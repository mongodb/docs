.. _llamaindex:

===========================================
Get Started with the LlamaIndex Integration
===========================================

.. default-domain:: mongodb

.. facet::
   :name: genre
   :values: tutorial

.. facet::
   :name: programming_language
   :values: python

.. meta::
   :description: Integrate Atlas Vector Search with LlamaIndex to build RAG applications.

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

You can integrate {+avs+} with `LlamaIndex <https://llamaindex.ai/>`__
to implement retrieval-augmented generation (RAG) in your |llm| 
application. This tutorial demonstrates how to start using {+avs+} 
with LlamaIndex to perform semantic search on your data and build a
|rag| implementation. Specifically, you perform the following actions:

#. Set up the environment.
#. Store custom data on |service|.
#. Create an {+avs+} index on your data.
#. Run the following vector search queries:

   - Semantic search.
   - Semantic search with metadata pre-filtering.

#. Implement |rag| by using {+avs+} to answer questions on your data.

Background
----------

LlamaIndex is an open-source framework designed to 
simplify how you connect custom data sources to |llm|\s. 
It provides several tools such as data connectors, indexes,
and query engines to help you load and 
prepare vector embeddings for |rag| applications.

By integrating {+avs+} with LlamaIndex, you can use 
|service| as a vector database and
implement |rag| by using {+avs+} to retrieve semantically 
similar documents from your data. To learn more about |rag|,
see :ref:`ai-key-concepts`.

Prerequisites
-------------

To complete this tutorial, you must have the following:

- An |service| {+cluster+} running MongoDB version 6.0.11, 7.0.2, or later
  (including :abbr:`RCs (Release Candidates)`).

- An OpenAI API Key. You must have a paid OpenAI account with credits
  available for API requests.

- A notebook to run your Python project such as `Colab <https://colab.research.google.com>`__.

.. _llamaindex-environment:

Set Up the Environment
----------------------

First, set up the environment for this tutorial by copying 
and pasting the following code snippets into your notebook.

.. include:: /includes/ai-integrations/llamaindex/llamaindex-set-up-environment.rst

Use |service| as a Vector Store
-------------------------------

Then, load custom data into |service| and instantiate |service| as 
a vector database, also called a `vector store 
<https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/>`__.
Copy and paste the following code snippets into your notebook.

.. include:: /includes/ai-integrations/llamaindex/llamaindex-create-vector-store.rst

.. _llamaindex-create-index:

Create the {+avs+} Index
------------------------------------

To enable vector search queries on your vector store,
create an {+avs+} index on the ``llamaindex_db.test`` collection.

Required Access
~~~~~~~~~~~~~~~

To create an {+avs+} index, you must have :authrole:`Project Data Access 
Admin` or higher access to the |service| project.

Procedure
~~~~~~~~~

.. include:: /includes/ai-integrations/llamaindex/llamaindex-create-index.rst

Run Vector Search Queries
-------------------------

Once |service| builds your index, return 
to your notebook and run vector search queries on your data. 
The following examples demonstrate different queries that you can 
run on your vectorized data.

.. include:: /includes/ai-integrations/llamaindex/llamaindex-query-examples.rst

Answer Questions on Your Data
-----------------------------

This section demonstrates how to implement |rag| in your 
application with {+avs+} and LlamaIndex. Now that you've learned
how to run vector search queries to retrieve semantically 
similar documents, run the following code to use 
{+avs+} to retrieve documents and a LlamaIndex `query engine 
<https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/>`__
to then answer questions based on those documents.

.. include:: /includes/ai-integrations/llamaindex/llamaindex-perform-qa.rst

Next Steps
----------

To explore LlamaIndex's full library of tools for |rag| applications, which includes 
data connectors, indexes, and query engines, see `LlamaHub <https://llamahub.ai>`__.

To extend the application in this tutorial to have back-and-forth conversations, see
`Chat Engine <https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/>`__.

MongoDB also provides the following developer resources:

- `How to Build a RAG System With LlamaIndex, OpenAI, and MongoDB Vector Database
  <https://www.mongodb.com/developer/products/atlas/rag-with-polm-stack-llamaindex-openai-mongodb/>`__
- :github:`MongoDB Developer GitHub Repository </mongodb-developer>`

.. seealso:: 
   
   - `LlamaIndex Documentation <https://docs.llamaindex.ai/en/stable/examples/vector_stores/MongoDBAtlasVectorSearchRAGOpenAI/>`__
   - `LlamaIndex API Reference <https://docs.llamaindex.ai/en/stable/api_reference/storage/vector_store/mongodb/>`__
