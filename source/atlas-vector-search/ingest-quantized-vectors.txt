.. _avs-bindata-vector-subtype:

===============================
How to Ingest Quantized Vectors
===============================

You can convert your embeddings to |bson| :manual:`BinData
</reference/method/BinData/>` ``vector`` subtype ``float32`` or
``vector`` subtype ``int8`` vectors. 

Use Cases
---------

We recommend the |bson| ``binData`` ``vector`` subtype for the following
use cases:  

- You need to index quantized vector output from embedding models.
- You have a large number of float vectors but want to reduce the
  storage and WiredTiger footprint (such as disk and memory usage) in
  ``mongod``.

Benefits 
--------

The :manual:`BinData </reference/method/BinData/>` ``vector`` format
requires about three times less disk space in your {+cluster+} compared
to arrays of elements. It allows you to index your vectors with
alternate types such as ``int8`` vectors, reducing the memory needed to
build the {+avs+} index for your collection. 

If you don't already have ``binData`` vectors, you can convert your
embeddings to this format by using any supported driver before writing
your data to a collection. This page walks you through the steps for
converting your embeddings to the :manual:`BinData
</reference/method/BinData/>` ``vector`` subtype.

Supported Drivers  
-----------------

|bson| :manual:`BinData </reference/method/BinData/>` ``vector`` subtype
``float32`` and ``int8`` vector conversion is supported by
:driver:`PyMongo Driver </pymongo/>` v4.10 or later. 

Prerequisites
-------------

To convert your embeddings to |bson| :manual:`BinData
</reference/method/BinData/>` ``vector`` subtype, you need the
following: 

- An |service| {+cluster+} running MongoDB version 6.0.11, 7.0.2, or
  later. 
        
  Ensure that your :abbr:`IP address (Internet Protocol address)` is
  included in your |service| project's :ref:`access list <access-list>`. 

- An environment to run interactive Python notebooks such as `Colab
  <https://colab.research.google.com>`__. 

- Access to an embedding model that supports byte vector output. 

  The following embedding model providers support ``int8``
  ``binData`` vectors: 

  .. list-table:: 
     :widths: 50 50
     :header-rows: 1

     * - Embedding Model Provider 
       - Embedding Model
     * - `Cohere <https://cohere.com/>`__ 
       - ``embed-english-v3.0``
     * - `Nomic <https://www.nomic.ai/>`__ 
       - ``nomic-embed-text-v1.5``
     * - `Jina <https://jina.ai/>`__ 
       - ``jina-embeddings-v2-base-en``
     * - `Mixedbread <https://www.mixedbread.ai/>`__ 
       - ``mxbai-embed-large-v1``

  You can use any of these embedding model providers to generate
  ``binData`` vectors. Scalar quantization preserves recall for these
  models because these models are all trained to be quantization aware.
  Therefore, :term:`recall` degradation for scalar quantized embeddings
  produced by these models is minimal even at lower dimensions like 384. 

Procedure
---------

The examples in this procedure use either new data or existing data and
`Cohere's <https://cohere.com/>`__ ``embed-english-v3.0`` model. The
example for new data uses sample text strings, which you can replace
with your own data. The example for existing data uses a subset of
documents without any embeddings from the ``listingsAndReviews``
collection in the ``sample_airbnb`` database, which you can replace with
your own database and collection (with or without any embeddings).
Select the tab based on whether you want to create ``binData`` vectors
for new data or for data you already have in your |service| {+cluster+}. 

Create an interactive Python notebook by saving a file with the
``.ipynb`` extension, and then perform the following steps in the 
notebook. To try the example, replace the placeholders with valid
values. 

.. tabs:: 

   .. tab:: New Data 
      :tabid: new 

      .. include:: /includes/steps-avs-create-bson-vectors-new-data-python.rst 

   .. tab:: Existing Data 
      :tabid: existing 

      .. include:: /includes/steps-avs-create-bson-vectors-existing-data-python.rst

For an advanced demonstration of this procedure on sample data using
Cohere's ``embed-english-v3.0`` embedding model, see
this :github:`notebook </mongodb-developer/GenAI-Showcase/blob/main/notebooks/techniques/quantized_vector_ingestion_with_cohere_and_mongodb.ipynb>`. 
