.. index:: administration; sharding
.. _sharding-administration:

==============================
Sharded Cluster Administration
==============================

.. default-domain:: mongodb

This document describes common administrative tasks for sharded
clusters. For complete documentation of sharded clusters see the
:doc:`/sharding` section of this manual.

.. contents:: Sharding Procedures:
   :backlinks: none
   :local:

.. _sharding-procedure-setup:

Set up a Sharded Cluster
------------------------

Before deploying a cluster, see :ref:`sharding-requirements`.

For testing purposes, you can run all the required shard :program:`mongod` processes on a
single server. For production, use the configurations described in
:doc:`/administration/replication-architectures`.

.. include:: /includes/warning-sharding-hostnames.rst

If you have an existing replica set, you can use the
:doc:`/tutorial/convert-replica-set-to-replicated-shard-cluster`
tutorial as a guide. If you're deploying a cluster from scratch, see
the :doc:`/tutorial/deploy-shard-cluster` tutorial for more detail or
use the following procedure as a quick starting point:

1. Create data directories for each of the three (3) config server instances.

#. Start the three config server instances. For example, to start a
   config server instance running on TCP port ``27018`` with the data
   stored in ``/data/configdb``, type the following:

   .. code-block:: sh

      mongod --configsvr --dbpath /data/configdb --port 27018

   For additional command options, see :doc:`/reference/mongod`
   and :doc:`/reference/configuration-options`.

   .. include:: /includes/note-config-server-startup.rst

#. Start a :program:`mongos` instance. For example, to start a
   :program:`mongos` that connects to config server instance running on the following hosts:

   - ``mongoc0.example.net``
   - ``mongoc1.example.net``
   - ``mongoc2.example.net``

   You would issue the following command:

   .. code-block:: sh

      mongos --configdb mongoc0.example.net:27018,mongoc1.example.net:27018,mongoc2.example.net:27018

#. Connect to one of the :program:`mongos` instances. For example, if
   a :program:`mongos` is accessible at ``mongos0.example.net`` on
   port ``27017``, issue the following command:

   .. code-block:: sh

      mongo mongos0.example.net

#. Add shards to the cluster.

   .. note:: In production deployments, all shards should be replica sets.

      To deploy a replica set, see the
      :doc:`/tutorial/deploy-replica-set` tutorial.

   From the :program:`mongo` shell connected
   to the :program:`mongos` instance, call the :method:`sh.addShard()`
   method for each shard to add to the cluster.

   For example:

   .. code-block:: javascript

      sh.addShard( "mongodb0.example.net:27027" )

   If ``mongodb0.example.net:27027`` is a member of a replica
   set, call the :method:`sh.addShard()` method with an argument that
   resembles the following:

   .. code-block:: javascript

      sh.addShard( "<setName>/mongodb0.example.net:27027" )

   Replace, ``<setName>`` with the name of the replica set, and
   MongoDB will discover all other members of the replica set. Repeat
   this step for each new shard in your cluster.

   .. optional::

      You can specify a name for the shard and a maximum size. See
      :dbcommand:`addShard`.

   .. note::

      .. versionchanged:: 2.0.3

      Before version 2.0.3, you must specify the shard in the following
      form:

      .. code-block:: sh

         replicaSetName/<seed1>,<seed2>,<seed3>

      For example, if the name of the replica set is ``repl0``, then
      your :method:`sh.addShard()` command would be:

      .. code-block:: javascript

         sh.addShard( "repl0/mongodb0.example.net:27027,mongodb1.example.net:27017,mongodb2.example.net:27017" )

#. Enable sharding for each database you want to shard.
   While sharding operates on a per-collection basis, you must enable
   sharding for each database that holds collections you want to shard.
   This step is a meta-data change and will not redistribute your data.

   MongoDB enables sharding on a per-database basis. This is only a
   meta-data change and will not redistribute your data. To enable
   sharding for a given database, use the :dbcommand:`enableSharding`
   command or the :method:`sh.enableSharding()` shell helper.

   .. code-block:: javascript

      db.runCommand( { enableSharding: <database> } )

   Or:

   .. code-block:: javascript

      sh.enableSharding(<database>)

   .. note::

      MongoDB creates databases automatically upon their first use.

      Once you enable sharding for a database, MongoDB assigns a
      :term:`primary shard` for that database, where MongoDB stores all data
      before sharding begins.

.. _sharding-administration-shard-collection:

#. Enable sharding on a per-collection basis.

   Finally, you must explicitly specify collections to shard. The
   collections must belong to a database for which you have enabled
   sharding. When you shard a collection, you also choose the shard
   key. To shard a collection, run the :dbcommand:`shardCollection`
   command or the :method:`sh.shardCollection()` shell helper.

   .. code-block:: javascript

      db.runCommand( { shardCollection: "<database>.<collection>", key: "<shard-key>" } )

   Or:

   .. code-block:: javascript

      sh.shardCollection("<database>.<collection>", "key")

   For example:

   .. code-block:: javascript

      db.runCommand( { shardCollection: "myapp.users", key: {username: 1} } )

   Or:

   .. code-block:: javascript

      sh.shardCollection("myapp.users", {username: 1})

   The choice of shard key is incredibly important: it affects
   everything about the cluster from the efficiency of your queries to
   the distribution of data. Furthermore, you cannot change a
   collection's shard key after setting it.

   See the :ref:`Shard Key Overview <sharding-shard-key>` and the
   more in depth documentation of :ref:`Shard Key Qualities
   <sharding-internals-shard-keys>` to help you select better shard
   keys.

   If you do not specify a shard key, MongoDB will shard the
   collection using the ``_id`` field.

Cluster Management
------------------

This section outlines procedures for adding and remove shards, as well
as general monitoring and maintenance of a :term:`sharded cluster`.

.. _sharding-procedure-add-shard:

Add a Shard to a Cluster
~~~~~~~~~~~~~~~~~~~~~~~~

To add a shard to an *existing* sharded cluster, use the following
procedure:

#. Connect to a :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. First, you need to tell the cluster where to find the individual
   shards. You can do this using the :dbcommand:`addShard` command or
   the :method:`sh.addShard()` helper:

   .. code-block:: javascript

      sh.addShard( "<hostname>:<port>" )

   Replace ``<hostname>`` and ``<port>`` with the hostname and TCP
   port number of where the shard is accessible.

   For example:

   .. code-block:: javascript

      sh.addShard( "mongodb0.example.net:27027" )

   .. note:: In production deployments, all shards should be replica sets.

   Repeat for each shard in your cluster.

   .. optional::

      You may specify a "name" as an argument to the
      :dbcommand:`addShard`, as follows:

      .. code-block:: javascript

         db.runCommand( { addShard: mongodb0.example.net, name: "mongodb0" } )

      You cannot specify a name for a shard using the
      :method:`sh.addShard()` helper in the :program:`mongo` shell. If
      you use the helper or do not specify a shard name, then MongoDB
      will assign a name upon creation.

   .. versionchanged:: 2.0.3
      Before version 2.0.3, you must specify the shard in the
      following form: the replica set name, followed by a forward
      slash, followed by a comma-separated list of seeds for the
      replica set. For example, if the name of the replica set is
      "myapp1", then your :method:`sh.addShard()` command might resemble:

      .. code-block:: javascript

         sh.addShard( "repl0/mongodb0.example.net:27027,mongodb1.example.net:27017,mongodb2.example.net:27017" )

.. note::

   It may take some time for :term:`chunks <chunk>` to migrate to the
   new shard.

   For an introduction to balancing, see :ref:`sharding-balancing`. For
   lower level information on balancing, see :ref:`sharding-balancing-internals`.

.. _sharding-procedure-remove-shard:

Remove a Shard from a Cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To remove a :term:`shard` from a :term:`sharded cluster`, you must:

- Migrate :term:`chunks <chunk>` to another shard or database.

- Ensure that this shard is not the :term:`primary shard` for any databases in
  the cluster. If it is, move the "primary" status for these databases
  to other shards.

- Finally, remove the shard from the cluster's configuration.

.. note::

   To successfully migrate data from a shard, the :term:`balancer`
   process **must** be active.

The procedure to remove a shard is as follows:

#. Connect to a :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Determine the name of the shard you will be removing.

   You must specify the name of the shard. You may have specified this
   shard name when you first ran the :dbcommand:`addShard` command. If not,
   you can find out the name of the shard by running the
   :dbcommand:`listShards` or :dbcommand:`printShardingStatus`
   commands or the :method:`sh.status()` shell helper.

   The following examples will remove a shard named ``mongodb0`` from the cluster.

#. Begin removing chunks from the shard.

   Start by running the :dbcommand:`removeShard` command. This will
   start "draining" or migrating chunks from the shard you're removing
   to another shard in the cluster.

   .. code-block:: javascript

      db.runCommand( { removeshard: "mongodb0" } )

   This operation will return the following response immediately:

   .. code-block:: javascript

      { msg : "draining started successfully" , state: "started" , shard :"mongodb0" , ok : 1 }

   Depending on your network capacity and the amount of data in the
   shard, this operation can take anywhere from a few minutes to several
   days to complete.

#. View progress of the migration.

   You can run the :dbcommand:`removeShard` again at any stage of the
   process to view the progress of the migration, as follows:

   .. code-block:: javascript

      db.runCommand( { removeShard: "mongodb0" } )

   The output should look something like this:

   .. code-block:: javascript

      { msg: "draining ongoing" , state: "ongoing" , remaining: { chunks: 42, dbs : 1 }, ok: 1 }

   In the ``remaining`` sub-document ``{ chunks: xx, dbs: y }``, a
   counter displays the remaining number of chunks that MongoDB must
   migrate to other shards and the number of MongoDB databases that have
   "primary" status on this shard.

   Continue checking the status of the :dbcommand:`removeShard` command
   until the remaining number of chunks to transfer is 0.

#. Move any databases to other shards in the cluster as needed.

   This is only necessary when removing a shard that is also the
   :term:`primary shard` for one or more databases.

   Issue the following command at the :program:`mongo` shell:

   .. code-block:: javascript

      db.runCommand( { movePrimary: "myapp", to: "mongodb1" })

   This command will migrate all remaining non-sharded data in the
   database named ``myapp`` to the shard named ``mongodb1``.

   .. warning::

      Do not run the :dbcommand:`movePrimary` until you have *finished*
      draining the shard.

   The command will not return until MongoDB completes moving all
   data. The response from this command will resemble the following:

   .. code-block:: javascript

      { "primary" : "mongodb1", "ok" : 1 }

#. Run :dbcommand:`removeShard` again to clean up all metadata
   information and finalize the shard removal, as follows:

   .. code-block:: javascript

      db.runCommand( { removeshard: "mongodb0" } )

   When successful, this command will return a document like this:

   .. code-block:: javascript

      { msg: "remove shard completed successfully" , stage: "completed", host: "mongodb0", ok : 1 }

Once the value of the ``stage`` field is "completed," you may safely
stop the processes comprising the ``mongodb0`` shard.

List Databases with Sharding Enabled
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To list the databases that have sharding enabled, query the
``databases`` collection in the :ref:`config-database`.
A database has sharding enabled if the value of the ``partitioned``
field is ``true``. Connect to a :program:`mongos` instance with a
:program:`mongo` shell, and run the following operation to get a full
list of databases with sharding enabled:

.. code-block:: javascript

   use config
   db.databases.find( { "partitioned": true } )

.. example:: You can use the following sequence of commands when to
   return a list of all databases in the cluster:

   .. code-block:: javascript

      use config
      db.databases.find()

   If this returns the following result set:

   .. code-block:: javascript

      { "_id" : "admin", "partitioned" : false, "primary" : "config" }
      { "_id" : "animals", "partitioned" : true, "primary" : "m0.example.net:30001" }
      { "_id" : "farms", "partitioned" : false, "primary" : "m1.example2.net:27017" }

   Then sharding is only enabled for the the ``animals`` database.

List Shards
~~~~~~~~~~~

To list the current set of configured shards, use the :dbcommand:`listShards`
command, as follows:

.. code-block:: javascript

   db.runCommand( { list shards : 1 } )

View Cluster Details
~~~~~~~~~~~~~~~~~~~~

To view cluster details, issue :method:`db.printShardingStatus()` or
:method:`sh.status()`. Both methods return the same output.

.. example:: In the following example output from :method:`sh.status()`

   - ``sharding version`` displays the version number of the shard
     metadata.

   - ``shards`` displays a list of the :program:`mongod` instances
     used as shards in the cluster.

   - ``databases`` displays all databases in the cluster,
     including database that do not have sharding enabled.

   - The ``chunks`` information for the ``foo`` database displays how
     many chunks are on each shard and displays the range of each chunk.

   .. code-block:: javascript

      --- Sharding Status ---
        sharding version: { "_id" : 1, "version" : 3 }
        shards:
          {  "_id" : "shard0000",  "host" : "m0.example.net:30001" }
          {  "_id" : "shard0001",  "host" : "m3.example2.net:50000" }
        databases:
          {  "_id" : "admin",  "partitioned" : false,  "primary" : "config" }
          {  "_id" : "animals",  "partitioned" : true,  "primary" : "shard0000" }
              foo.big chunks:
                      shard0001    1
                      shard0000    6
                  { "a" : { $minKey : 1 } } -->> { "a" : "elephant" } on : shard0001 Timestamp(2000, 1) jumbo
                  { "a" : "elephant" } -->> { "a" : "giraffe" } on : shard0000 Timestamp(1000, 1) jumbo
                  { "a" : "giraffe" } -->> { "a" : "hippopotamus" } on : shard0000 Timestamp(2000, 2) jumbo
                  { "a" : "hippopotamus" } -->> { "a" : "lion" } on : shard0000 Timestamp(2000, 3) jumbo
                  { "a" : "lion" } -->> { "a" : "rhinoceros" } on : shard0000 Timestamp(1000, 3) jumbo
                  { "a" : "rhinoceros" } -->> { "a" : "springbok" } on : shard0000 Timestamp(1000, 4)
                  { "a" : "springbok" } -->> { "a" : { $maxKey : 1 } } on : shard0000 Timestamp(1000, 5)
              foo.large chunks:
                      shard0001    1
                      shard0000    5
                  { "a" : { $minKey : 1 } } -->> { "a" : "hen" } on : shard0001 Timestamp(2000, 0)
                  { "a" : "hen" } -->> { "a" : "horse" } on : shard0000 Timestamp(1000, 1) jumbo
                  { "a" : "horse" } -->> { "a" : "owl" } on : shard0000 Timestamp(1000, 2) jumbo
                  { "a" : "owl" } -->> { "a" : "rooster" } on : shard0000 Timestamp(1000, 3) jumbo
                  { "a" : "rooster" } -->> { "a" : "sheep" } on : shard0000 Timestamp(1000, 4)
                  { "a" : "sheep" } -->> { "a" : { $maxKey : 1 } } on : shard0000 Timestamp(1000, 5)
          {  "_id" : "test",  "partitioned" : false,  "primary" : "shard0000" }

Chunk Management
----------------

This section describes various operations on :term:`chunks <chunk>` in
:term:`sharded clusters <sharded cluster>`. MongoDB automates most
chunk management operations. However, these chunk management
operations are accessible to administrators for use in some
situations, typically surrounding initial setup, deployment, and data
ingestion.

.. _sharding-procedure-create-split:

Split Chunks
~~~~~~~~~~~~

Normally, MongoDB splits a :term:`chunk` following inserts when a
chunk exceeds the :ref:`chunk size <sharding-chunk-size>`. The
:term:`balancer` may migrate recently split chunks to a new shard
immediately if :program:`mongos` predicts future insertions will
benefit from the move.

The MongoDB treats all chunks the same, whether split manually or
automatically by the system.

.. warning::

   You cannot merge or combine chunks once you have split them.

You may want to split chunks manually if:

- you have a large amount of data in your cluster and very few
  :term:`chunks <chunk>`,
  as is the case after deploying a cluster using existing data.

- you expect to add a large amount of data that would
  initially reside in a single chunk or shard.

.. example::

   You plan to insert a large amount of data with :term:`shard key`
   values between ``300`` and ``400``, *but* all values of your shard
   keys are between ``250`` and ``500`` are in a single chunk.

Use :method:`sh.status()` to determine the current chunks ranges across
the cluster.

To split chunks manually, use the :dbcommand:`split` command with
operators: ``middle`` and ``find``. The equivalent shell helpers are
:method:`sh.splitAt()` or :method:`sh.splitFind()`.

.. example::

   The following command will split the chunk that contains
   the value of ``63109`` for the ``zipcode`` field in the ``people``
   collection of the ``records`` database:

   .. code-block:: javascript

      sh.splitFind( "records.people", { "zipcode": 63109 } )

:method:`sh.splitFind()` will split the chunk that contains the
*first* document returned that matches this query into two equally
sized chunks. You must specify the full namespace
(i.e. "``<database>.<collection>``") of the sharded collection to
:method:`sh.splitFind()`. The query in :method:`sh.splitFind()` need
not contain the shard key, though it almost always makes sense to
query for the shard key in this case, and including the shard key will
expedite the operation.

Use :method:`sh.splitAt()` to split a chunk in two using the queried
document as the partition point:

.. code-block:: javascript

   sh.splitAt( "records.people", { "zipcode": 63109 } )

However, the location of the document that this query finds with
respect to the other documents in the chunk does not affect how the
chunk splits.

.. _sharding-administration-pre-splitting:
.. _sharding-administration-create-chunks:

Create Chunks (Pre-Splitting)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In most situations a :term:`sharded cluster` will create and distribute
chunks automatically without user intervention. However, in a limited
number of use profiles, MongoDB cannot create enough chunks or
distribute data fast enough to support required throughput. Consider
the following scenarios:

- you must partition an existing data collection that resides on a
  single shard.

- you must ingest a large volume of data into a cluster that
  isn't balanced, or where the ingestion of data will lead to an
  imbalance of data.

  This can arise in an initial data loading, or in a case where you
  must insert a large volume of data into a single chunk, as is the
  case when you must insert at the beginning or end of the chunk
  range, as is the case for monotonically increasing or decreasing
  shard keys.

Preemptively splitting chunks increases cluster throughput for these
operations, by reducing the overhead of migrating chunks that hold
data during the write operation. MongoDB only creates splits after an
insert operation, and can only migrate a single chunk at a time. Chunk
migrations are resource intensive and further complicated by large
write volume to the migrating chunk.

To create and migrate chunks manually, use the following procedure:

#. Split empty chunks in your collection by manually performing
   :dbcommand:`split` command on chunks.

   .. example::

      To create chunks for documents in the ``myapp.users``
      collection, using the ``email`` field as the :term:`shard key`,
      use the following operation in the :program:`mongo` shell:

        .. code-block:: javascript

           for ( var x=97; x<97+26; x++ ){
             for( var y=97; y<97+26; y+=6 ) {
               var prefix = String.fromCharCode(x) + String.fromCharCode(y);
               db.runCommand( { split : "myapp.users" , middle : { email : prefix } } );
             }
           }

      This assumes a collection size of 100 million documents.

#. Migrate chunks manually using the :dbcommand:`moveChunk` command:

   .. example::

      To migrate all of the manually created user profiles evenly,
      putting each prefix chunk on the next shard from the other, run
      the following commands in the mongo shell:

        .. code-block:: javascript

           var shServer = [ "sh0.example.net", "sh1.example.net", "sh2.example.net", "sh3.example.net", "sh4.example.net" ];
           for ( var x=97; x<97+26; x++ ){
             for( var y=97; y<97+26; y+=6 ) {
               var prefix = String.fromCharCode(x) + String.fromCharCode(y);
               db.adminCommand({moveChunk : "myapp.users", find : {email : prefix}, to : shServer[(y-97)/6]})
             }
           }

   You can also let the balancer automatically distribute the new
   chunks. For an introduction to balancing, see
   :ref:`sharding-balancing`. For lower level information on balancing,
   see :ref:`sharding-balancing-internals`.

.. _sharding-balancing-modify-chunk-size:

Modify Chunk Size
~~~~~~~~~~~~~~~~~

When you initialize a sharded cluster, the default chunk size is 64
megabytes. This default chunk size works well for most deployments. However, if you
notice that automatic migrations are incurring a level of I/O that
your hardware cannot handle, you may want to reduce the chunk
size. For the automatic splits and migrations, a small chunk size
leads to more rapid and frequent migrations.

To modify the chunk size, use the following procedure:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the :ref:`config-database`:

   .. code-block:: javascript

      use config

#. Issue the following :method:`save() <db.collection.save()>`
   operation:

   .. code-block:: javascript

      db.settings.save( { _id:"chunksize", value: <size> } )

   Where the value of ``<size>`` reflects the new chunk size in
   megabytes. Here, you're essentially writing a document whose values
   store the global chunk size configuration value.

.. note::

   The :setting:`chunkSize` and :option:`--chunkSize <mongos --chunkSize>`
   options, passed at runtime to the :program:`mongos` **do not**
   affect the chunk size after you have initialized the cluster.

   To eliminate confusion you should *always* set chunk size using the
   above procedure and never use the runtime options.

Modifying the chunk size has several limitations:

- Automatic splitting only occurs when inserting :term:`documents
  <document>` or updating existing documents.

- If you lower the chunk size it may take time for all chunks to split to
  the new size.

- Splits cannot be "undone."

If you increase the chunk size, existing chunks must grow through
insertion or updates until they reach the new size.

.. _sharding-balancing-manual-migration:

Migrate Chunks
~~~~~~~~~~~~~~

In most circumstances, you should let the automatic balancer
migrate :term:`chunks <chunk>` between :term:`shards <shard>`.
However, you may want to migrate chunks manually in a few cases:

- If you create chunks by :term:`pre-splitting` the data in your
  collection, you will have to migrate chunks manually to distribute
  chunks evenly across the shards. Use pre-splitting in limited
  situations, to support bulk data ingestion.

- If the balancer in an active cluster cannot distribute chunks within
  the balancing window, then you will have to migrate chunks manually.

For more information on how chunks move between shards, see
:ref:`sharding-balancing-internals`, in particular the section
:ref:`sharding-chunk-migration`.

To migrate chunks, use the :dbcommand:`moveChunk` command.

.. note::

   To return a list of shards, use the :dbcommand:`listShards`
   command.

   Specify shard names using the :dbcommand:`addShard` command
   using the ``name`` argument. If you do not specify a name in the
   :dbcommand:`addShard` command, MongoDB will assign a name
   automatically.

The following example assumes that the field ``username`` is the
:term:`shard key` for a collection named ``users`` in the ``myapp``
database, and that the value ``smith`` exists within the :term:`chunk`
you want to migrate.

To move this chunk, you would issue the following command from a :program:`mongo`
shell connected to any :program:`mongos` instance.

.. code-block:: javascript

   db.adminCommand({moveChunk : "myapp.users", find : {username : "smith"}, to : "mongodb-shard3.example.net"})

This command moves the chunk that includes the shard key value "smith" to the
:term:`shard` named ``mongodb-shard3.example.net``. The command will
block until the migration is complete.

See :ref:`sharding-administration-create-chunks` for an introduction
to pre-splitting.

.. versionadded:: 2.2
   :dbcommand:`moveChunk` command has the: ``_secondaryThrottle``
   parameter. When set to ``true``, MongoDB ensures that
   :term:`secondary` members have replicated operations before allowing
   new chunk migrations.

.. warning::

   The :dbcommand:`moveChunk` command may produce the following error
   message:

   .. code-block:: none

      The collection's metadata lock is already taken.

   These errors occur when clients have too many open :term:`cursors
   <cursor>` that access the chunk you are migrating. You can either
   wait until the cursors complete their operation or close the
   cursors manually.

   .. todo:: insert link to killing a cursor.

.. index:: bulk insert
.. _sharding-bulk-inserts:

Strategies for Bulk Inserts in Sharded Clusters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. todo:: Consider moving to the administrative guide as it's of an
   applied nature, or create an applications document for sharding

.. todo:: link the words "bulk insert" to the bulk insert topic when
   it's published


Large bulk insert operations including initial data ingestion or
routine data import, can have a significant impact on a :term:`sharded
cluster`. Consider the following strategies and possibilities for
bulk insert operations: 

- If the collection does not have data, then there is only one
  :term:`chunk`, which must reside on a single shard. MongoDB must
  receive data, create splits, and distribute chunks to the available
  shards. To avoid this performance cost, you can pre-split the
  collection, as described in :ref:`sharding-administration-pre-splitting`.

- You can parallels import by sending insert operations to more than
  one :program:`mongos` instance. If the collection is empty,
  pre-split first, as described in
  :ref:`sharding-administration-pre-splitting`.

- If your shard key increases monotonically during an insert then all
  the inserts will go to the last chunk in the collection, which will
  always end up on a single shard. Therefore, the insert capacity of the
  cluster will never exceed the insert capacity of a single shard. 

  If your insert volume is never larger than what a single shard can
  process, then there is no problem; however, if the insert volume
  exceeds that range, and you cannot avoid a monotonically
  increasing shard key, then consider the following modifications to
  your application: 

  - Reverse all the bits of the shard key to preserves the information
    while avoiding the correlation of insertion order and increasing
    sequence of values.

  - Swap the first and last 16-bit words to "shuffle" the inserts.

  .. example:: The following example, in C++, swaps the leading and
     trailing 16-bit word of :term:`BSON` :term:`ObjectIds <ObjectId>`
     generated so that they are no longer monotonically increasing.

     .. code-block:: cpp

        using namespace mongo;
        OID make_an_id() {
          OID x = OID::gen();
          const unsigned char *p = x.getData();
          swap( (unsigned short&) p[0], (unsigned short&) p[10] );
          return x;
        }

        void foo() {
          // create an object
          BSONObj o = BSON( "_id" << make_an_id() << "x" << 3 << "name" << "jane" );
          // now we might insert o into a sharded collection...
        }

  For information on choosing a shard key, see :ref:`sharding-shard-key`
  and see :ref:`Shard Key Internals <sharding-internals-shard-keys>` (in
  particular, :ref:`sharding-internals-operations-and-reliability` and
  :ref:`sharding-internals-choose-shard-key`).

.. index:: balancing; operations
.. _sharding-balancing-operations:

Balancer Operations
-------------------

This section describes provides common administrative procedures related
to balancing. For an introduction to balancing, see
:ref:`sharding-balancing`. For lower level information on balancing, see
:ref:`sharding-balancing-internals`.

.. _sharding-balancing-check-lock:

Check the Balancer Lock
~~~~~~~~~~~~~~~~~~~~~~~

To see if the balancer process is active in your :term:`cluster
<sharded cluster>`, do the following:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the :ref:`config-database`:

   .. code-block:: javascript

      use config

#. Use the following query to return the balancer lock:

   .. code-block:: javascript

      db.locks.find( { _id : "balancer" } ).pretty()

   You can also use the following shell helper to return the same
   information:

   .. code-block:: javascript

      sh.getBalancerState().pretty()

When this command returns, you will see output like the following:

.. code-block:: javascript

   {   "_id" : "balancer",
   "process" : "mongos0.example.net:1292810611:1804289383",
     "state" : 2,
        "ts" : ObjectId("4d0f872630c42d1978be8a2e"),
      "when" : "Mon Dec 20 2010 11:41:10 GMT-0500 (EST)",
       "who" : "mongos0.example.net:1292810611:1804289383:Balancer:846930886",
       "why" : "doing balance round" }


Here's what this tells you:

- The balancer originates from the :program:`mongos` running on the
  system with the hostname ``mongos0.example.net``.

- The value in the ``state`` field indicates that a :program:`mongos`
  has the lock. For version 2.0 and later, the value of an active lock
  is ``2``; for earlier versions the value is ``1``.

  .. note::

     Use the :method:`sh.isBalancerRunning()` helper in the
     :program:`mongo` shell to determine if the balancer is
     running, as follows:

     .. code-block:: javascript

        sh.isBalancerRunning()

.. _sharding-schedule-balancing-window:

Schedule the Balancing Window
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some situations, particularly when your data set grows slowly and a
migration can impact performance, it's useful to be able to ensure
that the balancer is active only at certain times.  Use the following
procedure to specify a window during which the :term:`balancer` will
be able to migrate chunks:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the :ref:`config-database`:

   .. code-block:: javascript

      use config

#. Use an operation modeled on the following example :method:`update()
   <db.collection.update()>` operation to modify the balancer's
   window:

   .. code-block:: javascript

      db.settings.update({ _id : "balancer" }, { $set : { activeWindow : { start : "<start-time>", stop : "<stop-time>" } } }, true )

   Replace ``<start-time>`` and ``<end-time>`` with time values using
   two digit hour and minute values (e.g ``HH:MM``) that describe the
   beginning and end boundaries of the balancing window.
   These times will be evaluated relative to the time zone of each individual
   :program:`mongos` instance in the sharded cluster.
   For instance, running the following
   will force the balancer to run between 11PM and 6AM local time only:

   .. code-block:: javascript

      db.settings.update({ _id : "balancer" }, { $set : { activeWindow : { start : "23:00", stop : "6:00" } } }, true )

.. note::

   The balancer window must be sufficient to *complete* the migration
   of all data inserted during the day.

   As data insert rates can change based on activity and usage
   patterns, it is important to ensure that the balancing window you
   select will be sufficient to support the needs of your deployment.

Remove a Balancing Window Schedule
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you have :ref:`set the balancing window
<sharding-schedule-balancing-window>` and wish to remove the schedule
so that the balancer is always running, issue the following sequence
of operations:

.. code-block:: javascript

   use config
   db.settings.update({ _id : "balancer" }, { $unset : { activeWindow : true })

.. _sharding-balancing-disable-temporally:

Disable the Balancer
~~~~~~~~~~~~~~~~~~~~

By default the balancer may run at any time and only moves chunks as
needed. To disable the balancer for a short period of time and prevent
all migration, use the following procedure:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

.. TODO Does the user have to switch to the config db to do this?

#. Issue *one* of the following operations to disable the balancer:

   .. code-block:: javascript

      sh.setBalancerState(false)
      sh.stopBalancer()

#. Later, issue *one* the following operations to enable the balancer:

   .. code-block:: javascript

      sh.setBalancerState(true)
      sh.startBalancer()

.. note::

   If a balancing round is in progress, the system will complete the
   current round before the balancer is officially disabled.  After
   disabling, you can use the :method:`sh.getBalancerState()` shell
   function to determine whether the balancer is in fact disabled.

The above process and the :method:`sh.setBalancerState()`,
:method:`sh.startBalancer()`, and :method:`sh.stopBalancer()` helpers provide
wrappers on the following process, which may be useful if you need to
run this operation from a driver that does not have helper functions:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the :ref:`config-database`:

   .. code-block:: javascript

      use config

#. Issue the following update to disable the balancer:

   .. code-block:: javascript

      db.settings.update( { _id: "balancer" }, { $set : { stopped: true } } , true );

#. To enable the balancer again, alter the value of "stopped" as follows:

   .. code-block:: javascript

      db.settings.update( { _id: "balancer" }, { $set : { stopped: false } } , true );

.. index:: config servers; operations
.. _sharding-procedure-config-server:

Config Server Maintenance
-------------------------

Config servers store all cluster metadata, most importantly,
the mapping from :term:`chunks <chunk>` to :term:`shards <shard>`.
This section provides an overview of the basic
procedures to migrate, replace, and maintain these servers.

.. seealso:: :ref:`sharding-config-server`

Deploy Three Config Servers for Production Deployments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For redundancy, all production :term:`sharded clusters <sharded cluster>`
should deploy three config servers processes on three different
machines.

Do not use only a single config server for production deployments.
Only use a single config server deployments for testing.  You should
upgrade to three config servers immediately if you are shifting to
production.  The following process shows how to convert a test
deployment with only one config server to production deployment with
three config servers.

#. Shut down all existing MongoDB processes. This includes:

   - all :program:`mongod` instances or :term:`replica sets <replica set>`
     that provide your shards.

   - the :program:`mongod` instance that provides your existing config
     database.

   - all :program:`mongos` instances in your cluster.

#. Copy the entire :setting:`dbpath` file system tree from the
   existing config server to the two machines that will provide the
   additional config servers. These commands, issued on the system
   with the existing :ref:`config-database`, ``mongo-config0.example.net`` may
   resemble the following:

   .. code-block:: sh

      rsync -az /data/configdb mongo-config1.example.net:/data/configdb
      rsync -az /data/configdb mongo-config2.example.net:/data/configdb

#. Start all three config servers, using the same invocation that you
   used for the single config server.

   .. code-block:: sh

      mongod --configsvr

#. Restart all shard :program:`mongod` and :program:`mongos` processes.

.. _sharding-process-config-server-migrate-same-hostname:

Migrate Config Servers with the Same Hostname
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use this process when you need to migrate a config server to a new
system but the new system will be accessible using the same host
name.

#. Shut down the config server that you're moving.

   This will render all config data for your cluster :ref:`read only
   <sharding-config-server>`.

#. Change the DNS entry that points to the system that provided the old
   config server, so that the *same* hostname points to the new
   system.

   How you do this depends on how you organize your DNS and
   hostname resolution services.

#. Move the entire :setting:`dbpath` file system tree from the old
   config server to the new config server. This command, issued on the
   old config server system, may resemble the following:

   .. code-block:: sh

      rsync -az /data/configdb mongo-config0.example.net:/data/configdb

#. Start the config instance on the new system. The default invocation
   is:

   .. code-block:: sh

      mongod --configsvr

When you start the third config server, your cluster will become
writable and it will be able to create new splits and migrate chunks
as needed.

.. _sharding-process-config-server-migrate-different-hostname:

Migrate Config Servers with Different Hostnames
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use this process when you need to migrate a :ref:`config-database` to a new
server and it *will not* be accessible via the same hostname. If
possible, avoid changing the hostname so that you can use the
:ref:`previous procedure <sharding-process-config-server-migrate-same-hostname>`.

#. Shut down the :ref:`config server <sharding-config-server>` you're moving.

   This will render all config data for your cluster "read only:"

   .. code-block:: sh

      rsync -az /data/configdb mongodb.config2.example.net:/data/configdb

#. Start the config instance on the new system. The default invocation
   is:

   .. code-block:: sh

      mongod --configsvr

#. Shut down all existing MongoDB processes. This includes:

   - all :program:`mongod` instances or :term:`replica sets <replica set>`
     that provide your shards.

   - the :program:`mongod` instances that provide your existing
     :ref:`config databases <config-database>`.

   - all :program:`mongos` instances in your cluster.

#. Restart all :program:`mongod` processes that provide the shard
   servers.

#. Update the :option:`--configdb <mongos --configdb>` parameter (or
   :setting:`configdb`) for all :program:`mongos` instances and
   restart all :program:`mongos` instances.

Replace a Config Server
~~~~~~~~~~~~~~~~~~~~~~~

Use this procedure only if you need to replace one of your config
servers after it becomes inoperable (e.g. hardware failure.) This
process assumes that the hostname of the instance will not change. If
you must change the hostname of the instance, use the process for
:ref:`migrating a config server to a different hostname
<sharding-process-config-server-migrate-different-hostname>`.

#. Provision a new system, with the same hostname as the previous
   host.

   You will have to ensure that the new system has the same IP address
   and hostname as the system it's replacing *or* you will need to
   modify the DNS records and wait for them to propagate.

#. Shut down *one* (and only one) of the existing config servers. Copy
   all this host's :setting:`dbpath` file system tree from the current system
   to the system that will provide the new config server. This
   command, issued on the system with the data files, may resemble the
   following:

   .. code-block:: sh

      rsync -az /data/configdb mongodb.config2.example.net:/data/configdb

#. Restart the config server process that you used in the previous
   step to copy the data files to the new config server instance.

#. Start the new config server instance. The default invocation is:

   .. code-block:: sh

      mongod --configsvr

Backup Cluster Metadata
~~~~~~~~~~~~~~~~~~~~~~~

The cluster will remain operational [#read-only]_ without one
of the config database's :program:`mongod` instances, creating a backup
of the cluster metadata from the config database is straight forward:

#. Shut down one of the :term:`config databases <config database>`.

#. Create a full copy of the data files (i.e. the path specified by
   the :setting:`dbpath` option for the config instance.)

#. Restart the original configuration server.

.. seealso:: :doc:`backups`.

.. [#read-only] While one of the three config servers unavailable, no
   the cluster cannot split any chunks nor can it migrate chunks
   between shards. Your application will be able to write data to the
   cluster. The :ref:`sharding-config-server` section of the
   documentation provides more information on this topic.

.. index:: troubleshooting; sharding
.. index:: sharding; troubleshooting
.. _sharding-troubleshooting:

Troubleshooting
---------------

The two most important factors in maintaining a successful sharded cluster are:

- :ref:`choosing an appropriate shard key <sharding-internals-shard-keys>` and

- :ref:`sufficient capacity to support current and future operations
  <sharding-capacity-planning>`.

You can prevent most issues encountered with sharding by ensuring that
you choose the best possible :term:`shard key` for your deployment and
ensure that you are always adding additional capacity to your cluster
well before the current resources become saturated. Continue reading
for specific issues you may encounter in a production environment.

.. _sharding-troubleshooting-not-splitting:

All Data Remains on One Shard
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Your cluster must have sufficient data for sharding to make
sense. Sharding works by migrating chunks between the shards until
each shard has roughly the same number of chunks.

The default chunk size is 64 megabytes. MongoDB will not begin
migrations until the imbalance of chunks in the cluster exceeds the
:ref:`migration threshold <sharding-migration-thresholds>`. While the
default chunk size is configurable with the :setting:`chunkSize`
setting, these behaviors help prevent unnecessary chunk migrations,
which can degrade the performance of your cluster as a whole.

If you have just deployed a sharded cluster, make sure that you have
enough data to make sharding effective. If you do not have sufficient
data to create more than eight 64 megabyte chunks, then all data will
remain on one shard. Either lower the :ref:`chunk size
<sharding-chunk-size>` setting, or add more data to the cluster.

As a related problem, the system will split chunks only on
inserts or updates, which means that if you configure sharding and do not
continue to issue insert and update operations, the database will not
create any chunks. You can either wait until your application inserts
data *or* :ref:`split chunks manually <sharding-procedure-create-split>`.

Finally, if your shard key has a low :ref:`cardinality
<sharding-shard-key-cardinality>`, MongoDB may not be able to create
sufficient splits among the data.

One Shard Receives too much Traffic
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some situations, a single shard or a subset of the cluster will
receive a disproportionate portion of the traffic and workload. In
almost all cases this is the result of a shard key that does not
effectively allow :ref:`write scaling <sharding-shard-key-write-scaling>`.

It's also possible that you have "hot chunks." In this case, you may
be able to solve the problem by splitting and then migrating parts of
these chunks.

In the worst case, you may have to consider re-sharding your data
and :ref:`choosing a different shard key <sharding-internals-choose-shard-key>`
to correct this pattern.

The Cluster does not Balance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you have just deployed your sharded cluster, you may want to
consider the :ref:`troubleshooting suggestions for a new cluster where
data remains on a single shard <sharding-troubleshooting-not-splitting>`.

If the cluster was initially balanced, but later developed an uneven
distribution of data, consider the following possible causes:

- You have deleted or removed a significant amount of data from the
  cluster. If you have added additional data, it may have a
  different distribution with regards to its shard key.

- Your :term:`shard key` has low :ref:`cardinality <sharding-shard-key-cardinality>`
  and MongoDB cannot split the chunks any further.

- Your data set is growing faster than the balancer can distribute
  data around the cluster. This is uncommon and
  typically is the result of:

  - a :ref:`balancing window <sharding-schedule-balancing-window>` that
    is too short, given the rate of data growth.

  - an uneven distribution of :ref:`write operations
    <sharding-shard-key-write-scaling>` that requires more data
    migration. You may have to choose a different shard key to resolve
    this issue.

  - poor network connectivity between shards, which may lead to chunk
    migrations that take too long to complete. Investigate your
    network configuration and interconnections between shards.

Migrations Render Cluster Unusable
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If migrations impact your cluster or application's performance,
consider the following options, depending on the nature of the impact:

#. If migrations only interrupt your clusters sporadically, you can
   limit the :ref:`balancing window
   <sharding-schedule-balancing-window>` to prevent balancing activity
   during peak hours. Ensure that there is enough time remaining to
   keep the data from becoming out of balance again.

#. If the balancer is always migrating chunks to the detriment of
   overall cluster performance:

   - You may want to attempt :ref:`decreasing the chunk size <sharding-balancing-modify-chunk-size>`
     to limit the size of the migration.

   - Your cluster may be over capacity, and you may want to attempt to
     :ref:`add one or two shards <sharding-procedure-add-shard>` to
     the cluster to distribute load.

It's also possible, that your shard key causes your
application to direct all writes to a single shard. This kind of
activity pattern can require the balancer to migrate most data soon after writing
it. Consider redeploying your cluster  with a shard key that provides
better :ref:`write scaling <sharding-shard-key-write-scaling>`.

Disable Balancing During Backups
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If MongoDB migrates a :term:`chunk` during a :doc:`backup
</administration/backups>`, you can end with an inconsistent snapshot
of your :term:`sharded cluster`. Never run a backup while the balancer is
active. To ensure that the balancer is inactive during your backup
operation:

- Set the :ref:`balancing window <sharding-schedule-balancing-window>`
  so that the balancer is inactive during the backup. Ensure that the
  backup can complete while you have the balancer disabled.

- :ref:`manually disable the balancer <sharding-balancing-disable-temporally>`
  for the duration of the backup procedure.

Confirm that the balancer is not active using the
:method:`sh.getBalancerState()` method before starting a backup
operation. When the backup procedure is complete you can reactivate
the balancer process.
