.. _backup-cloud-provider:

==========================================
{+Cloud-Backup+}s
==========================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

.. include:: /includes/fact-atlas-free-tier-only-limits.rst

|service| {+Cloud-Backup+}s provide localized backup storage using the
native snapshot functionality of the cluster's cloud service provider.

.. include:: /includes/admonitions/notes/backup-roles.rst

|service| supports {+cloud-backup+} for clusters served on:

- :ref:`Microsoft Azure <microsoft-azure>`
- :ref:`Amazon Web Services (AWS) <amazon-aws>`
- :ref:`Google Cloud Platform (GCP) <google-gcp>`

You can enable {+cloud-backup+} during the
:doc:`cluster creation </tutorial/create-new-cluster>` or during the
:doc:`modification of an existing cluster </scale-cluster>`.
From the cluster configuration modal, toggle
:guilabel:`Turn on Cloud Backup` to :guilabel:`Yes`.

If you need to retain any {+old-backup+} snapshots for archival
purposes, download them before you switch to {+Cloud-Backup+} from
{+old-backup+}s. To learn how to download a snapshot, see
:doc:`/backup/legacy-backup/restore`.

.. important:: Limitations of {+Cloud-Backup+}

   {+Cloud-Backup+}s:

   - Can support sharded clusters running MongoDB version 3.6 or later.

   - Cannot restore an existing snapshot to a cluster after you add or
     remove a shard from it. You may restore an existing snapshot to
     another cluster with a matching topology.

.. note:: Sharded Cluster Balancer, FCV 4.0 databases and {+Cloud-Backup+}

   With databases running |fcv-link| 4.0 or earlier, {+Cloud-Backup+}
   automatically disables the balancer for snapshots if it's running.
   This ensures an inactive balancer during the backup operation. When
   the snapshot completes, {+Cloud-Backup+} returns the balancer to its
   previous state.

.. _encrypted-cloud-provider-snapshot:

Encryption at Rest using Customer Key Management
------------------------------------------------

|service| encrypts the storage engine of all snapshot volumes, ensuring
the security of cluster data at rest. For projects and clusters using
:ref:`security-kms-encryption`, |service| applies an additional layer
of encryption to your snapshot storage volumes using the Key Management
Service (KMS) provider configured for the cluster.

.. tabs::

   .. tab:: AWS Identity and Access Management (IAM)
      :tabid: aws-kms

      For clusters using :doc:`AWS IAM </security-aws-kms>` as their
      Key Management Service, |service| uses the project's customer
      master key (CMK) and AWS |iam| user or role credentials at the time
      of the snapshot to automatically encrypt the snapshot data files.
      This is an additional layer of encryption on the existing
      encryption applied to all |service| storage and snapshot
      volumes. :term:`Oplog <oplog>` data collected for |pit| restores
      is also encrypted with the customer's |cmk|.

      |service| stores the unique ID of the |cmk| and the |aws| |iam|
      user credentials or :ref:`roles <set-up-unified-aws-access>` used
      to access the |cmk|. |service| uses this information when restoring
      the snapshot. For complete documentation on restoring an encrypted
      snapshot, see :ref:`restore-encrypted-snapshot`.

   .. tab:: Azure Key Vault
      :tabid: azure-kms

      For clusters using :doc:`Azure Key Vault </security-azure-kms>`
      as their Key Management Service, |service| uses the project's
      Key Identifier, Key Vault Credentials, and Active Directory
      application account credentials at the time of the snapshot to
      automatically encrypt the snapshot data files. This is an
      additional layer of encryption on the existing encryption
      applied to all |service| storage and snapshot volumes.

      |service| stores the unique ID of the Azure Key Identifier used
      the encrypt the snapshot. |service| also stores the Azure Key
      Vault credentials and the Active Domain application account
      credentials used to access the Key Identifier. |service| uses
      this information when restoring the snapshot. For complete
      documentation on restoring an encrypted snapshot, see
      :ref:`restore-encrypted-snapshot`.

   .. tab:: Google Cloud KMS
      :tabid: gcp-kms

      |service| uses your |gcp| Service Account Key to encrypt and
      decrypt your MongoDB master keys. These MongoDB master keys are
      used to encrypt cluster database files and :ref:`cloud providers
      snapshots <backup-cloud-provider>`. For complete documentation
      on restoring an encrypted snapshot, see
      :ref:`restore-encrypted-snapshot`.

To view the key used to encrypt a snapshot:

1. Click :guilabel:`Databases` in the top-left corner of |service|.
   
#. From the :guilabel:`{+Database-Deployments+}` view of the |service| UI, click
   on the cluster name.

#. Click the :guilabel:`Backup` tab, then click
   :guilabel:`Snapshots`.

#. Note the :guilabel:`Encryption Key ID` for each snapshot in
   the cluster. |service| lists the Key Identifier
   used to encrypt the snapshot. Unencrypted snapshots display
   :guilabel:`Not enabled`.

.. include:: /includes/fact-encrypted-backups-master-key-requirements.rst

For complete documentation on configuring Encryption at Rest using
your Key Management for an |service| project,
see :doc:`/security-kms-encryption`. You can then either
:ref:`deploy <create-cluster-enable-encryption>` a new cluster or
:ref:`enable <scale-cluster-enable-encryption>` an existing cluster
with Encryption at Rest using your Key Management.

.. _single-region-cloud-backup:

Single-Region Cluster Backups
-----------------------------

With single-region cluster backups, |service|:

- Determines the order of nodes to try to snapshot using the following 
  algorithm:

  i. Snapshots on a secondary. :sup:`1` Then,
  #. Snapshots the node with the lowest priority if possible. :sup:`2` 
     Then,
  #. Snapshots incrementally from one snapshot to the next if possible. 
     :sup:`3` Then,
  #. Snapshots node lexically first by hostname.

  :sup:`1` If there is a tie, |service| skips to the next step to determine the node to snapshot.

  :sup:`2` If there is a tie, |service| then favors the node that can be snapshotted incrementally from the previous snapshot (i.e., node using the same disk). 
  
  :sup:`3` If there is a tie, |service| then favors the node with the lexicographically smallest hostname.

- Once the node order is determined, tries to snapshot a node. If a 
  selected node is unhealthy, |service| tries to snapshot the next 
  node that it favors.

- Stores the snapshots in the same cloud region as the cluster.
- Retains snapshots based on your
  :ref:`retention policy <cloud-provider-retention-policy>`.

.. .. include:: /images/cloud-provider-snapshot-single-region-primary.rst

.. .. include:: /images/cloud-provider-snapshot-single-region-secondary.rst

|service| automatically creates a new snapshot storage volume if the
existing snapshot storage volume becomes invalid. |service| creates the
new volume in the same region as the cluster's current primary.
|service| then takes a full-copy snapshot to maintain backup
availability and continues using that member and its corresponding
region for further incremental snapshots.

Events that can cause an election to select a new node for the snapshot
storage volume include:

- Changing the |service| cluster tier,
- Modifying the |service| cluster's storage volume or speed,
- Changing the |service| cluster's region, and
- Maintenance performed by |service| or the cluster's cloud provider.

.. seealso::

   To learn more about snapshot retention, see
   :ref:`cloud-provider-retention-policy`.

.. _multi-region-cloud-backup:

Multi-Region Cluster Backups
----------------------------

With multi-region cluster backups, |service|:

- Determines the order of nodes to snapshot using the following 
  algorithm:

  i. Snapshots in the highest priority region if possible. :sup:`1` 
     Then,  
  #. Snapshots on a secondary. :sup:`2` Then,
  #. Snapshots the node with the lowest priority if possible. :sup:`3` 
     Then,
  #. Snapshots incrementally from one snapshot to the next if possible. 
     :sup:`4` Then,
  #. Snapshots node lexically first by hostname.

  :sup:`1` If there is a tie, |service| then compares based on the descending order of priority. 

  :sup:`2` If there is a tie, |service| skips to the next step to determine the node to snapshot.

  :sup:`3` If there is a tie, |service| then favors the node that can be snapshotted incrementally from the previous snapshot (i.e., node using the same disk). 
  
  :sup:`4` If there is a tie, |service| then favors the node with the lexicographically smallest hostname.

- Tries to snapshot a node once the node order is determined. If a 
  selected node is unhealthy, |service| tries to snapshot the next 
  node that it favors.

- Retains snapshots based on your :ref:`retention policy 
  <cloud-provider-retention-policy>`.

.. .. include:: /images/cloud-provider-snapshot-multi-region-primary.rst

.. .. include:: /images/cloud-provider-snapshot-multi-region-secondary.rst

|service| automatically creates a new snapshot storage volume if the
existing snapshot storage volume becomes invalid. |service| creates the
new volume in the same region as the cluster's current primary.
|service| then takes a full-copy snapshot to maintain backup
availability and continues using that member and its corresponding
region for further incremental snapshots.

Events that can cause an election to select a new node for the snapshot
storage volume include:

- Changing the |service| cluster tier,
- Modifying the |service| cluster's storage volume or speed,
- Changing the |service| cluster's highest priority region, and
- Maintenance performed by |service| or the cluster's cloud provider.

.. seealso::

   To learn more about snapshot retention, see
   :ref:`cloud-provider-retention-policy`.

.. _sharded-global-cluster-backup:

Global Cluster Backups
----------------------

|service| can back up :doc:`Global Clusters </global-clusters>` using
{+Cloud-Backup+}s as their backup method. |service| restores the shards
in the source cluster to the corresponding shards in the target cluster
using the same order as specified in the cluster configuration.

.. example::

   ``shard0`` in the source cluster is restored to ``shard0`` in the
   target cluster.

.. note::

   If you used the |api| to create your Global Cluster, the zones are
   defined in the ``replicationSpecs`` parameter in the
   :doc:`Create a Cluster </reference/api/clusters-create-one>` and
   :doc:`Modify a Cluster </reference/api/clusters-modify-one>`
   |api| endpoints.

If the cluster configurations of the source and target clusters do not
match, shard data may migrate to a different cloud provider zone than
where it resided in the source cluster. After |service| completes the
restore operation, the MongoDB :term:`balancer` for the target cluster
migrates the data back to the zone where it resided in the source
cluster if your clusters meet the following requirements:

- Both clusters have enabled a |global-write-cluster| on the same
  collection

- Both clusters use the same :term:`shard key` for the
  |global-write|-enabled collection

.. note::

   If the |global-write|-enabled collection on the target cluster does
   not contain any data, the MongoDB balancer for the cluster
   automatically distributes any data that you later add to the
   collection among the target cluster's shards.

To enable global writes on the target cluster:

1. Click :guilabel:`Databases` in the top-left corner of |service|.
   
#. Click :guilabel:`Browse Collections` beneath the target cluster on the
   :guilabel:`{+Database-Deployments+}` page.

#. Click :guilabel:`Enable Global Writes`.

.. _pit-restore:

{+PIT-Restore+}s
--------------------------------------------------------

{+PIT-Restore+}s replay the :term:`oplog` to restore a cluster from a
particular point in time within a window specified in the Backup
Policy.

You may opt to
:ref:`enable {+PIT-Restore+} restores <create-cluster-backups>`.
Configure your {+pit-restore+} window with the
:ref:`Backup Policy Editor <cps-backup-policies>`.

.. note::

   Enabling {+pit-restore+}s increases the monthly cost of your
   cluster.

   To learn more about the cost implications, see
   :ref:`billing <billing-backup-cloud-provider-snapshots>`.

Your cluster's snapshots stay within the cloud provider's storage
service under the cluster or shard's
:ref:`highest priority region <deploy-across-multiple-regions>`. Oplog
backups on |aws| clusters use standard |aws| |s3| encryption.

.. note::

   All clusters with {+pit-restore+}s enabled store :term:`oplog` data
   on |aws| |s3|, including clusters backed by Azure and |gcp|.

The following actions cause all existing oplog backups to be deleted.
All existing snapshots remain intact, but |service| removes previously
preserved oplog data when:

- You disable {+pit-restore+}s for your cluster.
- The cluster receives an excessive number of writes. The cluster 
  processes a large number of writes that causes the oplog to roll over 
  before backup collects it.

  .. example::

     1. You sized your oplog for one hour of its usual write traffic,
        say 1,000 operations.

     #. Database activity results in a large number of writes to the
        oplog, say 2,000 operations.

     #. The number of writes result in the oplog dropping older
        records. This example would lose 1,000 operations.

     #. Backup should collect operation #1, but it collects #1,001
        instead.

If you :ref:`change <move-cluster>` your cluster's :ref:`highest 
priority region <deploy-across-multiple-regions>` or if MongoDB 
migrates oplog data to a different region:

- |service| retains data in both the old and new regions until your 
  {+pit-restore+} window is represented in the new region. Once 
  the {+pit-restore+} window is represented in the new region, 
  |service| deletes the data in the old region.
- You will be billed for storage in both the old and new regions for   
  the days following the region change. You must disable 
  {+pit-restore+} and reenable it to prevent billing in both regions. 

  .. note:: 

   If you disable {+pit-restore+}, |service| will delete the 
   {+pit-restore+} history.

When you use {+pit-restore+}s to restore a {+cluster+} from a previous 
point in time, |service| retains the {+cluster+}'s :term:`oplog`. You 
can use {+pit-restore+}s repeatedly to restore the {+cluster+} to any point
in its {+pit-restore+} window **except** between when you initiated a
restore and when |service| completes a snapshot after the restore.

.. _cloud-provider-backup-schedule:
.. _cloud-provider-retention-policy:
.. _cps-backup-policies:

Snapshot Scheduling and Retention Policy
----------------------------------------

Use the Backup Policy Editor to configure a backup policy for {+Cloud-Backup+}s.

1. Click :guilabel:`Databases` in the top-left corner of |service|.
   
#. From the :guilabel:`{+Database-Deployments+}` view, click the cluster name.

#. Click the :guilabel:`Backup` tab.

#. Click :guilabel:`Backup Policy`.

A backup policy has the following sections:

- A time of day, in |utc|, at which to create snapshots.

- A frequency interval and duration of retention.

- If |pit| Restores are enabled, a |pit| window that allows you
  to restore to any point in time in the last X days where X is the window.

.. example::

   The default backup policy specifies a snapshot time of ``18:00``
   |utc| and the following four policy items:

   .. list-table::
      :widths: 15 15 20 30 20
      :header-rows: 1

      * - Policy Type
        - Tier
        - {+PIT-Restore+}
        - Snapshot Taken
        - Snapshot Retained

      * - Hourly
        - |nvme|
        - Enabled
        - Every 12 hours
        - 2 days

      * - Hourly
        - non-|nvme|
        - Enabled
        - Every 6 hours
        - 2 days

      * - Daily
        - All
        - Either
        - Every day
        - 7 days

      * - Weekly
        - All
        - Either
        - Every Saturday
        - 4 weeks

      * - Monthly
        - All
        - Either
        - Last day of the month
        - 12 months

To learn more about {+Cloud-Backup+} billing, see
:ref:`billing-backup-cloud-provider-snapshots`.

.. _changine-backup-policy-time:

Changing the Backup Policy Time
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To modify the backup policy time:

1. In the :guilabel:`Backup Policy Editor`, select the hour at
   which |service| takes a snapshot each day from :guilabel:`hr`
   beneath :guilabel:`Snapshot Time (UTC)`.

2. Select the number of minutes after :guilabel:`hr` at which |service|
   takes a snapshot from :guilabel:`min` beneath
   :guilabel:`Snapshot Time (UTC)`.

3. Click :guilabel:`Save Changes`.

.. _creating-backup-policy:

Configuring the Backup Policy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Each row in the :guilabel:`Backup Policy Frequency and Retention` table
represents a backup policy item. Configure the policy items and,
optionally, add new policy items to configure a new backup policy.

.. tip::

   |service| displays the estimated number of snapshots
   associated with your changes below the
   :guilabel:`Backup Policy Frequency and Retention` table.

To specify a backup policy item:

1. Select the frequency unit from :guilabel:`Frequency Unit` for a
   policy item.

   Alternatively, click :guilabel:`Add Frequency Unit` to add a new
   policy item to the backup policy.

   .. note::

      You cannot specify multiple :guilabel:`Hourly` and
      :guilabel:`Daily` backup policy items.

2. Select the frequency for the frequency unit from :guilabel:`Every`.

   .. note::

      If you delete an existing backup frequency unit, the snapshots
      for which the frequency was specified remain intact until they
      expire or you delete them.

3. Specify the retention time for the policy item in
   :guilabel:`Retention Time` and the units for the retention time from
   the list to the right.

   .. note::

      |service| requires that the value specified for
      :guilabel:`Retention Time` for items that are less frequent is
      equal to or larger than the value specified for items that are
      more frequent. For example, if the hourly policy item specifies
      a retention of two days or greater, the retention for the weekly
      snapshot must be two weeks or greater.

      You can't configure a :ref:`restore window <create-pit-policy>` 
      that is longer than the Hourly Snapshot 
      :guilabel:`Retention Time`.

4. (Optional) To apply the retention changes in the updated backup
   policy to snapshots that |service| took previously, check
   :guilabel:`Apply policy retention changes to existing snapshots`.

   .. note::

      This option affects only snapshots created by the updated policy
      items and whose retention has not been updated individually with
      the
      :doc:`/reference/api/cloud-backup/schedule/modify-one-schedule`
      API.

5. Click :guilabel:`Save Changes`.

.. note::

   To take a snapshot sooner than the next scheduled snapshot,
   use the :doc:`/reference/api/cloud-backup/backup/take-one-ondemand-backup` API.

.. note::

   .. include:: /includes/fact-overlapping-backup-policy-items.rst

.. important::

   If you disable {+Cloud-Backup+}s for a cluster or terminate a
   cluster that had snapshots enabled, |service| immediately
   deletes the backup snapshots for that cluster. For clusters not
   using :ref:`security-kms-encryption`
   you can :ref:`download the latest snapshot
   <restore-cloud-provider-snapshot-download>` to preserve any data
   stored in the cluster.

.. _create-pit-policy:

Configure the Restore Window
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can replay the :term:`oplog` to restore a cluster from any point in 
time within a specified restore window.

To specify the restore window duration, select how long you want
|service| to retain the oplog for point-in-time restores from
the :guilabel:`Restore Window` list.

.. note::

   You can't configure a restore window
   that is longer than the :ref:`Hourly Snapshot Retention Time
   <creating-backup-policy>`.

.. _viewing-snapshots:

Viewing Snapshots
~~~~~~~~~~~~~~~~~

|service| displays existing snapshots on the :guilabel:`Snapshots`
page. To view snapshots that |service| has already taken:

1. Click :guilabel:`Databases` in the top-left corner of |service|.
   
#. From the :guilabel:`{+Database-Deployments+}` view, click the cluster name.

#. Click the :guilabel:`Backup` tab.

#. Click :guilabel:`Snapshots`.

By default, |service| displays both on-demand and policy-based
snapshots. To view only policy-based snapshots:

1. Click :guilabel:`Policy` under :guilabel:`View Snapshots by`.

   Alternatively, click :guilabel:`On-demand` to display only snapshots
   taken by clicking :guilabel:`Take Snapshot Now`.

Snapshots taken according to the backup policy display the frequency of
the policy item that generated the snapshot in the
:guilabel:`Frequency` column: ``Monthly``, ``Weekly``, ``Daily``, or
``Hourly``.

.. note::

   .. include:: /includes/fact-overlapping-backup-policy-items.rst

.. _cloud-provider-snapshots-on-demand:
.. _on-demand-snapshots:

On-Demand Snapshots
-------------------

|service| takes on-demand snapshots immediately, unlike scheduled
snapshots which occur at
:doc:`regular intervals </reference/api/cloud-backup/schedule/schedules>`.
If there is already an on-demand snapshot with a status of ``queued``
or ``inProgress``, you must wait until |service| has completed the
on-demand snapshot before taking another. If there is already a
scheduled snapshot with a status of ``queued`` or ``inProgress``, you
may queue an on-demand snapshot. You must have the
:authrole:`Organization Owner` or :authrole:`Project Owner` role to
successfully call this endpoint.

To take an on-demand snapshot:

1. Click :guilabel:`Databases` in the top-left corner of |service|.
   
#. From the :guilabel:`{+Database-Deployments+}` view, click the :icon:`ellipsis-h`
   button below the cluster name then click
   :guilabel:`Take Snapshot Now`.

#. In the :guilabel:`On-Demand Snapshot` modal, enter the following:

   a. In the :guilabel:`Retention` box, enter the number of days that
      you want |service| to retain the snapshot.

   b. In the :guilabel:`Description` box, enter a descriptive name
      for the snapshot.

#. Click :guilabel:`Take Snapshot`.

Click the :guilabel:`Backup` tab, then click :guilabel:`Snapshots` for
the cluster to view the on-demand snapshot.

.. note::

   The :guilabel:`Take Snapshot Now` button also appears on the
   :guilabel:`Snapshots` page for the cluster.

.. toctree::
   :titlesonly:

   /backup/cloud-backup/restore
   /backup/cloud-backup/export
   /backup/cloud-backup/import-archive
