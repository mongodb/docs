.. index:: fundamentals; sharding
.. _sharding-fundamentals:

=================
Sharding Overview
=================

.. default-domain:: mongodb

Sharding is MongoDBâ€™s approach to scaling out. Sharding partitions a
database collection and stores the different portions on different
machines. Sharding automatically balances data and load across machines.
By splitting a collection up across multiple machines, MongoDB increases
write capacity, provides the ability to support larger working sets, and
raises the limits of total data size beyond the physical resources of a
single node.

When your collections become too large for existing storage, you need
only add a new machine and MongoDB automatically distributes data to the
new server.

Sharded Cluster
---------------

Sharding occurs within a :term:`sharded cluster`. A typical sharded cluster
consists of the following:

- Two or more :program:`mongod` instances that serve as the
  :term:`shards <shard>` that hold your database collections. Usually
  each shard comprises a :term:`replica set`, not a lone
  :program:`mongod` instance.

- Three :program:`mongod` instances that serve as config servers to hold
  metadata about the cluster. The metadata maps :term:`chunks
  <chunk>` to shards.

- Two or more :program:`mongos` instances that route the reads and
  writes to the shards. A :program:`mongos` instance is a lightweight
  routing process.

Sharding Features
-----------------

With sharding MongoDB automatically distributes data among a
collection of :program:`mongod` instances. Sharding, as implemented in
MongoDB has the following features:

.. glossary::

   Range-based Data Partitioning
      MongoDB distributes documents among :term:`shards <shard>` based
      on the value of the :ref:`shard key <sharding-shard-key>`. Each
      :term:`chunk` represents a block of :term:`documents <document>`
      with values that fall within a specific range. When chunks grow
      beyond the :ref:`chunk size <sharding-chunk-size>`, MongoDB
      divides the chunks into smaller chunks (i.e. :term:`splitting
      <split>`) based on the shard key.

   Automatic Data Volume Distribution
      The sharding system automatically balances data across the
      cluster without intervention from the application
      layer. Effective automatic sharding depends on a well chosen
      :ref:`shard key <sharding-shard-key>`, but requires no
      additional complexity, modifications, or intervention from
      developers.

   Transparent Query Routing
      Sharding is completely transparent to the application layer,
      because all connections to a cluster go through
      :program:`mongos`. Sharding in MongoDB requires some
      :ref:`basic initial configuration <sharding-procedure-setup>`,
      but ongoing function is entirely transparent to the application.

   Horizontal Capacity
      Sharding increases capacity in two ways:

      #. Effective partitioning of data can provide additional write
         capacity by distributing the write load over a number of
         :program:`mongod` instances.

      #. Given a shard key with sufficient :ref:`cardinality
         <sharding-shard-key-cardinality>`, partitioning data allows
         users to increase the potential amount of data to manage
         with MongoDB and expand the :term:`working set`.

.. index:: shard key
   single: sharding; shard key

.. _sharding-shard-key:
.. _shard-key:

Shard Keys
----------

MongoDB distributes documents among :term:`shards <shard>` based on
:term:`shard keys <shard key>`. A shard key must be :term:`field` that
exists in every :term:`document` in the collection. Shard keys, like
:term:`indexes <index>`, can be either a single field, or may be a
compound key, consisting of multiple fields.

MongoDB's sharding is range-based. Each :term:`chunk` holds
documents having specific range of values for the "shard key". Thus,
choosing the correct shard key can have a great impact on the
performance, capability, and functioning of your database and cluster.

Appropriate shard key choice depends on the schema of your data and
the way that your application queries and writes data to the database.

The ideal shard key:

- is easily divisible which makes it easy for MongoDB to distribute
  content among the shards. Shard keys that have a limited number of
  possible values are not ideal as they can result in some chunks that
  are "unsplitable." See the :ref:`sharding-shard-key-cardinality`
  section for more information.

- will distribute write operations among the cluster, to prevent any
  single shard from becoming a bottleneck. Shard keys that have a high
  correlation with insert time are poor choices for this reason;
  however, shard keys that have higher "randomness" satisfy this
  requirement better. See the :ref:`sharding-shard-key-write-scaling`
  section for additional background.

- will make it possible for the :program:`mongos` to return most query
  operations directly from a single *specific* :program:`mongod`
  instance. Your shard key should be the primary field used by your
  queries, and fields with a high degree of "randomness" are poor
  choices for this reason. See the :ref:`sharding-shard-key-query-isolation`
  section for specific examples.

The challenge when selecting a shard key is that there is not always
an obvious choice. Often, an existing field in your collection may not be
the optimal key. In those situations, computing a special purpose
shard key into an additional field or using a compound shard key may
help produce one that is more ideal.

.. index:: sharding; config servers
.. index:: config servers
.. _sharding-config-server:

Config Servers
--------------

Config servers maintain the shard metadata in a config
database.  The :term:`config database <config database>` stores
the relationship between :term:`chunks <chunk>` and where they reside
within a :term:`sharded cluster`. Without a config database, the
:program:`mongos` instances would be unable to route queries or write
operations within the cluster.

Config servers *do not* run as replica sets. Instead, a :term:`cluster
<sharded cluster>` operates with a group of *three* config servers that use a
two-phase commit process that ensures immediate consistency and
reliability.

For testing purposes you may deploy a cluster with a single
config server, but this is not recommended for production.

.. warning::

   If your cluster has a single config server, this
   :program:`mongod` is a single point of failure.  If the instance is
   inaccessible the cluster is not accessible. If you cannot recover
   the data on a config server, the cluster will be inoperable.

   **Always** use three config servers for production deployments.

The actual load on configuration servers is small because each
:program:`mongos` instances maintains a cached copy of the configuration
database. MongoDB only writes data to the config server to:

- create splits in existing chunks, which happens as data in
  existing chunks exceeds the maximum chunk size.

- migrate a chunk between shards.

Additionally, all config servers must be available on initial setup
of a sharded cluster, each :program:`mongos` instance must be able
to write to the ``config.version`` collection.

If one or two configuration instances become unavailable, the
cluster's metadata becomes *read only*. It is still possible to read
and write data from the shards, but no chunk migrations or splits will
occur until all three servers are accessible. At the same time, config
server data is only read in the following situations:

- A new :program:`mongos` starts for the first time, or an existing
  :program:`mongos` restarts.

- After a chunk migration, the :program:`mongos` instances update
  themselves with the new cluster metadata.

If all three config servers are inaccessible, you can continue to use
the cluster as long as you don't restart the :program:`mongos`
instances until after config servers are accessible again. If you
restart the :program:`mongos` instances and there are no accessible
config servers, the :program:`mongos` would be unable to direct
queries or write operations to the cluster.

Because the configuration data is small relative to the amount of data
stored in a cluster, the amount of activity is relatively low, and 100%
up time is not required for a functioning sharded cluster. As a result,
backing up the config servers is not difficult. Backups of config
servers are critical as clusters become totally inoperable when
you lose all configuration instances and data. Precautions to ensure
that the config servers remain available and intact are critical.

.. note::

   Configuration servers store metadata for a single sharded cluster.
   You must have a separate configuration server or servers for each
   cluster you administer.

.. index:: mongos
.. _sharding-mongos:
.. _sharding-read-operations:

:program:`mongos` and Querying
------------------------------

.. seealso:: :doc:`/reference/mongos` and the :program:`mongos`\-only
   settings: :setting:`test` and :setting:`chunkSize`.

Operations
~~~~~~~~~~

The :program:`mongos` provides a single unified interface to a sharded
cluster for applications using MongoDB. Except for the selection of a
:term:`shard key`, application developers and administrators need not
consider any of the :doc:`internal details of sharding <sharding-internals>`.

:program:`mongos` caches data from the :ref:`config server
<sharding-config-server>`, and uses this to route operations from
applications and clients to the :program:`mongod` instances.
:program:`mongos` have no *persistent* state and consume
minimal system resources.

The most common practice is to run :program:`mongos` instances on the
same systems as your application servers, but you can maintain
:program:`mongos` instances on the shards or on other dedicated
resources.

.. note::

   .. versionchanged:: 2.1

   Some aggregation operations using the :dbcommand:`aggregate`
   command (i.e. :method:`db.collection.aggregate()`,) will cause
   :program:`mongos` instances to require more CPU resources than in
   previous versions. This modified performance profile may dictate
   alternate architecture decisions if you use the :term:`aggregation
   framework` extensively in a sharded environment.

.. _sharding-query-routing:

Routing
~~~~~~~

:program:`mongos` uses information from :ref:`config servers
<sharding-config-server>` to route operations to the cluster as
efficiently as possible. In general, operations in a sharded
environment are either:

1. Targeted at a single shard or a limited group of shards based on
   the shard key.

2. Broadcast to all shards in the cluster that hold documents in a
   collection.

When possible you should design your operations to be as targeted as
possible. Operations have the following targeting characteristics:

- Query operations broadcast to all shards [#namespace-exception]_
  **unless** the :program:`mongos` can determine which shard or shard
  stores this data.

  For queries that include the shard key, :program:`mongos` can target
  the query at a specific shard or set of shards, if the portion
  of the shard key included in the query is a *prefix* of the shard
  key. For example, if the shard key is:

  .. code-block:: javascript

     { a: 1, b: 1, c: 1 }

  The :program:`mongos` *can* route queries that include the full
  shard key or either of the following shard key prefixes at a
  specific shard or set of shards:

  .. code-block:: javascript

     { a: 1 }
     { a: 1, b: 1 }

  Depending on the distribution of data in the cluster and the
  selectivity of the query, :program:`mongos` may still have to
  contact multiple shards [#possible-all]_ to fulfill these queries.

- All :method:`insert() <db.collection.insert()>` operations target to
  one shard.

- All single :method:`update() <db.collection.update()>` operations
  target to one shard. This includes :term:`upsert` operations.

- The :program:`mongos` broadcasts multi-update operations to every
  shard.

- The :program:`mongos` broadcasts :method:`remove()
  <db.collection.remove()>` operations to every shard unless the
  operation specifies the shard key in full.

While some operations must broadcast to all shards, you can improve
performance by using as many targeted operations as possible by
ensuring that your operations include the shard key.

.. [#namespace-exception] If a shard does not store chunks from a
   given collection, queries for documents in that collection are not
   broadcast to that shard.

.. [#a/c-as-a-case-of-a] In this example, a :program:`mongos` could
   route a query that included ``{ a: 1, c: 1 }`` fields at a specific
   subset of shards using the ``{ a: 1 }`` prefix. A :program:`mongos`
   cannot route any of the following queries to specific shards
   in the cluster:

   .. code-block:: javascript

      { b: 1 }
      { c: 1 }
      { b: 1, c: 1 }

.. [#possible-all] :program:`mongos` will route some queries, even
   some that include the shard key, to all shards, if needed.

Sharded Query Response Process
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To route a query to a :term:`cluster <sharded cluster>`,
:program:`mongos` uses the following process:

#. Determine the list of :term:`shards <shard>` that must receive the query.

   In some cases, when the :term:`shard key` or a prefix of the shard
   key is a part of the query, the :program:`mongos` can route the
   query to a subset of the shards. Otherwise, the :program:`mongos`
   must direct the query to *all* shards that hold documents for that
   collection.

   .. example::

      Given the following shard key:

      .. code-block:: javascript

         { zipcode: 1, u_id: 1, c_date: 1 }

      Depending on the distribution of chunks in the cluster, the
      :program:`mongos` may be able to target the query at a subset of
      shards, if the query contains the following fields:

      .. code-block:: javascript

         { zipcode: 1 }
         { zipcode: 1, u_id: 1 }
         { zipcode: 1, u_id: 1, c_date: 1 }

#. Establish a cursor on all targeted shards.

   When the first batch of results returns from the cursors:

   a. For query with sorted results (i.e. using
      :method:`cursor.sort()`) the :program:`mongos` performs a merge
      sort of all queries.

   b. For a query with unsorted results, the :program:`mongos` returns
      a result cursor that "round robins" results from all cursors on
      the shards.

      .. versionchanged:: 2.0.5
         Before 2.0.5, the :program:`mongos` exhausted each cursor,
         one by one.

.. index:: balancing
.. _sharding-balancing:

Balancing and Distribution
--------------------------

Balancing is the process MongoDB uses to redistribute data within a
:term:`sharded cluster`. When a :term:`shard` has a too many:
term:`chunks <chunk>` when compared to other shards, MongoDB balances
the shards.

The
balancing process attempts to minimize the impact that balancing can
have on the cluster, by:

- Moving only one chunk at a time.

- Only initiating a balancing round when the difference in number of
  chunks between the shard with the greatest and the shard with the
  least number of chunks exceeds the :ref:`migration threshold
  <sharding-migration-thresholds>`.

You may disable the balancer on a temporary basis for
maintenance and limit the window during which it runs to prevent the
balancing process from impacting production traffic.

.. seealso:: :ref:`sharding-balancing-operations` and
   :ref:`sharding-balancing-internals`.

.. note::

   The balancing procedure for :term:`sharded clusters <sharded cluster>`
   is entirely transparent to the user and application layer. This
   documentation is only included for your edification and possible
   troubleshooting purposes.

.. index:: sharding; security
.. _sharding-security:

Security Considerations for Sharded Clusters
--------------------------------------------

.. todo:: migrate this content to /administration/security.txt when that
   document exists. See DOCS-79 for tracking this document.

.. note::

   You should always run all :program:`mongod` components in trusted
   networking environments that control access to the cluster using
   network rules and restrictions to ensure that only known traffic
   reaches your :program:`mongod` and :program:`mongos` instances.

.. warning::  Limitations

   .. versionchanged:: 2.2
      Read only authentication is fully supported in shard
      clusters. Previously, in version 2.0, sharded clusters would not
      enforce read-only limitations.

   .. versionchanged:: 2.0
      Sharded clusters support authentication. Previously, in version
      1.8, sharded clusters will not support authentication and access
      control. You must run your sharded systems in trusted
      environments.

To control access to a sharded cluster, you must set the
:setting:`keyFile` option on all components of the sharded cluster. Use
the :option:`--keyFile <mongos --keyFile>` run-time option or the
:setting:`keyFile` configuration option for all :program:`mongos`,
configuration instances, and shard :program:`mongod` instances.

There are two classes of security credentials in a sharded cluster:
credentials for "admin" users (i.e. for the :term:`admin database`) and
credentials for all other databases. These credentials reside in
different locations within the cluster and have different roles:

#. Admin database credentials reside on the config servers, to receive
   admin access to the cluster you *must* authenticate a session while
   connected to a :program:`mongos` instance using the
   :term:`admin database`.

#. Other database credentials reside on the *primary* shard for the
   database.

This means that you *can* authenticate to these users and databases
while connected directly to the primary shard for a database. However,
for clarity and consistency all interactions between the client and
the database should use a :program:`mongos` instance.

.. note::

   Individual shards can store administrative credentials to their
   instance, which only permit access to a single shard. MongoDB
   stores these credentials in the shards' :term:`admin databases <admin database>` and these
   credentials are *completely* distinct from the cluster-wide
   administrative credentials.
