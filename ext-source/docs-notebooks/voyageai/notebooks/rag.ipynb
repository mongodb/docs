{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with Voyage AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a companion to the [RAG with Voyage AI](https://www.mongodb.com/docs/voyageai/tutorials/rag/) tutorial. Refer to the page for set-up instructions and detailed explanations.\n",
    "\n",
    "Retrieval-augmented generation (RAG) is an architecture that uses semantic search to augment large language models (LLMs) with additional data, enabling them to generate more accurate responses.\n",
    "\n",
    "While semantic search retrieves relevant documents based on meaning, RAG takes this a step further by providing those retrieved documents as context to an LLM. This additional context helps the LLM generate a more accurate response to a user's query, reducing hallucinations. Voyage AI provides best-in-class embedding and reranking models to power retrieval for your RAG applications.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mongodb/docs-notebooks/blob/main/voyageai/notebooks/rag.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To complete this tutorial, you must have the following:\n",
    "\n",
    "- Python 3.9+\n",
    "- A model API key to access Voyage AI models\n",
    "- An LLM API key (Anthropic or OpenAI)\n",
    "- For MongoDB storage: A MongoDB Atlas cluster with connection string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade voyageai numpy anthropic openai langchain-community langchain-text-splitters pypdf python-dotenv pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your API keys\n",
    "os.environ[\"VOYAGE_API_KEY\"] = \"<your-model-api-key>\"\n",
    "\n",
    "# Choose your LLM provider\n",
    "LLM_PROVIDER = \"anthropic\"  # Options: \"anthropic\" or \"openai\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"<your-anthropic-api-key>\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<your-openai-api-key>\"\n",
    "\n",
    "# If using MongoDB for vector search (optional)\n",
    "os.environ[\"MONGODB_URI\"] = \"<your-mongodb-connection-string>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from voyageai import Client as VoyageClient\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize Voyage client\n",
    "voyage_client = VoyageClient(api_key=os.environ.get(\"VOYAGE_API_KEY\"))\n",
    "\n",
    "# Initialize LLM client based on provider\n",
    "if LLM_PROVIDER == \"anthropic\":\n",
    "    from anthropic import Anthropic\n",
    "    llm_client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "    LLM_MODEL = \"claude-sonnet-4-5-20250929\"\n",
    "elif LLM_PROVIDER == \"openai\":\n",
    "    from openai import OpenAI\n",
    "    llm_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    LLM_MODEL = \"gpt-4o\"\n",
    "else:\n",
    "    raise ValueError(\"Unsupported LLM provider. Please choose 'anthropic' or 'openai'.\")\n",
    "\n",
    "# Model configuration\n",
    "VOYAGE_MODEL = \"voyage-4-large\"\n",
    "\n",
    "print(f\"Using LLM provider: {LLM_PROVIDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Memory Storage Implementation\n",
    "\n",
    "> **Note:** Storing vectors in memory is suitable for prototyping and experimentation. For production applications, use a vector database like MongoDB Atlas for efficient retrieval from larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-memory storage\n",
    "documents = []\n",
    "embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Data\n",
    "\n",
    "Load a PDF, split into chunks, and generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF\n",
    "loader = PyPDFLoader(\"https://investors.mongodb.com/node/13576/pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "# Generate embeddings and store in memory\n",
    "print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
    "for chunk in chunks:\n",
    "    result = voyage_client.embed(\n",
    "        [chunk.page_content],\n",
    "        model=VOYAGE_MODEL,\n",
    "        input_type=\"document\"\n",
    "    )\n",
    "    embedding = np.array(result.embeddings[0], dtype=np.float32)\n",
    "    documents.append(chunk.page_content)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "print(f\"Ingested {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Documents and Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = \"What are MongoDB's latest AI announcements?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Generate query embedding\n",
    "query_result = voyage_client.embed(\n",
    "    [query],\n",
    "    model=VOYAGE_MODEL,\n",
    "    input_type=\"query\"\n",
    ")\n",
    "query_embedding = np.array(query_result.embeddings[0], dtype=np.float32)\n",
    "\n",
    "# Calculate similarity scores using dot product\n",
    "embeddings_array = np.array(embeddings)\n",
    "similarities = np.dot(embeddings_array, query_embedding)\n",
    "\n",
    "# Get top-5 most similar documents\n",
    "top_k = 5\n",
    "top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "retrieved_docs = []\n",
    "for idx in top_indices:\n",
    "    retrieved_docs.append({\n",
    "        \"text\": documents[idx],\n",
    "        \"score\": float(similarities[idx])\n",
    "    })\n",
    "\n",
    "# Combine retrieved documents into context\n",
    "context = \"\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "\n",
    "# Create prompt with context\n",
    "prompt = f\"\"\"Based on the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Generate response based on LLM provider\n",
    "if LLM_PROVIDER == \"anthropic\":\n",
    "    response = llm_client.messages.create(\n",
    "        model=LLM_MODEL,\n",
    "        max_tokens=1024,\n",
    "        system=\"You are a helpful assistant that answers questions based on the provided context.\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    answer = response.content[0].text\n",
    "elif LLM_PROVIDER == \"openai\":\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "else:\n",
    "    answer = \"Unsupported LLM provider.\"\n",
    "\n",
    "print(f\"Response:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# RAG with MongoDB Vector Search\n",
    "\n",
    "The following section demonstrates how to implement RAG with MongoDB Atlas as the vector store for production applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize MongoDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.operations import SearchIndexModel\n",
    "import time\n",
    "\n",
    "# Initialize MongoDB client\n",
    "mongo_client = MongoClient(os.environ.get(\"MONGODB_URI\"))\n",
    "rag_db = mongo_client[\"rag_db\"]\n",
    "collection = rag_db[\"test\"]\n",
    "\n",
    "print(\"Connected to MongoDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Data into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF\n",
    "loader = PyPDFLoader(\"https://investors.mongodb.com/node/13576/pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split the data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "# Generate embeddings and prepare documents\n",
    "docs_to_insert = []\n",
    "print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
    "for doc in chunks:\n",
    "    result = voyage_client.embed(\n",
    "        [doc.page_content],\n",
    "        model=VOYAGE_MODEL,\n",
    "        input_type=\"document\"\n",
    "    )\n",
    "    embedding = np.array(result.embeddings[0], dtype=np.float32)\n",
    "    if embedding is not None:\n",
    "        docs_to_insert.append({\n",
    "            \"text\": doc.page_content,\n",
    "            \"embedding\": embedding.tolist()\n",
    "        })\n",
    "\n",
    "# Insert documents into the collection\n",
    "if docs_to_insert:\n",
    "    collection.insert_many(docs_to_insert)\n",
    "    print(f\"Inserted {len(docs_to_insert)} documents into MongoDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"vector_index\"\n",
    "\n",
    "# Check if index already exists\n",
    "existing_indexes = list(collection.list_search_indexes(index_name))\n",
    "if existing_indexes:\n",
    "    if existing_indexes[0].get(\"queryable\"):\n",
    "        print(\"Vector index already exists and is queryable\")\n",
    "else:\n",
    "    # Create the search index\n",
    "    print(\"Creating vector search index...\")\n",
    "    search_index_model = SearchIndexModel(\n",
    "        definition = {\n",
    "            \"fields\": [\n",
    "                {\n",
    "                    \"type\": \"vector\",\n",
    "                    \"numDimensions\": 1024,\n",
    "                    \"path\": \"embedding\",\n",
    "                    \"similarity\": \"dotProduct\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        name=index_name,\n",
    "        type=\"vectorSearch\"\n",
    "    )\n",
    "\n",
    "    collection.create_search_index(model=search_index_model)\n",
    "\n",
    "    # Wait for index to become queryable\n",
    "    print(\"Waiting for index to become queryable...\")\n",
    "    while True:\n",
    "        indices = list(collection.list_search_indexes(index_name))\n",
    "        if len(indices) and indices[0].get(\"queryable\") is True:\n",
    "            print(\"Index is ready!\")\n",
    "            break\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Documents and Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with MongoDB\n",
    "query = \"What are MongoDB's latest AI announcements?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Generate query embedding\n",
    "query_result = voyage_client.embed(\n",
    "    [query],\n",
    "    model=VOYAGE_MODEL,\n",
    "    input_type=\"query\"\n",
    ")\n",
    "query_embedding = np.array(query_result.embeddings[0], dtype=np.float32)\n",
    "\n",
    "# Define the aggregation pipeline\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": \"vector_index\",\n",
    "            \"queryVector\": query_embedding.tolist(),\n",
    "            \"path\": \"embedding\",\n",
    "            \"exact\": True,\n",
    "            \"limit\": 5\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"text\": 1\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute the query\n",
    "results = collection.aggregate(pipeline)\n",
    "context_docs = list(results)\n",
    "\n",
    "# Convert documents to string\n",
    "context_string = \" \".join([doc[\"text\"] for doc in context_docs])\n",
    "\n",
    "# Construct prompt for the LLM\n",
    "prompt = f\"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    {context_string}\n",
    "    Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "# Generate response based on LLM provider\n",
    "if LLM_PROVIDER == \"anthropic\":\n",
    "    message = llm_client.messages.create(\n",
    "        model=LLM_MODEL,\n",
    "        max_tokens=1024,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    answer = message.content[0].text\n",
    "elif LLM_PROVIDER == \"openai\":\n",
    "    completion = llm_client.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    answer = completion.choices[0].message.content\n",
    "else:\n",
    "    answer = \"Unsupported LLM provider.\"\n",
    "\n",
    "print(f\"Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
