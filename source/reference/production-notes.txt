==============================
|k8s-op-full| Production Notes
==============================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

This page details system configuration recommendations for the
|k8s-op-full| when running in production.

- All sizing and performance recommendations for common MongoDB deployments
  through the |k8s-op-short| in this section are subject to change. Do
  not treat these recommendations as guarantees or limitations of any kind.

- These recommendations reflect performance testing findings and represent
  our suggestions for production deployments. We ran the tests on a cluster
  comprised of seven AWS EC2 instances of type ``t2.2xlarge`` and a
  master node of type ``t2.medium``.

- The recommendations in this section don't discuss characteristics of
  any specific deployment. Your deployment's characteristics may differ
  from the assumptions made to create these recommendations. Contact
  MongoDB Support for further help with sizings.

.. _OPA-gatekeeper:

Control Your Deployments with Policies Set in OPA Gatekeeper
------------------------------------------------------------

To control, audit, and debug your production deployments, you can use policies
for the `Gatekeeper <https://github.com/open-policy-agent/gatekeeper>`__
Open Policy Agent (OPA). Gatekeeper contains |k8s-crds| for creating and extending
deployment constraints through the
:gatekeeper:`constraint templates </constrainttemplates/>`.

The |k8s-op-short| offers a :ref:`list of Gatekeeper policies <gatekeeper-policies-list>`
that you can customize and apply to your deployments.

Each Gatekeeper policy consists of:

- ``<policy_name>.yaml`` file
- ``constraints.yaml`` file that is based on the :gatekeeper:`constraint template </constrainttemplates/>`

You can use binary and configurable Gatekeeper policies:

- Binary policies allow or prevent specific configurations, such as
  preventing deployments that don't use TLS, or deploying only specific
  MongoDB or |onprem| versions.

- Configurable policies allow you to specify configurations, such as the
  total number of replica sets that will be deployed for a specific
  MongoDB or |onprem| custom resource.

To use and apply Gatekeeper sample policies with the |k8s-op-short|:

1. :gatekeeper:`Install the OPA Gatekeeper </install/>` on your Kubernetes cluster.

2. Review the list of available constraint templates and constraints:

   .. code-block:: sh
      
      kubectl get constrainttemplates
      kubectl get constraints

3. Navigate to the policy directory, select a policy from the list and
   apply it and its constraints file:

   .. code-block:: sh

      cd <policy_directory>
      kubectl apply -f <policy_name>.yaml
      kubectl apply -f constraints.yaml

4. Review the Gatekeeper policies that are currently applied:

   .. code-block:: sh

      kubectl get constrainttemplates
      kubectl get contstraints

.. _gatekeeper-policies-list:

List of Sample OPA Gatekeeper Policies
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The |k8s-op-short| offers the following sample policies in this
:github:`OPA examples </mongodb/mongodb-enterprise-kubernetes/tree/master/opa_examples>`
GitHub directory:

.. list-table::
   :widths: 40 60
   :header-rows: 1

   * - Location
     - Policy Description

   * - :github:`Debugging </mongodb/mongodb-enterprise-kubernetes/tree/master/opa_examples/debugging>`
     - Blocks all MongoDB and |onprem| resources. This allows you to use
       the log output to craft your own policies. To learn more, see
       :gatekeeper:`Gatekeeper Debugging </debug/>`.

   * - :github:`mongodb_allow_replicaset </mongodb/mongodb-enterprise-kubernetes/tree/master/opa_examples/mongodb_allow_replicaset>`
     - Allows deploying only replica sets for MongoDB resources and
       prevents deploying sharded clusters.

   * - :github:`mongodb_allowed_versions </mongodb/mongodb-enterprise-kubernetes/tree/master/opa_examples/mongodb_allowed_versions>`
     - Allows deploying only specific MongoDB versions.

   * - :github:`ops_manager_allowed_versions </mongodb-enterprise-kubernetes/tree/master/opa_examples/ops_manager_allowed_versions>`
     - Allows deploying only specific |onprem| versions.

   * - :github:`mongodb_strict_tls </10gen/mongodb-enterprise-kubernetes/tree/master/opa_examples/mongodb_strict_tls>`
     - Allows using strict TLS mode for MongoDB deployments.

   * - :github:`ops_manager_replica_members </10gen/mongodb-enterprise-kubernetes/tree/master/opa_examples/ops_manager_replica_members>`
     - Allows deploying a specified number of |onprem| replica set and
       Application Database members.

   * - :github:`ops_manager_wizardless </10gen/mongodb-enterprise-kubernetes/tree/master/opa_examples/ops_manager_wizardless>`
     - Allows installing |onprem| in a non-interactive mode.

Deploy the Recommended Number of MongoDB Replica Sets
-----------------------------------------------------

We recommend that you use a single instance of the |k8s-op-short|
to deploy up to 20 replica sets in parallel.

You **may** increase this number to 50 and expect a reasonable
increase in the time that the |k8s-op-short| takes to download,
install, deploy, and reconcile its resources.

For 50 replica sets, the time to deploy varies and might take up to
40 minutes. This time depends on the network bandwidth of the |k8s|
cluster and the time it takes each {+mdbagent+} to download MongoDB
installation binaries from the Internet for each MongoDB cluster member.

To deploy more than 50 MongoDB replica sets in parallel,
use multiple instances of the |k8s-op-short|.

Ensure Proper Persistence Configuration
---------------------------------------

The |k8s| deployments orchestrated by the |k8s-op-short| are
stateful. The |k8s| container uses |k8s-pvs| to maintain the
cluster state between restarts.

To satisfy the statefulness requirement, the |k8s-op-short| performs
the following actions:

- Creates |k8s-pvs| for your MongoDB deployment.
- Mounts storage devices to one or more directories
  called mount points.
- Creates one persistent volume for each MongoDB mount point.
- Sets the default path in each |k8s| container to ``/data``.

To meet your MongoDB cluster's storage needs, make the following
changes in your configuration for each replica set deployed with
the |k8s-op-short|:

- Verify that persistent volumes are enabled in
  :setting:`spec.persistent`. This setting defaults to ``true``.
- Specify a sufficient amount of storage for the |k8s-op-short|
  to allocate for each of the volumes. The volumes store the data
  and the logs.

  - To set multiple volumes, each for data, logs, and the ``oplog``, use
    :setting:`spec.podSpec.persistence.multiple.data`.
  - To set a single volume to store data, logs, and the ``oplog``,
    use :setting:`spec.podSpec.persistence.single`.

The following abbreviated example shows recommended persistent storage
sizes.

.. code-block:: yaml
   :linenos:
   :emphasize-lines: 8, 13-19
   

   apiVersion: mongodb.com/v1
   kind: MongoDB
   metadata:
     name: my-replica-cluster
   spec:
     
     ...
     persistent: true
     
     
     shardPodSpec:
     ...
       persistence:
         multiple:
           data:
             storage: "20Gi"
           logs:
             storage: "4Gi"
             storageClass: standard

For a full example of persistent volumes configuration, see
:github:`replica-set-persistent-volumes.yaml 
</mongodb/mongodb-enterprise-kubernetes/blob/master/samples/mongodb/persistent-volumes/replica-set-persistent-volumes.yaml>`
in the :github:`Persistent Volumes Samples
</mongodb/mongodb-enterprise-kubernetes/tree/master/samples/mongodb/persistent-volumes>` directory. This
directory also contains sample persistent volumes configurations for
sharded clusters and standalone deployments.

.. seealso::

   - :setting:`spec.persistent`
   - :setting:`spec.podSpec.persistence.single`
   - :setting:`spec.podSpec.persistence.multiple.data`

Name Your MongoDB Service with its Purpose
------------------------------------------

Set the :setting:`spec.service` parameter to a value that identifies
this deployment's purpose, as illustrated in the following example.

.. code-block:: yaml
   :copyable: false
   :linenos:
   :emphasize-lines: 8

   apiVersion: mongodb.com/v1
   kind: MongoDB
   metadata:
     name: my-replica-set
   spec:
     members: 3
     version: "4.4.0-ent"
     service: drilling-pumps-geosensors
     featureCompatibilityVersion: "4.0"

.. seealso::

   :setting:`spec.service`

Specify CPU and Memory Resource Requirements
--------------------------------------------

In |k8s|, each Pod includes parameters that allow you
to specify :k8sdocs:`CPU resources </tasks/configure-pod-container/assign-cpu-resource/>`
and :k8sdocs:`memory resources
</tasks/configure-pod-container/assign-memory-resource/>` for each
container in the Pod.

To indicate resource bounds, |k8s| uses the :k8sdocs:`requests and limits
</concepts/configuration/manage-resources-containers/#requests-and-limits>`
parameters, where:
  
- *request* indicates a lower bound of a resource.
- *limit* indicates an upper bound of a resource.

The following sections illustrate how to:

- :ref:`set CPU and Memory for the Operator Pod <operator_pod_resources>`.
- :ref:`set CPU and Memory for MongoDB Pods <mdb_pods_resources>`.

For the Pods hosting |onprem|, use the
:github:`default resource limits configurations
</mongodb/mongodb-enterprise-kubernetes/blob/master/samples/ops-manager/ops-manager-pod-spec.yaml#L38-L46>`.

.. _operator_pod_resources:

Set CPU and Memory Utilization Bounds for the |k8s-op-short| Pod
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When you deploy replica sets with the |k8s-op-short|, CPU usage for
Pod used to host the |k8s-op-short| is initially high during the
reconciliation process, however, by the time the deployment completes,
it lowers.

For production deployments, to satisfy deploying up to 50 MongoDB
replica sets or sharded clusters in parallel with the |k8s-op-short|,
set the CPU and memory resources and limits for the |k8s-op-short| Pod
as follows:

- ``spec.template.spec.containers.resources.requests.cpu`` to 500m
- ``spec.template.spec.containers.resources.limits.cpu`` to 1100m
- ``spec.template.spec.containers.resources.requests.memory`` to 200Mi
- ``spec.template.spec.containers.resources.limits.memory`` to 1Gi

If you don't include the unit of measurement for CPUs, |k8s| interprets
it as the number of cores. If you specify ``m``, such as 500m, |k8s|
interprets it as ``millis``. To learn more, see
:k8sdocs:`Meaning of CPU </concepts/configuration/manage-resources-containers/#meaning-of-cpu>`.
  
The following abbreviated example shows the configuration with
recommended CPU and memory bounds for the |k8s-op-short| Pod in your
deployment of 50 replica sets or sharded clusters. If you are
deploying fewer than 50 MongoDB clusters, you may use lower
numbers in the configuration file for the |k8s-op-short| Pod.

.. note::

   Monitoring tools report the size of the |k8s-node| rather than the
   actual size of the container.

.. example::

   .. code-block:: yaml
      :copyable: false
      :linenos:
      :emphasize-lines: 24-25, 34-40

      apiVersion: apps/v1
      kind: Deployment
      metadata:
       name: mongodb-enterprise-operator
       namespace: mongodb
      spec:
       replicas: 1
       selector:
        matchLabels:
           app.kubernetes.io/component: controller
           app.kubernetes.io/name: mongodb-enterprise-operator
           app.kubernetes.io/instance: mongodb-enterprise-operator
       template:
        metadata:
         labels:
           app.kubernetes.io/component: controller
           app.kubernetes.io/name: mongodb-enterprise-operator
           app.kubernetes.io/instance: mongodb-enterprise-operator
         spec:
           serviceAccountName: mongodb-enterprise-operator
           securityContext:
             runAsNonRoot: true
             runAsUser: 2000
           containers:
           - name: mongodb-enterprise-operator
             image: quay.io/mongodb/mongodb-enterprise-operator:1.9.2
             imagePullPolicy: Always
             args:
              - "-watch-resource=mongodb"
              - "-watch-resource=opsmanagers"
              - "-watch-resource=mongodbusers"
             command:
              - "/usr/local/bin/mongodb-enterprise-operator"
             resources:
               limits:
                 cpu: 1100m
                 memory: 1Gi
               requests:
                 cpu: 500m
                 memory: 200Mi

For a full example of CPU and memory utilization resources and limits
for the |k8s-op-short| Pod that satisfy parallel deployment of up to
50 MongoDB replica sets, see the :github:`mongodb-enterprise.yaml
</mongodb/mongodb-enterprise-kubernetes/blob/master/mongodb-enterprise.yaml#L219-L235>`
file.

.. seealso::

   - :k8sdocs:`Requests and Limits </concepts/configuration/manage-resources-containers/#requests-and-limits>`
   - :k8sdocs:`Assign CPU Resources to Containers and Pods </tasks/configure-pod-container/assign-cpu-resource/>`


.. _mdb_pods_resources:

Set CPU and Memory Utilization Bounds for MongoDB Pods
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The values for Pods hosting replica sets or sharded clusters map
to the :k8sdocs:`requests field </reference/generated/kubernetes-api/{+k8s-api-version+}/#resourcerequirements-v1-core>`
for CPU and memory for the created Pod. These values are consistent
with :manual:`considerations </administration/production-notes#allocate-sufficient-ram-and-cpu>`
stated for MongoDB hosts.

The |k8s-op-short| uses its allocated memory for processing, for the
WiredTiger cache, and for storing packages during the deployments.

For production deployments, set the CPU and memory resources and limits
for the MongoDB Pod as follows:

- ``spec.podSpec.podTemplate.spec.containers.resources.requests.cpu`` to 0.25
- ``spec.podSpec.podTemplate.spec.containers.resources.limits.cpu`` to 0.25
- ``spec.podSpec.podTemplate.spec.containers.resources.requests.memory`` to 512M
- ``spec.podSpec.podTemplate.spec.containers.resources.limits.memory`` to 512M

If you don't include the unit of measurement for CPUs, |k8s| interprets
it as the number of cores. If you specify ``m``, such as 500m, |k8s|
interprets it as ``millis``. To learn more, see
:k8sdocs:`Meaning of CPU </concepts/configuration/manage-resources-containers/#meaning-of-cpu>`.

The following abbreviated example shows the configuration with
recommended CPU and memory bounds for each Pod hosting a MongoDB
replica set member in your deployment.

.. example::

  .. code-block:: yaml
     :linenos:
     :emphasize-lines: 5, 12-20
     :copyable: false

     apiVersion: mongodb.com/v1
     kind: MongoDB
     metadata:
     name: my-replica-set
     spec:
       members: 3
       version: 4.0.0-ent
       service: my-service
       ...

       persistent: true
       podSpec:
         podTemplate:
           spec:
             containers:
             - name: mongodb-enterprise-database
               resources:
                 limits:
                   cpu: "0.25"
                   memory: 512M

For a full example of CPU and memory utilization resources and limits
for Pods hosting MongoDB replica set members, see the
:github:`replica-set-podspec.yaml </mongodb/mongodb-enterprise-kubernetes/blob/master/samples/mongodb/podspec/replica-set-podspec.yaml#L38-L45>`
file in the the :github:`MongoDB Podspec Samples </mongodb/mongodb-enterprise-kubernetes/tree/master/samples/mongodb/podspec>` directory.

This directory also contains sample CPU and memory limits
configurations for Pods used for:

- A sharded cluster, in the :github:`sharded-cluster-podspec.yaml </mongodb/mongodb-enterprise-kubernetesblob/master/samples/mongodb/podspec/sharded-cluster-podspec.yaml#L62-91>`.
- Standalone MongoDB deployments, in the :github:`standalone-podspec.yaml </mongodb/mongodb-enterprise-kubernetesblob/master/samples/mongodb/podspec/standalone-podspec.yaml#L36-39>`.

.. seealso::

   - :setting:`spec.podSpec.podTemplate.spec`
   - :k8sdocs:`Requests and Limits </concepts/configuration/manage-resources-containers/#requests-and-limits>`
   - :k8sdocs:`Assign CPU Resources to Containers and Pods </tasks/configure-pod-container/assign-cpu-resource/>`

Use Multiple Availability Zones
-------------------------------

Set the |k8s-op-short| and |k8s-statefulsets| to distribute all members
of one replica set to different |k8s-nodes| to ensure high
availability.

The following abbreviated example shows affinity and multiple
availability zones configuration.

.. example::

   .. code-block:: yaml
      :copyable: false
      :linenos:

      apiVersion: mongodb.com/v1
      kind: MongoDB
      metadata:
        name: my-replica-set
      spec:
        members: 3
        version: 4.2.1-ent
        service: my-service
        ...
          podAntiAffinityTopologyKey: nodeId
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
              matchExpressions:
              - key: security
                operator: In
                values:
                - S1
              topologyKey: failure-domain.beta.kubernetes.io/zone

          nodeAffinity:
             requiredDuringSchedulingIgnoredDuringExecution:
               nodeSelectorTerms:
               - matchExpressions:
                 - key: kubernetes.io/e2e-az-name
                 operator: In
                 values:
                 - e2e-az1
                 - e2e-az2

In this example, the |k8s-op-short| schedules the Pods deployment to
the nodes which have the label ``kubernetes.io/e2e-az-name`` in ``e2e-az1`` or
``e2e-az2`` availability zones. Change ``nodeAffinity`` to
schedule the deployment of Pods to the desired availability zones.

See the full example of multiple availability zones configuration in
:github:`replica-set-affinity.yaml </mongodb/mongodb-enterprise-kubernetes/blob/master/samples/mongodb/affinity/replica-set-affinity.yaml>`
in the :github:`Affinity Samples </mongodb/mongodb-enterprise-kubernetes/tree/master/samples/mongodb/persistent-volumes>`
directory.

This directory also contains sample affinity and multiple zones
configurations for sharded clusters and standalone MongoDB deployments.

.. seealso::

   - :k8sdocs:`Running in Multiple Zones </setup/best-practices/multiple-zones/>`
   - :k8sdocs:`Node affinity </concepts/scheduling-eviction/assign-pod-node/#node-affinity>`

Co-locate ``mongos`` Pods with Your Applications
------------------------------------------------

You can run the lightweight ``mongos`` instance on the same |k8s-node|
as your apps using MongoDB. The |k8s-op-short| supports standard |k8s|
:k8sdocs:`node affinity and anti-affinity </concepts/configuration/assign-pod-node/#affinity-and-anti-affinity>`
features. Using these features, you can force install the ``mongos``
on the same Pod as your application.

The following abbreviated example shows affinity and multiple
availability zones configuration.

The ``podAffinity`` key determines whether to install an application
on the same Pod, node, or data center as another application.

To specify Pod affinity:

1. Add a label and value in the ``spec.podSpec.podTemplate.metadata.labels``
   |yaml| collection to tag the deployment. See
   :setting:`spec.podSpec.podTemplate.metadata`,
   and the
   :k8sdocs:`Kubernetes PodSpec v1 core API </reference/generated/kubernetes-api/{+k8s-api-version+}/#podspec-v1-core>`.

#. Specify which label the ``mongos`` uses in the
   ``spec.mongosPodSpec.podAffinity``
   ``.requiredDuringSchedulingIgnoredDuringExecution.labelSelector``
   |yaml| collection. The ``matchExpressions`` collection defines the
   ``label`` that the |k8s-op-short| uses to identify the Pod for hosting
   the ``mongos``.

.. example::

   .. code-block:: yaml
      :copyable: false
      :linenos:

      apiVersion: mongodb.com/v1
      kind: MongoDB
      metadata:
        name: my-replica-set
      spec:
        members: 3
        version: 4.2.1-ent
        service: my-service

        ...
          podAntiAffinityTopologyKey: nodeId
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
              matchExpressions:
              - key: security
                operator: In
                values:
                - S1
              topologyKey: failure-domain.beta.kubernetes.io/zone

          nodeAffinity:
             requiredDuringSchedulingIgnoredDuringExecution:
               nodeSelectorTerms:
               - matchExpressions:
                 - key: kubernetes.io/e2e-az-name
                 operator: In
                 values:
                 - e2e-az1
                 - e2e-az2

See the full example of multiple availability zones and node affinity
configuration in
:github:`replica-set-affinity.yaml </mongodb/mongodb-enterprise-kubernetes/blob/master/samples/mongodb/affinity/replica-set-affinity.yaml>`
in the :github:`Affinity Samples </mongodb/mongodb-enterprise-kubernetes/tree/master/samples/mongodb/persistent-volumes>`
directory.

This directory also contains sample affinity and multiple
zones configurations for sharded clusters and standalone
MongoDB deployments.

.. seealso::

   - :k8sdocs:`Assigning Pods to Nodes </concepts/scheduling-eviction/assign-pod-node/#nodeselector>`
   - :k8sdocs:`Node affinity and anti-affinity </concepts/configuration/assign-pod-node/#affinity-and-anti-affinity>`
   - :k8sdocs:`Kubernetes PodSpec v1 core API </reference/generated/kubernetes-api/{+k8s-api-version+}/#podspec-v1-core>`

Use Labels to Differentiate Between Deployments
-----------------------------------------------

Use the :k8sdocs:`Pod affinity
</concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity>`
|k8s| feature to:

- Separate different MongoDB resources, such as ``test``, ``staging``,
  and ``production`` environments.

- Place |k8s-pods| on some specific nodes to take advantage of
  features such as |ssd| support.

.. code-block:: yaml
   :copyable: false
   :linenos:

   mongosPodSpec:
     podAffinity:
       requiredDuringSchedulingIgnoredDuringExecution:
         - labelSelector:
           matchExpressions:
           - key: security
             operator: In
             values:
             - S1
           topologyKey: failure-domain.beta.kubernetes.io/zone

.. seealso::

  :k8sdocs:`Pod affinity
  </concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity>`

Verify Permissions
------------------

Objects in the |k8s-op-short| configuration  use the following
default permissions.


.. list-table::
   :widths: 25 75
   :header-rows: 1

   * - Kubernetes Resources
     - Verbs

   * - Configmaps
     - Require the following permissions:
  
       - ``get``, ``list``, ``watch``. The |k8s-op-short| reads the organization
         and project data from the specified ``configmap``.
 
       - ``create``, ``update``. The |k8s-op-short| creates and updates ``configmap``
         objects for configuring the :ref:`appdb-om-arch` instances.
  
       - ``delete``. The |k8s-op-short| needs the ``delete`` ``configmap`` permission
         to support its :ref:`older versions <k8s-support-lifecycle>`.
         This permission will be deleted when older versions reach their
         End of Life Date.

   * - Secrets
     - Require the following permissions:
  
       - ``get``, ``list``, ``watch``. The |k8s-op-short| reads secret objects to
         retrieve sensitive data, such as :ref:`TLS <secure-tls>` or
         :ref:`X.509 <create-x509-certs>` access information. For example, it
         reads the credentials from a secret object to connect to the |onprem|.

       - ``create``, ``update``. The |k8s-op-short| creates secret
         objects holding :ref:`TLS <secure-tls>` or
         :ref:`X.509 <create-x509-certs>` access information.
    
       - ``delete``. The |k8s-op-short| deletes secret objects (containing passwords)
         related to the :ref:`appdb-om-arch`.
    
   * - Services
     - Require the following permissions:
   
       - ``get``, ``list``, ``watch``. The |k8s-op-short| reads and watches
         MongoDB services. For example, to communicate with the Ops Manager service,
         the |k8s-op-short| needs ``get``, ``list`` and ``watch``
         permissions to use the |onprem| service's URL.
 
       - ``create``, ``update``. To communicate with services, the |k8s-op-short|
         creates and updates service objects corresponding to |onprem|
         and MongoDB custom resources.
    
   * - StatefulSets
     - Require the following permissions:
  
       - ``get``, ``list``, ``watch``. The |k8s-op-short| reacts to the changes in the
         StatefulSets it creates for the MongoDB custom resources. It also reads
         the fields of  the StatefulSets it manages.

       - ``create``, ``update``. The |k8s-op-short| creates and updates StatefulSets
         corresponding to the mongoDB custom resources.
    
       - ``delete``. The |k8s-op-short| needs permissions to delete the StatefulSets
         when you delete the MongoDB custom resource.

   * - Pods
     - Require the following permissions:
  
       - ``get``, ``list``, ``watch``. The |k8s-op-short| queries the
         Application Database Pods to get information about its state.
  
   * - Namespaces
     - Require the following permissions:
  
       - ``list``, ``watch``. When you run the |k8s-op-short| in the cluster-wide mode,
         it needs ``list`` and ``watch`` permissions to all namespaces
         for the MongoDB custom resources.

.. seealso::

   :ref:`meko-om-arch`

Enable HTTPS
------------

The |k8s-op-short| supports configuring |onprem| to run over 
:ref:`HTTPS <config-https>`.

Enable |https| before deploying your |onprem| resources to avoid a situation 
where the |k8s-op-short| reports your resources' status as ``Failed``.

.. seealso::
   
   - :ref:`https-enablement-issues`

Enable TLS
----------

The |k8s-op-short| supports |tls| encryption.
Use |tls| with your MongoDB deployment to encrypt your data over
the network.

The configuration in the following example enables |tls| for the replica
set. When |tls| is enabled, all traffic between members of the replica
set and clients is encrypted using |tls| certificates.

To learn more about securing your MongoDB deployments using |tls|, see 
:ref:`secure-tls`.

The default |tls| mode is ``requireTLS``. You can customize it using the
:setting:`spec.additionalMongodConfig.net.ssl.mode` configuration
parameter, as shown in the following abbreviated example.

.. code-block:: yaml
   :copyable: false
   :emphasize-lines: 15-18,21-24
   :linenos:
   
   apiVersion: mongodb.com/v1
   kind: MongoDB
   metadata:
   name: my-tls-enabled-rs
   spec:
     type: ReplicaSet
     members: 3
     version: 4.4.0-ent

    opsManager:
      configMapRef:
        name: my-project
    credentials: my-credentials

    security:
      tls:
        enabled: true
        ca: <custom-ca>

    ...
    additionalMongodConfig:
      net:
        ssl:
         mode: "preferSSL"

See the full |tls| configuration example in
:github:`replica-set.yaml </mongodb/mongodb-enterprise-kubernetes/tree/master/samples/mongodb/tls/replica-set>`
in the :github:`TLS </mongodb/mongodb-enterprise-kubernetes/tree/master/samples/mongodb/tls>`
samples directory. This directory also contains sample |tls| configurations for
sharded clusters and standalone deployments.

Enable Authentication
---------------------

The |k8s-op-short| supports :ref:`X.509 <create-x509-certs>`, LDAP,
and :ref:`SCRAM <add-db-user-scram>` user authentication.

.. note::
   For LDAP configuration, see the
   :setting:`spec.security.authentication.ldap.automationLdapGroupDN`
   setting.
   
You must create an additional |k8s-crd| for your
MongoDB users and the {+mdbagent+} instances.
The |k8s-op-short| generates and distributes the certificate.

See the full X.509 certificates configuration examples in the
:github:`x509 Authentication
</mongodb/mongodb-enterprise-kubernetes/tree/master/samples/mongodb/authentication/x509>` directory in
the :github:`Authentication </mongodb/mongodb-enterprise-kubernetes/tree/master/samples/mongodb/authentication>`
samples directory. This directory also contains sample LDAP and SCRAM configurations.

Example Deployment CRD
~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: yaml
   :copyable: false
   :emphasize-lines: 14-17
   :linenos:

   apiVersion: mongodb.com/v1
   kind: MongoDB
   metadata:
     name: my-tls-enabled-rs
   spec:
     type: ReplicaSet
     members: 3
     version: "4.0.4-ent"
     project: my-project
     credentials: my-credentials
     security:
       tls:
         enabled: true
       authentication:
         enabled: true
         modes: ["X509"]
         internalCluster: "X509"

Example User CRD
~~~~~~~~~~~~~~~~

.. code-block:: yaml
   :copyable: false
   :linenos:

   apiVersion: mongodb.com/v1
   kind: MongoDBUser
   metadata:
     name: user-with-roles
   spec:
     username: "CN=mms-user-1,OU=cloud,O=MongoDB,L=New York,ST=New York,C=US"
     db: "$external"
     project: my-project
     roles:
       - db: "admin"
         name: "clusterAdmin"

.. seealso::

   - :setting:`spec.security.authentication.ldap.automationLdapGroupDN`
   - :ref:`Manage Database Users Using X.509 Authentication <create-x509-certs>`
   - :ref:`Manage Database Users Using SCRAM Authentication <add-db-user-scram>`
