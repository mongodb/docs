.. _kafka-sink-data-formats:

=================================
Kafka Sink Connector Data Formats
=================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol


Supported Data Formats
----------------------

The MongoDB Kafka Sink Connector ``converter`` setting specifies the
deserialization method for data it reads from a topic. The converter
can deserialize the following data formats:

.. list-table::
   :header-rows: 1
   :stub-columns: 1
   :widths: 1 4

   * - Format Name
     - Description

   * - AVRO
     - An `open source serialization system <https://avro.apache.org>`_
       that provides a compact binary format and a JSON-like API.
       Integrates with the
       `Confluent Schema Registry <https://www.confluent.io/confluent-schema-registry>`_
       to manage schema definitions.

   * - JSON with Schema
     - JSON record structure with explicit schema information to
       ensure the data matches the expected format.

   * - JSON (plain)
     - JSON record structure without an attached schema.

   * - RAW JSON
     - Serialized as a String. The JSON structure is not managed by
       Kafka Connect.

.. note::

   Even when specifying the `StringConverter
   <https://kafka.apache.org/21/javadoc/index.html?org/apache/kafka/connect/storage/StringConverter.html>`_
   format, the **RAW JSON** mode fields must contain valid JSON to be
   parsed by the connector correctly.

For more information on Kafka data serialization, see `Kafka Connect
Serialization Explained
<https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained#choosing-serialization-format>`_.

Configuration Example for AVRO
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following configuration provides example settings that use the
**AVRO data format** with a Schema Registry:

.. code-block:: properties

   key.converter=io.confluent.connect.avro.AvroConverter
   key.converter.schema.registry.url=http://localhost:8081

   value.converter=io.confluent.connect.avro.AvroConverter
   value.converter.schema.registry.url=http://localhost:8081

For more information on using a Schema Registry, see `Schema Management
<https://docs.confluent.io/current/schema-registry/index.html>`_.

Configuration Example for JSON with Schema
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following configuration provides example settings that use the
**JSON with schema** data format. The Kafka topic data must be in
JSON format and contain top-level objects ``schema`` and ``payload``.

.. code-block:: properties

   key.converter=org.apache.kafka.connect.json.JsonConverter
   key.converter.schemas.enable=true

   value.converter=org.apache.kafka.connect.json.JsonConverter
   value.converter.schemas.enable=true

Configuration Example for JSON without Schema
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following configuration provides example settings that use the **JSON
without schema** data format. The Kafka topic data must be in JSON format.

.. code-block:: properties

   key.converter=org.apache.kafka.connect.json.JsonConverter
   key.converter.schemas.enable=false

   value.converter=org.apache.kafka.connect.json.JsonConverter
   value.converter.schemas.enable=false

.. note:: Choose the appropriate data format

   When you specify **JSON without Schema**, any JSON schema objects such
   as ``schema`` or ``payload`` are read explicitly rather than as a
   validation schema.

Configuration Example for RAW JSON
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following configuration provides example settings that use the
**RAW JSON** data format.

.. code-block:: properties

   key.converter=org.apache.kafka.connect.storage.StringConverter
   key.converter.schemas.enable=false

   value.converter=org.apache.kafka.connect.storage.StringConverter
   value.converter.schemas.enable=false

Supported Sink Record Structure
-------------------------------

Once the converter has deserialized the data from the Kafka topic,
Kafka Connect creates a :kafka-21-javadoc:`SinkRecord </connect/sink/SinkRecord.html>`
object.

The MongoDB Kafka Connector converts the ``SinkRecord`` into
a ``SinkDocument`` which contains the key and value in BSON format. The
converter determines the types using schema, if provided.

The connector supports all the core schema types listed in
:kafka-21-javadoc:`Schema.Type </connect/data/Schema.Type.html>`:

- **Array**
- **Boolean**
- **Bytes**
- **Float32**
- **Float64**
- **Int16**
- **INT32**
- **INT64**
- **INT8**
- **MAP**
- **STRING**
- **STRUCT**

The MongoDB Kafka Connector also supports the following `AVRO logical types
<http://avro.apache.org/docs/1.8.2/spec.html#Logical+Types>`_:

- **Decimal**
- **Date**
- **Time** (millis/micros)
- **Timestamp** (millis/micros)

For a sample AVRO schema that uses logical types, see
:ref:`avro-logical-types-example`.


Nested Field Schema Example
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The converter handles schemas with nested key or value structures.
The following is an example AVRO schema with nested fields:

.. code-block:: json

   {
     "type": "record",
     "name": "Customer",
     "namespace": "com.mongodb.kafka.data.kafka.avro",
     "fields": [
       {
         "name": "name",
         "type": "string"
       },
       {
         "name": "age",
         "type": "int"
       },
       {
         "name": "active",
         "type": "boolean"
       },
       {
         "name": "address",
         "type": {
           "type": "record",
           "name": "AddressRecord",
           "fields": [
             {
               "name": "city",
               "type": "string"
             },
             {
               "name": "country",
               "type": "string"
             }
           ]
         }
       },
       {
         "name": "food",
         "type": {
           "type": "array",
           "items": "string"
         }
       },
       {
         "name": "data",
         "type": {
           "type": "array",
           "items": {
             "type": "record",
             "name": "Whatever",
             "fields": [
               {
                 "name": "k",
                 "type": "string"
               },
               {
                 "name": "v",
                 "type": "int"
               }
             ]
           }
         }
       },
       {
         "name": "lut",
         "type": {
           "type": "map",
           "values": "double"
         }
       },
       {
         "name": "raw",
         "type": "bytes"
       }
     ]
   }

Dot Notation for Nested Fields
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the above schema example, the ``address`` field is a document with two
sub-fields, ``city`` and ``country``. It's also possible to specify those
fields with dot notation:

.. code-block:: json

   {
     "name": "address.city",
     "type": "string"
   },
   { 
     "name": "address.country",
     "type": string"
   }

Dot notation allows you to specify nested fields without using separate
lines for the top-level document and its sub-documents.

.. _avro-logical-types-example:

AVRO Logical Type Example
~~~~~~~~~~~~~~~~~~~~~~~~~

The following AVRO schema demonstrates use of each supported logical types
(``Decimal``, ``Date``, ``Time``, and ``Timestamp``):

.. code-block:: json

   {
     "type": "record",
     "name": "MyLogicalTypesRecord",
     "namespace": "com.mongodb.kafka.data.kafka.avro",
       "fields": [
         {
           "name": "myDecimalField",
           "type": {
             "type": "bytes",
             "logicalType": "decimal",
             "connect.parameters": {
               "scale": "2"
             }
           }
         },
         {
           "name": "myDateField",
           "type": {
             "type": "int",
             "logicalType": "date"
           }
         },
         {
           "name": "myTimeMillisField",
           "type": {
             "type": "int",
             "logicalType": "time-millis"
           }
         },
         {
           "name": "myTimeMicrosField",
           "type": {
             "type": "long",
             "logicalType": "time-micros"
           }
         },
         {
           "name": "myTimestampMillisField",
           "type": {
             "type": "long",
             "logicalType": "timestamp-millis"
           }
         },
         {
           "name": "myTimestampMicrosField",
           "type": {
             "type": "long",
             "logicalType": "timestamp-micros"
           }
         }
       ]
     }

.. note::

   If you compile your schema using `AVRO code generation
   <https://avro.apache.org/docs/1.8.2/gettingstartedjava.html#Serializing+and+deserializing+with+code+generation>`_
   for your Kafka producer application, your logical types are mapped to the
   following non-standard Java classes:

   .. list-table::
      :header-rows: 1
      :stub-columns: 1
      :widths: 1 2

      * - Schema Type
        - Java Type

      * - date
        - ``org.joda.time.LocalDate``

      * - time-millis
        - ``org.joda.time.LocalTime``

      * - time-micros
        - ``long``

      * - datetime-millis
        - ``org.joda.time.DateTime``

      * - datetime-micros
        - ``long``
