.. _local-rag:

=========================================================
Build a Local RAG Implementation with {+avs+}
=========================================================

.. default-domain:: mongodb

.. facet::
   :name: genre
   :values: tutorial

.. facet::
   :name: programming_language
   :values: python

.. meta::
   :description: How to implement retrieval-augmented generation (RAG) for Atlas Vector Search using local embedding models and chat models.

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

This tutorial demonstrates how to implement
retrieval-augmented generation (RAG)
locally, without the need for |api| keys or credits. 
To learn more about |rag|, see :ref:`ai-key-concepts`. 

Specifically, you perform the following actions:

#. Create a local |service| {+deployment+}.
#. Set up the environment.
#. Use a local embedding model to generate vector embeddings.
#. Create an {+avs+} index on your data.
#. Deploy a local |llm| to answer questions on your data.

Background
----------

To complete this tutorial, you can either create a local |service| 
{+deployment+} by using the :ref:`{+atlas-cli+} 
<atlas-programmatic-access-cli-overview>` or :ref:`deploy a 
{+cluster+} <create-new-cluster>` on the cloud. The {+atlas-cli+} 
is the command-line interface for |service-fullname|, and you can use 
the {+atlas-cli+} to interact with |service| from the terminal for 
various tasks, including creating local |service| {+deployments+}. 
To learn more, see 
:atlascli:`Manage Local and Cloud Deployments from the Atlas CLI </atlas-cli-local-cloud>`.

.. note::
   
   Local |service| {+deployments+} are intended for testing only. 
   For production environments, :ref:`deploy a {+cluster+} <create-new-cluster>`.

You also use the following open-source models in this tutorial:

- `bge-large-en-v1.5 <https://huggingface.co/BAAI/bge-large-en-v1.5>`__ embedding model 
- `Mistral 7B <https://docs.mistral.ai/getting-started/models/>`__ generative model

There are several ways to download and deploy |llm|\s locally.
In this tutorial, you download the Mistral 7B model
by using `GPT4All <https://gpt4all.io/index.html>`__,
an open-source ecosystem for local |llm| development. 

This tutorial also uses `LangChain <https://langchain.com/>`__, a popular 
open-source |llm| framework, to connect to these models and 
integrate them with {+avs+}. If you prefer different models or a 
different framework, you can adapt this tutorial by replacing 
the model names and LangChain-specific components 
with their equivalents for your preferred setup.

To learn more about how to leverage LangChain in your |rag| applications,
see :ref:`langchain`. To learn more about other frameworks you can
use with {+avs+}, see :ref:`ai-integrations`.

Prerequisites
-------------

To complete this tutorial, you must have the following:

- The :atlascli:`{+atlas-cli+} </>` installed and running v1.14.3 or later.

- An interactive Python notebook that you can run locally. 
  You can run interactive Python notebooks in `VS Code 
  <https://code.visualstudio.com/docs/datascience/jupyter-notebooks>`__.
  Ensure that your environment runs Python v3.10 or later.

  .. note::

     If you use a hosted service such as `Colab 
     <https://colab.research.google.com>`__, ensure that 
     you have enough RAM to run this tutorial. Otherwise,
     you might experience performance issues.

Create a Local |service| Deployment
-----------------------------------

In this section, you create a local |service| {+deployment+} to
use as a vector database. If you have an |service| {+cluster+} 
running MongoDB version 6.0.11, 7.0.2, or later with the 
:ref:`sample data <sample-data>` loaded, you can skip this step.

To create the local {+deployment+}:

.. include:: /includes/steps-create-local-deployment-atlas-cli.rst

Set Up the Environment
----------------------

In this section, you set up the environment for this tutorial.
Create an interactive Python notebook by saving a file 
with the ``.ipynb`` extension, and then run the following code snippets 
in the notebook.

.. include:: /includes/ai-integrations/langchain/local-rag-set-up-environment.rst

Generate Embeddings with a Local Model
--------------------------------------

In this section, you load an embedding model locally and 
create vector embeddings using data from the 
:ref:`sample_airbnb <sample-airbnb>` database, 
which contains a single collection called ``listingsAndReviews``. 
The following code snippet performs the following 
actions:

- Establishes a connection to your local |service| {+deployment+} or 
  your |service| {+cluster+} and the ``sample_airbnb.listingsAndReviews`` collection. 
- Loads the ``bge-large-en-v1.5`` embedding model from LangChain's 
  ``HuggingFaceEmbeddings`` library. 
- Creates a filter to include only documents that have a ``summary`` field
  and don't have an ``embeddings`` field.
- For each document in the collection that satisfies the filter:

  - Generates an embedding from the document's ``summary`` field
    by using the ``bge-large-en-v1.5`` embedding model.
  - Updates the document by creating a new field called 
    ``embeddings`` that contains the embedding.

Run the following code in your notebook:

.. literalinclude:: /includes/ai-integrations/langchain/local-rag-create-embeddings.py
   :language: python

.. tabs::
   :hidden: true

   .. tab:: Local {+Deployment+}
      :tabid: local

      This code takes some time to run. After it's finished, you can
      connect to your local {+deployment+} 
      from {+mongosh+} or your application by using your {+deployment+}\'s 
      connection string. Then, to view your vector embeddings,
      run :manual:`read operations 
      </crud/#read-operations>` on the 
      ``sample_airbnb.listingsAndReviews`` collection. 
      
   .. tab:: Cloud {+Deployment+}
      :tabid: cloud

      This code takes some time to run. After it's finished, you can
      view your vector embeddings :ref:`in the {+atlas-ui+} <atlas-ui-view-collections>`
      by navigating to the ``sample_airbnb.listingsAndReviews`` collection in your 
      {+cluster+} and expanding the fields in a document.

Create the {+avs+} Index
------------------------------------

To enable vector search on the ``sample_airbnb.listingsAndReviews`` 
collection, create an {+avs+} index.

.. tabs::

   .. tab:: Local {+Deployment+}
      :tabid: local

      If you're using a local |service| {+deployment+}, complete the following steps:

      .. include:: /includes/ai-integrations/langchain/local-rag-create-index-cli.rst

   .. tab:: Cloud {+Deployment+}
      :tabid: cloud
      
      .. note:: 

         To create an {+avs+} index, you must have :authrole:`Project Data Access 
         Admin` or higher access to the |service| project.

      If you're using an |service| {+cluster+}, complete the following steps:

      .. include:: /includes/ai-integrations/langchain/local-rag-create-index-ui.rst

Answer Questions with a Local LLM
---------------------------------

This section demonstrates a sample |rag| implementation 
that you can run locally by using {+avs+}, LangChain, and GPT4All.
In your interactive Python notebook, run the following code snippets:

.. include:: /includes/ai-integrations/langchain/local-rag-perform-qa.rst
