ref: list-command-line-specification
content: |
   - the ``--packages`` option to download the MongoDB Spark Connector
     package.  The following package is available:

     - ``mongo-spark-connector_{+scala-version+}`` for use with Scala 2.12.x

   - the ``--conf`` option to configure the MongoDB Spark Connnector.
     These settings configure the ``SparkConf`` object.

     .. note:: 

        When specifying the Connector configuration via ``SparkConf``, you
        must prefix the settings appropriately. For details and other
        available MongoDB Spark Connector options, see the
        :doc:`/configuration`.
---
ref: list-configuration-explanation
content: |
   - The :ref:`spark.mongodb.read.uri <spark-input-conf>` specifies the
     MongoDB server address (``127.0.0.1``), the database to connect
     (``test``), and the collection (``myCollection``) from which to read
     data, and the read preference.
   - The :ref:`spark.mongodb.write.uri <spark-output-conf>` specifies the
     MongoDB server address (``127.0.0.1``), the database to connect
     (``test``), and the collection (``myCollection``) to which to write
     data. Connects to port ``27017`` by default.
   - The ``packages`` option specifies the Spark Connector's
     Maven coordinates, in the format ``groupId:artifactId:version``.
---
ref: command-line-start-spark-shell
content: |

   .. include:: /includes/extracts/list-command-line-specification.rst

   For example,

   .. code-block:: sh

      ./bin/spark-shell --conf "spark.mongodb.read.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred" \
                        --conf "spark.mongodb.write.uri=mongodb://127.0.0.1/test.myCollection" \
                        --packages org.mongodb.spark:mongo-spark-connector_{+scala-version+}:{+current-version+}
 
   .. include:: /includes/extracts/list-configuration-explanation.rst

---
ref: command-line-start-pyspark
content: |

   .. include:: /includes/extracts/list-command-line-specification.rst

   The following example starts the ``pyspark`` shell from the command
   line:

   .. code-block:: sh

      ./bin/pyspark --conf "spark.mongodb.read.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred" \
                    --conf "spark.mongodb.write.uri=mongodb://127.0.0.1/test.myCollection" \
                    --packages org.mongodb.spark:mongo-spark-connector_{+scala-version+}:{+current-version+}
 
   .. include:: /includes/extracts/list-configuration-explanation.rst

   The examples in this tutorial will use this database and collection.
---
ref: command-line-start-sparkR
content: |

   .. include:: /includes/extracts/list-command-line-specification.rst

   For example,

   .. code-block:: sh

      ./bin/sparkR  --conf "spark.mongodb.read.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred" \
                    --conf "spark.mongodb.write.uri=mongodb://127.0.0.1/test.myCollection" \
                    --packages org.mongodb.spark:mongo-spark-connector_{+scala-version+}:{+current-version+}

   .. include:: /includes/extracts/list-configuration-explanation.rst
...
