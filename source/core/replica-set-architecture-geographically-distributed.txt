.. _replica-set-geographical-distribution:

=======================================
Geographically Distributed Replica Sets
=======================================

.. default-domain:: mongodb

Adding members to a replica set in multiple data centers adds
redundancy and provides fault tolerance if one data center is
unavailable. Members in additional data centers should have a
:doc:`priority of 0 </core/replica-set-priority-0-member>` to prevent
them from becoming primary.

For example: the architecture of a geographically distributed replica
set may be:

- One :term:`primary <primary>` in the main data center.

- One :term:`secondary <secondary>` member in the main data center. This
  member can become primary at any time.

- One :doc:`priority 0 </core/replica-set-priority-0-member>` member in
  a second data center. This member cannot become primary.

In the following replica set, the primary and one secondary are in *Data
Center 1*, while *Data Center 2* has a :doc:`priority 0
</core/replica-set-priority-0-member>` secondary that cannot become a
primary.

.. include:: /images/replica-set-three-members-geographically-distributed.rst

If the primary is unavailable, the replica set will elect a new
primary from *Data Center 1*. If the data centers cannot connect to
each other, the member in *Data Center 2* will not become the primary.

If *Data Center 1* becomes unavailable, you can manually recover the
data set from *Data Center 2* with minimal downtime. With sufficient
:ref:`write concern <write-concern>`, there will be no data loss.

To facilitate elections, the main data center should hold a majority
of members. Also ensure that the set has an odd number of members. If
adding a member in another data center results in a set with an even
number of members, deploy an :ref:`arbiter
<replica-set-arbiters>`. For more information on elections, see
:doc:`/core/replica-set-elections`.

.. seealso:: :doc:`/tutorial/deploy-geographically-distributed-replica-set`.
