.. _avs-quantization:

===================
Vector Quantization 
===================

.. default-domain:: mongodb

.. facet::
   :name: programming_language
   :values: python, javascript/typescript

.. meta::
   :description: Use the Atlas Vector Search to ingest quantized vectors or automatically quantize vectors.
   :keywords: atlas ui, node.js, code example, atlas api, atlas cli, java sync, go, python, mongodb shell, sample dataset

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

.. note:: 

   .. include:: /includes/fact-avs-int1-ingestion-preview.rst

{+avs+} supports automatic quantization of your 32-bit float vector
embeddings. It also supports ingesting and indexing your 
pre-quantized scalar and binary vectors from certain embedding models.

.. _avs-quantization-about:

About Quantization 
------------------

Quantization is the process of shrinking full-fidelity vectors into
fewer bits. It reduces the amount of main memory required to store each
vector in an {+avs+} index by indexing the reduced representation
vectors instead. This allows for storage of more vectors or vectors with
higher dimensions. Therefore, quantization reduces resource 
consumption and improves speed. We recommend quantization for
applications with a large number of vectors, such as over 10M.  

Scalar Quantization 
~~~~~~~~~~~~~~~~~~~

:term:`Scalar quantization <scalar quantization>` involves first
identifying the minimum and maximum values for each dimension of the
indexed vectors to establish a range of values for a dimension. Then,
the range is divided into equally sized intervals or bins. Finally, each
float value is mapped to a bin to convert the continuous float values
into discrete integers. In {+avs+}, this quantization reduces the vector
embedding's RAM cost to one fourth (``1/4``) of the pre-quantization cost. 

Binary Quantization 
~~~~~~~~~~~~~~~~~~~

Binary quantization involves assuming a
midpoint of ``0`` for each dimension, which is typically appropriate for
embeddings normalized to length ``1`` such as  OpenAI's 
``text-embedding-3-large``. Then, each value in the vector is
compared to the midpoint and assigned a binary value of ``1`` if it's
greater than the midpoint and a binary value of ``0`` if it's less than
or equal to the midpoint. In {+avs+}, this quantization reduces the
vector embedding's RAM cost to one twenty-fourth (``1/24``) of the
pre-quantization cost. The reason it's not ``1/32`` is because the data
structure containing the |hnsw| graph itself, separate from the vector
values, isn't compressed.

When you run a query, {+avs+} converts the float value in the query
vector into a binary vector using the same midpoint for efficient
comparison between the query vector and indexed binary vectors. It then
rescores by reevaluating the identified candidates in the binary
comparison using the original float values associated with those results
from the binary index to further refine the results. The full fidelity
vectors are stored in their own data structure on disk, and are only 
referenced during rescoring when you configure binary quantization or
when you perform exact search against either binary or scalar quantized
vectors.  

.. seealso:: 

   - `What is vector quantization? <https://www.mongodb.com/developer/products/atlas/ingesting_quantized_vectors_with_cohere/#what-is-vector-quantization->`__

Requirements
------------

The following table shows the requirements for automatically quantizing
and ingesting quantized vectors: 

.. list-table:: 
   :header-rows: 1 

   * - Requirement
     - For ``int1`` Ingestion 
     - For ``int8`` Ingestion 
     - For Automatic Scalar Quantization 
     - For Automatic Binary Quantization  

   * - Requires index definition settings 
     - No 
     - No 
     - Yes 
     - Yes 

   * - Requires |bson| ``binData`` format
     - Yes
     - Yes
     - No
     - No

   * - Storage on mongod
     - ``binData(int1)``
     - ``binData(int8)``
     - | ``binData(float32)``
       | ``array(float32)``

     - | ``binData(float32)``
       | ``array(float32)``

   * - Supported Similarity method
     - ``euclidean``
     
     - | ``cosine``
       | ``euclidean``
       | ``dotProduct``
     
     - | ``cosine``
       | ``euclidean``
       | ``dotProduct``
     
     - | ``cosine``
       | ``euclidean``
       | ``dotProduct``

   * - Supported Number of Dimensions
     - Multiple of 8
     - 1 to 4096
     - 1 to 4096
     - Multiple of 8

   * - Supports |enn| Search
     - |enn| on ``int1``
     - |enn| on ``int8``
     - |enn| on ``float32``
     - |enn| on ``float32``

.. _avs-automatic-quantization:

How to Enable Automatic Quantization of Vectors
-----------------------------------------------

You can configure {+avs+} to automatically quantize 32-bit float vector
embeddings in your collection to reduced representation types, such as
``int8`` (scalar) and ``binary`` in your vector indexes.

To set or change the quantization type, specify a ``quantization`` field
value of either ``scalar`` or ``binary`` in your index definition. This
triggers an index rebuild similar to any other index definition change.
The specified quantization type applies to all indexed vectors and
query vectors at query-time.

For most embedding models, we recommend binary quantization with
rescoring. If you want to use lower dimension models that are not
:abbr:`QAT (Quantization-Aware-Trained)`, use scalar quantization 
because it has less representational loss and therefore, incurs less 
representational capacity loss. 

Benefits 
~~~~~~~~

{+avs+} provides native capabilities for scalar quantization as well as
binary quantization with rescoring. Automatic quantization increases 
scalability and cost savings for your applications by reducing the
storage and computational resources for efficient processing of your
vectors. Automatic quantization reduces the RAM for ``mongot`` by 3.75x
for scalar and by 24x for binary; the vector values shrink by 4x and 32x
respectively, but |hnsw| graph itself does not shrink. This improves
performance, even at the highest volume and scale. 

Use Cases 
~~~~~~~~~

We recommend automatic quantization if you have large number of full
fidelity vectors, typically over 10M vectors. After quantization, you
index reduced representation vectors without compromising the accuracy
when retrieving vectors. 

Procedure 
~~~~~~~~~

To enable automatic quantization:

.. procedure::
   :style: normal

   .. step:: Specify the type of quantization you want in your {+avs+} index.
    
      In a new or existing {+avs+} index, specify one of the following 
      quantization types in the ``fields.quantization`` field
      for your :ref:`index definition <avs-index-definition>`:

      - ``scalar``: to produce byte vectors from 32-bit input vectors.
      - ``binary``: to produce bit vectors from 32-bit input vectors.

      If you specify automatic quantization on data that is not an array of
      32-bit ``float`` values, {+avs+} silently ignores that
      vector instead of indexing it, and those vectors will be skipped. 

   .. step:: Create or update the index.
    
      .. include:: /includes/fact-index-build-initial-sync.rst

      The specified quantization type applies to all indexed
      vectors and query vectors at query-time.


.. _avs-bindata-vector-subtype:

How to Ingest Pre-Quantized Vectors
-----------------------------------

{+avs+} also supports ingestion and indexing of scalar and
binary quantized vectors from certain embedding models. If you don't already
have quantized vectors, you can convert your embeddings to |bson|
:manual:`BinData </reference/method/BinData/>` ``vector`` subtype
``float32``, ``int1``, or ``int8`` vectors. 

.. note:: 

   .. include:: /includes/fact-avs-int1-ingestion-preview.rst

Use Cases 
~~~~~~~~~

We recommend the |bson| ``binData`` ``vector`` subtype for the following
use cases:  

- You need to index quantized vector output from embedding models.
- You have a large number of float vectors but want to reduce the
  storage and WiredTiger footprint (such as disk and memory usage) in
  ``mongod``.

Benefits 
~~~~~~~~

The :manual:`BinData </reference/method/BinData/>` ``vector`` format
requires about three times less disk space in your {+cluster+} compared
to arrays of elements. It allows you to index your vectors with
alternate types such as ``int1`` or ``int8`` vectors, reducing the
memory needed to build the {+avs+} index for your collection. It reduces
the RAM for ``mongot`` by 3.75x for scalar and by 24x for binary; the
vector values shrink by 4x and 32x respectively, but the |hnsw| graph
itself doesn't shrink. 

If you don't already have ``binData`` vectors, you can convert your
embeddings to this format by using any supported driver before writing
your data to a collection. This page walks you through the steps for
converting your embeddings to the :manual:`BinData
</reference/method/BinData/>` ``vector`` subtype.

Supported Drivers  
~~~~~~~~~~~~~~~~~

|bson| :manual:`BinData </reference/method/BinData/>` ``vector`` subtype
``float32``, ``int1``, and ``int8`` vector conversion is supported by
the following drivers:

- :driver:`PyMongo Driver </pymongo/>` v4.10 or later
- :driver:`Node.js Driver </node/current/>` v6.11 or later

----------

.. |arrow| unicode:: U+27A4

|arrow| Use the **Select your language** drop-down menu to set the 
language of the procedure on this page.

----------

.. tabs-selector:: drivers
   :default-tabid: python

Prerequisites
~~~~~~~~~~~~~

To convert your embeddings to |bson| :manual:`BinData
</reference/method/BinData/>` ``vector`` subtype, you need the
following: 

- An |service| {+cluster+} running MongoDB version 6.0.11, 7.0.2, or
  later. 
        
  Ensure that your :abbr:`IP address (Internet Protocol address)` is
  included in your |service| project's :ref:`access list <access-list>`. 

- Access to an embedding model that supports byte vector output. 

  The outputs from the following embedding models
  can be used to generate |bson| ``binData`` vectors with
  a supported MongoDB driver:

  .. list-table:: 
     :widths: 50 50
     :header-rows: 1

     * - Embedding Model Provider 
       - Embedding Model
     * - `Cohere <https://cohere.com/>`__ 
       - ``embed-english-v3.0``
     * - `Nomic <https://www.nomic.ai/>`__ 
       - ``nomic-embed-text-v1.5``
     * - `Jina <https://jina.ai/>`__ 
       - ``jina-embeddings-v2-base-en``
     * - `Mixedbread <https://www.mixedbread.ai/>`__ 
       - ``mxbai-embed-large-v1``

  Scalar quantization preserves recall for these
  models because these models are all trained to be quantization aware.
  Therefore, :term:`recall` degradation for scalar quantized embeddings
  produced by these models is minimal even at lower dimensions like 384. 

.. tabs-drivers::

   .. tab::
      :tabid: nodejs
  
      - A terminal and code editor to run your Node.js project.
      - `npm and Node.js <https://docs.npmjs.com/downloading-and-installing-node-js-and-npm>`__
        installed.

   .. tab::
      :tabid: python

      - An environment to run interactive Python notebooks
        such as `VS Code <https://code.visualstudio.com/docs/datascience/jupyter-notebooks>`__
        or `Colab <https://colab.research.google.com>`__.

Procedure
~~~~~~~~~

The examples in this procedure use either new data or existing data and
embeddings generated by using `Cohere's <https://cohere.com/>`__
``embed-english-v3.0`` model. The example for new data uses sample text
strings, which you can replace with your own data. The example for
existing data uses a subset of documents without any embeddings from the
``listingsAndReviews`` collection in the ``sample_airbnb`` database,
which you can replace with your own database and collection (with or
without any embeddings). 

Select the tab based on whether you want to
create ``binData`` vectors for new data or for data you already have in
your |service| {+cluster+}.  

.. tabs-drivers:: 

   .. tab::
      :tabid: nodejs

      .. tabs::

         .. tab:: New Data 
            :tabid: new
         
            .. include:: /includes/avs-examples/bson-bindata-vectors/steps-avs-create-bson-vectors-new-data-nodejs.rst 

         .. tab:: Existing Data 
            :tabid: existing

            .. include:: /includes/avs-examples/bson-bindata-vectors/steps-avs-create-bson-vectors-existing-data-nodejs.rst 
   
   .. tab::
      :tabid: python

      Create an interactive Python notebook by saving a file with
      the ``.ipynb`` extension, and then perform the following
      steps in the notebook. To try the example, replace the
      placeholders with valid values. 

      .. tabs::

         .. tab:: New Data 
            :tabid: new 

            .. cta-banner::
               :url: https://github.com/mongodb/docs-notebooks/blob/main/quantization/new-data.ipynb?tck=docs
               :icon: Code

               Work with a runnable version of this tutorial as a :github:`Python notebook <mongodb/docs-notebooks/blob/main/quantization/new-data.ipynb?tck=docs>`.

            .. include:: /includes/avs-examples/bson-bindata-vectors/steps-avs-create-bson-vectors-new-data-python.rst 

         .. tab:: Existing Data 
            :tabid: existing 

            .. cta-banner::
               :url: https://github.com/mongodb/docs-notebooks/blob/main/quantization/existing-data.ipynb?tck=docs
               :icon: Code

               Work with a runnable version of this tutorial as a :github:`Python notebook <mongodb/docs-notebooks/blob/main/quantization/existing-data.ipynb?tck=docs>`.

            .. include:: /includes/avs-examples/bson-bindata-vectors/steps-avs-create-bson-vectors-existing-data-python.rst

      For an advanced demonstration of this procedure on sample data using
      Cohere's ``embed-english-v3.0`` embedding model, see this
      :github:`notebook </mongodb-developer/GenAI-Showcase/blob/main/notebooks/techniques/quantized_vector_ingestion_with_cohere_and_mongodb.ipynb>`.

Evaluate Your Query Results 
---------------------------

You can measure the accuracy of your {+avs+} query by evaluating how
closely the results for an |ann| search match the results of an |enn|
search against your quantized vectors. That is, you can compare the
results of |ann| search with the results of |enn| search for the same
query criteria and measure how frequently the |ann| search results
include the nearest neighbors in the results from the |enn| search.

For a demonstration of evaluating your query results, see
:ref:`avs-improve-results`.
