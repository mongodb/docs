================================
Replica Set Fundamental Concepts
================================

.. default-domain:: mongodb

A MongoDB :term:`replica set` is a cluster of :program:`mongod` instances that
replicate amongst one another and ensure automated failover. Most replica
sets consist of two or more :program:`mongod` instances with at most one
of these designated as the primary and the rest as secondary
members. Clients direct all writes to the primary, while
the secondary members replicate from the primary asynchronously.

Database replication with MongoDB adds redundancy, helps to
ensure high availability, simplifies certain administrative tasks
such as backups, and may increase read capacity. Most production
deployments use replication.

If you're familiar with other database systems, you may think about
replica sets as a more sophisticated form of traditional master-slave
replication. [#master-slave]_ In master-slave replication, a
:term:`master` node accepts writes while one or more :term:`slave`
nodes replicate those write operations and thus maintain data sets
identical to the master. For MongoDB deployments, the member that accepts write
operations is the **primary**, and the replicating members are **secondaries**.

MongoDB's replica sets provide automated failover. If a
:term:`primary` fails, the remaining members will automatically
try to elect a new primary.

A replica set can have up to 12 members, but only 7 members can have
votes. For information regarding non-voting members, see
:ref:`non-voting members <replica-set-non-voting-members>`

.. seealso:: The :doc:`/replication` index for a list of the documents in
   this manual that describe the operation and use of replica sets.

.. [#master-slave] MongoDB also provides conventional master/slave
   replication. Master/slave replication operates by way of the same
   mechanism as replica sets, but lacks the automatic failover
   capabilities. While replica sets are the recommended solution for
   production, a replica set can support only 12 members in total.
   If your deployment requires more than 11 :term:`slave` members, you'll
   need to use master/slave replication.

.. index:: configuration; replica set members
.. _replica-set-member-configuration:

Member Configurations
---------------------

You can configure replica set members in a variety of ways, as listed
here. In most cases, members of a replica set have the default
proprieties.

.. index:: replica set members; secondary only
.. _replica-set-secondary-only-configuration:
.. _replica-set-secondary-only-members:

Secondary-Only Members
~~~~~~~~~~~~~~~~~~~~~~

These members have data but cannot become primary under any
circumstance. To configure a member to be secondary-only, see
:doc:`/tutorial/configure-secondary-only-replica-set-member`.

.. index:: replica set members; hidden
.. _replica-set-hidden-configuration:
.. _replica-set-hidden-members:

Hidden Members
~~~~~~~~~~~~~~

These members cannot become primary and are invisible to client
applications. *However,* hidden members **do** vote in :ref:`elections
<replica-set-elections>`.

Hidden members are ideal for instances that will have significantly
different usage patterns than the other members and require separation
from normal traffic. Typically, hidden members provide reporting,
dedicated backups, and dedicated read-only testing and integration
support.
  
To configure a member to be a hidden member, see
:doc:`/tutorial/configure-a-hidden-replica-set-member`.

.. index:: replica set members; delayed
.. _replica-set-delayed-configuration:
.. _replica-set-delayed-members:

Delayed Members
~~~~~~~~~~~~~~~

Delayed members copy and apply operations from the primary's :term:`oplog` with
a specified delay. If a member has a delay of one hour, then
the latest entry in this member's oplog will not be more recent than
one hour old, and the state of data for the member will reflect the state of the
set an hour earlier.

.. example:: If the current time is 09:52 and the secondary is a
   delayed by an hour, no operation will be more recent than 08:52.

Delayed members may help recover from various kinds of human error. Such
errors may include inadvertently deleted databases or botched
application upgrades. Consider the following factors when determining
the amount of slave delay to apply:

- Ensure that the length of the delay is equal to or greater than your
  maintenance windows.

- The size of the oplog is sufficient to capture *more than* the
  number of operations that typically occur in that period of
  time. For more information on oplog size, see
  :ref:`replica-set-oplog-sizing`.

Delayed members must have a :term:`priority` set to ``0`` to prevent
them from becoming primary in their replica sets. Also these members
should be :ref:`hidden <replica-set-hidden-members>` to prevent your
application from seeing or querying this member.

To configure a member to be a delayed member, see
:doc:`/tutorial/configure-a-delayed-replica-set-member`.

.. index:: replica set members; arbiters
.. _replica-set-arbiter-configuration:
.. _replica-set-arbiters:

Arbiters
~~~~~~~~

These members have no data and exist solely to participate in
:ref:`elections <replica-set-elections>`. Arbiters have the following
interactions with the rest of the replica set:

- Credential exchanges that authenticate the arbiter with the replica
  set. All MongoDB processes within a replica set use keyfiles. These
  exchanges are encrypted.

  MongoDB only transmits the authentication credentials in a
  cryptographically secure exchange, and encrypts no other exchange.

- Exchanges of replica set configuration data and of votes. These are
  not encrypted.

  If your MongoDB deployment uses SSL, then all communications between
  arbiters and the other members of the replica set are secure. See the
  documentation :doc:`/tutorial/configure-ssl` for more
  information. As with all MongoDB components, run arbiters on secure
  networks.
  
To add an arbiter to the replica set, see
:doc:`/tutorial/add-replica-set-arbiter`.

.. index:: replica set members; non-voting
.. _replica-set-non-voting-configuration:
.. _replica-set-non-voting-members:

Non-Voting Members
~~~~~~~~~~~~~~~~~~

These members do not vote in elections. Non-voting members are only
used for larger sets with more than 12 members. To configure a member
as non-voting, see
:doc:`/tutorial/configure-a-non-voting-replica-set-member`.

.. index:: pair: replica set; failover
.. _replica-set-failover-administration:
.. _replica-set-failover:
.. _failover:

Failover and Recovery
---------------------

Replica sets feature automated failover. If the :term:`primary`
goes offline or becomes unresponsive and a majority of the original
set members can still connect to each other, the set will elect a new
primary.

While :term:`failover` is automatic, :term:`replica set`
administrators should still understand exactly how this process
works. This section below describe failover in detail.

In most cases, failover occurs without administrator intervention
seconds after the :term:`primary` either steps down, becomes inaccessible,
or becomes otherwise ineligible to act as primary. If your MongoDB deployment
does not failover according to expectations, consider the following
operational errors:

- No remaining member is able to form a majority. This can happen as a
  result of network partitions that render some members
  inaccessible. Design your deployment to ensure that a majority of
  set members can elect a primary in the same facility as core
  application systems.

- No member is eligible to become primary. Members must have a
  :data:`~local.system.replset.settings.gmembers[n].priority` setting
  greater than ``0``, have a state that is less than ten seconds behind
  the last operation to the :term:`replica set`, and generally be *more*
  up to date than the voting members.

In many senses, :ref:`rollbacks <replica-set-rollbacks>` represent a
graceful recovery from an impossible failover and recovery situation.

Rollbacks occur when a primary accepts writes that other members of
the set do not successfully replicate before the primary steps
down. When the former primary begins replicating again it performs a
"rollback."  *If* the operations replicate to another member *and*
that member remains available and accessible to a majority of the
replica set, there will be no rollback.

Rollbacks remove those operations from the instance that were never
replicated to the set so that the data set is in a consistent state.
The :program:`mongod` program writes rolled back data to a :term:`BSON`
file that you can view using :program:`bsondump`, applied manually
using :program:`mongorestore`.

You can prevent rollbacks using a :ref:`replica acknowledged
<write-concern-replica-acknowledged>` write concern. These write
operations require not only the :term:`primary` to acknowledge the
write operation, sometimes even the majority of the set to confirm the
write operation before returning.

.. include:: /includes/seealso-elections.rst

.. index:: replica set; elections
.. index:: failover; elections
.. _replica-set-elections:

Elections
~~~~~~~~~

When any failover occurs, an election takes place to decide which
member should become primary.

Elections provide a mechanism for the members of a :term:`replica set`
to autonomously select a new :term:`primary` without
administrator intervention. The election allows replica sets to
recover from failover situations very quickly and robustly.

Whenever the primary becomes unreachable, the secondary members
trigger an election. The first member to
receive votes from a majority of the set will become primary. The most
important feature of replica set elections is that a majority of the
original number of members in the replica set must be present for
election to succeed. If you have a three-member replica set, the set can
elect a primary when two or three members can connect to each other. If
two members in the replica go offline, then the remaining member will
remain a secondary.

.. note::

   - When the current :term:`primary` steps down and triggers an
     election, the :program:`mongod` instances will close all client
     connections. This ensures that the clients maintain an accurate
     view of the :term:`replica set` and helps prevent :term:`rollbacks
     <rollback>`.

   - Members on either side of a network partition cannot see each
     other when determining whether a majority is available to hold an
     election.

     That means that if a primary steps down and neither side of the
     partition has a majority on its own, the set will not elect a new
     primary and the set will become read only. To avoid this
     situation, attempt to place a majority of instances in one data
     center with a minority of instances in a secondary facility.

For more information on elections and failover, see the
:ref:`replica-set-failover-administration` section in the
:doc:`/tutorial/troubleshoot-replica-sets` document.

.. index:: replica set; network partitions
.. index:: replica set; elections

.. index:: replica set; priority
.. _replica-set-node-priority:
.. _replica-set-member-priority:

Member Priority
~~~~~~~~~~~~~~~

In a replica set, every member has a "priority," that helps determine
eligibility for :ref:`election <replica-set-elections>` to
:term:`primary`. By default, all members have a priority of ``1``,
unless you modify the
:data:`~local.system.replset.members[n].priority` value. All members
have a single vote in elections.

.. warning::

   Always configure the
   :data:`~local.system.replset.members[n].priority` value to control
   which members will become primary. Do not configure
   :data:`~local.system.replset.members[n].votes` except to permit
   more than 7 secondary members.

For more information on member priorities, see the
:doc:`/tutorial/adjust-replica-set-member-priority` document.

.. index:: pair: replica set; consistency
.. _replica-set-consistency:

Consistency
-----------

This section provides an overview of the concepts that underpin
database consistency and the MongoDB mechanisms to ensure that users
have access to consistent data.

In MongoDB, all read operations issued to the primary of a
replica set are :term:`consistent <strict consistency>` with the last
write operation.

If clients configure the :term:`read preference` to permit secondary reads,
read operations cannot return from :term:`secondary` members that have not
replicated more recent updates or operations. In these situations the
query results may reflect a previous state.

This behavior is sometimes characterized as :term:`eventual
consistency` because the secondary member's state will *eventually*
reflect the primary's state and MongoDB cannot guarantee :term:`strict
consistency` for read operations from secondary members.

There is no way to guarantee consistency for reads from *secondary
members,* except by configuring the :term:`client` and :term:`driver` to
ensure that write operations succeed on all members before completing
successfully.

.. index:: rollbacks
   single: replica set; rollbacks
   single: consistency; rollbacks

.. _replica-set-rollbacks:
.. _replica-set-rollback:

Rollbacks
~~~~~~~~~

In some :term:`failover` situations :term:`primaries <primary>` will have
accepted write operations that have *not* replicated to the
:term:`secondaries <secondary>` after a failover occurs. This case is
rare and typically occurs as a result of a network partition with
replication lag. When this member (the former primary) rejoins the
:term:`replica set` and attempts to continue replication as a
secondary the former primary must revert these operations or "roll
back" these operations to maintain database consistency across the
replica set.

MongoDB writes the rollback data to a :term:`BSON` file in the
database's :setting:`dbpath` directory. Use :doc:`bsondump
</reference/program/bsondump>` to read the contents of these rollback files
and then manually apply the changes to the new primary. There is no
way for MongoDB to appropriately and fairly handle rollback situations
automatically. Therefore you must intervene manually to apply rollback
data. Even after the member completes the rollback and returns to
secondary status, administrators will need to apply or decide to
ignore the rollback data. MongoDB writes rollback data to a
``rollback/`` folder within the :setting:`dbpath` directory to files
with filenames in the following form:

.. code-block:: none

   <database>.<collection>.<timestamp>.bson

For example:

.. code-block:: none

   records.accounts.2011-05-09T18-10-04.0.bson

The best strategy for avoiding all rollbacks is to ensure :ref:`write
propagation <replica-set-write-concern>` to all or some of the
members in the set. Using
these kinds of policies prevents situations
that might create rollbacks.

.. warning::

   A :program:`mongod` instance will not rollback more than 300
   megabytes of data. If your system needs to rollback more than 300
   MB, you will need to manually intervene to recover this data. If
   this is the case, you will find the following line in your
   :program:`mongod` log:

   .. code-block:: none

      [replica set sync] replSet syncThread: 13410 replSet too much data to roll back

   In these situations you will need to manually intervene to either
   save data or to force the member to perform an initial sync from a
   "current" member of the set by deleting the content of the existing
   :setting:`dbpath` directory.

For more information on failover, see:

- The :ref:`replica-set-failover` section in this document.

- The :ref:`replica-set-failover-administration` section in the
  :doc:`/administration/replica-sets` document.

Application Concerns
~~~~~~~~~~~~~~~~~~~~

Client applications are indifferent to the configuration and operation
of replica sets. While specific configuration depends to some extent
on the client :doc:`drivers </applications/drivers>`, there is often
minimal or no difference between applications using
:term:`replica sets <replica set>` or standalone instances.

There are two major concepts that *are* important to consider when
working with replica sets:

1. :ref:`Write Concern <write-concern>`.

   Write concern sends a MongoDB client a response from the server to
   confirm successful write operations. In replica sets you can
   configure :ref:`replica acknowledged <write-concern-replica-acknowledged>`
   write concern to ensure that secondary members of the set have
   replicated operations before the write returns.

2. :ref:`Read Preference <replica-set-read-preference>`

   By default, read operations issued against a replica set return
   results from the :term:`primary`. Users may
   configure :term:`read preference` on a per-connection basis to
   prefer that read operations return on the :term:`secondary`
   members.

:term:`Read preference` and :term:`write concern` have particular
:ref:`consistency <replica-set-consistency>` implications.

For a more detailed discussion of application concerns, see
:doc:`/applications/replication`.

Administration and Operations
-----------------------------

This section provides a brief overview of concerns relevant to
administrators of :term:`replica set` deployments.

For more information on replica set administration, operations, and
architecture, see:

- :doc:`/tutorial/deploy-replica-set`
- :doc:`/tutorial/expand-replica-set`
- :doc:`/tutorial/remove-replica-set-member`
- :doc:`/tutorial/replace-replica-set-member`
- :doc:`/tutorial/adjust-replica-set-member-priority`
- :doc:`/tutorial/resync-replica-set-member`
- :doc:`/tutorial/configure-replica-set-secondary-sync-target`
- :doc:`/tutorial/configure-a-delayed-replica-set-member`
- :doc:`/tutorial/configure-a-hidden-replica-set-member`
- :doc:`/tutorial/configure-a-non-voting-replica-set-member`
- :doc:`/tutorial/configure-secondary-only-replica-set-member`
- :doc:`/core/replica-set-architectures`

.. index:: replica set; oplog
.. _replica-set-oplog-sizing:

Oplog
~~~~~
..
  Actual oplog sizing as of 2012-07-02:

  32 bit systems = ~48 megabytes
  64 bit = larger of 5% of disk or ~1 gigabyte
  64 bit OS X = ~183 megabytes

The :term:`oplog` (operations log) is a special :term:`capped
collection` that keeps a rolling record of all operations that modify
that data stored in your databases. MongoDB applies database operations
on the :term:`primary` and then records the operations on the primary's
oplog. The :term:`secondary` members then replicate this log and apply
the operations to themselves in an asynchronous process. All replica set
members contain a copy of the oplog, allowing them to maintain the
current state of the database. Operations in the oplog are :term:`idempotent`.

By default, the size of the oplog is as follows:

- For 64-bit Linux, Solaris, FreeBSD, and Windows systems, MongoDB
  will allocate 5% of the available free disk space to the oplog.

  If this amount is smaller than a gigabyte, then MongoDB will
  allocate 1 gigabyte of space.

- For 64-bit OS X systems, MongoDB allocates 183 megabytes of space to
  the oplog.

- For 32-bit systems, MongoDB allocates about 48 megabytes of space to
  the oplog.

Before oplog creation, you can specify the size of your oplog with the
:setting:`oplogSize` option. After you start a replica set member for
the first time, you can only change the size of the oplog by using the
:doc:`/tutorial/change-oplog-size` tutorial.

In most cases, the default oplog size is sufficient. For example, if an
oplog that is 5% of free disk space fills up in 24 hours of operations, then
secondaries can stop copying entries from the oplog for up to 24 hours
without becoming stale. However, most replica sets have much
lower operation volumes, and their oplogs can hold a much larger
number of operations.

The following factors affect how MongoDB uses space in the oplog:

- Update operations that affect multiple documents at once.

  The oplog must translate multi-updates into individual operations,
  in order to maintain :term:`idempotency <idempotent>`. This can use
  a great deal of oplog space without a corresponding increase
  in disk utilization.

- If you delete roughly the same amount of data as you insert.

  In this situation the database will not grow significantly in disk
  utilization, but the size of the operation log can be quite large.

- If a significant portion of your workload entails in-place updates.

  In-place updates create a large number of operations but do not
  change the quantity data on disk.

If you can predict your replica set's workload to resemble one
of the above patterns, then you may want to consider creating an oplog
that is larger than the default. Conversely, if the predominance of
activity of your MongoDB-based application are reads and you are
writing a small amount of data, you may find that you need a much
smaller oplog.

To view oplog status, including the size and the time range of
operations, issue the :method:`db.printReplicationInfo()` method. For
more information on oplog status, see
:ref:`replica-set-troubleshooting-check-oplog-size`.

For additional information about oplog behavior, see
:ref:`replica-set-oplog` and :ref:`replica-set-syncing`.

Replica Set Deployment
~~~~~~~~~~~~~~~~~~~~~~

Without replication, a standalone MongoDB instance represents a single
point of failure and any disruption of the MongoDB system will render
the database unusable and potentially unrecoverable. Replication
increase the reliability of the database instance, and replica sets
are capable of distributing reads to :term:`secondary` members depending
on :term:`read preference`. For database work loads dominated by read
operations, (i.e. "read heavy") replica sets can greatly increase the
capability of the database system.

The minimum requirements for a replica set include two members with
data, for a :term:`primary` and a secondary, and an :ref:`arbiter
<replica-set-arbiters>`. In most circumstances, however, you will want
to deploy three data members.

For those deployments that rely heavily on distributing reads to
secondary instances, add additional members to the set as load
increases. As your deployment grows, consider adding or moving
replica set members to secondary data centers or to geographically
distinct locations for additional redundancy. While many architectures
are possible, always ensure that the quorum of members required to elect
a primary remains in your main facility.

Depending on your operational requirements, you may consider adding
members configured for a specific purpose including, a :term:`delayed
member` to help provide protection against human errors and change
control, a :term:`hidden member` to provide an isolated member for
reporting and monitoring, and/or a :ref:`secondary only member
<replica-set-secondary-only-members>` for dedicated backups.

The process of establishing a new replica set member can be resource
intensive on existing members. As a result, deploy new members to
existing replica sets significantly before current demand saturates
the existing members.

.. note::

   :term:`Journaling <journal>`, provides single-instance write
   durability. The journaling greatly improves the reliability and
   durability of a database. Unless MongoDB runs with journaling, when
   a MongoDB instance terminates ungracefully, the database can end in
   a corrupt and unrecoverable state.

   You should assume that a database, running without journaling, that
   suffers a crash or unclean shutdown is in corrupt or inconsistent
   state.

   **Use journaling**, however, do not forego proper replication
   because of journaling.

   64-bit versions of MongoDB after version 2.0 have journaling
   enabled by default.

.. index:: pair: replica set; security

.. _replica-set-security:

Security
~~~~~~~~

In most cases, :term:`replica set` administrators do not have to keep
additional considerations in mind beyond the normal security
precautions that all MongoDB administrators must take. However, ensure
that:

- Your network configuration will allow every member of the replica
  set to contact every other member of the replica set.

- If you use MongoDB's authentication system to limit access to your
  infrastructure, ensure that you configure a
  :setting:`keyFile` on all members to permit authentication.

For most instances, the most effective ways to control access and to
secure the connection between members of a :term:`replica set` depend
on network-level access control. Use your environment's firewall and
network routing to ensure that traffic *only* from clients and other
replica set members can reach your :program:`mongod` instances. If
needed, use virtual private networks (VPNs) to ensure secure
connections over wide area networks (WANs.)

Additionally, MongoDB provides an authentication mechanism for
:program:`mongod` and :program:`mongos` instances connecting to
replica sets. These instances enable authentication but specify a
shared key file that serves as a shared password.

.. versionadded:: 1.8
   Added support authentication in replica set deployments.

.. versionchanged:: 1.9.1
   Added support authentication in sharded replica set deployments.

To enable authentication add the following option to your configuration file:

.. code-block:: cfg

   keyFile = /srv/mongodb/keyfile

.. note::

   You may chose to set these run-time configuration options using the
   :option:`--keyFile <mongod --keyFile>` (or :option:`mongos --keyFile`)
   options on the command line.

Setting :setting:`keyFile` enables authentication and specifies a key
file for the replica set members to use when authenticating to each
other. The content of the key file is arbitrary but must be the same
on all members of the replica set and on all :program:`mongos`
instances that connect to the set.

The key file must be less one kilobyte in size and may only contain
characters in the base64 set. The key file must not have group or "world"
permissions on UNIX systems. Use the following command to use the
OpenSSL package to generate "random" content for use in a key file:

.. code-block:: bash

   openssl rand -base64 753

.. note::

   Key file permissions are not checked on Windows systems.

.. _replica-set-deployment-overview:
.. _replica-set-architecture:

Architectures
~~~~~~~~~~~~~

The architecture and design of the :term:`replica set` deployment can
have a great impact on the set's capacity and capability. This section
provides a general overview of the architectural possibilities for
replica set deployments.  However, for most production deployments a
conventional 3-member replica set with
:data:`~local.system.replset.members[n].priority` values of ``1`` are
sufficient.

While the additional flexibility discussed is below helpful for
managing a variety of operational complexities, it always makes sense
to let those complex requirements dictate complex architectures,
rather than add unnecessary complexity to your deployment.

Consider the following factors when developing an architecture for
your replica set:

- Ensure that the members of the replica set will always be able to
  elect a :term:`primary`. Run an odd number of members or run an
  :term:`arbiter` on one of your application servers if you have an
  even number of members.

- With geographically distributed members, know where the "quorum" of
  members will be in the case of any network partitions.  Attempt to
  ensure that the set can elect a primary among the members in the
  primary data center.

- Consider including a :ref:`hidden <replica-set-hidden-members>`
  or :ref:`delayed member <replica-set-delayed-members>` in your replica
  set to support dedicated functionality, like backups, reporting, and
  testing.

- Consider keeping one or two members of the set in an off-site data
  center, but make sure to configure the :ref:`priority
  <replica-set-node-priority>` to prevent it from becoming primary.

- Create custom write concerns with :ref:`replica set tags
  <replica-set-configuration-tag-sets>` to ensure that applications
  can control the threshold for a successful write operation. Use
  these write concerns to ensure that operations propagate to specific
  data centers or to machines of different functions before returning
  successfully.

For more information regarding replica set configuration and
deployments see :doc:`/core/replica-set-architectures`.
