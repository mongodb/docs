:tabs-selector-position: main

.. facet::
   :name: programming_language
   :values: csharp, go, java, javascript/typescript, python

.. _avs-rag:
.. _ai-key-concepts:

=============================================================
Retrieval-Augmented Generation (RAG) with {+avs+}
=============================================================

.. default-domain:: mongodb

.. facet::
   :name: genre
   :values: tutorial

.. meta::
   :description: Use MongoDB Atlas Vector Search to implement retrieval-augmented-generation (RAG) in your generative AI applications.
   :keywords: RAG, retrieval-augmented generation, AI, LLM, vector database, vector search, semantic search, generative search, code example, java, python, go, node.js, Hugging Face, C#, .NET, OpenAI

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

Retrieval-augmented generation (RAG) is an architecture used
to augment large language models (LLMs) with additional data
so that they can generate more accurate responses.
You can implement |rag| in your generative AI applications
by combining an |llm| with a retrieval system powered by {+avs+}.

.. button:: Get Started
   :uri: https://www.mongodb.com/docs/atlas/atlas-vector-search/rag/#get-started

Why use RAG?
------------

When working with |llm|\s, you might encounter the
following limitations:

- Stale data: |llm|\s are trained on a static dataset
  up to a certain point in time. This means that they
  have a limited knowledge base and might use outdated
  data.

- No access to local data: |llm|\s don't have access to
  local or personalized data. Therefore, they can
  lack knowledge about specific domains.

- Hallucinations: When training data is incomplete or
  outdated, |llm|\s can generate inaccurate information.

You can address these limitations by taking
the following steps to implement |rag|:

1. **Ingestion:** Store your custom data as :term:`vector embeddings`
   in a vector database, such as |service-fullname|. This allows you
   to create a knowledge base of up-to-date and personalized data.

#. **Retrieval:** Retrieve semantically similar
   documents from the database based on the user's question by using
   a search solution, such as {+avs+}. These
   documents augment the |llm| with additional, relevant data.

#. **Generation:** Prompt the |llm|. The |llm| uses
   the retrieved documents as context to generate a more accurate
   and relevant response, reducing hallucinations.

Because |rag| enables tasks such as question answering and
text generation, it's an effective architecture for building
AI chatbots that provide personalized, domain-specific responses.
To create production-ready chatbots, you must configure a server
to route requests and build a user interface on top of your |rag|
implementation.

RAG with {+avs+}
----------------------------

To implement |rag| with {+avs+}, you ingest data into |service|,
retrieve documents with {+avs+}, and generate responses
using an |llm|. This section describes the components
of a basic, or naive, |rag| implementation
with {+avs+}. For step-by-step instructions, see :ref:`basic-rag-example`.

.. figure:: /images/rag-flowchart.svg
   :figwidth: 100%
   :alt: RAG flowchart with Atlas Vector Search

.. _rag-ingestion:

Ingestion
~~~~~~~~~

Data ingestion for |rag| involves processing your custom
data and storing it in a vector database to prepare it
for retrieval. To create a basic ingestion pipeline with
|service| as the vector database, do the following:

.. tabs-drivers::

   .. tab::
      :tabid: csharp

      .. include:: /includes/avs/rag/avs-rag-ingest-data-csharp.rst

   .. tab::
      :tabid: go

      .. include:: /includes/avs/rag/avs-rag-ingest-data-go.rst

   .. tab::
      :tabid: java-sync

      .. include:: /includes/avs/rag/avs-rag-ingest-data-java.rst

   .. tab::
      :tabid: nodejs

      .. include:: /includes/avs/rag/avs-rag-ingest-data-javascript.rst

   .. tab::
      :tabid: python

      .. include:: /includes/avs/rag/avs-rag-ingest-data-python.rst

.. _rag-retrieval:

Retrieval
~~~~~~~~~

Building a retrieval system involves searching for and returning the
most relevant documents from your vector database to augment the |llm|
with. To retrieve relevant documents with {+avs+}, you convert the user's
question into vector embeddings and run a :ref:`vector search query
<avs-queries>` against your data in |service| to find documents with the
most similar embeddings.

To perform basic retrieval with {+avs+}, do the following:

1. Define an :ref:`{+avs+} index <avs-indexes>`
   on the collection that contains your vector embeddings.

#. Choose one of the following methods to retrieve documents
   based on the user's question:

   - Use an :ref:`{+avs+} integration <ai-integrations>`
     with a popular framework or service.
     These integrations include built-in libraries and tools
     that enable you to easily build retrieval systems
     with {+avs+}.

   - Build your own retrieval system. You can define
     your own functions and pipelines to run
     :ref:`{+avs+} queries <avs-queries>`
     specific to your use case.

     To learn how to build a basic retrieval system with {+avs+},
     see :ref:`basic-rag-example`.

.. _rag-generation:

Generation
~~~~~~~~~~

To generate responses, combine your retrieval system with an |llm|.
After you perform a vector search to retrieve relevant documents,
you provide the user's question along with the relevant documents as
context to the |llm| so that it can generate a more accurate response.

Choose one of the following methods to connect to an |llm|:

- Use an :ref:`{+avs+} integration <ai-integrations>`
  with a popular framework or service. These
  integrations include built-in libraries and tools to
  help you connect to |llm|\s with minimal set-up.

- Call the |llm|\'s |api|.
  Most AI providers offer |api|\s to their generative models
  that you can use to generate responses.

- Load an open-source |llm|.
  If you don't have |api| keys or credits,
  you can use an open-source |llm| by loading it locally
  from your application. For an example implementation, see the
  :ref:`local-rag` tutorial.

.. collapsible:: 
   :heading: Learn by Watching 
   :sub_heading: Learn how to develop a RAG system with {+avs+}.

   *Duration: 1.16 Minutes*

   .. video:: https://youtu.be/uOMCffRB08s?si=dxl8uNGifIxi2UnI

.. _basic-rag-example:

Get Started
-----------

The following example demonstrates how to implement |rag| with a
retrieval system powered by {+avs+}.

----------

.. |arrow| unicode:: U+27A4

|arrow| Use the **Select your language** drop-down menu to set the
language of the examples on this page.

.. tabs-selector:: drivers
   :default-tabid: python

----------

.. tabs-drivers::

   .. tab::
      :tabid: csharp

   .. tab::
      :tabid: go

   .. tab::
      :tabid: java-sync

   .. tab::
      :tabid: nodejs

   .. tab::
      :tabid: python

      .. cta-banner::
         :url: https://github.com/mongodb/docs-notebooks/blob/main/use-cases/rag.ipynb?tck=docs
         :icon: Code

         Work with a runnable version of this tutorial as a :github:`Python notebook <mongodb/docs-notebooks/blob/main/use-cases/rag.ipynb?tck=docs>`.

Prerequisites
~~~~~~~~~~~~~

To complete this example, you must have the following:

.. tabs-drivers::

   .. tab::
      :tabid: csharp

      .. include:: /includes/avs/rag/avs-rag-prerequisites-csharp.rst

   .. tab::
      :tabid: go

      .. include:: /includes/avs/rag/avs-rag-prerequisites-go.rst

   .. tab::
      :tabid: java-sync

      .. include:: /includes/avs/rag/avs-rag-prerequisites-java.rst

   .. tab::
      :tabid: nodejs

      .. include:: /includes/avs/rag/avs-rag-prerequisites-javascript.rst

   .. tab::
      :tabid: python

      .. include:: /includes/avs/rag/avs-rag-prerequisites-python.rst

Procedure
~~~~~~~~~

.. tabs-drivers::

   .. tab::
      :tabid: csharp

      .. include:: /includes/avs/rag/steps-avs-rag-openai-csharp.rst

   .. tab::
      :tabid: go

      .. include:: /includes/avs/rag/steps-avs-rag-huggingface-go.rst

   .. tab::
      :tabid: java-sync

      .. include:: /includes/avs/rag/steps-avs-rag-huggingface-java.rst

   .. tab::
      :tabid: nodejs

      .. include:: /includes/avs/rag/steps-avs-rag-huggingface-javascript.rst

   .. tab::
      :tabid: python

      .. include:: /includes/avs/rag/steps-avs-rag-huggingface-python.rst

.. _rag-examples:

Next Steps
----------

For more detailed |rag| tutorials, see the following resources:

- To learn how to implement |rag| with popular |llm| frameworks
  and AI services, see :ref:`ai-integrations`.

- To learn how to implement |rag| using a local |service| {+deployment+}
  and local models, see :ref:`local-rag`.

- For use-case based tutorials and interactive Python notebooks,
  see :github:`Generative AI Use Cases Repository
  </mongodb-developer/GenAI-Showcase/tree/main>`.

To start building production-ready chatbots with {+avs+}, you can use
the `MongoDB Chatbot Framework <https://mongodb.github.io/chatbot/>`__.
This framework provides a set of libraries that enable you to
quickly build AI chatbot applications.

.. _rag-fine-tuning:

Fine-Tuning
~~~~~~~~~~~

To optimize and fine-tune your |rag| applications, 
see :ref:`avs-improve-results` and :ref:`avs-performance-tuning`.

You can also experiment with
different embedding models, chunking strategies, and |llm|\s.
To learn more, see the following resources:

- :website:`How to Choose the Right Embedding Model for Your LLM Application
  </developer/products/atlas/choose-embedding-model-rag/>`
- :website:`How to Choose the Right Chunking Strategy for Your LLM Application
  </developer/products/atlas/choosing-chunking-strategy-rag/>`
- :website:`How to Evaluate Your LLM Application
  </developer/products/atlas/evaluate-llm-applications-rag>`

Additionally, {+avs+} supports advanced retrieval systems.
Because you can seamlessly index vector data along with your other
data in |service|, you can fine-tune your retrieval results
by :ref:`pre-filtering <vectorSearch-agg-pipeline-filter>`
on other fields in your collection or performing
:ref:`hybrid search <as_hybrid-search>` to combine semantic search
with full-text search results.
