ref: list-command-line-specification
content: |
   - the ``--packages`` option to download the MongoDB Spark Connector
     package.  The following packages are available:

     - ``mongo-spark-connector_2.10`` for use with Scala 2.10.x
     - ``mongo-spark-connector_2.11`` for use with Scala 2.11.x

   - the ``--conf`` option to configure the MongoDB Spark Connnector.
     These settings configure the ``SparkConf`` object.

     .. note:: 

        When specifying the Connector configuration via ``SparkConf``, you
        must prefix the settings appropriately. For details and other
        available MongoDB Spark Connector options, see the
        :doc:`/configuration`.
---
ref: list-configuration-explantation
content: |
   - The :ref:`spark.mongodb.input.uri <spark-input-conf>` specifies
     the MongoDB server address (``127.0.0.1``), the database to connect
     (``test``), and the collection (``myCollection``) from which to
     read data, and the read preference. Connects to port ``27017`` by
     default. To connect to another port, specify it after the server
     address (``127.0.0.1:27018``).
   - The :ref:`spark.mongodb.output.uri <spark-output-conf>` specifies the
     MongoDB server address(``127.0.0.1``), the database to connect
     (``test``), and the collection (``myCollection``) to which to write
     data. Connects to port ``27017`` by default.
   - The ``packages`` option specifies the Spark Connector's
     Maven coordinates, in the format ``groupId:artifactId:version``.
---
ref: command-line-start-spark-shell
content: |

   .. include:: /includes/extracts/list-command-line-specification.rst

   For example,

   .. code-block:: sh

      ./bin/spark-shell --conf "spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred" \
                        --conf "spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection" \
                        --packages org.mongodb.spark:mongo-spark-connector_2.11:2.0.0
 
   .. include:: /includes/extracts/list-configuration-explantation.rst

---
ref: command-line-start-pyspark
content: |

   .. include:: /includes/extracts/list-command-line-specification.rst

   The following example starts the ``pyspark`` shell from the command
   line:

   .. code-block:: sh

      ./bin/pyspark --conf "spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred" \
                    --conf "spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection" \
                    --packages org.mongodb.spark:mongo-spark-connector_2.10:2.0.0
 
   .. include:: /includes/extracts/list-configuration-explantation.rst
---
ref: command-line-start-sparkR
content: |

   .. include:: /includes/extracts/list-command-line-specification.rst

   For example,

   .. code-block:: sh

      ./bin/sparkR  --conf "spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred" \
                    --conf "spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection" \
                    --packages org.mongodb.spark:mongo-spark-connector_2.10:1.1.0

   .. include:: /includes/extracts/list-configuration-explantation.rst

   The examples in this tutorial will use this database and collection.
...
