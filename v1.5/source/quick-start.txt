.. _pymongo-arrow-quick-start:

===========
Quick Start
===========

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

.. facet::
   :name: genre
   :values: reference
 
.. meta::
   :keywords: tutorial, introduction, setup, begin  

This tutorial is intended as an introduction to working with
**{+driver-short+}**. The tutorial assumes the reader is familiar with basic
`PyMongo <https://pymongo.readthedocs.io/en/stable/tutorial.html>`__ and
`MongoDB <https://docs.mongodb.com>`__ concepts.

Prerequisites
-------------

Ensure that you have the {+driver-short+} distribution
:ref:`installed <pymongo-arrow-install>`. In the Python shell, the following should
run without raising an exception:

.. code-block:: python

   >>> import pymongoarrow as pma

This tutorial also assumes that a MongoDB instance is running on the
default host and port. After you have `downloaded and installed
<https://docs.mongodb.com/manual/installation/>`__ MongoDB, you can start
it as shown in the following code example:

.. code-block:: bash

   $ mongod

Extending PyMongo
~~~~~~~~~~~~~~~~~

The ``pymongoarrow.monkey`` module provides an interface to patch PyMongo
in place, and add {+driver-short+} functionality directly to
``Collection`` instances:

.. code-block:: python

   from pymongoarrow.monkey import patch_all
   patch_all()

After you run the ``monkey.patch_all()`` method, new instances of
the ``Collection`` class will contain the {+driver-short+} APIs--
for example, the ``pymongoarrow.api.find_pandas_all()`` method.

.. note::
  
   You can also use any of the {+driver-short+} APIs
   by importing them from the ``pymongoarrow.api`` module. If you do,
   you must pass the instance of the ``Collection`` on which the operation is to be
   run as the first argument when calling the API method.

Test Data
~~~~~~~~~

The following code uses PyMongo to add sample data to your cluster:

.. code-block:: python

   from datetime import datetime
   from pymongo import MongoClient
   client = MongoClient()
   client.db.data.insert_many([
     {'_id': 1, 'amount': 21, 'last_updated': datetime(2020, 12, 10, 1, 3, 1), 'account': {'name': 'Customer1', 'account_number': 1}, 'txns': ['A']},
     {'_id': 2, 'amount': 16, 'last_updated': datetime(2020, 7, 23, 6, 7, 11), 'account': {'name': 'Customer2', 'account_number': 2}, 'txns': ['A', 'B']},
     {'_id': 3, 'amount': 3,  'last_updated': datetime(2021, 3, 10, 18, 43, 9), 'account': {'name': 'Customer3', 'account_number': 3}, 'txns': ['A', 'B', 'C']},
     {'_id': 4, 'amount': 0,  'last_updated': datetime(2021, 2, 25, 3, 50, 31), 'account': {'name': 'Customer4', 'account_number': 4}, 'txns': ['A', 'B', 'C', 'D']}])

Defining the Schema
-------------------

{+driver-short+} relies on a data schema to marshall
query result sets into tabular form. If you don't provide this schema, {+driver-short+}
infers one from the data. You can define the schema by
creating a ``Schema`` object and mapping the field names
to type-specifiers, as shown in the following example:

.. code-block:: python

   from pymongoarrow.api import Schema
   schema = Schema({'_id': int, 'amount': float, 'last_updated': datetime})

MongoDB uses embedded documents to represent nested data. {+driver-short+} offers
first-class support for these documents:

.. code-block:: python

   schema = Schema({'_id': int, 'amount': float, 'account': { 'name': str, 'account_number': int}})

{+driver-short+} also supports lists and nested lists:

.. code-block:: python

   from pyarrow import list_, string
   schema = Schema({'txns': list_(string())})
   polars_df = client.db.data.find_polars_all({'amount': {'$gt': 0}}, schema=schema)

.. tip::
  
   {+driver-short+} includes multiple permissible type-identifiers for each supported BSON
   type. For a full list of these data types and their associated type-identifiers, see
   :ref:`<pymongo-arrow-data-types>`.

Find Operations
---------------

The following code example shows how to load all records that have a non-zero
value for the ``amount`` field as a ``pandas.DataFrame`` object:

.. code-block:: python

   df = client.db.data.find_pandas_all({'amount': {'$gt': 0}}, schema=schema)

You can also load the same result set as a ``pyarrow.Table`` instance:

.. code-block:: python

   arrow_table = client.db.data.find_arrow_all({'amount': {'$gt': 0}}, schema=schema)

Or as a ``polars.DataFrame`` instance:

.. code-block:: python

   df = client.db.data.find_polars_all({'amount': {'$gt': 0}}, schema=schema)

Or as a NumPy ``arrays`` object:

.. code-block:: python
  
   ndarrays = client.db.data.find_numpy_all({'amount': {'$gt': 0}}, schema=schema)

When using NumPy, the return value is a dictionary where the keys are field
names and the values are the corresponding ``numpy.ndarray`` instances.

.. note::

   In all of the preceding examples, you can omit the schema as shown in the following
   example:

   .. code-block:: python

      arrow_table = client.db.data.find_arrow_all({'amount': {'$gt': 0}})

   If you omit the schema, {+driver-short+} tries to automatically apply a schema based on
   the data contained in the first batch.

Aggregate Operations
--------------------

Running an aggregate operation is similar to running a find operation, but it takes a
sequence of operations to perform.

The following is a simple example of the ``aggregate_pandas_all()`` method that outputs a
new dataframe in which all ``_id`` values are grouped together and their ``amount`` values
summed:

.. code-block:: python

   df = client.db.data.aggregate_pandas_all([{'$group': {'_id': None, 'total_amount': { '$sum': '$amount' }}}])

You can also run aggregate operations on embedded documents.
The following example unwinds values in the nested ``txn`` field, counts the number of each
value, then returns the results as a list of NumPy ``ndarray`` objects, sorted in
descending order:

.. code-block:: python

   pipeline = [{'$unwind': '$txns'}, {'$group': {'_id': '$txns', 'count': {'$sum': 1}}}, {'$sort': {"count": -1}}]
   ndarrays = client.db.data.aggregate_numpy_all(pipeline)

.. tip::
  
   For more information about aggregation pipelines, see the
   :manual:`MongoDB Server documentation </core/aggregation-pipeline/>`.

Writing to MongoDB
------------------

You can use the ``write()`` method to write objects of the following types to MongoDB:

- Arrow ``Table``
- Pandas ``DataFrame``
- NumPy ``ndarray``
- Polars ``DataFrame``

.. code-block:: python
 
   from pymongoarrow.api import write
   from pymongo import MongoClient
   coll = MongoClient().db.my_collection
   write(coll, df)
   write(coll, arrow_table)
   write(coll, ndarrays)

.. note::
  
   NumPy arrays are specified as ``dict[str, ndarray]``.

Ignore Empty Values
~~~~~~~~~~~~~~~~~~~

The ``write()`` method optionally accepts a boolean ``exclude_none`` parameter.
If you set this parameter to ``True``, the driver doesn't write empty values to
the database. If you set this parameter to ``False`` or leave it blank, the 
driver writes ``None`` for each empty field.

The code in the following example writes an Arrow ``Table`` to MongoDB twice. One
of the values in the ``'b'`` field is set to ``None``. 

The first call to the ``write()`` method omits the ``exclude_none`` parameter, 
so it defaults to ``False``. All values in the ``Table``, including ``None``, 
are written to the database. The second call to the ``write()`` method sets 
``exclude_none`` to ``True``, so the empty value in the ``'b'`` field is ignored.

.. io-code-block::
   :copyable: true

   .. input:: 
      :language: python
      :emphasize-lines: 12, 16
      
      data_a = [1, 2, 3]
      data_b = [1, None, 3]

      data = Table.from_pydict(
         {
            "a": data_a,
            "b": data_b,
         },
      )

      coll.drop()
      write(coll, data)
      col_data = list(coll.find({}))

      coll.drop()
      write(coll, data, exclude_none=True)
      col_data_exclude_none = list(coll.find({}))

      print(col_data)

      print(col_data_exclude_none)

   .. output::
      :language: json
      :visible: false
      :emphasize-lines: 2, 6

      {'_id': ObjectId('...'), 'a': 1, 'b': 1}
      {'_id': ObjectId('...'), 'a': 2, 'b': None}
      {'_id': ObjectId('...'), 'a': 3, 'b': 3}

      {'_id': ObjectId('...'), 'a': 1, 'b': 1}
      {'_id': ObjectId('...'), 'a': 2}
      {'_id': ObjectId('...'), 'a': 3, 'b': 3}

Writing to Other Formats
------------------------

Once result sets have been loaded, you can then write them to any format that the package
supports.

For example, to write the table referenced by the variable ``arrow_table`` to a Parquet
file named ``example.parquet``, run the following code:

.. code-block:: python
  
   import pyarrow.parquet as pq
   pq.write_table(arrow_table, 'example.parquet')

Pandas also supports writing ``DataFrame`` instances to a variety
of formats, including CSV and HDF. To write the data frame
referenced by the variable ``df`` to a CSV file named ``out.csv``, run the following
code:

.. code-block:: python
  
   df.to_csv('out.csv', index=False)

The Polars API is a mix of the two preceding examples:

.. code-block:: python
 
   import polars as pl
   df = pl.DataFrame({"foo": [1, 2, 3, 4, 5]})
   df.write_parquet('example.parquet')

.. note::

   Nested data is supported for parquet read and write operations, but is not well
   supported by Arrow or Pandas for CSV read and write operations.
