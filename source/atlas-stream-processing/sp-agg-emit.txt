.. _streams-agg-pipeline-emit:
.. _atlas-sp-agg-emit:

=========
``$emit``
=========

.. default-domain:: mongodb

.. meta::
   :keywords: atlas stream processing, $emit aggregation pipeline stage 
   :description: Learn how to use the $emit stage to output processed data
                 to streaming data platforms.

.. facet::
   :name: genre
   :values: reference

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

.. _atlas-sp-agg-emit-def:

Definition
----------

The ``$emit`` stage specifies a connection in the
:ref:`Connection Registry <manage-spi-connection-add>` to emit
messages to. The connection must be either an {+kafka+} broker or a
:manual:`time series collection </core/timeseries-collections>`.

.. _atlas-sp-agg-emit-syntax:

Syntax
------

.. _sp-emit-kafka:

Apache Kafka Broker
~~~~~~~~~~~~~~~~~~~

To write processed data to an {+kafka+} broker, use the ``$emit``
pipeline stage with the following prototype form:

.. code-block:: json

   {
     "$emit": {
       "connectionName": "<registered-connection>",
       "topic" : "<target-topic>" | <expression>,
       "config": {
         "headers": "<expression>",
         "key": "<key-string>" | { key-document },
	 "keyFormat": "<deserialization-type>",
	 "outputFormat": "<json-format>"
       }
     }  
   }

The ``$emit`` stage takes a document with the following fields: 

.. list-table:: 
   :header-rows: 1
   :widths: 10 15 15 70

   * - Field 
     - Type 
     - Necessity 
     - Description

   * - ``connectionName`` 
     - string
     - Required 
     - Name, as it appears in the 
       :ref:`Connection Registry <manage-spi-connection-add>`, of the
       connection to ingest data from.

   * - ``topic``
     - string | expression
     - Required 
     - Name of the {+kafka+} topic to emit messages to.

   * - ``config``
     - document 
     - Optional
     - Document containing fields that override various default 
       values.

   * - ``config.headers``
     - expression
     - Optional
     - Headers to add to the output message. The expression must evaluate
       to either an object or an array.

       If the expression evaluates to an object, {+atlas-sp+}
       constructs a header from each key-value pair in that object, where
       the key is the header name, and the value is the header value.

       If the expression evaluates to an array, it must take the form of
       an array of key-value pair objects. For example:       
          
       .. code-block:: json
	  
    	    [
            {k: "name1", v: ...},
    	      {k: "name2", v: ...},
    	      {k: "name3", v: ...} 
          ]

       {+atlas-sp+} constructs a header from each object in the array, 
       where the key is the header name, and the value is the header value. 

       {+atlas-sp+} supports header values of the following types:

       - ``binData``
       - ``string``
       - ``object``
       - ``int``
       - ``long``
       - ``double``
       - ``null``

   * - ``config.key``
     - object | string
     - Optional 
     - Expression that evaluates to a {+kafka+} message key.

       If you specify ``config.key``, you must specify
       ``config.keyFormat``.

   * - ``config.keyFormat``
     - string
     - Conditional
     - Data type used to deserialize {+kafka+} key data. Must be one
       of the following values:

       - ``"binData"``
       - ``"string"``
       - ``"json"``
       - ``"int"``
       - ``"long"``

       Defaults to ``binData``. If you specify ``config.key``, you
       must specify ``config.keyFormat``. If the ``config.key`` of a
       document does not deserialize successfully to the specified
       data type, {+atlas-sp+} sends it to your :ref:`dead letter
       queue <atlas-sp-dlq>`.

   * - ``config.outputFormat``
     - string
     - Optional 
     - JSON format to use when emitting messages to {+kafka+}. Must be one of the
       following values:

       - ``"relaxedJson"``
       - ``"canonicalJson"``

       Defaults to ``"relaxedJson"``.

.. _sp-emit-timeseries:

{+service+} Time Series Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To write processed data to an {+service+} time series collection,
use the ``$emit`` pipeline stage with the following prototype form:

.. code-block:: json

   {
     "$emit": {
       "connectionName": "<registered-connection>",
       "db" : "<target-db>",
       "coll" : "<target-coll>",
       "timeseries" : {
	 <options>
       } 
     }  
   }

The ``$emit`` stage takes a document with the following fields: 

.. list-table:: 
   :header-rows: 1
   :widths: 10 15 15 70

   * - Field 
     - Type 
     - Necessity 
     - Description

   * - ``connectionName`` 
     - string
     - Required 
     - Name, as it appears in the 
       :ref:`Connection Registry <manage-spi-connection-add>`, of the
       connection to ingest data from.

   * - ``db``
     - string
     - Required 
     - Name of the {+service+} database that contains the target
       time series collection.

   * - ``coll``
     - string
     - Required 
     - Name of the {+service+} time series collection to write
       to.

   * - ``timeseries``
     - document
     - Required 
     - Document defining the :manual:`time series fields
       </core/timeseries/timeseries-procedures/#time-series-field-reference>`
       for the collection.

.. note::

   The maximum size for documents within a time series collection is 4 MB. 
   To learn more, see :ref:`manual-timeseries-collection-limitations`.

.. _atlas-sp-agg-emit-behavior:

Behavior
--------

``$emit`` must be the last stage of any pipeline it appears in. You can
use only one ``$emit`` stage per pipeline.

You can only write to a single {+service+} time series collection per
stream processor. If you specify a collection that doesn't exist,
{+service+} creates the collection with the time series fields you
specified. You must specify an existing database.

You can use a :manual:`dynamic expression
</reference/operator/aggregation/#expression-operators>` as the value
of the ``topic`` field to enable your stream processor to write to
different target {+kafka+} topics on a message-by-message basis. The
expression must evaluate to a string.

.. example::

   You have a stream of transaction events that generates messages of 
   the following form:

   .. code-block:: json

      { 
        "customer": "Very Important Industries",
        "customerStatus": "VIP",
        "tenantId": 1,
        "transactionType": "subscription"
      }

      { 
        "customer": "N. E. Buddy",
        "customerStatus": "employee",
        "tenantId": 5,
        "transactionType": "requisition"
      }

      { 
        "customer": "Khan Traktor",
        "customerStatus": "contractor",
        "tenantId": 11,
        "transactionType": "billableHours"
      }

   To sort each of these into a distinct {+kafka+} topic, you can write
   the following ``$emit`` stage:

   .. code-block:: json

      $emit: {
        connectionName: "kafka1",
        topic: "$customerStatus"
      }

   This ``$emit`` stage:

   - Writes the ``Very Important Industries`` message to a topic named 
     ``VIP``.
   - Writes the ``N. E. Buddy`` message to a topic named ``employee``.
   - Writes the ``Khan Traktor`` message to a topic named 
     ``contractor``.

For more information on dynamic expressions, see :manual:`expression
operators </reference/operator/aggregation/#expression-operators>`.

If you specify a topic that doesn't already exist, {+kafka+}
automatically creates the topic when it receives the first message
that targets it.

If you specify a topic with a dynamic expression, but 
{+atlas-sp+} cannot evaluate the expression for a given message, 
{+atlas-sp+} sends that message to the :term:`dead letter queue` if configured 
and processes subsequent messages. If there is no :term:`dead letter queue` 
configured, then {+atlas-sp+} skips the message completely and processes 
subsequent messages.

Examples
~~~~~~~~

A streaming data source generates detailed weather reports from
various locations, conformant to the schema of the :ref:`Sample
Weather Dataset <sample-weather>`. The following aggregation has three
stages:

1. The :pipeline:`$source` stage establishes a connection with the
   {+kafka+} broker collecting these reports in a topic named
   ``my_weatherdata``, exposing each record as it is ingested to the
   subsequent aggregation stages. This stage also overrides the name
   of the timestamp field it projects, setting it to
   ``ingestionTime``.

2. The :pipeline:`$match` stage excludes documents that have an
   ``airTemperature.value`` of greater than or equal to ``30.0`` and
   passes along the documents with an ``airTemperature.value`` less
   than ``30.0`` to the next stage.

3. The ``$emit`` stage writes the output to a topic named
   ``stream`` over the ``weatherStreamOutput`` Kafka broker
   connection.
   
.. code-block:: json
   :copyable: true

   {
     '$source': {
       connectionName: 'sample_weatherdata',
       topic: 'my_weatherdata',
       tsFieldName: 'ingestionTime'
     }
   },
   { '$match': { 'airTemperature.value': { '$lt': 30 } } },
   {
     '$emit': {
       connectionName: 'weatherStreamOutput',
       topic: 'stream'
     }
   }

Documents in the ``stream`` topic take the following form:

.. code-block:: json
   :copyable: false

   {
     "st":"x+34700+119500",
     "position": {
       "type": "Point",
       "coordinates": [122.8,116.1]
     },
     "elevation": 9999,
     "callLetters": "6ZCM",
     "qualityControlProcess": "V020",
     "dataSource": "4",
     "type": "SAO",
     "airTemperature": {
       "value": 6.7,
       "quality": "9"
     },
     "dewPoint": {
       "value": 14.1,
       "quality": "1"
     },
     "pressure": {
       "value": 1022.2,
       "quality": "1"
     },
     "wind": {
       "direction": {
         "angle": 200,
	 "quality": "9"
       },
       "type": "C",
       "speed": {
         "rate": 35,
	 "quality": "1"
       }
     },
     "visibility": {
       "distance": {
         "value": 700,
	 "quality": "1"
       },
       "variability": {
         "value": "N",
         "quality": "1"
       }
     },
     "skyCondition": {
       "ceilingHeight": {
         "value": 1800,
	 "quality": "9",
	 "determination": "9"
       },
       "cavok": "N"
     },
     "sections": ["AA1","AG1","UG1","SA1","MW1"],
     "precipitationEstimatedObservation": {
       "discrepancy": "0",
       "estimatedWaterDepth": 999
     },
     "atmosphericPressureChange": {
       "tendency": {
         "code": "4",
	 "quality": "1"
       },
       "quantity3Hours": {
         "value": 3.8,
	 "quality": "1"
       },
       "quantity24Hours": {
         "value": 99.9,
	 "quality": "9"
       }
     },
     "seaSurfaceTemperature": {
       "value": 9.7,
       "quality": "9"
     },
     "waveMeasurement": {
       "method": "M",
       "waves": {
         "period": 8,
	 "height": 3,
	 "quality": "9"
       },
       "seaState": {
         "code": "00",
	 "quality": "9"
       }
     },
     "pastWeatherObservationManual": {
       "atmosphericCondition": {
         "value": "6",
	 "quality": "1"
       },
       "period": {
         "value": 3,
	 "quality": "1"
       }
     },
     "skyConditionObservation": {
       "totalCoverage": {
         "value": "02",
	 "opaque": "99",
	 "quality": "9"
       },
       "lowestCloudCoverage": {
         "value": "00",
	 "quality": "9"
       },
       "lowCloudGenus": {
         "value": "00",
	 "quality": "1"
       },
       "lowestCloudBaseHeight":{
         "value": 1750,
	 "quality": "1"
       },
       "midCloudGenus": {
         "value": "99",
	 "quality": "1"
       },
       "highCloudGenus": {
         "value": "00",
	 "quality": "1"
       }
     },
     "presentWeatherObservationManual": {
       "condition": "52",
       "quality": "1"
     },
     "atmosphericPressureObservation": {
       "altimeterSetting": {
         "value": 1015.9,
	 "quality": "9"
       },
       "stationPressure": {
         "value": 1026,
	 "quality": "1"
       }
     },
     "skyCoverLayer": {
       "coverage": {
         "value": "08",
	 "quality": "1"
       },
       "baseHeight": {
         "value": 2700,
	 "quality": "9"
       },
       "cloudType": {
         "value": "99",
	 "quality": "9"
       }
     },
     "liquidPrecipitation": {
       "period": 12,
       "depth": 20,
       "condition": "9",
       "quality": "9"
     },
     "extremeAirTemperature": {
       "period": 99.9,
       "code": "N",
       "value": -30.4,
       "quantity": "1"
     },
     "ingestionTime":{
       "$date":"2024-09-26T17:34:41.843Z"
     },
     "_stream_meta":{
       "source":{
         "type": "kafka",
	 "topic": "my_weatherdata",
	 "partition": 0,
	 "offset": 4285
       }
     }
   }

.. note::

   The preceding is a representative example. Streaming data are
   not static, and each user sees distinct documents.
