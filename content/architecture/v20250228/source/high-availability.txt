.. meta::
   :robots: noindex, nosnippet 

.. _arch-center-high-availability:

==========================================
Guidance for {+service+} High Availability
==========================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: onecol

Consult this page to plan the appropriate cluster configuration that optimizes 
your availability and performance while aligning with your enterprise's 
cost controls and access needs.

Features for {+service+} High Availability
------------------------------------------

When you launch a new cluster in |service|, {+service+} automatically configures a 
minimum three-node replica set and distributes it in the region you deploy to. If a 
primary member experiences an outage, the MongoDB database automatically detects this failure 
and elects a secondary member as a replacement, and promotes this secondary member 
to become the new primary. |service| then restores or replaces the failing member 
to ensure that the cluster is returned to its target configuration as soon as possible. 
The MongoDB client driver also automatically switches all client connections. The 
entire selection and failover process happens within seconds, without manual 
intervention. MongoDB optimizes the algorithms used to detect failure and elect a 
new primary, reducing the failover interval. 

Clusters that you deploy within a single region are spread across availability 
zones within that region, so that they can withstand partial region outages without 
an interruption of read or write availability.

If maintaining write operations in your 
preferred region at all times is a high priority, MongoDB recommends deploying the 
cluster so that at least two electable members are in at least two data centers within 
your preferred region.

For the best database performance in a worldwide deployment, users can configure a 
:ref:`global cluster <global-clusters>` which uses location-aware sharding to minimize 
read and write latency. If you have geographical storage requirements, you can also 
ensure that {+service+} stores data in a particular geographical area.

.. _arch-center-ha-recs:

Recommendations for {+service+} High Availability
-------------------------------------------------

.. _arch-center-deployment-topologies:

Recommended Deployment Topology for Single Region
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Single Region, 3 Node Replica Set / Shard (Primary Secondary Secondary)
```````````````````````````````````````````````````````````````````````

.. figure:: /includes/images/highavailabilityPSS.svg
   :figwidth: 750px
   :alt: A topology with a single region with 3 nodes: one primary and two secondaries.

This topology is appropriate if low latency is required but high availability requirements 
are limited to a single region. This topology can tolerate any 1 node failure and easily satisfy majority write 
concern within region secondaries. This will maintain a primary in the preferred region upon any node 
failure, limits cost, and is the least complicated from an application architecture perspective. 
The reads and writes from secondaries in this topology encounter low latency in your preferred region.

This topology, however, can't tolerate a regional outage.

.. _arch-center-ha-configurations:

Recommended Configurations for High Availability and Recovery
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use the following recommendations to configure your
{+service+} deployments and backups for high availability and to
expedite recovery from disasters.

To learn more about how to plan for and respond to disasters, see
:ref:`arch-center-dr`.

Members of the Same Replica Sets Should Not Share Resources
```````````````````````````````````````````````````````````

MongoDB provides high availability by having multiple copies of data in replica sets.
Members of the same replica set don't share the same resources. For example, members
of the same replica set don't share the same physical hosts and disks.
|service| satisfies this requirement by default: it deploys nodes in
different availability zones, on different physical hosts and disks.

Ensure that replica sets don't share resources by :ref:`distributing data across data centers <arch-center-distribute-data>`.

Use an Odd Number of Replica Set Members
````````````````````````````````````````

To elect a :manual:`primary </core/replica-set-members>`, you need a majority of :manual:`voting </core/replica-set-elections>` replica set members available. We recommend that you create replica sets with an
odd number of voting replica set members. There is no benefit in having
an even number of voting replica set members. |service| satisfies this
requirement by default, as |service| requires having 3, 5, or 7 nodes.

Fault tolerance is the number of replica set members that can become
unavailable with enough members still available for a primary election. 
Fault tolerance of a four-member replica set is the same as for a three-member replica set because both can withstand a single-node
outage.

To learn more about replica set members, see :manual:`Replica Set Members </core/replica-set-members>`. To learn more about replica set elections and 
voting, see :manual:`Replica Set Elections </core/replica-set-elections>`.

.. _arch-center-distribute-data:

Distribute Data Across At Least Three Data Centers in Different Availability Zones
````````````````````````````````````````````````````````````````````````````````````

To guarantee that a replica set can elect a primary if a data center
becomes unavailable, you must distribute nodes across at least three
data centers, but we recommend that you use five data centers.

If you choose a region for your data centers that supports
availability zones, you can distribute nodes in data centers in different
availability zones. This way you can have multiple separate physical data
centers, each in its own availability zone and in the same region.

This section aims to illustrate the need for a deployment with five data centers.
To begin, consider deployments with two and three data centers.

Consider the following diagram, which shows data distributed across
two data centers:

.. figure:: /includes/images/two-data-centers.png
   :figwidth: 750px
   :alt: An image showing two data centers: Data Center 1, with a primary and a secondary node, and Data Center 2, with only a secondary node

In the previous diagram, if Data Center 2 becomes unavailable, a majority of replica set members remain available and {+service+} can elect a primary. However, if you lose Data Center 1, you have only one
out of three replica set members available, no majority, and the system degrades into read-only mode.

Consider the following diagram, which shows data distributed across
three data centers:

.. figure:: /includes/images/three-data-centers.png
   :figwidth: 750px
   :alt: An image showing three data centers: Data Center 1, with a primary node, Data Center 2, with a secondary node, and Data Center 3, with a secondary node

When you distribute nodes across three data centers, if one data
center becomes unavailable, you still have two out of three replica set
members available, which maintains a majority to elect a primary.

In addition to ensuring high availability, we recommend that you ensure the continuity of write
operations. For this reason, we recommend that you deploy five data centers,
to achieve a 2+2+1 topology required for the majority write concern.
See the following section on :ref:`majority write concern <arch-center-majority-write-concern>` in this topic for detailed explanations of this
requirement.

You can distribute data across at least three data centers within the same region by choosing a region with at least three availability zones. Each
availability zone contains one or more discrete data centers, each with redundant power, networking and connectivity, often housed in separate
facilities.

{+service+} uses availability zones for all cloud providers
automatically when you deploy a dedicated cluster to a region that supports availability zones. |service| splits the cluster's nodes across
availability zones. For example, for a three-node replica set {+cluster+} deployed to a three-availability-zone region, {+service+} deploys one node
in each zone. A local failure in the data center hosting one node doesn't
impact the operation of data centers hosting the other nodes because MongoDB
performs automatic failover and leader election. Applications automatically
recover in the event of local failures.

We recommend that you deploy replica sets to the following regions because they support at least three availability zones:

Recommended AWS Regions
#######################

.. include:: /includes/aws-recommended-regions.rst

Recommended Azure Regions
#########################

.. include:: /includes/azure-recommended-regions.rst

Recommended Google Cloud Regions
################################

.. include:: /includes/gcp-recommended-regions.rst

Use ``mongos`` Redundancy for Sharded {+Clusters+}
```````````````````````````````````````````````````

When a client connects to a sharded {+cluster+}, we recommend that you include multiple :manual:`mongos </reference/program/mongos/>`
processes, separated by commas, in the connection URI. To learn more,
see :manual:`MongoDB Connection String Examples </reference/connection-string-examples/#self-hosted-replica-set-with-members-on-different-machines>`.
This setup allows operations to route to different ``mongos`` instances
for load balancing, but it is also important for disaster recovery.

Consider the following diagram, which shows a sharded {+cluster+}
spread across three data centers. The application connects to the {+cluster+} from a remote location. If Data Center 3 becomes unavailable, the application can still connect to the ``mongos``
processes in the other data centers.

.. figure:: /includes/images/mongos-three-data-centers.png
   :figwidth: 750px
   :alt: An image showing three data centers: Data Center 1, with a primary shards and two mongos, Data Center 2, with secondary shards and two mongos, and Data Center 3, with secondary shards and two mongos. The application connects to all six mongos instances.

You can use 
:manual:`retryable reads </core/retryable-reads/>` and
:manual:`retryable writes</core/retryable-writes/>` to simplify the required error handling for the ``mongos``
configuration.

.. _arch-center-majority-write-concern:

Use ``majority`` Write Concern
``````````````````````````````

MongoDB allows you to specify the level of acknowledgment requested
for write operations by using :manual:`write concern 
</reference/write-concern/>`. For example, if
you had a three-node replica set and had a write concern of
``majority``, every write operation would need to be persisted on 
two nodes before an acknowledgment of completion sends to the driver
that issued said write operation. For the best protection from a regional
node outage, we recommend that you set the write concern to ``majority``.

Even though using ``majority`` write concern increases latency, compared
with write concern ``1``, we recommend that you use ``majority`` write
concern because it allows your data centers to continue having write
operations even if a replica set loses the primary.

To understand the importance of ``majority`` write concern, consider a
five-node replica set spread across three separate regions with a 2-2-1
topology (two regions with two nodes and one region with one node),
with a write concern of ``4``. If one of the regions with two nodes becomes
unavailable due to an outage and only three nodes are available, no write
operations complete and the operation hangs because it is unable to persist data on four nodes.
In this scenario, despite the availability of the majority of nodes in the replica set, the database
behaves the same as if a majority of the nodes in the replica set were
unavailable. If you use ``majority`` write concern rather than a numeric value, it prevents this scenario.

Consider Backup Configuration
`````````````````````````````
Frequent data backups is critical for business continuity and disaster recovery. Frequent backups ensure that data loss and downtime is
minimal if a disaster or cyber attack disrupts normal operations. 

We recommend that you:

- Set your backup frequency to meet your desired business continuity
  objectives. Continuous backups may be needed for some systems, while less frequent snapshots may be desirable for others.
- Store backups in a different physical location than the
  source data.
- Test your backup recovery process to ensure that you can restore
  backups in a repeatable and timely manner.
- Confirm that your {+clusters+} run the same MongoDB versions for
  compatibility during restore.
- Configure a :atlas:`backup compliance policy 
  </backup/cloud-backup/backup-compliance-policy/#std-label-backup-compliance-policy>` to prevent deleting backup
  snapshots, prevent decreasing the snapshot retention time, and more.

For more backup recommendations, see :ref:`arch-center-backups`.

Plan Your Resource Utilization
``````````````````````````````

To avoid resource capacity issues, we recommend that you monitor
resource utilization and hold regular capacity planning sessions.
MongoDB Professional Services offers these sessions.

Over-utilized clusters could fail causing a disaster. 
Scale up clusters to higher tiers if your utilization is regularly alerting at a steady state,
such as above 60%+ utilization for system CPU and system memory.

To view your resource utilization, see :atlas:`Monitor Real-Time Performance </real-time-performance-panel>`. To view metrics with the {+atlas-admin-api+}, see :oas-atlas-tag:`Monitoring and Logs </Monitoring-and-Logs>`.

To learn best practices for alerts and monitoring for resource
utilization, see :ref:`arch-center-monitoring-alerts`.

If you encounter resource capacity issues, see :ref:`arch-center-resource-capacity`.

Plan Your MongoDB Version Changes
`````````````````````````````````

We recommend that you run the latest MongoDB version as it allows you to
take advantage of new features and provides improved security guarantees
compared with previous versions.

Ensure that you perform MongoDB major version upgrades far before your
current version reaches `end of life <https://www.mongodb.com/legal/support-policy/lifecycles>`__. 

You can't downgrade your MongoDB version using the {+atlas-ui+}. Because of this,
when planning and executing a major version upgrade, we recommend that you
work directly with MongoDB Professional or Technical Services to help you
avoid any issues that might occur during the upgrade process.

Automation Examples: {+service+} High Availability
--------------------------------------------------

The following examples configure the Single Region, 3 Node Replica Set / Shard 
deployment topology using |service| :ref:`tools for automation <arch-center-automation>`.

These examples also apply other recommended configurations, including:

.. tabs::

   .. tab:: Dev and Test Environments
      :tabid: devtest

      .. include:: /includes/shared-settings-clusters-devtest.rst

   .. tab:: Staging and Prod Environments
      :tabid: stagingprod

      .. include:: /includes/shared-settings-clusters-stagingprod.rst

.. tabs::

   .. tab:: CLI
      :tabid: cli

      .. note::

         Before you
         can create resources with the {+atlas-cli+}, you must:

         - :atlas:`Create your paying organization 
           </billing/#configure-a-paying-organization>` and :atlas:`create an API key </configure-api-access/>` for the
           paying organization.
         - :atlascli:`Install the {+atlas-cli+} </install-atlas-cli/>` 
         - :atlascli:`Connect from the {+atlas-cli+} 
           </connect-atlas-cli/>` using the steps for :guilabel:`Programmatic Use`.

      Create One Deployment Per Project
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

      .. tabs::

         .. tab:: Dev and Test Environments
            :tabid: devtest

            For your development and testing environments, run the following command for each project. Change
            the IDs and names to use your values:

            .. include:: /includes/examples/cli-example-create-clusters-devtest.rst

         .. tab:: Staging and Prod Environments
            :tabid: stagingprod

            For your staging and production environments, create the following ``cluster.json`` file for each project. 
            Change the IDs and names to use your values:

            .. include:: /includes/examples/cli-json-example-create-clusters.rst

            After you create the ``cluster.json`` file, run the
            following command for each project. The
            command uses the ``cluster.json`` file to create a cluster.

            .. include:: /includes/examples/cli-example-create-clusters-stagingprod.rst 

      For more configuration options and info about this example, 
      see :ref:`atlas-clusters-create`.

   .. tab:: Terraform
      :tabid: Terraform

      .. note::

         Before you
         can create resources with Terraform, you must:

         - :atlas:`Create your paying organization 
           </billing/#configure-a-paying-organization>` and :atlas:`create an API key </configure-api-access/>` for the
           paying organization. Store your API key as environment
           variables by running the following command in the terminal:

           .. code-block::

              export MONGODB_ATLAS_PUBLIC_KEY="<insert your public key here>"
              export MONGODB_ATLAS_PRIVATE_KEY="<insert your private key here>"

         - `Install Terraform 
           <https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli>`__ 

      Create the Projects and Deployments
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

      .. tabs::

         .. tab:: Dev and Test Environments
            :tabid: devtest

            For your development and testing environments, create the
            following files for each application and environment 
            pair. Place the files for each application and environment
            pair in their own directory. Change the IDs and names to use your values:

            main.tf
            ```````

            .. include:: /includes/examples/tf-example-main-devtest.rst 

            variables.tf
            ````````````

            .. include:: /includes/examples/tf-example-variables.rst

            terraform.tfvars
            ````````````````

            .. include:: /includes/examples/tf-example-tfvars-devtest.rst

            provider.tf
            ```````````

            .. include:: /includes/examples/tf-example-provider.rst

            After you create the files, navigate to each application and environment pair's directory and run the following
            command to initialize Terraform:

            .. code-block::

               terraform init

            Run the following command to view the Terraform plan:

            .. code-block::

               terraform plan
            
            Run the following command to create one project and one deployment for the application and environment pair. The command uses the files and the |service-terraform| to
            create the projects and clusters:

            .. code-block::

               terraform apply

            When prompted, type ``yes`` and press :kbd:`Enter` to apply
            the configuration.

         .. tab:: Staging and Prod Environments
            :tabid: stagingprod

            For your staging and production environments, create the
            following files for each application and environment 
            pair. Place the files for each application and environment
            pair in their own directory. Change the IDs and names to use your values:

            main.tf
            ```````

            .. include:: /includes/examples/tf-example-main-stagingprod.rst 

            variables.tf
            ````````````

            .. include:: /includes/examples/tf-example-variables.rst

            terraform.tfvars
            ````````````````

            .. include:: /includes/examples/tf-example-tfvars-stagingprod.rst

            provider.tf
            ```````````

            .. include:: /includes/examples/tf-example-provider.rst

            After you create the files, navigate to each application and environment pair's directory and run the following
            command to initialize Terraform:

            .. code-block::

               terraform init

            Run the following command to view the Terraform plan:

            .. code-block::

               terraform plan
            
            Run the following command to create one project and one deployment for the application and environment pair. The command uses the files and the |service-terraform| to
            create the projects and clusters:

            .. code-block::

               terraform apply

            When prompted, type ``yes`` and press :kbd:`Enter` to apply
            the configuration. 
      
      For more configuration options and info about this example, 
      see |service-terraform| and the `MongoDB Terraform Blog Post
      <https://www.mongodb.com/developer/products/atlas/deploy-mongodb-atlas-terraform-aws/>`__.

