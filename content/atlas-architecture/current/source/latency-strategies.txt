.. _arch-center-latency-strategies:

====================================
Guidance for Atlas Latency Reduction
====================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: onecol

The following sections list configuration choices you can make to reduce
latency in your {+service+} deployment.

Physical Distance
-----------------

Physical distance is the primary cause of latency. The distance between
users and your application, between your application and your data, and
between cluster nodes all impact system latency and application
performance.

To reduce latency for read and write operations, it's crucial to place
both your application and data geographically closer to users.
Data-bearing {+service+} nodes are server nodes within an {+service+}
cluster that store your database's data and handle read and write
operations. To support faster data access for your application users,
deploy data-bearing {+service+} nodes to cloud provider regions that are
geographically close to the majority of your application users.

If your application's users are distributed across multiple geographies,
such as between the US and Europe, we recommend that you deploy to one
or more regions in each geography to reduce latency for users in each
location. To learn more about multi-region deployments, see
:ref:`arch-center-paradigms-multi-region`. 

If your data is divided by geography, such that users in each geography
access different sets of data, you can also :manual:`shard </sharding>`
your data by region or geography in order to optimize read and write
performance for users in each geography. This approach allows you to
handle large data sets and high throughput while ensuring data locality. 

Replication Configuration
-------------------------

:manual:`Replication </replication>` is the copying of data from the
primary node to secondary nodes. The following replication configuration
options can be adjusted to minimize latency for read and write
operations in a replica set:

- **Write Concern Levels:** There is a trade-off between write latency
  and write durability when you configure :manual:`write concern
  </reference/write-concern>`. MongoDB's default global write concern is
  set to ``majority``, which requires each write operation to be
  replicated across the majority of voting, data-bearing nodes in your
  replica set before {+service+} acknowledges the operation as complete
  and successful to the client. Setting a higher write concern increases
  write latency, but also improves write durability and prevents
  :ref:`rollbacks <replica-set-rollback>` during a replica set failover.

- **Read Concern and Read Preference**: There is a trade-off between
  query latency, data availability, and the consistency of your query
  responses when you configure :manual:`read concern
  </reference/read-concern>` and :manual:`read preference
  </core/read-preference>`. MongoDB's default global read concern is
  ``local``, which requires read operations to read from only one node
  in the local replica set without waiting to confirm that data is
  replicated across other nodes. Whether this node is the primary or a
  secondary node is determined by your read preference, which
  {+service+} sets to ``primary`` by default. This default read concern
  and preference combination optimizes for the lowest latency reads from
  the most up-to-date node in the replica set, but it also runs the risk
  that the primary node's data may not be durable and can potentially be
  rolled back during a failover, and that users will not be able to
  query data if there is no available primary node.

  You can change your read preference to ``primaryPreferred`` to allow
  read operations to read from a secondary node when there is no
  available primary node, or if there is a geographically closer
  secondary node, but this runs the risk of returning stale data if a
  secondary node is not up-to-date. This risk can be mitigated by
  increasing your :manual:`write concern </reference/write-concern>` to
  ensure that more secondary nodes are up-to-date, with the trade-off
  that this increases your write latency.

  .. important:: 
  
     Keep in mind that there is the possibility of a secondary node returning 
     stale data due to replication lag.
     
- **Query Timeout Limits**: You can set global and operation-level
  :manual:`query timeout limits
  </tutorial/query-documents/specify-query-timeout/>` on your deployment
  to reduce the amount of time your application waits for a response
  before timing out. This can prevent ongoing queries from negatively
  impacting deployment performance for long periods of time.

- **Node Election Priority**: To increase the likelihood that the
  members in your main data center be elected primary before the members
  in an alternate data center during a :manual:`replica set election
  </core/replica-set-elections>`, you can set the
  :rsconf:`members[n].priority <rsconf.members[n].priority>` of the
  members in the alternate data center to be lower than that of the
  members in the primary data center. For example, if you deploy your
  cluster across the {+aws+} regions ``us-east-1`` (Northern Virginia)
  and ``us-west-1`` (Northern California), and the majority of your
  users are in California, you can prioritize nodes in the {+aws+}
  ``us-west-1`` (Northern California) region to ensure that the primary
  node is always geographically close to the majority of your users and
  can respond to read and write operations with minimal latency.

- **Mirrored Reads:** Mirrored reads reduce the impact of primary
  elections following an outage by pre-warming the caches on the
  secondary nodes. For more information, see :manual:`Mirrored Reads
  </replication/#std-label-mirrored-reads>`.

For more guidance on implementing the best replication configuration for
your needs, contact {+ps+}.

Network Configuration
---------------------

You can increase security and further reduce latency using the following network 
connectivity options: 

- **Private Endpoints:** 
  :atlas:`Private endpoints </data-federation/admin/manage-private-endpoint/>` 
  establish direct and secure connections between your application's virtual 
  network and your |service| cluster, potentially reducing network hops and 
  improving latency.

- **VPC Peering:** 
  Configure :atlas:`VPC peering </atlas-stream-processing/manage-vpc-peering-connections/>` 
  in your replica sets to allow applications to connect to peered regions. 
  In the event of a failover, VPC peering provides a way for an application 
  server to detect a new primary node and route traffic accordingly. 

Data Modeling and Query Optimization
------------------------------------

The speed at which your application accesses data contributes to latency. 
Good :manual:`data modeling </data-modeling>` and 
:atlas:`query optimization </atlas-search/performance/query-performance/>` can 
improve data access speeds. For example, you can:

- **Reduce Document Size:** Consider shortening field names and value lengths to 
  decrease the amount of data transferred over the network.

- **Optimize Query Patterns:** Use indexes effectively to minimize the amount 
  of data that needs to be read across regions.

Monitoring and Testing Latency
------------------------------

{+service+} provides the :atlas:`Real-Time Performance Panel (RTPP)
</real-time-performance-panel/>` to observe latency metrics for
different regions. You can also implement application-level monitoring
to track end-to-end latency to and from the application. Before final
production deployment, we suggest conducting performance testing under
the various :ref:`multi-region scenarios
<arch-center-paradigms-multi-region>` to identify and address latency
bottlenecks. 

To learn more about monitoring your deployment, see
:ref:`arch-center-monitoring-alerts`.

Connection Configuration
------------------------

We recommend that you use a connection method built on the most
:driver:`current driver version <>` for your application's programming
language whenever possible. While the default connection string
{+service+} provides is a good place to start, you can add
:manual:`connection string options
</reference/connection-string-options/>` to your connection string to
improve performance in the context of your specific application and
deployment architecture. 

For enterprise-level application deployments, it's especially important
to :manual:`tune your connection pool settings
</tutorial/connection-pool-performance-tuning/>` to meet user demand
while minimizing operational latency. For example, you can use the
``minPoolSize`` and ``maxPoolSize`` options to adjust how and when the
majority of your database client connections open, allowing you to
prevent or plan for the latency spikes that come with the associated
network overhead. 

The extent to which you can configre these settings depends on your
deployment architecture. For example, if your application deployment is
leveraging single-threaded resources, like {+aws+} Lambda, your
application will only ever be able to open and use one client
connection. To learn more about how and where to create and use a
connection pool, and where to specify connection pool settings, see
:manual:`Connection Pool Overview
</administration/connection-pool-overview/>`. 

Example Low-Latency Application
-------------------------------

.. include:: /includes/cloud-docs/example-resilient-app.rst
