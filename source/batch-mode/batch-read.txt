.. _batch-read-from-mongodb:

===============================
Read from MongoDB in Batch Mode
===============================

.. toctree::
   :caption: Batch Read Configuration Options

   /batch-mode/batch-read-config

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol 

Overview
--------

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         .. include:: /java/read-from-mongodb.rst

     - id: python
       content: |

         .. include:: /python/read-from-mongodb.rst

     - id: scala
       content: |

         .. include:: /scala/read-from-mongodb.rst

Schema Inference
----------------

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         .. include:: /java/schema-inference.rst

     - id: python
       content: |

         .. include:: /python/schema-inference.rst

     - id: scala
       content: |

         .. include:: /scala/schema-inference.rst

.. _spark-schema-hint:

Specify Known Fields with Schema Hints
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can specify a schema containing known field values to use during
schema inference by specifying the ``schemaHint`` configuration option. You can
specify the ``schemaHint`` option in any of the following Spark formats:

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - Type
     - Format

   * - DDL
     - ``<field one name> <FIELD ONE TYPE>, <field two name> <FIELD TWO TYPE>``

   * - SQL DDL
     - ``STRUCT<<field one name>: <FIELD ONE TYPE>, <field two name>: <FIELD TWO TYPE>``

   * - JSON
     - .. code-block:: json
          :copyable: false
           
          { "type": "struct", "fields": [
          { "name": "<field name>", "type": "<field type>", "nullable": <true/false> },
          { "name": "<field name>", "type": "<field type>", "nullable": <true/false> }]}

The following example shows how to specify the ``schemaHint`` option in each
format by using the Spark shell. The example specifies a string-valued field named
``"value"`` and an integer-valued field named ``"count"``.

.. code-block:: scala

   import org.apache.spark.sql.types._

   val mySchema = StructType(Seq(
       StructField("value", StringType), 
       StructField("count", IntegerType))
   
   // Generate DDL format
   mySchema.toDDL

   // Generate SQL DDL format
   mySchema.sql

   // Generate Simple String DDL format
   mySchema.simpleString

   // Generate JSON format
   mySchema.json

You can also specify the ``schemaHint`` option in the Simple String DDL format,
or in JSON format by using PySpark, as shown in the following example:

.. code-block:: python

   from pyspark.sql.types import StructType, StructField, StringType, IntegerType
   
   mySchema = StructType([ 
      StructField('value', StringType(), True), 
      StructField('count', IntegerType(), True)])

   # Generate Simple String DDL format
   mySchema.simpleString()

   # Generate JSON format
   mySchema.json()

Filters
-------

.. tabs-drivers::

   tabs:

     - id: python
       content: |

         .. include:: /python/filters.rst

     - id: scala
       content: |

         .. include:: /scala/filters.rst
         
SQL Queries
-----------

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         .. include:: /java/sql.rst

     - id: python
       content: |

         .. include:: /python/sql.rst

     - id: scala
       content: |

         .. include:: /scala/sql.rst
  
API Documentation
-----------------

To learn more about the types used in these examples, see the following Apache Spark
API documentation:

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         - `Dataset<T> <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html>`__
         - `DataFrameReader <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html>`__

     - id: python
       content: |

         - `DataFrame <https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html>`__
         - `DataFrameReader <https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.html>`__

     - id: scala
       content: |

         - `Dataset[T] <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html>`__
         - `DataFrameReader <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameReader.html>`__