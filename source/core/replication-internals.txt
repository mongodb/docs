=====================
Replication Internals
=====================

.. default-domain:: mongodb

Synopsis
--------

This document provides a more in-depth explanation of the internals and
operation of :term:`replica set` features. This material is not necessary for
normal operation or application development but may be useful for
troubleshooting and for further understanding MongoDB's behavior and approach.

.. index:: replica set; local database
.. _replica-set-oplog:

The Local Database
------------------

The ``local`` database exists on every :program:`mongod` instance and
stores replication data specific to that instance. The ``local``
database is invisible to replication. The collections in the ``local``
database are not replicated.

When authentication is used, authenticating against the ``local`` database
is equivalent to authenticating against the admin database. It gives you
permissions across all databases, not just ``local``.

In replication, the ``local`` database is used to store internal replication
data for each member of a :term:`replica set`. The ``local`` database contains the
following collections used for replication:

.. data:: local.system.replset

   This stores the replica set's configuration object. To view the
   object's configuration information, issue :method:`rs.conf()` from
   the :program:`mongo` shell. You can also query this collection
   directly.

.. data:: local.oplog.rs

   This is the capped collection that is the :term:`oplog`. You set its
   size at creation using the :setting:`oplogSize` setting. To change
   its size once it has been created, see
   :doc:`/tutorial/change-oplog-size`. For additional information, see
   the :ref:`replica-set-internals-oplog` topic in this document and the
   :ref:`replica-set-oplog-sizing` topic in the :doc:`/core/replication`
   document.

.. data:: local.replset.minvalid

   This contains an object used internally by replica sets to track sync
   status.

In :term:`master`-:term:`slave` replication, the ``local`` database contains
the following collections:

- On the master:

  .. data:: local.oplog.$main

     This is the oplog for the master-slave configuration.

  .. data:: local.slaves

     This contains information about each slave.

- On each slave:

  .. data:: local.sources

     This contains information about the slave's master server.

.. index:: replica set; oplog
.. _replica-set-internals-oplog:

Oplog
-----

For an explanation of the oplog, see the :ref:`replica-set-oplog-sizing`
topic in the :doc:`/core/replication` document.

Under various exceptional
situations, updates to a :term:`secondary's <secondary>` oplog might 
lag behind the desired performance time. See
:ref:`Replication Lag <replica-set-replication-lag>` for details.

All members of a :term:`replica set` send heartbeats (pings) to all
other members in the set and can import operations to the local oplog
from any other member in the set.

Replica set oplog operations are :term:`idempotent`. The following
operations require idempotency:

- initial sync
- post-rollback catch-up
- sharding chunk migrations

.. todo:: Verify that "sharding chunk migrations" (above) requires
   idempotency. The wiki was unclear on the subject.

.. In 2.0, replicas would import entries from the member lowest
.. "ping," This wasn't true in 1.8 and will likely change in 2.2.

.. _replica-set-data-integrity:
.. _replica-set-implementation:

Data Integrity
--------------

.. index:: replica set; read preferences

Read Preferences
~~~~~~~~~~~~~~~~

MongoDB uses :term:`single-master replication` to ensure that the
database remains consistent. However, clients may modify the
:ref:`read preferences <replica-set-read-preference>` on a
per-connection basis in order to distribute read operations to the
:term:`secondary` members of a :term:`replica set`. Read-heavy deployments may achieve
greater query throughput by distributing reads to secondary members. But
keep in mind that replication is asynchronous; therefore, reads from
secondaries may not always reflect the latest writes to the
:term:`primary`.

.. seealso:: :ref:`replica-set-consistency`

.. note::

   Use :method:`db.getReplicationInfo()` from a secondary member
   and the :doc:`replication status </reference/replication-info>`
   output to asses the current state of replication and determine if
   there is any unintended replication delay.

.. index:: replica set; configurations
.. _replica-set-member-configurations-internals:

Member Configurations
---------------------

Replica sets can include members with the following four special
configurations that affect membership behavior:

- :ref:`Secondary-only <replica-set-secondary-only-members>` members have
  their :data:`priority <members[n].priority>` values set to ``0`` and thus
  are not eligible for election as primaries.

- :ref:`Hidden <replica-set-hidden-members>` members do not appear in the
  output of :method:`db.isMaster()`. This prevents clients
  from discovering and potentially querying the member in question.

- :ref:`Delayed <replica-set-delayed-members>` members lag a fixed period
  of time behind the primary. These members are typically used
  for disaster recovery scenarios. For example, if an administrator
  mistakenly truncates a collection, and you discover the mistake within
  the lag window, then you can manually fail over to the delayed member.

- :ref:`Arbiters <replica-set-arbiters>` exist solely to participate
  in elections. They do not replicate data from the primary.

In almost every case, replica sets simplify the process of
administering database replication. However, replica sets still have a
unique set of administrative requirements and concerns. Choosing the
right :doc:`system architecture </administration/replication-architectures>`
for your data set is crucial.

.. seealso:: The :ref:`replica-set-member-configurations` topic in the
   :doc:`/administration/replica-sets` document.

.. index:: replica set; security

Security
--------

Administrators of replica sets also have unique :ref:`monitoring
<replica-set-monitoring>` and :ref:`security <replica-set-security>`
concerns. The :ref:`replica set functions <replica-set-functions>` in
the :program:`mongo` shell, provide the tools necessary for replica set
administration. In particular use the :method:`rs.conf()` to return a
:term:`document` that holds the :doc:`replica set configuration
</reference/replica-configuration>` and use :method:`rs.reconfig()` to
modify the configuration of an existing replica set.

.. index:: replica set; elections
.. index:: replica set; failover
.. _replica-set-election-internals:

Elections
---------

Elections are the process :term:`replica set` members use to select which member should
become :term:`primary`. A primary is the only member in the replica
set that can accept write operations, including :method:`insert()
<db.collection.insert()>`, :method:`update() <db.collection.update()>`,
and :method:`remove() <db.collection.remove()>`.

The following events can trigger an election:

- You initialize a replica set for the first time.

- A primary steps down. A primary will step down in response to the
  :dbcommand:`replSetStepDown` command or if it sees that one of the
  current secondaries is eligible for election *and* has a higher
  priority. A primary also will step down when it cannot contact a
  majority of the members of the replica set. When the current primary
  steps down, it closes all open client connections to prevent clients
  from unknowingly writing data to a non-primary member.

- A :term:`secondary` member loses contact with a primary. A secondary
  will call for an election if it cannot establish a connection to a
  primary.

- A :term:`failover` occurs.

In an election, all members have one vote,
including :ref:`hidden <replica-set-hidden-members>` members, :ref:`arbiters
<replica-set-arbiters>`, and even recovering members.
Any :program:`mongod` can veto an election.

In the default configuration, all members have an equal chance of
becoming primary; however, it's possible to set :data:`priority
<members[n].priority>` values that weight the election. In some
architectures, there may be operational reasons for increasing the
likelihood of a specific replica set member becoming primary. For
instance, a member located in a remote data center should *not* become
primary. See: :ref:`replica-set-node-priority` for more
information.

Any member of a replica set can veto an election, even if the
member is a :ref:`non-voting member <replica-set-non-voting-members>`.

A member of the set will veto an election under the following
conditions:

- If the member seeking an election is not a member of the voter's set.

- If the member seeking an election is not up-to-date with the most
  recent operation accessible in the replica set.

- If the member seeking an election has a lower priority than another member
  in the set that is also eligible for election.

- If the current primary member has more recent operations
  (i.e. a higher "optime") than the member seeking election, from the
  perspective of the voting member.

- The current primary will veto an election if it has the same or
  more recent operations (i.e. a "higher or equal optime") than the
  member seeking election.

The first member to receive votes from a majority of members in a set
becomes the next primary until the next election. Be
aware of the following conditions and possible situations:

- Replica set members send heartbeats (pings) to each other every 2
  seconds. If a heartbeat does not return for more than 10 seconds,
  the other members mark the delinquent member as inaccessible.

- Replica set members compare priorities only with other members of
  the set. The absolute value of priorities does not have any impact on
  the outcome of replica set elections, with the exception of the value ``0``,
  which indicates the member cannot become primary and cannot seek election.
  For details, see :ref:`replica-set-node-priority-configuration`.

- A replica set member cannot become primary *unless* it has the
  highest "optime" of any visible member in the set.

- If the member of the set with the highest priority is within 10
  seconds of the latest :term:`oplog` entry, then the set will *not* elect a
  primary until the member with the highest priority catches up
  to the latest operation.

.. seealso:: :ref:`Non-voting members in a replica
   set <replica-set-non-voting-members>`,
   :ref:`replica-set-node-priority-configuration`, and
   :data:`replica configuration <members[n].votes>`.

.. index:: replica set; network partitions
.. index:: replica set; elections
.. _replica-set-elections-and-network-partitions:

Elections and Network Partitions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. todo:: The following two paragraphs needs review -BG

Members on either side of a network partition cannot see each other when
determining whether a majority is available to hold an election.

That means that if a primary steps down and neither side of the
partition has a majority on its own, the set will not elect a new
primary and the set will become read only. The best practice is to have
and a majority of servers in one data center and one server in another.

.. index:: replica set; sync


Syncing
-------

In order to remain up-to-date with the current state of the :term:`replica set`,
set members sync, or copy, :term:`oplog` entries from other members.

When a new member joins a set or an existing member restarts, the
member waits to receive heartbeats from other members. By
default, the member syncs from the *the closest* member of the
set that is either the primary or another secondary with more recent
oplog entries. This prevents two secondaries from syncing from each other.

In version 2.0, secondaries only change sync targets if the connection
between secondaries drops or produces an error.

For example:

#. If you have two secondary members in one data center and a primary in
   a second facility, and if you start all three instances at roughly
   the same time (i.e. with no existing data sets or oplog,) both
   secondaries will likely sync from the primary, as neither secondary
   has more recent oplog entries.

   If you restart one of the secondaries, then when it rejoins the set it
   will likely begin syncing from the other secondary, because of proximity.

#. If you have a primary in one facility and a secondary in an
   alternate facility, and if you add another secondary to the alternate
   facility, the new secondary will likely sync from the existing
   secondary because it is closer than the primary.
