=====================
|backup| Requirements
=====================

.. default-domain:: mongodb

Hardware Requirements
---------------------

Sizing By Component
~~~~~~~~~~~~~~~~~~~

MMS Package (Front-end)
```````````````````````

This package requires a minimum of 4 x 2ghz+ CPU cores and 16GB of RAM
to get started. This setup will have enough capacity to monitor and
backup approximately 200 servers. All replica set members, config
servers, and mongos servers are counted. There are no specific hard
disk requirements as all data used by this package is persisted in the
configured MongoDB databases. As more servers are monitored and backed
up additional front-end instances can be added.

Backup Daemon Package (Back-end)
````````````````````````````````

A server running the deamon package will be acting as a hidden
secondary for every replica set that it is assigned. 4 x 2ghz+ CPU
cores and 16GB of RAM will be adequate for most load generated by this
activity. Since it will not be dealing with read traffic, a server
running a Backup Daemon will typically be able to be assigned many
more replica sets then a server with production traffic. The limiting
factor on how many backups a deamon can be assigned will be disk. The
server running this package will need enough disk to hold a full copy
of every databases it is backing up. It will also need enough write
I/O throughput to handle applying oplogs to each one of those
backups.

As an example, take a sharded cluster with four shards, each being
200GB. Looking at a secondary for each one of the shards it appears
the disk averages 15MB/sec of write traffic. A Backup Daemon assigned
these four shards would need at least 800GB of disk space (in reality
more to handle growth) and that disk partition would need to be able
to write more than 60MB/sec.

Point in time restore capability requires enough space to reconstruct
a snapshot of a backup on the deamon. In the example above, doing a
point in time restore of this cluster would required another 800GB of
temporary space on the deamon during the restore. Snapshot restores do
not require any additional disk space.

MMS Metadata Database
`````````````````````

Each replica set member should have 4 x 2ghz+ CPU cores and 16GB of
RAM. 200GB of disk space will be adequate for the first 200
servers. Since this data is updated very frequently high-end disk,
preferably SSDs are recommended. If the capacity of this server is
reached it can either be upgraded or additional replica sets can be
brought online and different types of MMS data can be split between
them by changing the MMS configuration.

Snapshot Storage / Blockstore Database
``````````````````````````````````````

The amount of storage needed to store a replica set backup in the
Blockstore database is calculated by looking at the file size of the
replica set being backed up, gigabytes of oplog per hour generated by
the replica set, compression ratio of the data, and the configured
snapshot retention schedule.

Using the four shard, 200GB per shard cluster example from above, also
add 2GB/day of oplogs generated per shard or 8GB/day total across the
cluster. If the longest snapshot being stored has a one year retention
period the approximate amount of data in the blockstore will be
800GB + (8GB * 365 days) or 3720GB. If the data gets a 4:1 compression
ratio, which is an average seen in the hosted MMS, the blockstore
space required will actually be 930GB.

930GB is a very conservative number because it assumes that 8GB of
oplog in one day changes 8GB of data on disk. The other extreme is
that 8GB of oplog could all be $inc operations on the same
document. In that case, 8GB of oplog could only change 4 bytes on
disk. In practice the number will be somewhere in between depending on
the replica sets insert/update/delete patterns.

A good rule of thumb of looking at the MMS hosted service is a replica
set will take up 2x - 3x its size in the Blockstore.

Medium grade HDDs will have enough i/o throughput to handle the load
of the Blockstore. Each replica set member should have 4 x 2ghz+ CPU
cores. 8GB of RAM for every 1TB disk of Blockstore is recommended to
provide good snapshot and restore speed.

Combining Components
~~~~~~~~~~~~~~~~~~~~

Each of these components does not require its own server, the CPU and
RAM requirements can be aggregated together. Each component should
still have its own disk partition with the recommended amount of
storage space and i/o throughput.

A configuration used in the MMS hosted environment is to have multiple
RAIDs attached to each high-end physical server. Each server may be
running a combination of a Blockstore primary/secondary, a Backup
Daemon, and a MMS Metadata primary/secondary. The front-end package
can be run on a much smaller server as it only has modest CPU and RAM
requirements.

High Availability Setup
-----------------------

MMS Package (Front-end)
~~~~~~~~~~~~~~~~~~~~~~~

This component is stateless and can be made highly available using
similar strategies to other web servers. A load balancer (either layer
4 or layer 7) can be placed in front of the instances of this package
to distribute requests. If N servers are needed for the desired
capacity, more than N servers should be behind the load balancer so
that a certain number of losses does not degrade performance. N+1 or
N+2 are common configurations.

Backup Daemon Package (Back-end)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

While the system as a whole protects the deamon against data loss,
manual intervention is required if a deamon is lost. A deamon
exclusively owns a set of backups. If it is down, those backups will
not continue. There are built-in MMS alerts that will notify the
system administrator that backups for falling behind. If the deamon
can be be repaired, the backups will continue where they left off. If
it must be replaced, the backups need to be manually assigned to a
different deamon in the Admin section of the MMS UI.

The process of moving a backup from one deamon to another involves
doing a self restore of the previous snapshot from the
Blockstore. This makes the daemons somewhat ephemeral and the
durability of the backup data lives in the Blockstore replica set.

MMS Metadata Database / Blockstore Database
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

These should be standard MongoDB replica sets. All MongoDB best
practices should be followed to make these replica sets highly
available and durable.

Software Requirements
---------------------

MMS requires 64-bit Linux. MMS supports the following distributions:

- CentOS 5 or later

- Red Hat Enterprise Linux 5 or later

- Amazon Linux (latest version only)

- SLES 11 or later

- Ubuntu 12.04 or later

The MongoDB databases backing MMS best be MongoDB 2.4.6 or later.

The MongoDB replica sets and sharded clusters tobe backed up must be
running MongoDB 2.4.3 or later.

OS Configuration
----------------

Ulimit
~~~~~~

It is extremely important that ulimits be configured correctly. The
MMS/Backup HTTP server under load and the Backup Daemon launching many
``mongod`` instances will quickly hit most linux distributions default ulimits. The
following settings are recommended for the ``/etc/security/limits.conf``
file on all servers running the front-end or back-end package.

.. code-block:: none

   mongodb-mms        soft   nofile           64000
   mongodb-mms        hard   nofile           64000
   mongodb-mms        soft   nproc            32000
   mongodb-mms        hard   nproc            32000

   mongod           soft   nofile           64000
   mongod           hard   nofile           64000
   mongod           soft   nproc            32000
   mongod           hard   nproc            32000

Be sure to check for a ``/etc/security/limits.d/90-nproc.conf`` file
that may override the configured limits.

For any servers running a MMS Metadata Database or a Blockstore
Database, see the :manual:`production notes
</administration/production-notes/>` page for more specific
recommendations.

Firewall
~~~~~~~~

The front-end package will default to running web servers on ports
8080 and 8081.
