:orphan:

========================================================
Release Notes for MongoDB 2.6 (Development Series 2.5.x)
========================================================

.. default-domain:: mongodb

MongoDB 2.6 is currently in development, as part of the 2.5
development release series. While 2.5-series releases are currently
available, these versions of MongoDB, including the 2.6 release
candidate builds, are for **testing only and
not for production use**.

This document will eventually contain the full release notes for
MongoDB 2.6; before its release this document covers the 2.5
development series as a work-in-progress.

.. contents:: See the :doc:`full index of this page <2.6-changes>` for
              a complete list of changes included in 2.6 (Development
              Series 2.5.x).
   :backlinks: none
   :local:
   :depth: 2

Downloading
-----------

You can download the 2.6 release candidate on the `downloads page`_ in the
:guilabel:`Development Release (Unstable)` section. There are no
distribution packages for development releases, but you can use the
binaries provided for testing purposes. See
:doc:`/tutorial/install-mongodb-on-linux`,
:doc:`/tutorial/install-mongodb-on-windows`, or
:doc:`/tutorial/install-mongodb-on-os-x` for the basic installation
process.

.. _`downloads page`: http://www.mongodb.org/downloads

Changes
-------

.. important:: The MongoDB 2.5-series, which will become MongoDB 2.6,
   is for testing and development **only**. All identifiers, names,
   interfaces are subject to change. Do **not** use a MongoDB 2.5
   release in production situations.

SNMP Enterprise Identifier Changed
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. DOCS-1696

In 2.5.1, the IANA enterprise identifier for MongoDB changed from
37601 to 34601. Users of SNMP monitoring must modify their SNMP
configuration (i.e. MIB) accordingly.

Default ``bind_ip`` for RPM and DEB Packages
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the packages provided by 10gen for MongoDB in RPM (Red Hat, CentOS,
Fedora Linux, and derivatives) and DEB (Debian, Ubuntu, and
derivatives,) the default :setting:`bind_ip` value attaches MongoDB
components to the localhost interface *only*. These packages set this
default in the default configuration file
(i.e. ``/etc/mongodb.conf``.)

If you use one of these packages and have *not* modified the default
``/etc/mongodb.conf`` file, you will need to set :setting:`bind_ip`
before or during the upgrade.

There is no default ``bind_ip`` setting in any other 10gen distributions
of MongoDB.

Aggregation Pipeline Changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``$out`` Stage to Write Data to a Collection
````````````````````````````````````````````

.. pipeline:: $out

   In an aggregation pipeline you may specify the name of a collection
   to a :pipeline:`$out` stage, which will write all documents in the
   aggregation pipeline to that collection.

   .. important:: The input collection for the aggregation may be
      sharded; however, you may not specify a sharded collection to
      :pipeline:`$out`.

   :pipeline:`$out` will create a new collection if one does not
   already exist in the current database. This collection is not
   visible until the aggregation completes. If the aggregation fails,
   MongoDB will not create any collection.
   
   If the output collection already exists, when the aggregation
   completes *successfully*, the :pipeline:`$out` atomically replaces
   the output collection with the new results
   collection. :pipeline:`$out` does not change any indexes that
   existed on the previous collection.

   .. important:: The pipeline will fail to complete if the documents
      produced by the pipeline would violate any unique indexes,
      including the index on the ``_id`` field, that existed on the
      original output collection.
   
   You may *only* specify :pipeline:`$out` at the end of a pipeline.

With :pipeline:`$out`, the aggregation framework can return result
sets of any size.

.. example::

   The following operation will insert all documents in ``records``
   collection into a collection named ``users``. The inserted
   documents will *only* have the ``_id``, ``uid``, and ``email``
   fields from the source documents:

   .. code-block:: javascript

      db.records.aggregate( { $project: { uid: 1, email: 1 } },
                            { $out: "users" } )

.. [#same-as-replace] When the :pipeline:`$out` operation specifies
   a collection that already exists in the current database,
   :pipeline:`$out` behaves like the replace mode of
   :dbcommand:`mapReduce`.

Aggregation Operation May Return a Cursor
`````````````````````````````````````````

The :dbcommand:`aggregate` may now return a cursor rather than a
result document. Specify the ``cursor`` operation and an initial batch
size as options to the :dbcommand:`aggregate` command to return a
cursor object. In 2.5.2, the *temporary* :program:`mongo` shell helper
``db.collection.aggregateCursor()`` provides access to this
capability.

By returning a cursor, aggregation pipelines can return result sets of
any size. In previous versions, the result of an aggregation operation
could be no larger than 16 megabytes.

The following command will return an aggregation cursor:

.. code-block:: javascript

   db.runCommand(
      { aggregate: "records",
        pipeline: [
           { $project: { name: { $toUpper: "$_id" }, _id: 0 } },
           { $sort: { name: 1 } }
        ],
        cursor: {}
      }
   )

In the :program:`mongo` shell, you may use the ``aggregateCursor()``
method:

.. code-block:: javascript

   db.records.aggregateCursor(
     [
        { $project : { name: { $toUpper: "$_id" }, _id: 0 } },
        { $sort : { name: 1 } }
     ]
   )


Cursors returned from the aggregation command are equivalent to
cursors returned from :method:`~db.collection.find()` queries in every
way.

You may specify an initial batch size using the following form:

.. code-block:: javascript

   db.runCommand(
      { aggregate: "records",
        pipeline: [
           { $project : { name: { $toUpper: "$_id" }, _id: 0 } },
           { $sort : { name : 1 } }
        ],
        cursor: { batchSize: 10 }
      }
   )

The ``{batchSize: 10 }`` document specifies the size of the *initial*
batch size only. Specify subsequent batch sizes to :ref:`OP_GET_MORE
<wire-op-get-more>` operations as with other MongoDB cursors.

Improved Sorting
````````````````

.. versionadded:: 2.5.2

The :pipeline:`$sort` and :pipeline:`$group` stages now use a more
efficient sorting system within the :program:`mongod` process. This
improves performance for sorting operations that cannot rely on an
index or that exceed the maximum memory use limit, see
:limit:`Aggregation Sort Operation` for more information.

For large sort operations these "external sort" operations can write
data to the ``_tmp`` directory in the :setting:`dbpath` directory. To
enable external sort use the new ``allowDiskUsage`` argument to the
:dbcommand:`aggregate`, as in the following prototype:

.. code-block:: javascript

   {
     aggregate: "<collection>",
     pipeline: [<pipeline>],
     allowDiskUsage: <boolean>
   }

You can run this pipeline operation, using the following command:

.. code-block:: javascript

   db.runCommand(
      { aggregate: "records",
        pipeline: [
           { $project : { name: { $toUpper: "$_id" }, _id: 0 } },
           { $sort : { name : 1 } }
        ],
        cursor: { batchSize: 10 },
        allowDiskUsage: true
      }
   )

``$redact`` Stage to Provide Filtering for Field-Level Access Control
`````````````````````````````````````````````````````````````````````

.. pipeline:: $redact

   .. versionadded:: 2.5.2

   Provides a method to restrict the content of a returned document on
   a per-field level.

   .. example::

      Given a collection with the following document in a ``test``
      collection:

      .. code-block:: javascript

         { a: {
             level: 1,
             b: {
                 level: 5,
                 c: {
                     level: 1,
                     message: "Hello"
                 }
             },
             d: "World."
         }

     Consider the following aggregation operation:

     .. code-block:: javascript

        db.test.aggregate(
           { $match: {} },
           { $redact: { $cond: [ { $lt: [ '$level', 3 ] },
                                 "$$CONTINUE",
                                 "$$PRUNE" ]
                      }
           }
        )

     This operation evaluates every field at every level for all
     documents in the ``test`` collection, and uses the
     :expression:`$cond` expression and the variables ``$$CONTINUE``
     and ``$$PRUNE`` to specify the redaction of document parts in the
     aggregation pipeline.

     .. important:: ``$$CONTINUE`` will become ``$$DESCEND`` in
        2.5.3.

     Specifically, if the field ``level`` is less than 3,
     (i.e. ``{ $lt: [ '$level', 3' ] }``) :pipeline:`$redact`
     continues (i.e. ``$$CONTINUE``) evaluating the fields and
     sub-documents at this level of the input document. If the value
     of ``level`` is greater than ``3``, then :pipeline:`$redact`
     removes all data at this level of the document.

     The result of this aggregation operation is as follows:

      .. code-block:: javascript

         { a: {
             level: 1,
             d: "World."
         }

     You may also specify ``$$KEEP`` as a variable to
     :expression:`$cond`, which returns the entire sub-document
     without transversing, as ``$$CONTINUE``.

     .. START-COMMENT

        In 2.5.3, :expression:`$cond` will accept a document with
        named parameters in addition to the current ordered array of
        parameters.

        In 2.5.3, ``$$CONTINUE`` will become ``$$DESCEND``.

     .. END-COMMENT

     .. see:: :expression:`$cond`.

Set Expression Operations in ``$project``
`````````````````````````````````````````

In 2.5.2, the :pipeline:`$project` aggregation pipeline stage now
supports the following set expressions:

.. important:: Set operators take arrays as their arguments and treat
   these arrays as sets. Except for :expression:`$setDifference`, the
   set operators ignore duplicate entries in an input array and
   produce arrays containing unique entries.

.. expression:: $setIsSubset

   Takes two arrays and returns ``true`` when the first array is a
   subset of the second and ``false`` otherwise.

.. expression:: $setEquals

   Takes two arrays and returns ``true`` when they contain the same
   elements, and ``false`` otherwise.

.. expression:: $setDifference

   Takes two arrays and returns an array containing the elements that
   only exist in the first array. :expression:`$setDifference` may
   produce arrays containing duplicate items in 2.5.2 if the input
   array contains duplicate items.

.. expression:: $setIntersection

   Takes any number of arrays and returns an array that contains the
   elements that appear in every input array.

.. expression:: $setUnion

   Takes any number of arrays and returns an array that containing the
   elements that appear in any input array.

.. expression:: $all

   Takes a single array and returns ``true`` if all its values are
   ``true`` and ``false`` otherwise.

   :expression:`$all` will become ``$allElementsTrue`` in 2.5.3.

.. expression:: $any

   Takes a single expression that returns an array and returns
   ``true`` if any of its values are ``true`` and ``false`` otherwise.

   :expression:`$any` will become ``$anyElementTrue`` in 2.5.3.

``$map`` and ``$let`` Expressions in Aggregation Pipeline Stages
````````````````````````````````````````````````````````````````

.. tip:: For :expression:`$let` and :expression:`$map`, the
   aggregation framework introduces variables. To specify a variable,
   use the name of the variable prefixed by **2** dollar signs
   (i.e. ``$$``) as in: ``$$<name>``.

.. expression:: $let

   :expression:`$let` binds variables for use in sub-expressions. For
   example:

   .. code-block:: javascript

      { $project: { $let: { vars: { tally: 75, count: 50 } },
                            in: { remaining: { $subtract: [ "$$tally", "$$count" ] } } } }

   Would return a document with the following:

   .. code-block:: javascript

      { remaining: 25 }

   :expression:`$let` is available the in :pipeline:`$project`,
   :pipeline:`$group`, and :pipeline:`$redact` pipeline stages.

.. expression:: $map

   :expression:`$map` applies a sub-expression to each item in an
   array and returns an array with the result of the sub-expression

   :expression:`$map` is available the in :pipeline:`$project`,
   :pipeline:`$group`, and :pipeline:`$redact` pipeline stages.

   Given an input document that resembles the following:

   .. code-block:: javascript

      { skews: [ 1, 1, 2, 3, 5, 8 ] }

   And the following :pipeline:`$project` statement:

   .. code-block:: javascript

      { $project: { adjustments: { $map: { input: $skews,
                                           as: "adj",
                                           in: { $add: [ "$$adj", 12 ] } } } } }

   The :expression:`$map` would transform the input document into the
   following output document:

   .. code-block:: javascript

      { adjustments: [ 13, 13, 14, 15, 17, 20 ] }

``$literal`` Expression for Aggregation Pipeline Stages
```````````````````````````````````````````````````````

The new :expression:`$literal` operator allows users to explicitly
specify documents in aggregation operations that the pipeline stage
would otherwise interpret directly. For example, use
:expression:`$literal` to project fields with dollar signs
(e.g. ``$``) in their values.

.. expression:: $literal

   Wraps an expression to prevent the aggregation pipeline from
   interpreting an object directly.

Consider the following example:

.. code-block:: javascript

   db.runCommand( { aggregate: "records",
                    pipeline: [  { $project: { costsOneDollar:
                                      { $eq: [ "$price", { $literal: "$1.00" } ] } }
                                 } ] } )

This projects documents with a field named ``costsOneDollar`` that
holds a boolean value if the value of the field is the string
``$1.00``.

Update Improvements
~~~~~~~~~~~~~~~~~~~

MongoDB 2.5.2 includes a new subsystem which provides more reliable
and extensible update operations. 2.5.2 introduces two new changes to
the update language:

``$mul`` Update Operator
````````````````````````

.. operator:: $mul

   The new :operator:`$mul` allows you to multiply the value of a
   field by the specified amount. If the field does not exist in a
   document, :operator:`$mul` sets field to the specified
   amount. Consider the following prototype:

   the value of the field to the value
   specified to :operator:`$mul`

   .. example::

      Given a collection named ``records`` with the following
      documents:

      .. code-block:: javascript

         { _id: 1, a: 1, b: 4 }
         { _id: 2, a: 1, b: 3 }
         { _id: 3, a: 1 }

      Consider the following update operation:

      .. code-block:: javascript

         db.records.update( { a: 1 },
                            { $mul: { b: 5 } },
                            { multi: true } )

      Following this operation, the collection would resemble the
      following:

      .. code-block:: javascript

         { _id: 1, a: 1, b: 20 }
         { _id: 2, a: 1, b: 15 }
         { _id: 3, a: 1, b: 5 }

``xor`` operation for ``$bit`` Operator
```````````````````````````````````````

The :operator:`$bit` now supports bitwise updates using a logical
``xor`` operation. See the documentation of :operator:`$bit` for more
information on bitwise updates. Consider the following operation:

.. code-block:: javascript

   db.collection.update( { field: NumberInt(1) }, { $bit: { field: { xor: NumberInt(5) } } } );

Sharding Improvements
~~~~~~~~~~~~~~~~~~~~~

Support for Removing Orphan Data From Shards
````````````````````````````````````````````

.. versionadded:: 2.5.2

The new :dbcommand:`cleanupOrphaned` is available to remove orphaned
data from a sharded cluster. Orphaned data are those documents in a
collection that exist on shards that belong to another shard in the
cluster. Orphaned data are the result of failed migrations or incomplete
migration cleanup due to abnormal shutdown.

.. dbcommand:: cleanupOrphaned

   :dbcommand:`cleanupOrphaned` removes documents from *one* orphaned
   chunk range on shard and runs directly on the shard's
   :program:`mongod` instance, and requires the
   :authrole:`clusterAdmin` for systems running with :setting:`auth`.
   You do **not** need to disable the :term:`balancer` to run
   :dbcommand:`cleanupOrphaned`.

   In the common case, :dbcommand:`cleanupOrphaned` takes the
   following form:

   .. code-block:: javascript

      { cleanupOrphaned: <namespace>, startingFromKey: { } }

   The ``startingFromField`` field specifies the lowest value of the
   :term:`shard key` to begin searching for orphaned chunks. The empty
   document (i.e. ``{ }``) is equivalent to the minimum value for the
   shard key (i.e. ``$minValue``).

   The :dbcommand:`cleanupOrphaned` command returns a document that
   contains a field named ``stoppedAtKey`` that you can use to
   construct a loop, as in the following example in the
   :program:`mongo` shell:

   .. code-block:: javascript

      use admin

      var nextKey = {};

      while (
         nextKey = db.runCommand( { cleanupOrphaned : "test.user", startingFromKey : nextKey } ).stoppedAtKey
         ) { printjson(nextKey); }

   This loop remove all orphaned data on the shard.

Ability to Merge Co-located Contiguous Chunks
`````````````````````````````````````````````

.. versionadded:: 2.5.2

The :dbcommand:`mergeChunks` provides the ability for users to combine
contiguous chunks located on a single shard. This makes it possible to
combine chunks in situations where document removal leaves a sharded
collection with too many empty chunks.

.. dbcommand:: mergeChunks

   Combines two contiguous chunks located on the same shard. The
   :dbcommand:`mergeChunks` takes the following form:

   .. code-block:: javascript

      { mergeChunks: <namespace>, bounds: [ <minKey>, <maxKey> ] }

   The ``bounds`` option specified to :dbcommand:`mergeChunks` *must*
   be the boundaries of existing chunks in the collections. See the
   output of :method:`sh.status()` to see the current shard
   boundaries.

   .. important:: Both chunks **must** reside on the same shard.

   .. tip:: In 2.5.2, :dbcommand:`mergeChunks` has the following
      restrictions, which are subject to change in future releases:

      - :dbcommand:`mergeChunks` will only merge two chunks.

      - one chunk **must** not hold any documents (i.e. an "empty
        chunk").

Support for Auditing
~~~~~~~~~~~~~~~~~~~~

.. versionadded:: 2.5.2

MongoDB adds features to audit server and client activity for
:program:`mongod` and :program:`mongos` instances.

.. important:: Auditing, like all new features in 2.5.2, is in ongoing
   development. Specifically the interface, output format, audited
   events and the structure of audited events will change
   significantly in the in the 2.5 series before the release of 2.6.

To enable auditing, start :program:`mongod` or :program:`mongos` with
the following argument:

.. code-block:: sh

   --setParameter auditLogPath=<option>

The value of ``<option>`` is one of the following:

- a path. This may be the same as the :setting:`logpath` for MongoDB's
  process log. If you specify the same log file, then the path
  specification must be *exactly* the same as the :setting:`logpath`
  specification.

- the string ``:console`` to output audit log messages to standard
  output.

- the string ``:syslog`` to output audit log messages to the system's
  syslog facility.

.. tip:: As of 2.5.2 auditing does not support filtering of audit
   events.

``isMaster`` Comand includes Wire Protocol Versions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionadded:: 2.5.2

In order to support changes to the wire protocol both now and in the
future, the output of :dbcommand:`isMaster` now contains two new
fields that report the earliest version of the wire protocol that this
:program:`mongod` instance supports and the highest version of the
wire protocol that this :program:`mongo` instance supports.

.. data:: isMaster.maxWireVersion

   The latest version of the wire protocol that this :program:`mongod`
   or :program:`mongos` instance is capable of using to communicate
   with clients.

.. data:: isMaster.minWireVersion

   The earliest version of the wire protocol that this
   :program:`mongod` or :program:`mongos` instance is capable of using
   to communicate with clients.

SASL Library Change
~~~~~~~~~~~~~~~~~~~

MongoDB Enterprise uses Cyrus SASL instead of GNU SASL (``libgsasl``).
This change has the following SASL2 and Cyrus SASL library and GSSAPI
plugin dependencies:

For Debian or Ubuntu, install the following:

.. code-block:: sh

   sudo apt-get install cyrus-sasl2-dbg cyrus-sasl2-mit-dbg libsasl2-2 libsasl2-dev libsasl2-modules libsasl2-modules-gssapi-mit

For CentOS, Red Hat Enterprise Linux, and Amazon AMI, install the
following:

.. code-block:: sh

   sudo yum install cyrus-sasl cyrus-sasl-lib cyrus-sasl-devel cyrus-sasl-gssapi

For SUSE, install the following:

.. code-block:: sh

   sudo zypper install cyrus-sasl cyrus-sasl-devel cyrus-sasl-gssapi

LDAP Support for Authentication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

MongoDB Enterprise provides support for proxy authentication of users.  This
change allows administrators to configure a MongoDB cluster to authenticate
users via Linux PAM or by proxying authentication requests to a specified LDAP
service.

.. warning::

   Because this change uses ``SASL PLAIN`` mechanism to transmit the
   user password to the MongoDB server, you should, in general, use
   only on a trusted channel (VPN, SSL, trusted wired network).

Configuration
`````````````

LDAP support for user authentication requires proper configuration of
the ``saslauthd`` daemon process as well as introduces a new server
parameter, ``saslauthdPath``. ``saslauthdPath`` is the path to the Unix
Domain Socket of the ``saslauthd`` instance to use for proxy
authentication.

``saslauthd`` Configuration
^^^^^^^^^^^^^^^^^^^^^^^^^^^

On systems that configure ``saslauthd`` with a
``/etc/sysconfig/saslauthd`` file, such as Red Hat Enterprise Linux,
Fedora, CentOS, Amazon Linux AMI, set the mechanism ``MECH`` to
``ldap``:

.. code-block:: none

   MECH=ldap

On systems that configure ``saslauthd`` with a
``/etc/default/saslauthd`` file, set the mechanisms option to
``ldap``:

.. code-block:: none

   MECHANISMS="ldap"

To use with *ActiveDirectory*, start ``saslauthd`` with the following
configuration options:

.. code-block:: none

   ldap_servers: <ldap uri, e.g. ldaps://ad.example.net>
   ldap_use_sasl: yes
   ldap_mech: DIGEST-MD5
   ldap_auth_method: fastbind

To connect to an OpenLDAP server, use a test ``saslauthd.conf`` with
the following content:

.. code-block:: none

   ldap_servers: <ldap uri, e.g. ldaps://ad.example.net>
   ldap_search_base: ou=Users,dc=example,dc=com
   ldap_filter: (uid=%u)

To use this sample OpenLDAP configuration, create users with a ``uid``
attribute (login name) and place under the ``Users`` organizational
unit (``ou``).

To test the ``saslauthd`` configuration, use ``testsaslauthd`` utility,
as in the following example:

.. code-block:: sh

   testsaslauthd -u testuser -p testpassword -s mongod -f /var/run/saslauthd/mux

For more information on ``saslauthd`` configuration, see
`<http://www.openldap.org/doc/admin24/guide.html#Configuring saslauthd>`_.

MongoDB Server Configuration
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Configure the MongoDB server with the ``authenticationMechanisms``
parameter and the ``saslauthdPath`` parameters using either the command
line option :option:`--setParameter <mongod --setParameter>` or the
:doc:`configuration file </reference/configuration-options>`:

- If ``saslauthd`` has a socket path of ``/<some>/<path>/saslauthd``,
  set the ``saslauthdPath`` parameter to
  ``/<some>/<path>/saslauthd/mux`` and the ``authenticationMechanisms``
  parameter to ``PLAIN``, as in the following command line example:

  .. code-block:: sh

     mongod --setParameter saslauthdPath=/<some>/<path>/saslauthd/mux --setParameter authenticationMechanisms=PLAIN

  Or to set the configuration in the :doc:`configuration file
  </reference/configuration-options>`, add the parameters:

  .. code-block:: sh

     setParameter=saslauthdPath=/<some>/<path>/saslauthd/mux
     setParameter=authenticationMechanisms=PLAIN

- Otherwise, set the ``saslauthdPath`` to the empty string ``""`` to use
  the library's default value and the ``authenticationMechanisms``
  parameter to ``PLAIN``, as in the following command line example:

  .. code-block:: sh

     mongod --setParameter saslauthdPath="" --setParameter authenticationMechanisms=PLAIN

  Or to set the configuration in the :doc:`configuration file
  </reference/configuration-options>`, add the parameters:

  .. code-block:: sh

     setParameter=saslauthdPath=""
     setParameter=authenticationMechanisms=PLAIN

Authenticate in the ``mongo`` Shell
```````````````````````````````````

To use this authentication mechanism in the :program:`mongo` shell, you
**must** pass ``digestPassword: false`` to :method:`db.auth()` when
authenticating on the ``$external`` database, since the server must
receive an undigested password to forward on to ``saslauthd``, as in
the following example:

.. code-block:: javascript

   use $external
   db.auth(
            {
              mechanism: "PLAIN",
              user: "application/reporting@EXAMPLE.NET",
              pwd: "some1nterestingPwd",
              digestPassword: false
            }
          )

x.509 Authentication
~~~~~~~~~~~~~~~~~~~~

MongoDB introduces x.509 certificate authentication for use with a
secure :doc:`SSL connection </tutorial/configure-ssl>`.

.. important:: To use SSL, you must either use MongoDB Enterprise or
   build MongoDB locally using ``scons`` with the ``--ssl`` option.

The x.509 authentication allows clients to authenticate to servers with
certificates instead of with username and password.

The x.509 authentication also allows sharded cluster members and
replica set members to use x.509 certificates to verify their
membership to the cluster or the replica set instead of using key
files. The membership authentication is an internal process.

.. _`default distribution of MongoDB`: http://www.mongodb.org/downloads
.. _`MongoDB Enterprise`: http://www.mongodb.com/products/mongodb-enterprise

x.509 Certificate
`````````````````

The x.509 certificate for client authentication and the x.509
certificate for internal authentication have different properties.

The client certificate must have the following
properties:

- A single Certificate Authority (CA) must issue the certificates
  for both the client and the server.

- Client certificates must contain the following fields:

  .. code-block:: none

     keyUsage = digitalSignature
     extendedKeyUsage = clientAuth

The member certificate, used for internal authentication to verify
membership to the sharded cluster or a replica set, must have the
following properties:

- A single Certificate Authority (CA) must issue all the x.509
  certificates for the members of a sharded cluster or a replica set.

- The member certificate's ``subject``, which contains the
  Distinguished Name (``DN``), must match the ``subject`` of the
  certificate on the server, *starting from and including* the
  Organizational Unit (``OU``) of the certificate on the server.

New Protocol and Parameters
```````````````````````````

The change for x.509 authentication introduces a new ``MONGODB-X509``
protocol. For internal authentication for membership, the change also
introduces the ``--clusterAuthMode``, ``--sslClusterFile`` and the
``--sslClusterPassword`` options.

Use the ``--clusterAuthMode`` option to enable internal x.509
authentication for membership. The ``--clusterAuthMode`` option can
have one of the following values:

.. list-table::
   :header-rows: 1
   :widths: 20 40

   * - Value

     - Description

   * - ``keyfile``

     - Default value. Use keyfile for authentication.

   * - ``sendKeyfile``

     - For rolling upgrade purposes. Send the keyfile for
       authentication but can accept either keyfile or x.509
       certificate.

   * - ``sendX509``

     - For rolling upgrade purposes. Send the x.509 certificate for
       authentication but can accept either keyfile or x.509
       certificate.

   * - ``x509``

     - Recommended. Send the x.509 certificate for authentication and
       accept **only** x.509 certificate.

For the ``--sslClusterFile`` option, specify the full path to the x.509
certificate and key PEM file for the cluster or set member. If the key
is encrypted, specify the password with the ``--sslClusterPassword``
option.

Configure MongoDB Server to Use x.509
`````````````````````````````````````

Configure the MongoDB server from the command line, as in the following
[#additionalOptions]_:

.. code-block:: sh

   mongod --sslOnNormalPorts --sslPEMKeyFile <path to sslCertificate and key PEM file> --sslCAFile <path to root CA PEM file>

You may also specify these options in the :doc:`configuration file
</reference/configuration-options>`:

.. code-block:: none

   sslOnNormalPorts = true
   sslPEMKeyFile = <path to sslCertificate and key PEM file>
   sslCAFile = <path to the root CA PEM file>

To specify the x.509 certificate for internal cluster member
authentication, append the additional SSL options ``--clusterAuthMode``
and ``--sslClusterFile``, as in the following example for a member of a
replica set [#additionalOptions]_:

.. code-block:: sh

   mongod --replSet <name> --sslOnNormalPorts --sslPEMKeyFile <path to sslCertificate and key PEM file>  --sslCAFile <path to root CA PEM file>  --clusterAuthMode x509 --sslClusterFile <path to membership certificate and key PEM file>

.. [#additionalOptions] Include any additional options, SSL
   or otherwise, that are required for your specific configuration.

Authenticate with a x.509 Certificate
`````````````````````````````````````

To authenticate with a client certificate, you must first add a MongoDB
user that corresponds to the client certificate. See
:ref:`addX509SubjectUser`.

To authenticate, use the :method:`db.auth()` method in the
``$external`` database. For the ``mechanism`` field, specify
``"MONGODB-X509"``, and for the ``user`` field, specify the user, or
the ``subject``, that corresponds to the client certificate.

For example, if using the :program:`mongo` shell,

1. Connect :program:`mongo` shell to the :program:`mongod` set up for
   SSL:

   .. code-block:: sh

      mongo --ssl --sslPEMKeyFile <path to CA signed client PEM file>

#. To perform the authentication, use the :method:`db.auth()` method in the
   ``$external`` database.

   .. code-block:: javascript

      db.getSiblingDB("$external").auth(
                                         {
                                           mechanism: "MONGODB-X509",
                                           user: "CN=myName,OU=myOrgUnit,O=myOrg,L=myLocality,ST=myState,C=myCountry"
                                         }
                                       )

.. _addX509SubjectUser:

Add x.509 Certificate ``subject`` as a User
```````````````````````````````````````````

To authenticate with a client certificate, you must first add the value
of the ``subject`` from the client certificate as a MongoDB user.

1. You can retrieve the ``subject`` from the client certificate with
   the following command:

   .. code-block:: sh

      openssl x509 -in <pathToClient PEM> -inform PEM -subject -nameopt RFC2253

   The command returns the ``subject`` string as well as certificate:

   .. code-block:: sh

      subject= CN=myName,OU=myOrgUnit,O=myOrg,L=myLocality,ST=myState,C=myCountry
      -----BEGIN CERTIFICATE-----
      # ...
      -----END CERTIFICATE-----

2. Add the value of the ``subject``, omitting the spaces, from the
   certificate as a user. For example, in the :program:`mongo`
   shell, to add the user to the ``test`` database:

   .. code-block:: javascript

      use test
      db.addUser({
                  user: "CN=myName,OU=myOrgUnit,O=myOrg,L=myLocality,ST=myState,C=myCountry",
                  userSource: '$external',
                  roles: ['readAnyDatabase', 'readWriteAnyDatabase']
                })

See :doc:`/tutorial/add-user-to-database` for details on adding a user
with roles using :doc:`privilege documents
</reference/privilege-documents>`.

Upgrade Clusters to x.509 Authentication
````````````````````````````````````````

To upgrade clusters that are currently using keyfile authentication to
x.509 authentication, use a rolling upgrade process:

1. For each node of a cluster, set ``--clusterAuthMode`` to
   ``sendKeyFile``. With this setting, each node continues to use its
   keyfile to authenticate itself as a member. However, each node can now
   accept either a keyfile or the x.509 certificate from other members to
   authenticate those members. Upgrade all nodes of the cluster to this
   setting.

2. Then, for each node of a cluster, set ``--clusterAuthMode`` to
   ``sendX509`` and set ``--sslClusterFile`` to the appropriate path of
   the node's certificate. [#encryptedKey]_ With this setting, each
   node uses its x.509 certificate to authenticate itself as a member.
   However, each node continues to accept either a keyfile or the x.509
   certificate from other members to authenticate those members.
   Upgrade all nodes of the cluster to this setting.

3. Optional but recommended. Finally, for each node of the cluster, set
   ``--clusterAuthMode`` to ``x509`` to only use the x.509 certificate
   for authentication.

.. [#encryptedKey] If the key is encrypted, set the
   ``--sslClusterPassword`` to the password to decrypt the key.

Limit for ``maxConns`` Removed
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Starting in MongoDB 2.5.0, there is no longer any upward limit for the
:setting:`maxConns`, or :program:`mongod --maxConns` and
:program:`mongos --maxConns` options. Previous versions capped the
maximum possible :setting:`maxConns` setting at ``20,000``
connections.

Background Index Builds Replicate to Secondaries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Starting in MongoDB 2.5.0, if you initiate a :ref:`background index
build <index-creation-background>` on a :term:`primary`, the
secondaries will replicate the index build in the background.
In previous versions of MongoDB, secondaries built all indexes in the
foreground, even if the primary built an index in the background.

For all index builds, secondaries will not begin building indexes
until the primary has successfully completed the index build.

``mongod`` Automatically Continues in Progress Index Builds Following Restart
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If your :program:`mongod` instance was building an index when it
shutdown or terminated, :program:`mongod` will now continue building
the index when the :program:`mongod` restarts. Previously, the index
build *had* to finish building before :program:`mongod` shutdown.

To disable this behavior the 2.5 series adds a new run time option,
:setting:`noIndexBuildRetry` (or via, ``--noIndexBuildRetry`` on the
command line,) for :program:`mongod`. :setting:`noIndexBuildRetry`
prevents :program:`mongod` from continuing rebuilding indexes that did
not finished building when the :program:`mongod` last shut down.

.. setting:: noIndexBuildRetry

   By default, :program:`mongod` will attempt to rebuild indexes upon
   start-up *if* :program:`mongod` shuts down or stops in the middle
   of an index build. When enabled, this option prevents this
   behavior.
