===================
Backup Preparations
===================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: twocols

.. |san| replace:: :abbr:`SAN (Storage Area Network)`
.. |nas| replace:: :abbr:`NAS (Network Attached Storage)`

Overview
--------

Before backing up your cluster or replica set, decide how to back up
the data and what data to back up. This page describes items you must
consider before starting a backup.

.. important::

   .. include:: /includes/fact-backup-standalone-restriction.rst

For an overview of how Backup works, see
:ref:`mms-backup-functional-overview`.

.. include:: /includes/extracts/backup-config-options.rst

.. _snapshot-frequency-and-retention:

Snapshot Frequency and Retention Policy
---------------------------------------

.. only:: onprem

   By default, |mms| takes a base snapshot of your data every 24 hours. 

.. only:: cloud

   By default, |mms| takes a base snapshot of your data every 6 hours. 

If desired, administrators can change the frequency of base snapshots to 6, 8,
12, or 24 hours. |mms| creates snapshots automatically on a schedule. You
cannot take snapshots on demand.

|mms| retains snapshots for the time periods listed in the following
table. If you terminate a backup, |mms| immediately deletes the backup's
snapshots.

.. only:: onprem

   .. list-table::
      :widths: 30 30 40
      :header-rows: 1

      * - Snapshot
        - Default Retention Policy
        - Maximum Retention Policy
      * - Base snapshot
        - 2 days
        - 5 days
      * - Daily snapshot
        - 0 days
        - 1 year
      * - Weekly snapshot
        - 2 weeks
        - 1 year
      * - Monthly snapshot
        - 1 month
        - 3 years

.. only:: cloud

   .. list-table::
      :widths: 30 30 40
      :header-rows: 1

      * - Snapshot
        - Default Retention Policy
        - Maximum Retention Policy
      * - Base snapshot
        - 2 days
        - 5 days
      * - Daily snapshot
        - 7 days
        - 1 year
      * - Weekly snapshot
        - 4 weeks
        - 1 year
      * - Monthly snapshot
        - 13 months
        - 3 years

.. include:: /includes/extracts/backup-preparations-pricing.rst

You can change a backed-up deployment's schedule through its
:guilabel:`Edit Snapshot Schedule` menu option, available through the
:guilabel:`Backup` page. Administrators can change snapshot frequency and
retention through the :doc:`snapshotSchedule resource in the API
</reference/api/snapshot-schedule>`. If you change the schedule to save
fewer snapshots, |mms| does **not** delete existing snapshots to conform
to the new schedule. To delete unneeded snapshots, see
:doc:`/tutorial/delete-backup-snapshots`.

.. _namespaces-filter:

Namespaces Filter
-----------------

The namespaces filter lets you specify which databases and collections to back
up. You create either a :guilabel:`Blacklist` of those to exclude or a
:guilabel:`Whitelist` of those to include. You make your selections when
starting a backup and can later edit them as needed. If you change the filter
in a way that adds data to your backup, a resync is required.

Use the blacklist to prevent backup of collections that contain logging data,
caches, or other ephemeral data. Excluding these kinds of databases and
collections will allow you to reduce backup time and costs.
Using a blacklist is often preferable to using a whitelist as a whitelist
requires you to intentionally opt in to every namespace you want backed up.

.. important::

   If you use the namespaces filter, your backup's restore data will not include
   the ``seedSecondary`` script. The seedSecondary script provides
   an alternative to initial sync on new or restored replica set members. For
   more information, see :doc:`/tutorial/use-restore-to-seed-secondary`.

.. _considerations-backup-storage-engine:

Storage Engine
--------------

When you enable backups for a sharded cluster or a replica set that runs on
MongoDB 3.0 or later, you can choose the storage engine for the backups. MongoDB provides both the MMAPv1 and WiredTiger storage engines. If you do not specify a
storage engine, |mms| uses MMAPv1 by default. The |onprem| backup database schemas achieve maximum performance on MMAPv1.

The MMAPv1 engine minimizes storage in the :term:`blockstore`: all blocks are
already compressed and padding is disabled. WiredTiger compression offers no
further storage benefit.

.. admonition:: Insert Only MongoDB Workloads
   :class: note

   WiredTiger may be preferred when backing up insert only MongoDB workloads
   that benefit from high levels of block-level de-duplication in the
   :term:`blockstore`. You may see a reduction in storage when running the
   :term:`backup database` on WiredTiger.

You can choose a different storage engine for a backup than you do for the
original data. The backup storage engine does not need to match match that of
the original data. If your original data uses MMAPv1, you can choose
WiredTiger for backing up, and vice versa.

You can change the storage engine for a cluster or replica set's backups
at any time, but doing so requires an :term:`initial sync` of the backup
on the new engine.

Encryption
~~~~~~~~~~

.. include:: /includes/extracts/fact-encrypted-backups.rst

WiredTiger Options
~~~~~~~~~~~~~~~~~~

If you choose the WiredTiger engine to back up a collection that
already uses WiredTiger, the initial sync replicates all the
collection's WiredTiger options. For information on these options,
see the ``storage.wiredTiger.collectionConfig`` section of the
:manual:`Configuration File Options </reference/configuration-options>`
page in the MongoDB manual.

.. include:: /includes/extracts/backup-preparations-backup-daemon-storage-engine.rst

Index collection options are never replicated.

For more information on storage engines, see :manual:`Storage
</core/storage>` in the MongoDB manual.


Resyncing Production Deployments
--------------------------------

For production deployments, it is recommended that as a best practice you
periodically (annually) :doc:`resync </tutorial/resync-backup>` all
backed-up replica sets. When you resync, data is read from a secondary in
each replica set. During resync, no new snapshots are generated.

You may also want to resync your backup after:

- A reduction in data size, such that the size on disk of |mms|'s copy
  of the data is also reduced. This scenario also includes if you:

  - Have a :manual:`TTL index </core/index-ttl>` in place, which
    periodically deletes documents.

  - :manual:`Drop a collection </reference/method/db.collection.drop>`
    (MMAPv1 only).

  - Run a :term:`sharded cluster <sharded cluster>`, and there have been
    a lot of chunks moved off a particular shard.

- A switch in storage engines, if you want |mms| to provide snapshots
  in the new storage engine format.

- A manual build of an index on a replica set in a rolling fashion
  (as per :manual:`Build Indexes on Replica Sets
  </tutorial/build-indexes-on-replica-sets>` in the MongoDB manual).

.. _checkpoint:

Checkpoints
-----------

For :term:`sharded clusters <sharded cluster>`, checkpoints provide
additional restore points between snapshots. With
checkpoints enabled, |mms| creates restoration points at
configurable intervals of every 15, 30 or 60 minutes between
snapshots. To enable checkpoints, see :ref:`enable checkpoints
<enable-cluster-checkpoints>`.

To create a checkpoint, |mms| stops the :term:`balancer`
and inserts a token into the :term:`oplog` of each
:term:`shard` and :term:`config server` in the cluster. These
checkpoint tokens are lightweight and do not have a consequential
impact on performance or disk use.

Backup does not require checkpoints, and they are disabled by default.

Restoring from a checkpoint requires |mms| to apply the oplog of
each shard and config server to the last snapshot captured before the
checkpoint. Restoration from a checkpoint takes longer than
restoration from a snapshot.

.. _snapshot-while-balancer-enabled:

Snapshots when Agent Cannot Stop Balancer
-----------------------------------------

For :term:`sharded clusters <sharded cluster>`,
|mms| disables the :term:`balancer` before taking a
cluster snapshot. In certain situations, such as a long migration or no
running :program:`mongos`, |mms| tries to disable the balancer but cannot.
In such cases, |mms| will continue to take cluster snapshots but will flag
the snapshots with a warning that data may be incomplete and/or
inconsistent. Cluster snapshots taken during an active balancing
operation run the risk of data loss or orphaned data.

Snapshots when Agent Cannot Contact a ``mongod``
------------------------------------------------

For :term:`sharded clusters <sharded cluster>`,
if the Backup Agent cannot reach a :program:`mongod` process, whether a
shard or config server, then the agent cannot insert a synchronization
:term:`oplog` token. If this happens, |mms| will not create the snapshot and
will display a warning message.
