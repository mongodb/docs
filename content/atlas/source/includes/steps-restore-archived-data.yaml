title: "Pause the {+Online-Archive+} associated with the collection 
       whose archived data you wish to restore."
level: 4
ref: oa-restore-step1
stepnum: 1
content: | 
  See :ref:`pause-resume-online-archive` for more information.
--- 
title: "Connect to {+Online-Archive+} using the connection string. "
level: 4
ref: oa-restore-step2
stepnum: 2
content: | 
  You must use the :guilabel:`Archive Only` connection string to connect
  to the {+Online-Archive+}. To learn more, see
  :ref:`connect-online-archive-atlas-ui`. 
---
title: "Use :pipeline:`$merge` to move the data from your archive to 
       your |service| cluster."
level: 4
ref: oa-restore-step4
stepnum: 4
content: |

  To learn more about the ``$merge`` pipeline stage syntax and usage 
  for moving data back into your |service| cluster, see the :ref:`$merge
  <adf-merge-stage>` pipeline stage.

  .. example:: 

     Consider the following documents in an |s3| archive: 

     .. code-block:: json 
        
        {
          "_id" : 1,
          "item": "cucumber",
          "source": "nepal",
          "released": ISODate("2016-05-18T16:00:00Z")
        }
        {
          "_id" : 2,
          "item": "miso",
          "source": "canada",
          "released": ISODate("2016-05-18T16:00:00Z")
        }
        {
          "_id" : 3,
          "item": "oyster",
          "source": "luxembourg",
          "released": ISODate("2016-05-18T16:00:00Z")
        }
        {
          "_id" : 4,
          "item": "mushroom",
          "source": "ghana",
          "released": ISODate("2016-05-18T16:00:00Z")
        }

     Suppose the ``$merge`` syntax for restoring these documents 
     into the |service| cluster identifies documents based on the 
     ``item`` and ``source`` fields during the ``$merge`` stage.

     .. code-block:: json 

        db.<collection>.aggregate([ 
          { 
            "$merge": {
              "into": {
                "atlas": {
                  "clusterName": "<atlas-cluster-name>",
                  "db": "<db-name>",
                  "coll": "<collection-name>"
                }
              },
              "on": [ "item", "source" ],
              "whenMatched": "keepExisting",
              "whenNotMatched": "insert"
            } 
          }
        ])

     In this example, when an archived document matches a document on 
     the |service| cluster on those two fields, |service| keeps the 
     existing document in the cluster because the copy of the document 
     on the |service| cluster is more recent than the copy of the 
     document in the archive. When an archived document doesn't match 
     any document in the |service| cluster, |service| inserts the 
     document into the specified collection on the |service| cluster.

     When restoring data back into the |service| cluster, the archived 
     data might have duplicate ``_id`` fields. For this example, we can 
     include a :pipeline:`$sort` stage for sorting on the ``_id`` and 
     ``released`` fields before the :pipeline:`$merge` stage to ensure 
     that |service| chooses the documents with the recent date if there 
     are duplicates to resolve. 

     .. note:: 

        If there are multiple ``on`` fields, you must create a compound
        :manual:`unique index </core/index-unique>` on the ``on``
        identifier fields: 

        .. code-block:: shell 

           db.<collection>.createIndex( { item: 1, source: 1 }, {
           unique: true } ) 
           
        Alternatively, specify merges sequentially, one for each ``on``
        identifier field, to a temporary collection. Then merge the data
        in the temporary collection to the target collection using the
        {+cluster+}'s connection string. You must still create a unique
        index for each ``on`` identifier field. 
     
     The aggregation stage can be run in the background by setting the
     ``background`` flag to ``true``. To run this command in
     {+mongosh+}, use the ``db.runCommand``. 

     .. code-block:: json 

        db.runCommand({
          "aggregate": "<collection>",
          "pipeline": [
            {
              $sort: {
                "_id": 1,
                "released": 1,
              } 
            },
            { 
              "$merge": {
                "into": {
                  "atlas": {
                    "clusterName": "<atlas-cluster-name>",
                    "db": "<db-name>",
                    "coll": "<collection-name>"
                  }
                },
                "on": [ "item", "source" ],
                "whenMatched": "keepExisting",
                "whenNotMatched": "insert"
              } 
            }
          ], 
          "cursor": { }
        },
        { "background": true }
      )
    
     To learn more about resolving duplicate fields, see the 
     :ref:`$merge considerations <adf-merge-stage>`.
---
title: "Verify data in the |service| cluster and delete the 
       online archive."
level: 4
ref: oa-restore-step5
stepnum: 5
content: | 
  See :ref:`delete-online-archive` for more information.
...
