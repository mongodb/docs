.. _voyage-rag:
.. _voyage-rag-example:

===================================================
Retrieval-Augmented Generation (RAG) with Voyage AI
===================================================

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

Retrieval-augmented generation (RAG) is an architecture that uses
:ref:`semantic search <voyage-semantic-search>` to augment 
large language models (LLMs) with additional data, 
enabling them to generate more accurate responses. 

While semantic search retrieves relevant documents based on meaning,
RAG takes this a step further by providing those retrieved documents as context
to an LLM. This additional context helps the LLM generate a more accurate 
response to a user's query, reducing hallucinations. {+voyageai+} provides best-in-class 
embedding and reranking models to power retrieval for your RAG applications.

.. cta-banner::
   :url: https://search-playground.mongodb.com/tools/chatbot-demo-builder/snapshots/new
   :icon: wizard

   To try RAG without writing any code, use the
   `Playground <https://search-playground.mongodb.com/tools/chatbot-demo-builder/snapshots/new>`__
   to build an AI chatbot powered by {+voyageai+}.
   To learn more, see :ref:`Chatbot Demo Builder <avs-playground>`.

.. image:: /images/rag-diagram.png
   :figwidth: 1444px
   :alt: Diagram of RAG architecture

Tutorial
--------

The following tutorial demonstrates how to implement RAG with {+voyage+}
embeddings. 

.. cta-banner::
   :url: https://github.com/mongodb/docs-notebooks/blob/main/voyageai/
   :icon: Code

   You can also work with the code for this tutorial by cloning the :github:`GitHub repository <docs-notebooks/blob/main/voyageai/>`.

.. composable-tutorial::
   :options: language-no-interface, llm-provider, vector-storage
   :defaults: python, openai, in-memory

   .. selected-content::
      :selections: python, openai, in-memory

      Prerequisites
      ~~~~~~~~~~~~~

      To complete this tutorial, you must have the following:

      .. include:: /includes/rag/shared/prerequisites-python-base.rst

      Procedure
      ~~~~~~~~~

      Complete the following steps to implement RAG with {+voyageai+}
      embeddings and in-memory vector storage.

      .. include:: /includes/rag/shared/note-in-memory-limitations.rst

      .. include:: /includes/rag/steps-voyage-rag-python-openai-in-memory.rst

   .. selected-content::
      :selections: python, anthropic, in-memory

      Prerequisites
      ~~~~~~~~~~~~~

      To complete this tutorial, you must have the following:

      .. include:: /includes/rag/shared/prerequisites-python-base.rst

      Procedure
      ~~~~~~~~~

      Complete the following steps to implement RAG with {+voyageai+}
      embeddings and in-memory vector storage.

      .. include:: /includes/rag/shared/note-in-memory-limitations.rst

      .. include:: /includes/rag/steps-voyage-rag-python-anthropic-in-memory.rst

   .. selected-content::
      :selections: python, openai, mongodb

      Prerequisites
      ~~~~~~~~~~~~~

      To complete this tutorial, you must have the following:

      .. include:: /includes/rag/shared/prerequisites-python-base.rst

      .. include:: /includes/rag/shared/prerequisites-python-mongodb.rst

      Procedure
      ~~~~~~~~~

      Complete the following steps to implement RAG with {+voyageai+}
      embeddings and MongoDB.

      .. include:: /includes/rag/steps-voyage-rag-python-openai.rst

   .. selected-content::
      :selections: python, anthropic, mongodb

      Prerequisites
      ~~~~~~~~~~~~~

      To complete this tutorial, you must have the following:

      .. include:: /includes/rag/shared/prerequisites-python-base.rst

      .. include:: /includes/rag/shared/prerequisites-python-mongodb.rst

      Procedure
      ~~~~~~~~~

      Complete the following steps to implement RAG with {+voyageai+}
      embeddings and MongoDB.
      
      .. include:: /includes/rag/steps-voyage-rag-python-anthropic.rst


Why use RAG?
------------

When working with LLMs, you might encounter the
following limitations:

- **Stale data**: LLMs are trained on a static dataset
  up to a certain point in time. This means that they
  have a limited knowledge base and might use outdated
  data.

- **No access to additional data**: LLMs don't have access to
  local, personalized, or domain-specific data. Therefore, they can
  lack knowledge about specific domains.

- **Hallucinations**: When using incomplete or
  outdated data, LLMs can generate inaccurate responses.

RAG addresses these limitations by adding a retrieval step,
typically powered by :ref:`semantic search <voyage-semantic-search>`,
to get relevant documents in real time. Providing additional context helps LLMs
generate more accurate responses. This makes RAG an effective architecture for 
building AI chatbots that deliver personalized,
domain-specific question answering and text generation.

What are Vector Databases?
~~~~~~~~~~~~~~~~~~~~~~~~~~

Vector databases are specialized databases designed to store and efficiently
retrieve vector embeddings. While storing vectors in memory is suitable for
prototyping and experimentation, production RAG applications typically require
a vector database to perform efficient retrieval from a larger corpus.

MongoDB has native support for vector storage and retrieval, making it a 
convenient choice for storing and searching vector embeddings alongside
your other data. To learn more, see :ref:`avs-overview`.


Next Steps
----------

For additional tutorials, see the following resources:
  
- To learn how to implement RAG with popular LLM frameworks
  and AI services, see :ref:`ai-integrations`.

- To build AI agents and implement agentic RAG, see :ref:`ai-agents`. 
