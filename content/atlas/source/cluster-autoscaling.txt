.. _cluster-autoscaling:

======================
Configure Auto-Scaling
======================

.. meta::
   :description: Enable auto-scaling, including predictive auto-scaling, to optimize resource utilization and cost by adjusting cluster tier. Enable cluster storage auto-scaling to increase storage capacity.

.. default-domain:: mongodb

.. facet::
   :name: genre
   :values: reference

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

|service| uses auto-scaling for cluster compute to help you optimize resource utilization and cost
by adjusting your {+cluster+} tier. |service| uses storage auto-scaling to help you
automatically increase your {+cluster+} storage capacity.

In this section, you can learn about:

- :ref:`Cluster Tier Auto-Scaling: Predictive and Reactive <cluster-autoscaling-reactive-overview>`
- :ref:`Reactive Auto-Scaling for Cluster Tier <cluster-autoscaling-reactive-overview>`
- :ref:`Predictive Auto-Scaling for Cluster Tier <predictive-autoscaling-overview>`
- :ref:`Auto-Scaling for Cluster Storage <howitworks-scale-cluster-storage>`
- :ref:`Configure Auto-Scaling Options <configure-cluster-autoscaling>`
- :ref:`Alerts on Auto-Scaling Events <configure-autoscaling-alert>`
- :ref:`View Auto-Scaling Events in the Activity Feed <activity-feed-auto-scaling-events>`
- :ref:`MongoDB Support for Atlas Auto-Scaling <mongodb-support-autoscaling>`     

.. _cluster-tier-autoscaling-predictive-and-reactive:

Cluster Tier Auto-Scaling
--------------------------

|service| uses reactive and predictive auto-scaling for cluster tiers.
|service| chooses its auto-scaling mechanism based on your cluster's type,
tier, and workload pattern.

- Reactive auto-scaling. |service| uses thresholds, and not prediction,
  to trigger scaling events based on current resource usage. Reactive
  auto-scaling occurs **after** sustained high or low resource usage.
  To learn more, see :ref:`cluster-autoscaling-reactive-overview`.

- Predictive auto-scaling. |service| uses machine learning to
  anticipate future scaling needs based on historical usage patterns
  and attempts to trigger scaling events **before** the forecasted workload
  spike arrives. 
  
  Predictive auto-scaling is an extension of cluster tier auto-scaling and
  falls back to reactive auto-scaling. |service| continues to rely on
  reactive auto-scaling to manage **unexpected** spikes in workload that
  aren't cyclical or predictable. |service| uses predictive auto-scaling
  for :ref:`eligible clusters <eligible-clusters-predictive-autoscaling>`.
  To learn more, see :ref:`predictive-autoscaling-overview`.

.. important::

   If you create a cluster in |service| and it's :ref:`eligible for reactive auto-scaling <eligible-clusters-reactive-autoscaling>`
   and :ref:`eligible for predictive auto-scaling <eligible-clusters-predictive-autoscaling>`,
   both predictive and reactive auto-scaling mechanisms are enabled by
   default for the new cluster if you use the {+atlas-ui+}. |service| then uses
   auto-scaling mechanisms based on your cluster's type, tier, and workload.
   If you use the {+atlas-admin-api+}, you must explicitly :ref:`enable auto-scaling <configure-cluster-autoscaling>`.

.. _cluster-autoscaling-reactive-overview:

Reactive Auto-Scaling for Cluster Tier
----------------------------------------

.. include:: /includes/fact-reactive-autoscaling.rst

You can configure the {+cluster+} tier ranges that |service| uses to
automatically scale your {+cluster+} tier, storage capacity, or both in
response to {+cluster+} usage.

To optimize resource utilization and improve cost profile, |service|
reactive auto-scaling detects sustained higher demand and short-term peak
traffic and adjusts {+cluster+} tier based on real-time resource usage.

To help control costs, you can specify a range of maximum and minimum
{+cluster+} sizes that your {+cluster+} can automatically scale to.

Reactive auto-scaling works on a rolling basis, and the process doesn't incur any
downtime. {+service+} maintains a primary node during this process, but
the  nodes are upgraded one-by-one and are unavailable while being upgraded.

To learn about recommendations for scalability, including avoiding resource
drift when using infrastructure as code tools with reactive auto-scaling,
see :ref:`arch-center-scalability-recs` in the {+atlas-arch-center+}.

.. _eligible-clusters-reactive-autoscaling:

Eligible Clusters for Reactive Auto-Scaling
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/fact-auto-scaling-availability.rst

.. _howitworks-scale-cluster-tier:
.. _system-memory-calculation:

How |service| Scales {+Cluster+} Tier
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. note:: Tier Availability

   Reactive auto-scaling works on {+cluster+} tiers in :guilabel:`General` and
   :guilabel:`Low-CPU` classes, but *not* on {+clusters+} in
   the :guilabel:`Local NVMe SSD` class.

|service| analyzes the following {+cluster+} metrics to
determine when to reactively scale a {+cluster+}, and whether to scale the
{+cluster+} tier up or down:

- Normalized System CPU Utilization

- System Memory Utilization

|service| calculates system System Memory Utilization based on available node memory
and total memory as follows:

(``memoryTotal`` - (``memoryFree`` + ``memoryBuffers`` + ``memoryCached``)) / (``memoryTotal``) * 100

In the previous calculation, ``memoryFree``, ``memoryBuffers``, and ``memoryCached`` are
amounts of available memory that |service| can reclaim for other
purposes. To learn more, see :guilabel:`System Memory` in
:ref:`review-available-metrics`.

|service| won't scale your {+cluster+} tier if the new {+cluster+} 
tier would fall outside of your specified :guilabel:`Minimum` and 
:guilabel:`Maximum Cluster Size` range. 

|service| scales your {+cluster+} to another tier in the same class.
For example, |service| scales :guilabel:`General` {+clusters+}
to other :guilabel:`General` {+cluster+} classes, but doesn't scale 
:guilabel:`General` {+clusters+} to :guilabel:`Low-CPU` {+cluster+}
classes.

The exact reactive auto-scaling criteria are subject to change in order to ensure
appropriate cluster resource utilization.

.. include:: /includes/fact-auto-scaling-and-migration.rst

If you deploy :term:`read-only nodes <read-only node>` and want your
{+cluster+} to scale faster, consider adjusting your :ref:`Replica
Set Scaling Mode <replica-set-scaling-mode>`.

Scaling Up a {+Cluster+} Tier Reactively 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To manage dynamic workloads for your applications, |service| reactively
scales up nodes in your {+cluster+} under the conditions described in this
section.

If the next {+cluster+} tier is within your :guilabel:`Maximum Cluster Size`
range, |service| scales :term:`operational nodes <operational node>` in your
{+cluster+} up to the next tier if at least *one* of the following criteria
is true for *any* {+cluster+} node of this type.

.. note::

  The following list groups together CPU-related criteria, followed by
  the memory-related criteria. Within each group, criteria appear in
  the order from most restrictive to least restrictive, and criteria for
  specific cloud providers appear first, if they exist.

- ``M10`` and ``M20`` {+clusters+}:

  - |aws|. The average normalized :term:`Relative System CPU Utilization <relative system CPU utilization>`
    has exceeded 90% for the past 20 minutes and the average non-normalized
    :term:`Absolute System CPU Utilization <absolute system CPU utilization>`
    for :term:`CPU steal` has exceeded 30% for the past 3 minutes.

  - |azure|. The average normalized :term:`Relative System CPU Utilization <relative system CPU utilization>`
    has exceeded 90% for the past 20 minutes and the average non-normalized
    :term:`Absolute System CPU Utilization <absolute system CPU utilization>`
    for :term:`softIRQ` has exceeded 10% for the past 3 minutes.

  - The average normalized :term:`Absolute System CPU Utilization <absolute system CPU utilization>`
    has exceeded 90% of resources available to the {+cluster+} for the past 20 minutes.

  - The average normalized :term:`Relative System CPU Utilization <relative system CPU utilization>`
    has exceeded 75% of resources available to the {+cluster+} for the past one hour.

  - The average :guilabel:`System Memory Utilization` has exceeded 90% of resources
    available to the {+cluster+} for the past 10 minutes. To learn how
    |service| calculates the amount of system memory utilization, see
    :ref:`system-memory-calculation`.

  - The average :guilabel:`System Memory Utilization` has exceeded 75% of
    resources available to the {+cluster+} for the past one hour.

- ``M30+`` {+clusters+}:

  - The average :guilabel:`System CPU Utilization` has exceeded 90% of
    resources available to the {+cluster+} for the past 10 minutes.

  - The average :guilabel:`System CPU Utilization` has exceeded 75% of
    resources available to the {+cluster+} for the past one hour.

  - The average :guilabel:`System Memory Utilization` has exceeded 90% of
    resources available to the {+cluster+} for the past 10 minutes.

  - The average :guilabel:`System Memory Utilization` has exceeded 75% of
    resources available to the {+cluster+} for the past one hour.

These thresholds ensure that your {+cluster+} scales up quickly in
response to high loads, and your application can handle spikes in
traffic or usage, maintaining its performance and reliability.

.. note::

   The conditions in this section describe operational nodes.
   For :term:`analytics nodes <analytics node>` on any cloud provider,
   |service| scales them up to the next tier if the average normalized
   :guilabel:`System CPU Utilization` or
   the :guilabel:`System Memory Utilization` has exceeded 75% of resources
   available to any {+cluster+} node for the past one hour.

To achieve optimal resource utilization and cost profile, |service| avoids
scaling up the {+cluster+} to the next tier if:

- The ``M10`` or ``M20`` {+cluster+} has been scaled up in the past 20 minutes
  or one hour, depending on thresholds.
- The ``M30+`` {+cluster+} has been scaled up in the past 10 minutes
  or one hour, depending on thresholds.

For example, if the cluster tier has not been changed since ``12:00``,
|service| will scale an ``M30+`` {+cluster+} at ``12:10``, if the
{+cluster+}\'s current normalized System CPU Utilization is greater than 90%.

.. important:: Sudden Workload Spikes

   Scaling up to a greater {+cluster+} tier requires enough time to
   prepare backing resources. Automatic scaling may not occur when a
   {+cluster+} receives a burst of activity, such as a bulk insert.
   To reduce the risk of running out of resources, plan to scale up
   {+clusters+} before bulk inserts and other workload spikes.

Scaling Down a {+Cluster+} Tier Reactively 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To optimize costs, |service| reactively scales down nodes in your {+cluster+} under
the conditions described in this section.

If the next lowest {+cluster+} tier is within your
:guilabel:`Minimum Cluster Size` range, |service| scales the nodes in your
{+cluster+} down to the next lowest tier if *all* of the following criteria
are true for *all* nodes of the specified {+cluster+} type:

- All nodes:

  - |service| hasn't scaled the cluster down (manually or automatically) in
    the past 24 hours.

  - |service| hasn't provisioned or unpaused the cluster in the past 24 hours.
  - |service| hasn't stopped and restarted any cluster nodes in the past 12 hours.

- :term:`Operational nodes <operational node>`:

  - The average normalized :guilabel:`System CPU Utilization` is below 45% of resources
    available to the {+cluster+} over at least the last 10 minutes **AND**
    the last 4 hours. |service| uses the "4 hours average" checkpoint as
    an indication that the CPU load has settled down on the observed level.
    |service| uses the "10 minutes average" checkpoint as an indication
    that no recent CPU spikes have occurred that |service| didn't capture
    with the "4 hour average" checkpoint.

  - The average :manual:`WiredTiger cache </reference/command/serverStatus/#serverstatus.wiredTiger.cache>`
    usage is below 90% of the maximum WiredTiger cache size for at least
    the last 10 minutes **AND** the last 4 hours at the **current** {+cluster+}
    tier size. This indicates to |service| that the current {+cluster+} isn't overloaded.

  - The **projected total System Memory Utilization** at the new lower {+cluster+}
    tier is below 60% for at least the last 10 minutes **AND** the last 4 hours.
    |service| calculates the **projected total memory usage** mentioned
    in the preceding statement as follows.

    |service| measures the current memory usage and replaces the current
    :manual:`WiredTiger cache </reference/command/serverStatus/#serverstatus.wiredTiger.cache>`
    usage size with 80% of the WiredTiger cache size on the **new** lower tier {+cluster+}.

    Next, |service| checks whether the **projected total memory usage**
    would be below 60% for at least the last 4 hours and at least the
    last 10 minutes on the new tier size.

    .. note::

       |service| includes the WiredTiger cache in its memory calculation
       to make it more likely that {+clusters+} with a full cache, but
       otherwise low traffic, will scale down. In other words,
       |service| examines the size of the WiredTiger cache to determine that it can
       safely down scale an otherwise idle {+cluster+} with low Normalized System CPU Utilization
       in cases where the {+cluster+}\'s WiredTiger caches might reach 90% of
       the {+cluster+}\'s maximum WiredTiger cache size.

  These conditions ensure that |service| scales down operational nodes
  in your {+cluster+} to prevent high utilization states.

- :term:`Analytics nodes <analytics node>`:

  - The average :guilabel:`Normalized System CPU Utilization` and
    :guilabel:`System Memory Utilization` over the past 24 hours is below 50% of
    resources available to the {+cluster+}.

  .. note::

     ``M10`` and ``M20`` {+clusters+} use lower thresholds to account for
     caps on CPU usage set by cloud providers after burst periods. These
     thresholds vary depending on your cloud provider and {+cluster+} tier.

.. _independently-scaling-shards:

Scaling a Sharded {+Cluster+}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

|service| auto-scales the {+cluster+} tier for sharded {+clusters+}
using the same criteria as replica sets. |service| applies the 
following rules:

- For clusters with independent shard scaling enabled, auto-scaling in
  |service| evaluates and scales each shard independently. Independent shard
  scaling requires that the smallest shard size remains no smaller than two cluster
  tiers below the largest shard, to maintain availability and performance.
  If |service| triggers auto-scaling for such a cluster, and
  scales up the largest shard, it also scales up the smaller shards if necessary
  to ensure consistent availability and performance.

- If the operational or analytics nodes within a shard
  meet the criteria to auto-scale,
  only the operational or analytics nodes on that particular shard change tier.
- The Config server replica set doesn't auto-scale.


API for Independently Scaling Shards in {+Clusters+}
``````````````````````````````````````````````````````````

As of API resource version 2024-08-05 of the :ref:`Versioned Atlas Administration API <api-versioning-overview>`,
you can independently scale the {+cluster+} tier of each shard individually.
This API version is a significant change to the underlying scaling model of |service| {+clusters+}.

.. warning::

   The 2024-08-05 API version is a significant breaking change.
   If you send a request with the new API to describe the shards within the {+cluster+} asymmetrically,
   *the previous symmetrical-only API will no longer be available for that {+cluster+}*.
   To return to a previous API version,
   first reconfigure the cluster to have all shards operate on the same tier.

The new API is capable of describing asymmetric {+clusters+}.
The ``replicationSpec.numShards`` field is not present in the new API schema.
Instead, each shard is specified by a separate ``replicationSpec``,
even for symmetric {+clusters+} in which all shards are configured the same.

.. _predictive-autoscaling-overview:

Predictive Auto-Scaling for Cluster Tier
-----------------------------------------

Predictive auto-scaling is an extension of auto-scaling.

|service| uses demand-forecasting for host resource utilization and performs
preemptive scaling up of your cluster compute to ensure optimal resource
utilization. With predictive auto-scaling, |service| attempts to scale up
your cluster proactively, ahead of cyclical workload spikes.

Predictive auto-scaling is powered by a machine learning model based on
historical patterns. |service| scales the cluster up if the model predicts
high resource utilization. MongoDB updates the model and its criteria continuously
to optimize |service| performance.

Predictive auto-scaling has the following benefits for clusters with predictive, cyclical workloads:

- Automatically scale up your cluster for cyclical workload patterns, for example, daily or weekly spikes.
- Maintain consistent performance and availability during predictable high-demand periods.
- Reduce manual scaling tasks or scheduled scripts by letting |service| manage capacity increases.
- Seamlessly fall back to reactive auto-scaling when changes to the cluster
  workload fall outside of predictable patterns and are non-cyclical or unpredictable.

To trigger predictive auto-scaling, your cluster must maintain continuous 
activity logs for two weeks. Once it meets this criterion, the system enables 
predictive auto-scaling.

.. note::
   If you pause your cluster, predictive auto-scaling requires 
   two consecutive weeks of activity before it can resume.

Behavior of Predictive Auto-Scaling
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following statements describe how predictive auto-scaling works:

- |service| attempts to scale up your cluster instance size **before** the forecasted load arrives.

- When |service| scales your cluster predictively based on forecasted metrics,
  it can scale up by at most two tiers at a time.

- Predictive auto-scaling applies only to compute, not storage.
- Predictive auto-scaling respects existing auto-scaling minimum and maximum instance sizes.
- In cases when |service| can't use predictive auto-scaling to scale up the cluster,
  it falls back to using reactive auto-scaling.

- Predictive auto-scaling only supports upscaling. There is no predictive down-scaling.
  |service| uses reactive auto-scaling to automatically scale down the cluster
  when the workload decreases.

- If predictive up-scaling is scheduled to happen within the next 1 hour,
  |service| skips reactive down-scaling.

- If you use :ref:`independent shard scaling <independently-scaling-shards>`,
  and add one or more shards after predictive auto-scaling was already
  enabled and active on the cluster, these new shards don't auto-scale
  predictively until their workload patterns have been established,
  two weeks later. In the meantime, these shards use reactive auto-scaling
  behavior in |service|.

.. _eligible-clusters-predictive-autoscaling:

Eligible Clusters for Predictive Auto-Scaling
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

|service| uses predictive auto-scaling for eligible clusters.
Eligible clusters for predictive auto-scaling must meet **all**  of the following criteria:

- Belong to :guilabel:`General` and :guilabel:`Low-CPU` cluster classes.
- Have a tier that is ``M30`` or greater.
- Have auto-scaling enabled. If you enable down-scaling, auto-scaling
  minimum instance size should be equal to or greater than ``M30``.
- Have been active for at least two weeks.
- **Not** use  :ref:`NVMe storage <nvme-storage>` or belong to the
  :guilabel:`Local NVMe SSD` cluster class.

In addition, the following criteria affect whether |service| uses
predictive auto-scaling for an eligible cluster:

- Predictive auto-scaling applies only to electable and read-only nodes. 
  |service|  **doesn't** use predictive auto-scaling for search or analytics nodes.

- Predictive auto-scaling might not be able to predict non-cyclical and highly
  dynamic workload spikes in any eligible cluster. In these cases, |service|
  relies on reactive auto-scaling.

.. _howitworks-scale-cluster-storage:

How |service| Scales Cluster Storage
-------------------------------------

|service| enables {+cluster+} storage auto-scaling by default.
|service| automatically increases {+cluster+} storage when disk space used
reaches 90% for *any* node in the {+cluster+}.

To opt out of cluster storage scaling, un-check the
:guilabel:`Storage Scaling` checkbox in the :guilabel:`Auto-scale`
section.

The following considerations apply:

- |service| auto-scales {+cluster+} storage up only. You can manually
  reduce your {+cluster+} storage from the :ref:`Edit Cluster <scale-cluster>` page.

- On :ref:`AWS <amazon-aws>`, :ref:`Azure <microsoft-azure>`, and
  :ref:`GCP <google-gcp>` {+clusters+}, |service| increases {+cluster+}
  storage capacity to achieve 70% disk space used. To learn more, see
  :ref:`change-storage-capacity-aws`, :ref:`change-storage-capacity-azure`,
  and :ref:`change-storage-capacity-gcp`.

- Avoid high-speed write activity if you plan to scale up {+clusters+}.
  Scaling up a {+cluster+} to greater storage capacity requires sufficient
  time to  prepare and copy data to new disks. If a {+cluster+} receives
  a burst of high-speed write activity, such as a bulk insert, automatic
  scaling might not occur due to a temporary spike in disk storage capacity.
  To reduce the risk of running out of disk  storage, plan to scale up
  {+clusters+} in advance of bulk inserts and other instances of high-speed
  write activity.

- |service| disables disk auto-scaling if you specify one cluster tier
  class for the base nodes and another, different cluster tier class for
  the analytics nodes. For example, if you specify a :guilabel:`General`
  cluster class for :term:`operational nodes <operational node>` in
  the :guilabel:`Base Tier`, and a :guilabel:`Low-CPU` cluster class for
  analytics nodes in the :guilabel:`Analytics Tier`, |service| disables
  disk auto-scaling with the following error message: ``Disk auto-scaling
  is not yet available for clusters with mixed instance classes``.

- |service| disables disk reactive auto-scaling if you deploy the :guilabel:`Base Tier`
  and :guilabel:`Analytics Tier` nodes in different cloud provider regions.

{+Cluster+} Tier and {+Cluster+} Storage Might Scale in Parallel
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When |service| attempts to automatically scale your {+cluster+} storage
capacity as part of auto-scaling, it might need to scale your storage outside of the bounds that
your current {+cluster+} tier supports. To help ensure that your {+cluster+}
doesn't experience any downtime, |service| scales your {+cluster+} tier
(in addition to {+cluster+} storage) to accommodate the new storage capacity.

On |azure|, if you enable auto-scaling on a {+cluster+} deployed in one of
the :ref:`regions that support extended storage <microsoft-azure-storage-supported-regions>`,
and the current |iops| is lower than the default |iops| for the auto-scaled
disk size, |service| increases the alloted number of |iops| in the
:guilabel:`IOPS` slider and notifies you in the UI. To learn more, see
:ref:`change-storage-capacity-azure`.

.. example::

   The maximum storage capacity for an ``M30`` {+cluster+} is 480 GB. If
   you have an ``M30`` {+cluster+} with the maximum storage allocated and
   your disk space used reaches 90%, a storage auto-scaling event requires
   raising your storage capacity to 600 GB. In this case, |service| scales
   your {+cluster+} tier up to ``M40`` because this is the lowest {+cluster+}
   tier that can support the new required storage capacity. On |azure|,
   if you deployed the {+cluster+} in one of the :ref:`regions that support extended storage <microsoft-azure-storage-supported-regions>`,
   |service| also automatically increases |iops| to match the |iops| level
   for that tier's {+cluster+}.

In the event that your specified maximum {+cluster+} tier can't support
the new storage capacity, |service|:

1. Raises your maximum {+cluster+} tier to the next lowest tier that can
   accommodate the new storage capacity.

#. Scales your {+cluster+} tier to that new maximum tier.

.. note::

   When |service| overrides your maximum {+cluster+} tier, it also disables
   your {+cluster+} from automatically scaling down. To re-enable downward
   auto-scaling, configure it in :ref:`Cluster Settings <scale-cluster>`.
   See also :ref:`downward-scaling-considerations`.

If |service| attempts to scale your {+cluster+} tier down and the target
tier can't support your current disk capacity, provisioned |iops|, or both,
|service| doesn't scale your {+cluster+} down. In this scenario, |service|
updates your auto-scaling settings based on the relationship between your
current {+cluster+} tier and the configured maximum {+cluster+} tier:

- If the {+cluster+} is currently at the configured maximum {+cluster+}
  tier, |service| disables the {+cluster+} from automatically scaling
  down because all smaller tiers wouldn't be able to accommodate
  the necessary storage settings. If you want to re-enable downward
  auto-scaling, you must do so manually from your :ref:`Cluster
  Settings <scale-cluster>`.

- If the {+cluster+} isn't currently at the configured maximum {+cluster+}
  tier, |service| raises the minimum {+cluster+} tier to the current {+cluster+}
  tier. In this case, |service| doesn't disable downward auto-scaling.

This auto-scaling logic reduces the downtime in cases when your storage
settings don't match your workload.

Oplog Considerations
~~~~~~~~~~~~~~~~~~~~

Depending on whether you choose to use storage auto-scaling, |service| manages
the oplog entries based on either the minimum oplog retention window, or
the oplog size. To learn more, see :ref:`Oplog Size Behavior <oplog-size-behavior>`.
|service| enables storage auto-scaling by default.

.. _downward-scaling-considerations:

Considerations for Downward Auto-Scaling of Cluster Tier and Storage
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- When |service| scales down the storage capacity of your {+cluster+},
  this can take longer than expanding storage capacity due to the 
  mechanics of the scaling process.
- Estimate your deployment's range of workloads and then set the
  :guilabel:`Minimum Cluster Size` value to the {+cluster+} tier that
  has enough capacity to handle your deployment's workload. Account for
  any possible spikes or dips in {+cluster+} activity.

- You can't scale to a {+cluster+} tier smaller than ``M10``.

- You can't select a minimum {+cluster+} tier that is below the current
  disk configuration of your {+cluster+}. If your storage increases
  beyond what is supported by your minimum {+cluster+} tier, and |service|
  increases your {+cluster+}\s storage configuration beyond what your
  minimum {+cluster+} tier supports, then |service| automatically 
  adjusts your minimum {+cluster+} tier to a tier that
  supports the current storage requirements of your {+cluster+}.

  .. example::

     You have set your auto-scaling bounds to ``M20`` - ``M60`` and your
     current {+cluster+} tier is ``M40`` with a disk capacity of 200GB. 
     |service| triggers a disk auto-scaling event to increase capacity to 
     320GB because current disk usage exceeds 180GB, which is more than 
     90% of the 200GB capacity.

     |service| takes the following actions:
     
     1. Raises your minimum cluster tier to the next lowest tier, ``M30``,
        that can accommodate the new storage capacity. ``M20`` supports a 
        maximum storage capacity of 256GB, so it is no longer a 
        valid auto-scaling bound.
     2. Determines that the current instance size, ``M40``,
        supports the new disk configuration. The disk auto-scaling event 
        succeeds.

.. _configure-autoscaling-options:

Configure Auto-Scaling Options
------------------------------

You can configure auto-scaling options when you
:ref:`create <create-new-cluster>` or :ref:`modify <scale-cluster>` a
{+cluster+}. For new clusters, |service| automatically enables
{+cluster+} tier auto-scaling and storage auto-scaling.

You can do one of the following:

- Review and adjust the upper and lower {+cluster+} tiers that |service|
  should use when auto-scaling your {+cluster+}, or
- :ref:`Opt out <opt-out-autoscaling>` of using auto-scaling.

|service| displays auto-scaling options in the :guilabel:`Auto-scale`
section of the {+cluster+} builder for :guilabel:`General` and
:guilabel:`Low-CPU` tier {+clusters+}.

.. _configure-cluster-autoscaling:

Auto-Scaling Enabled by Default
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When you create a new {+cluster+}, |service| enables auto-scaling (predictive and reactive) for
{+cluster+} tier and {+cluster+} storage. (Predictive auto-scaling only affects
cluster tier and doesn't affect storage.) You don't need
to explicitly enable auto-scaling. If you prefer, you can
:ref:`opt out <opt-out-autoscaling>` for {+cluster+} tier and cluster
storage.

.. note::

   |service| enables cluster tier auto-scaling by default when you
   create {+clusters+} in the {+atlas-ui+}. If you
   :oas-bump-atlas-op:`create clusters with the API <creategroupcluster>`
   cluster auto-scaling isn't selected by default and you must explicitly
   enable it, using the options in the ``autoScaling`` object of
   the :oas-bump-atlas-op:`Update One Cluster in One Project <updategroupcluster>`
   endpoint.

With auto-scaling enabled, your {+cluster+} can automatically:

- Scale up to increase capability with a higher {+cluster+} tier, using either
  reactive or predictive auto-scaling, depending on cluster and workload eligibility.
- Decrease the current {+cluster+} tier to a lower {+cluster+} tier using reactive
  auto-scaling.

In the :guilabel:`Cluster tier` section of the :guilabel:`Auto-scale`
options, you can specify the :guilabel:`Maximum Cluster Size` and
:guilabel:`Minimum Cluster Size` values that your {+cluster+} can
automatically scale to. |service| sets these values as follows:

- The :guilabel:`Maximum Cluster Size` is set to one tier above your
  current {+cluster+} tier.

- The :guilabel:`Minimum Cluster Size` is set to the current {+cluster+}
  tier.

In addition, |service| might use :ref:`predictive auto-scaling <predictive-autoscaling-overview>`
if your cluster is eligible and its workload is cyclical and predictable.

.. _enable-autoscaling-example-cli-api:

Enable Auto-Scaling with {+atlas-cli+} and {+atlas-admin-api+}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can enable auto-scaling for both compute and storage when creating or
updating a cluster using the :atlascli:`{+atlas-cli+} </>` or {+atlas-admin-api+}.
The following examples show how to enable auto-scaling for both electable
nodes and analytics nodes. Replace the cluster tiers and provider settings
with the ones you need.

.. tabs::

   .. tab:: {+atlas-cli+}
      :tabid: atlascli

      To configure auto-scaling with the {+atlas-cli+}, create a JSON file
      that contains the auto-scaling configuration and then specify it in the :atlascli:`atlas api clusters updateCluster </command/atlas-api-clusters-updateCluster/>` command.

      .. collapsible::
         :heading: clusters updateCluster
         :sub_heading: Enable auto-scaling with the updateCluster command
         :expanded: false

         Use the :atlascli:`atlas api clusters updateCluster </command/atlas-api-clusters-updateCluster/>`
         command to directly call the API and enable auto-scaling settings
         on an existing cluster. To enable auto-scaling when creating a new
         cluster, use the :atlascli:`atlas api clusters createCluster </command/atlas-api-clusters-createCluster/>` command.
         
         .. procedure::
            :style: normal

            .. step:: Create the payload file.
               
               Create a ``payload.json`` file with the following content. Replace the placeholder values with your specific cluster configuration:

               .. code-block:: json
                  :copyable: true
                  :emphasize-lines: 17, 29

                  {
                    "replicationSpecs": [
                      {
                        "regionConfigs": [
                          {
                            "providerName": "{CLOUD-PROVIDER}",
                            "regionName": "{REGION-NAME}",
                            "priority": 7,
                            "electableSpecs": {
                              "instanceSize": "{INSTANCE-SIZE}",
                              "nodeCount": 3
                            },
                            "analyticsSpecs": {
                              "instanceSize": "{ANALYTICS-INSTANCE-SIZE}",
                              "nodeCount": 1
                            },
                            "autoScaling": {
                              "compute": {
                                "enabled": true,
                                "scaleDownEnabled": true,
                                "minInstanceSize": "{MIN-INSTANCE-SIZE}",
                                "maxInstanceSize": "{MAX-INSTANCE-SIZE}"
                              },
                              "diskGB": {
                                "enabled": true
                              }
                            },
                            "analyticsAutoScaling": {
                              "compute": {
                                "enabled": true,
                                "scaleDownEnabled": true,
                                "minInstanceSize": "{MIN-ANALYTICS-INSTANCE-SIZE}",
                                "maxInstanceSize": "{MAX-ANALYTICS-INSTANCE-SIZE}"
                              },
                              "diskGB": {
                                "enabled": true
                              }
                            }
                          }
                        ]
                      }
                    ]
                  }

            .. step:: Run the update command.

               After creating the ``payload.json`` file, run the following command to enable auto-scaling on an existing cluster and specify the JSON file with the ``--file`` flag:

               .. code-block:: bash
                  :copyable: true
                  :emphasize-lines: 5

                  atlas api clusters updateCluster \
                    --version 2024-10-23 \
                    --clusterName {CLUSTER-NAME} \
                    --groupId {GROUP-ID} \
                    --file payload.json

   .. tab:: {+atlas-admin-api+}
      :tabid: api

      You can use the {+atlas-admin-api+} to enable auto-scaling by specifying 
      the auto-scaling configuration in the request body.

      .. collapsible::
         :heading: Update One Cluster in One Project
         :sub_heading: Enable auto-scaling with the Atlas Administration API
         :expanded: false

         Use the :oas-bump-atlas-op:`Update One Cluster in One Project <updategroupcluster>`
         endpoint to enable auto-scaling by including the ``autoScaling`` object.

         .. code-block:: bash
            :copyable: true
            :emphasize-lines: 18, 33

            curl --user "{PUBLIC-KEY}:{PRIVATE-KEY}" --digest \
                 --header "Accept: application/vnd.atlas.2025-03-12+json" \
                 --header "Content-Type: application/json" \
                 --include \
                 --request PATCH "https://cloud.mongodb.com/api/atlas/v2/groups/{GROUP-ID}/clusters/{CLUSTER-NAME}" \
                 --data '{
                 "replicationSpecs": [
                   {
                     "regionConfigs": [
                       {
                         "providerName": "{CLOUD-PROVIDER}",
                         "regionName": "{REGION-NAME}",
                         "priority": 7,
                         "electableSpecs": {
                           "instanceSize": "{INSTANCE-SIZE}",
                           "nodeCount": 3
                         },
                         "autoScaling": {
                           "compute": {
                             "enabled": true,
                             "scaleDownEnabled": true,
                             "minInstanceSize": "{MIN-INSTANCE-SIZE}",
                             "maxInstanceSize": "{MAX-INSTANCE-SIZE}"
                           },
                           "diskGB": {
                             "enabled": true
                           }
                         },
                         "analyticsSpecs": {
                           "instanceSize": "{ANALYTICS-INSTANCE-SIZE}",
                           "nodeCount": 1
                         },
                         "analyticsAutoScaling": {
                           "compute": {
                             "enabled": true,
                             "scaleDownEnabled": true,
                             "minInstanceSize": "{MIN-ANALYTICS-INSTANCE-SIZE}",
                             "maxInstanceSize": "{MAX-ANALYTICS-INSTANCE-SIZE}"
                           },
                           "diskGB": {
                             "enabled": true
                           }
                         }
                       }
                     ]
                   }
                 ]
               }'

Review the {+Cluster+} Tier Auto-Scaling Options
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To review the enabled auto-scaling options for {+cluster+} tier and storage:

.. procedure::
  :style: normal

  .. step:: Review the cluster tier values.
      
     In the selected :guilabel:`Auto-Scale` checkbox, review the :guilabel:`Maximum Cluster Size` and :guilabel:`Minimum Cluster Size` values, and adjust them if needed.

  .. step:: Review the :guilabel:`Allow cluster to be scaled down` option.
      
     Review the :guilabel:`Allow cluster to be scaled down` option that is checked by default when you create a new {+cluster+} in the {+atlas-ui+}.

  .. step:: Review the :guilabel:`Storage Scaling` options.
      
     Review the options under the :guilabel:`Storage Scaling` checkbox that is checked by default.

.. _opt-out-autoscaling:

Opt Out of {+Cluster+} Tier Auto-Scaling
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To opt out of {+cluster+} auto-scaling (increasing the cluster tier),
when :ref:`creating a new cluster <create-new-cluster>`, navigate to the
:guilabel:`Cluster Tier` menu, and un-check the
:guilabel:`Cluster Tier Scaling` checkbox in the :guilabel:`Auto-scale`
section.

To opt out of {+cluster+} auto-scaling (decreasing the cluster tier),
when :ref:`creating a new cluster <create-new-cluster>`, navigate to
:guilabel:`Cluster Tier` menu, and un-check the
:guilabel:`Allow cluster to be scaled down` checkbox in the
:guilabel:`Auto-scale` section.

.. _activity-feed-auto-scaling-events:

Review Auto-Scaling Activity Feed
----------------------------------

You can :ref:`view Activity Feed <view-activity-feed>` to review the events
for each |service| project. When any auto-scaling event occurs, |service| logs
the event in the project :guilabel:`Activity Feed`.

|service| uses the following audit :ref:`auto-scaling events <alert-conditions-autoscaling>`.

To view or download only auto-scaling events:

.. procedure::
   :style: normal

   .. include:: /includes/nav/list-project-activity-feed.rst

   .. step:: Filter the activity feed for Atlas events.

      In the :guilabel:`Activity Feed`, click the :guilabel:`Filter by event(s)` menu and check :guilabel:`Atlas`.

   .. step:: Search for auto-scaling events.

      In the search box above the list, start typing ``auto-scaling``.

      In the right-hand side of the menu, all auto-scaling events display.
      Deselect any that you don't want to see.
      The feed list automatically updates with each change you make.

.. _effective-fields:

Enhance Auto-Scaling with Effective Fields in {+terraform+}
-----------------------------------------------------------------

.. note::
   This feature only applies to {+dedicated-clusters+} in the ``M10`` tier and upwards, and it is
   not supported for {+Flex-clusters+}.

When auto-scaling is enabled on a {+cluster+}, {+service+} automatically adjusts instance
sizes and storage capacity based on workload. 

If you use |service-terraform| for your {+cluster+} instance sizing and storage
configurations, you can specify effective fields for a subset of {+cluster+} sizing
values. We recommend this approach, this way: 

- Spec attributes remain exactly as you defined in your {+terraform+}
  configuration. 

- Default and auto-scaled values are available separately in effective specs (for
  example, ``effectiveElectableSpecs``), and no resource drift occurs when |service|
  auto-scales your cluster. 

- Your configuration stays clean and represents your intent, while effective specs show
  the reality of what |service| has provisioned. 

Effective fields ignore only changes to fields managed by specific types of auto-scaling.
The following table details the fields ignored when effective fields and specific types of
auto-scaling are enabled: 

.. list-table::
   :header-rows: 1

   * - Type of Auto-Scaling Enabled
     - Specs
     - Ignored Fields
     - Node Type
   * - Compute or Storage
     - ``electableSpecs``
     - ``instanceSize``, ``diskSizeGB``, ``diskIOPS``
     - :term:`Electable node <electable node>`
   * - Compute or Storage
     - ``readOnlySpecs``
     - ``instanceSize``, ``diskSizeGB``, ``diskIOPS``
     - :term:`Read-only node <read-only node>`
   * - Analytics
     - ``analyticsSpecs``
     - ``instanceSize``
     - :term:`Analytics node <analytics node>`

You cannot use effective fields for :ref:`search nodes <fts-search-node-cost-overview>`.  

Enable Effective Fields
~~~~~~~~~~~~~~~~~~~~~~~~~

.. tabs::

  .. tab:: API
    :tabid: api

    To enable effective fields, include the following header in the API request:
    
    .. code-block:: bash
      :copyable: true

      --header "Use-Effective-Instance-Fields: true"

  .. tab:: Terraform
    :tabid: terraform

    Set the ``use_effective_fields`` argument to ``true`` in your resource definition for
    the {+cluster+} as the following example shows:

    .. code-block:: bash
      :copyable: true

        resource "mongodbatlas_advanced_cluster" "this" {
            project_id           = mongodbatlas_project.this.id
            name                 = var.cluster_name
            cluster_type         = var.cluster_type
            use_effective_fields = true
            replication_specs    = var.replication_specs
            tags                 = var.tags
          }

    To learn more, see :github:`Effective Fields Module Example <mongodb/terraform-provider-mongodbatlas/tree/master/examples/mongodbatlas_advanced_cluster/effective_fields>`. 

    .. include:: /includes/facts/lifecycle-ignore-changes-admonition.rst

Behavior of Effective Fields
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Overall, effective fields behaviour falls into one of three scenarios:

.. collapsible::
  :heading: Effective fields and auto-scaling are enabled
  :sub_heading: Spec fields store your last node configuration, while effective fields show the values
   after latest auto-scaling event. Click to see a code example of this scenario.
  :expanded: false 

  .. note::
     We recommend enabling effective fields as a best practice.

  .. code-block:: json
      :copyable: true

      {
      "replicationSpecs": [
        {
          "regionConfigs": [
            {
              "analyticsAutoScaling": {
                "compute": {
                  "maxInstanceSize": "M30",
                  "minInstanceSize": "M10",
                  "enabled": true,
                  "scaleDownEnabled": true
                },
                "diskGB": {
                  "enabled": true
                }
              },
              "autoScaling": {
                "compute": {
                  "maxInstanceSize": "M30",
                  "minInstanceSize": "M10",
                  "enabled": true,
                  "scaleDownEnabled": true
                },
                "diskGB": {
                  "enabled": true
                }
              },
              "effectiveAnalyticsSpecs": {
                "instanceSize": "M30",
                "diskIOPS": 3000,
                "diskSizeGB": 100.0,
                "ebsVolumeType": "STANDARD",
                "nodeCount": 0
              },
              "effectiveElectableSpecs": {
                "instanceSize": "M30",
                "diskIOPS": 3000,
                "diskSizeGB": 100.0,
                "ebsVolumeType": "STANDARD",
                "nodeCount": 3
              },
              "effectiveReadOnlySpecs": {
                "instanceSize": "M30",
                "diskIOPS": 3000,
                "diskSizeGB": 100.0,
                "ebsVolumeType": "STANDARD",
                "nodeCount": 0
              },
              "electableSpecs": {
                "instanceSize": "M20",
                "diskSizeGB": 50.0,
                "nodeCount": 3
              },
              "priority": 7,
              "providerName": "AWS",
              "regionName": "US_EAST_1"
            }
          ],
          "zoneId": "6924a70c67695449ba5625ce",
          "zoneName": "Zone 1"
        }
      ]
    }

.. collapsible::
  :heading: Effective fields are enabled and auto-scaling is disabled
  :sub_heading: Spec fields store your last node configuration, while effective 
   fields show the values that |service| stores internally. Click to see a code example of
   this scenario.
  :expanded: false 

  .. note::
     We recommend enabling effective fields as a best practice.

  .. code-block:: json
      :copyable: true

      {
      "replicationSpecs": [
        {
          "regionConfigs": [
            {
              "effectiveAnalyticsSpecs": {
                "instanceSize": "M30",
                "diskIOPS": 3000,
                "diskSizeGB": 100.0,
                "ebsVolumeType": "STANDARD",
                "nodeCount": 0
              },
              "effectiveElectableSpecs": {
                "instanceSize": "M30",
                "diskIOPS": 3000,
                "diskSizeGB": 100.0,
                "ebsVolumeType": "STANDARD",
                "nodeCount": 3
              },
              "effectiveReadOnlySpecs": {
                "instanceSize": "M30",
                "diskIOPS": 3000,
                "diskSizeGB": 100.0,
                "ebsVolumeType": "STANDARD",
                "nodeCount": 0
              },
              "electableSpecs": {
                "instanceSize": "M30",
                "diskSizeGB": 100.0,
                "nodeCount": 3
              },
              "priority": 7,
              "providerName": "AWS",
              "regionName": "US_EAST_1"
            }
          ],
          "zoneId": "6924a70c67695449ba5625ce",
          "zoneName": "Zone 1"
        }
      ]
    }

.. collapsible::
  :heading: Effective fields are disabled and auto-scaling is enabled (Not recommended)
  :sub_heading: Effective field values always match exactly those set in the corresponding
   specifications. Click to see a code example of this scenario.
  :expanded: false 

  .. include:: /includes/facts/lifecycle-ignore-changes-admonition.rst

  .. code-block:: json
      :copyable: true

      {
        "replicationSpecs": [
          {
            "regionConfigs": [
              {
                "analyticsAutoScaling": {
                  "compute": {
                    "maxInstanceSize": "M30",
                    "minInstanceSize": "M10",
                    "enabled": true,
                    "scaleDownEnabled": true
                  },
                  "diskGB": {
                    "enabled": true
                  }
                },
                "autoScaling": {
                  "compute": {
                    "maxInstanceSize": "M30",
                    "minInstanceSize": "M10",
                    "enabled": true,
                    "scaleDownEnabled": true
                  },
                  "diskGB": {
                    "enabled": true
                  }
                },
                "effectiveAnalyticsSpecs": {
                  "instanceSize": "M30",
                  "diskIOPS": 3000,
                  "diskSizeGB": 100.0,
                  "ebsVolumeType": "STANDARD",
                  "nodeCount": 0
                },
                "effectiveElectableSpecs": {
                  "instanceSize": "M30",
                  "diskIOPS": 3000,
                  "diskSizeGB": 100.0,
                  "ebsVolumeType": "STANDARD",
                  "nodeCount": 3
                },
                "effectiveReadOnlySpecs": {
                  "instanceSize": "M30",
                  "diskIOPS": 3000,
                  "diskSizeGB": 100.0,
                  "ebsVolumeType": "STANDARD",
                  "nodeCount": 0
                },
                "analyticsSpecs": {
                  "instanceSize": "M30",
                  "diskIOPS": 3000,
                  "diskSizeGB": 100.0,
                  "ebsVolumeType": "STANDARD",
                  "nodeCount": 0
                },
                "electableSpecs": {
                  "instanceSize": "M30",
                  "diskIOPS": 3000,
                  "diskSizeGB": 100.0,
                  "ebsVolumeType": "STANDARD",
                  "nodeCount": 3
                },
                "readOnlySpecs": {
                  "instanceSize": "M30",
                  "diskIOPS": 3000,
                  "diskSizeGB": 100.0,
                  "ebsVolumeType": "STANDARD",
                  "nodeCount": 0
                },
                "priority": 7,
                "providerName": "AWS",
                "regionName": "US_EAST_1"
              }
            ],
            "zoneId": "6924a70c67695449ba5625ce",
            "zoneName": "Zone 1"
          }
        ]
        }

.. _custom-alerts-auto-scaling-events:

Configure Alerts for Auto-Scaling Events
-----------------------------------------

.. important::

   .. include:: /includes/opt-out-of-auto-autoscaling-emails.rst

Auto-scaling activities are a subset of :ref:`Atlas alerts <alert-conditions>`.

Each time |service| triggers any of the :ref:`auto-scaling events <alert-conditions-autoscaling>`,
you receive default |service| alerts.

You can opt out of or change alert configuration for some or all auto-scaling
events at a project level.

To modify an alert configuration, in the :guilabel:`Category` section,
select :guilabel:`Atlas Auto Scaling` and then select the :guilabel:`Condition/Metric`
from the list. You can then modify roles for alert recipients, change a
notification method, such as email or SMS, and add a notifier, such as Slack.
To learn more, see :ref:`Configure an Auto-Scaling Alert <configure-autoscaling-alert>`.

.. _mongodb-support-autoscaling:

MongoDB Support for |service| Auto-Scaling
-------------------------------------------

If you have any questions or concerns, :ref:`contact support <request-support>`.
