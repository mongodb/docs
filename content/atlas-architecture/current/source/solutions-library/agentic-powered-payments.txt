.. _arch-center-is-agentic-powered-payments:  

=========================================
Agentic AI-Powered Payments Orchestration
=========================================

.. facet::  
   :name: genre  
   :values: tutorial  

.. meta:: 
   :keywords: Agentic AI Payments, ISO 20022 Modernization, Payments
    Orchestration, MongoDB Atlas Payments, Canonical Payment Model,
    Multi-rail Settlement
   :description: Modernize payments with Agentic AI and MongoDB. Bridge
    ISO 20022, 8583, and Stablecoins for autonomous routing, repair, and
    adaptive conversion.

.. contents:: On this page  
   :local:  
   :backlinks: none  
   :depth: 1  
   :class: singlecol 

Modernize payments with Agentic AI, a JSON-based canonical model, and
MongoDB Atlas to bridge ISO 20022, ISO 8583, and Stablecoins.

**Use cases:** `Artificial Intelligence
<https://www.mongodb.com/solutions/use-cases/artificial-intelligence>`__,
`Payments <https://www.mongodb.com/solutions/use-cases/payments>`__

**Industries:** `Financial Services
<https://www.mongodb.com/solutions/industries/financial-services>`__

**Products and tools:** `MongoDB Atlas
<https://www.mongodb.com/products/platform/atlas-database>`__, `MongoDB
Atlas Vector Search
<https://www.mongodb.com/products/platform/atlas-vector-search>`__,
`MongoDB Atlas Search
<https://www.mongodb.com/products/platform/atlas-search>`__

**Partners:** `Amazon Bedrock
<https://cloud.mongodb.com/ecosystem/amazon-bedrock>`__, `LangChain
<https://cloud.mongodb.com/ecosystem/langchain>`__, `LangGraph
<https://www.langchain.com/langgraph>`__

Solution Overview  
-----------------  

Payment processors today face an interoperability problem. They must
handle legacy card messages (ISO 8583), modern rich data standards (ISO
20022), and evolving crypto payment rails. Traditional systems rely on
rigid extract, transform, and load (ETL) pipelines that break when a new
field appears or a local regulation changes.

To connect legacy infrastructure and the future of finance, a
**JSON-native canonical payment model** acts as the essential foundation
for modern orchestration. In a post-ISO 20022 landscape, it solves the
semantic dialect fragmentation seen in varying implementations, such as
CBPR+ and FedNow. It prevents **data truncation risks identified by
SWIFT**, where legacy internal systems strip away rich ISO 20022 data
because they lack the capacity to store expanded fields. Because the
model captures the full message depth in a flexible JSON format, the
system loses no data during translation.

Coupled with a **universal API abstraction**, the platform enables IT
teams to move away from opaque binary bitmaps and fragmented standards
to a developer-friendly environment. This environment allows for
**accelerated development and reuse**, as new payment rails—from India’s
UPI to stablecoins—can be integrated as modular extensions rather than
core code rewrites.

Beyond development speed, this model establishes **centralized payment
scheme and data governance**, enabling **single-point updates** where a
new regulatory field is defined once and inherited across all
transformation logic. This shift from rigid SQL schemas to a flexible
document model eliminates the **technical debt** and maintenance burden
typical of legacy hubs. By normalizing data into a truly standardized
and consistent data model early in the life cycle, the model facilitates
**early compliance engagement** and proactive in-flight repair.
Ultimately, it provides the defined stewardship and operational agility
needed to navigate a multirail world that is constantly evolving under
new regulations such as the Markets in Crypto-Assets (MiCA).

The following video gives an example of a cross-border DE to JP payment
with agent-driven Japanese Katakana transliteration of the creditor
name:

.. video:: https://youtu.be/BOAmlTSlU2w?t=305

This solution demonstrates how a canonical payment model with Agentic AI
can redefine payments processing. By moving away from rigid SQL schemas
to a flexible, JSON-based canonical model, financial institutions can
empower AI agents to autonomously ingest, normalize, route, and repair
transactions across any payment rail, such as a SWIFT wire or USDC
transfer on a public blockchain.

JSON Foundation
~~~~~~~~~~~~~~~

A JSON-based data model is the critical foundation for such a
multipayment rail processing system because of the following properties:

- **API Native**: Modern payments, especially in crypto and fintech, use
  API-driven communication (REST/GraphQL/RPC), which is inherently
  JSON-based. A native format reduces translation latency and complexity.

- **Schema Flexibility**: JSON handles common fields (amount, sender)
  while accommodating rail-specific fields (for example, gas limits for
  crypto, zengin codes for Japan, remittance information for ISO 20022)
  without breaking the core schema.

- **AI Readiness**: LLMs train on code and JSON. Agents can analyze the
  data structure directly, detecting anomalies and performing complex
  mappings more effectively than with opaque binary bitmaps or rigid SQL
  tables.

Reference Architectures  
-----------------------  

This payments processing demo includes a **Payment Processing Platform**
that handles the complete transaction lifecycle from format onboarding
to execution and repair. The architecture delegates specific
responsibilities to specialized components:

Build
~~~~~

- **Config Builder**: Accelerates format onboarding during development.
  Aggregation pipelines extract field mappings from existing
  configurations, while an LLM suggests mappings for unknown fields. New
  formats that once took months to integrate can be configured in weeks.
  Once configurations are in place, the platform handles live
  transaction processing. 

Execute
~~~~~~~

- **Message Translation Service**: Incoming messages pass through a
  configuration-driven processing system. Most fields follow
  deterministic rules stored in MongoDB that are constructed in the
  build process described previously. Complex unstructured fields such
  as remittance information route to an AI lane powered by Claude.

- **Agentic Resolution Service**: A multiagent system that repairs
  transactions failing validation. A supervisor routes tasks to
  specialized agents: the Resolution Agent finds correct values for
  missing or invalid fields (using database lookups, Atlas Search, or AI
  inference), and  the Execution Agent applies approved changes and logs
  an audit trail.

.. figure:: /includes/images/industry-solutions/agentic-powered-payments.svg
   :figwidth: 1200px
   :alt: Payment Processing Platform architecture showing message
         translation, canonical JSON routing, and agentic resolution with
         MongoDB Atlas

   Figure 1. Payment Processing Platform architecture showing message
   translation, canonical JSON routing, and agentic resolution with
   MongoDB Atlas

As shown in the architecture diagram, MongoDB serves as the Operational
Data Layer (ODL). The system stores each transaction as a single
document containing normalized canonical JSON, and a complete audit
trail of agent decisions. Conversion configurations live in a separate
collection, making format changes data-driven. This structure provides
the auditability required for regulatory compliance.

Core Capabilities
~~~~~~~~~~~~~~~~~

This demo showcases four core capabilities where the MongoDB flexible
schema and agentic AI transform payment operations.

Data-Driven Message Translation
```````````````````````````````

Legacy systems often hard-code translation logic (for example, "field 4
maps to field 32A"). This solution stores translation rules and metadata
in MongoDB alongside the data, making the translation logic data-driven.

Instead of recompiling code to change a mapping, you update a
configuration document in MongoDB. The converter service reads these
rules dynamically—extract patterns, field mappings, and output paths—to
normalize incoming ISO 8583 or SWIFT MT messages into the canonical JSON
format.

Config-Driven Transformation:

.. code-block:: python

   # Load config from MongoDB
   config = db.conversion_configs.find_one({"_id": "MT103_to_JSON"})

   # Extract fields using patterns from config["extract"]
   for field_id, pattern in config["extract"].items():
       extracted[field_id] = re.search(pattern, message).group(1)

   # Transform using rules from config["map"]
   for mapping in config["map"]:
       value = extracted[mapping["from"]]

       if "ai" not in mapping:                  # RULES lane
           output[mapping["to"]] = value
       else:                                    # AI lane
           ai_fields.append({"value": value, "field_type": mapping["ai"]})        

Some fields resist regex patterns such as SWIFT Field 70 (remittance
information) can contain free-text payment purposes, invoice references,
and instructions across multiple lines. The configuration marks these
fields for AI processing ("ai": "remittance"), and the LLM extracts
structured data from the unstructured text. This approach keeps
deterministic rules for most fields but handles the complex remainder
through inference.

New message formats are enabled through document inserts—the
converter code stays unchanged, keeping translation logic out of
the deployment cycle.

Gen AI-Powered Adaptive Conversion
``````````````````````````````````

With rules and historical metadata stored in the database, you can use
Generative AI (LLMs) to accelerate onboarding of new payment formats.

To add support for a new message type, provide a sample message. The
Adaptive Learning Service loads all existing conversion configurations
from MongoDB and extracts fields using format-specific parsing. It
matches detected field IDs against mappings from existing
configurations. For example, if field "32A" exists in MT103_to_JSON,
that exact mapping is reused.

For fields not found in any existing config, an LLM suggests mappings
that you review and refine. This generates a draft configuration ready
for human review and deployment. Payment rails that once took months to
integrate can now be onboarded faster.

Adaptive Learning with Aggregation and LLM:

.. code-block:: python

   # Aggregation pipeline: extract field mappings from ALL existing configs
   pipeline = [
       {"$addFields": {"extract_array": {"$objectToArray": "$extract"}}},
       {"$unwind": "$map"},
       {"$project": {"field_id": "$map.from", "mapping": "$map", "source_config": "$_id"}},
       {"$group": {"_id": "$field_id", "mapping": {"$first": "$mapping"}}}
   ]
   field_lookup = {doc["_id"]: doc for doc in db.conversion_configs.aggregate(pipeline)}

   # Match detected fields against learned patterns
   for field_id in detected_fields:
       if field_id in field_lookup:
           matched[field_id] = field_lookup[field_id]["mapping"]
       else:
           unknown.append(field_id)

   # For unknown fields: LLM suggests mappings constrained by format spec
   suggestions = bedrock.invoke_claude(f"Map {unknown} to {target_format} fields")

The aggregation pipeline builds a field-to-mapping lookup from all
configs in one query. Known fields get instant matches. Unknown fields
get LLM suggestions constrained by the target format's supported fields.

Agentic Payment Repair
``````````````````````

Cross-border payments frequently fail because of local clearing
requirements, such as missing codes, incorrect scripts, or incomplete
institution data. Traditional systems queue these exceptions for manual
review, often taking days to resolve.

The Resolution Agent handles these exceptions by using tools that the
LLM autonomously selects based on docstrings and system prompts. The
pattern guides the LLM toward cheaper, faster options first.

Resolution Agent Tool Selection:

.. code-block:: python

   from langchain_core.tools import tool
   from langgraph.prebuilt import create_react_agent

   @tool
   def atlas_search(collection: str, query: str, search_fields: list, fuzzy: bool = True) -> dict:
       """Search MongoDB using Atlas Search. Use fuzzy=False for exact match first."""
       pipeline = [{"$search": {
           "index": f"{collection}_search",
           "text": {"query": query, "path": search_fields, "fuzzy": {"maxEdits": 2} if fuzzy else {}}
       }}]
       results = list(db[collection].aggregate(pipeline))
       return {"found": bool(results), "top_result": results[0] if results else None}

   @tool
   def vector_search(collection: str, query: str) -> dict:
       """Semantic search using Atlas Vector Search - matches by meaning, not keywords."""
       embedding = voyage_client.embed([query], model="voyage-3").embeddings[0]
       pipeline = [{"$vectorSearch": {
           "index": f"{collection}_vector", "path": "embedding",
           "queryVector": embedding, "numCandidates": 100, "limit": 3
       }}]
       results = list(db[collection].aggregate(pipeline))
       return {"found": bool(results), "top_result": results[0] if results else None}

   agent = create_react_agent(model=llm, tools=[atlas_search, vector_search])

The LLM reads tool docstrings and system prompts to determine which tool
to call. Atlas Search handles exact lookups and typo-tolerant matching.
Vector Search handles semantic classification where keywords don't match
but meaning does. To illustrate these capabilities in practice, consider
the following payment scenarios:

- **Japan (Katakana Transliteration)**: A payment to Tokyo
  requires the beneficiary name in Katakana script, but the incoming
  message contains Latin characters (for example, Heinrich Mueller). The
  agent first checks the ``bank_details`` collection for a known
  translation. If not found, it uses Atlas Search for fuzzy matching. As
  a fallback, it calls an LLM to transliterate the name into Katakana ("
  ハインリッヒ・ミュラー").

- **India (IFSC Code Lookup)**: A payment to India requires an
  IFSC code to route to the correct branch, but the sender provides only
  a bank name and city. The agent queries the ``ifsc_codes`` collection
  for an exact match. If the input contains typos (for example, "HDFC
  Connaught" instead of "Connaught Place"), Atlas Search handles the
  fuzzy matching to find the correct code.

Human-in-the-Loop Review
++++++++++++++++++++++++

Banks operate in a high-risk environment where autonomous changes
require oversight. Before any repair is applied, the LangGraph workflow
pauses by using an ``interrupt()`` mechanism, presenting the proposed
change for human approval.

The user sees:

- **Original Value**: The field as it arrived (for example,
  "Heinrich Mueller").

- **Proposed Value**: What the agent found (for example, "ハイン
  リッヒ・ミュラー").

- **Reasoning**: Which tool was used and why.

The user can then:

- **Approve**: The Execution Agent applies the change and logs it
  to the audit trail.

- **Reject**: The workflow ends, and the original value is
  preserved.

- **Modify**: The user provides a corrected value, which the
  system then applies.

This process ensures full auditability—users review every agent decision
before execution, and the system logs every change with before and after
values and timestamps.

LangGraph HITL Implementation:

.. code-block:: python

   from langgraph.types import interrupt

   def human_review_node(state: dict) -> dict:
       """Pause workflow for human approval."""
       # interrupt() halts execution, returns data to caller, waits for resume
       review_decision = interrupt({
           "original_value": state.get("original_value"),
           "proposed_value": state.get("solution", {}).get("proposed_value"),
           "confidence": state.get("solution", {}).get("confidence")
       })
       # When resumed: {"approved": True/False, "modified_value": "..."}
       return {"approved": review_decision.get("approved"), "human_review": review_decision}

   # Workflow: resolution → human_review → execution
   workflow.add_edge("resolution", "human_review")  # Always review before execution
   app = workflow.compile(checkpointer=MemorySaver())  # Checkpointer persists state

The ``interrupt()`` function is the key mechanism. When the workflow
reaches this node, it serializes the current state, returns the review
data to the API caller, and waits. The frontend displays this data to
the user. When the user submits the decision, the API resumes the
workflow with the decision injected as the return value of
``interrupt()``.

Multi-Rail Extensibility: Blockchain Settlement Example
```````````````````````````````````````````````````````

The canonical JSON model provides a stable internal abstraction for
integrating multiple payment rails by normalizing core payment
intent—such as parties, assets, amounts, and instructions—into a single,
consistent structure. Using the MongoDB polymorphic document model,
rail-specific execution details and metadata are stored alongside
canonical fields without requiring separate schemas or data models. This
keeps protocol complexity out of upstream and downstream systems while
enabling different rails to consume the same payment representation.

The demo shows this pattern in action with a Solana integration:
payments containing crypto settlement fields are routed to a Solana
service that executes real transfers on devnet.

Solana Service Integration:

.. code-block:: python

   # Step 1: Convert incoming message to canonical JSON
   hop1_result = await converter.convert(
       source_format="pacs.008",  # or MT103, ISO8583, etc.
       target_format="JSON",
       message=raw_message
   )

   # Step 2: Parse the canonical JSON from conversion result
   canonical_json = json.loads(hop1_result['converted_message'])

   # Step 3: Check canonical JSON for crypto settlement fields
   if canonical_json.get('crypto_blockchain') and canonical_json.get('crypto_receiver_wallet'):
       # Route to Solana
       solana_service.transfer(
           to_pubkey=canonical_json['crypto_receiver_wallet'],
           amount_sol=50000
       )

   # step 4: Solana Service Class
   class SolanaService:
       def transfer(self, to_pubkey: str, amount_sol: float) -> dict:
           response = self.client.send_raw_transaction(bytes(tx))
           self.client.confirm_transaction(response.value)
           return {
               "signature": str(response.value),
               "explorer_url": f"https://explorer.solana.com/tx/{response.value}?cluster=devnet"
           }

Unlike traditional payment rails, blockchain provides built-in proof of
settlement. Every transfer returns a Solana Explorer URL where users
verify the transaction on-chain sender, receiver, amount, timestamp, and
confirmation status. This transparency isn't available with SWIFT or
card networks, where settlement proof requires back-office
reconciliation.

The demo runs on Solana devnet, which uses the same protocol as mainnet:
Transactions are cryptographically signed, propagated to validators, and
permanently recorded.

Why MongoDB
~~~~~~~~~~~

MongoDB provides the unified data platform for the entire Payment
Processing Platform—supporting both development (format configuration)
and runtime (transaction processing, agent decisions, audit trails) in a
single system.

The platform uses several core MongoDB capabilities to streamline these
complex financial workflows:

- **Schema Flexibility**: The MongoDB document model aligns naturally
  with canonical payment formats. Developers can evolve data models
  without migrations, store related data (canonical JSON, metadata,
  audit trail) in a single document, and handle diverse message
  structures without schema conflicts.

- **Atlas Search for Typo-Tolerant Resolution**: The Resolution Agent
  uses Atlas Search when exact lookups fail—typo tolerance up to 2 edit
  distance, compound queries across multiple fields, no external search
  infrastructure required.

- **Atlas Vector Search for Semantic Matching**: For classification
  beyond exact text—semantic similarity to categorize payment
  descriptions (for example, "paying wages" maps to purpose code SALA),
  built into Atlas without a separate vector database.

- **Aggregation Pipelines for Adaptive Learning**: The Adaptive Learning
  Service uses ``$objectToArray`` and ``$unwind`` to build
  field-to-mapping lookups from all configurations in one
  query—database-level processing for instant pattern reuse.

- **Automatic Document Updates**: Single-document atomicity ensures the
  Execution Agent updates payment fields and logs the audit trail in one
  operation—critical for regulatory compliance in high-risk financial
  services.

Together, these capabilities make MongoDB the ODL for AI-driven payment
processing—supporting the demands of modern multirail finance.

Data Model Approach
-------------------

MongoDB stores two types of documents that power the payment processing
pipeline.

.. procedure::
   :style: normal

   .. step:: Canonical JSON (Payment Document)

      .. code-block:: JSON

         {
           "transaction_ref": "MED-CH-ZA-2024-001",
           "value_date": "2024-12-15",
           "amount": "180000.00",
           "currency": "CHF",
           "debtor_name": "SWISS PHARMA INTERNATIONAL AG",
           "debtor_account": "CH9300762011623852957",
           "debtor_bank": "UBSWCHZH80A",
           "creditor_name": "SOUTH AFRICAN HEALTH SUPPLIES PTY LTD",
           "creditor_account": "ZA123456789012345678901",
           "creditor_bank": "ABSAZAJJXXX",
           "remittance_info": "INVOICE MED-ZA-2024-5678",
           "charge_bearer": "SHA"
           "metadata": {...}
         }

      This flat structure acts as a universal bridge with the following
      mappings:

      - MT103 maps to JSON, which maps to pacs.008.
      - MT202 maps to JSON, which maps to pacs.009.

      One semantic concept equals one field name and the same
      mapping across all formats. Each document also includes a
      ``metadata`` object containing processing details and an
      ``audit_trail`` that logs every agent decision (old value, new
      value, tool used, timestamp).

   .. step:: Conversion Configuration

      The system supports both multihop conversions through JSON and
      direct conversions for common paths. The following example shows a
      direct conversion config:   

      .. code-block:: JSON

         {
           "_id": "MT103_to_pacs.008",
           "extract": {
             "20": ":20:([^\\n:]+)",
             "32A": ":32A:([^\\n:]+)",
             "50K": ":50K:([\\s\\S]*?)(?=:[0-9])"
           },
           "map": [
             {"from": "20", "to": ["transaction_ref"]},
             {"from": "32A", "to": ["value_date", "currency", "amount"], "split": [6, 9]},
             {"from": "50K", "to": ["debtor_account", "debtor_name"], "multiline": true},
             {"from": "70", "to": "remittance_info", "ai": "remittance"}
           ],
           "output": {
             "transaction_ref": "Document.FIToFICstmrCdtTrf.CdtTrfTxInf.PmtId.InstrId",
             "amount": "Document.FIToFICstmrCdtTrf.CdtTrfTxInf.IntrBkSttlmAmt.#text"
           }
         }

      Three sections define the conversion: 
      
      1. ``extract``: Pulls fields by using regex. 
      #. ``map``: Transforms them (splitting composites, formatting
         dates, routing to AI). 
      #. ``output``: Places values in the target format. 
      
      Document inserts enable new message formats, removing translation
      logic from the deployment cycle. For new payment rails, the
      canonical model accelerates connectivity by serving as a universal
      JSON integration point, ensuring that core business logic remains
      untouched as you scale.

Build the Solution
------------------

For the complete implementation, follow the instructions in the `GitHub
repository
<https://github.com/mongodb-industry-solutions/fsi-payments-processing>`__.    

The Payment Processing Platform consists of two microservices: a **Message
Translation Service** that handles format conversion through a three-lane
pipeline—including a Configuration Builder for development-time format
onboarding—and a **Payment Agent** that repairs invalid transactions by
using a LangGraph multiagent workflow. Both services share MongoDB Atlas
as their ODL.

.. procedure::
   :style: normal

   .. step:: Implement the Payment Converter Service

      The converter transforms messages between formats (MT103, ISO
      20022, ISO 8583) by using configuration stored in MongoDB rather
      than hard-coded logic.

      a. **Set up the Translation Pipeline**: Configure the extractor to
         route fields based on complexity: deterministic regex patterns
         for structured fields such as transaction references and
         amounts, and LLM calls through AWS Bedrock for unstructured
         fields such as remittance information.

      b. **Store conversion configurations in MongoDB**: Each
         configuration document contains three sections: ``extract``
         (regex patterns), ``map`` (field transformations), and
         ``output`` (target format paths). The converter reads these at
         runtime—no code changes needed for new formats.

      c. **Build the output generator**: The builder takes transformed
         fields and constructs the target format. For ISO 20022, the
         builder generates valid XML with proper namespaces. For JSON
         targets, it outputs the flat canonical structure.

   .. step:: Implement the Payment Agent with LangGraph

      The agent repairs transactions that fail validation—missing IFSC
      codes, incorrect scripts, incomplete beneficiary data—using a
      multiagent workflow.

      a. **Create the Supervisor Agent**: This agent receives a repair
         task (for example, "creditor_name needs Katakana for Japan")
         and determines whether to route to the Resolution Agent for
         research or directly to Execution if the fix is known.

      b. **Build the Resolution Agent with tools**: quip it with three
         tools: ``atlas_search`` (database lookup with exact or fuzzy
         matching), ``vector_search`` (semantic similarity for
         classification), and ``transliterate_text`` (LLM-based script
         conversion). The agent determines which tool to use based on
         the problem context.

      c. **Add the Human Review checkpoint**: Use LangGraph's
         ``interrupt()`` function to pause the workflow before applying
         changes. The user views the original value, proposed fix,
         confidence score, and reasoning. They can approve, reject, or
         modify before the workflow resumes.

      d. **Implement the Execution Agent**: On approval, this agent
         writes the corrected value to the canonical JSON and appends an
         entry to the audit trail with old value, new value, tool used,
         and timestamp.

   .. step:: Enable Adaptive Learning for New Formats

      When onboarding a new payment format, the system auto-generates
      configuration by learning from existing patterns.

      a. **Load existing configurations**: The Adaptive Learning Service
         queries MongoDB for all conversion configurations and builds a
         map of field IDs to their canonical mappings (for example,
         field "32A" maps to ``value_date``, ``currency``, and
         ``amount``).

      b. **Parse the sample message**: By using format-specific
         extractors—such as SWIFT tag patterns, ISO 8583 field
         positions, and XML paths—the service identifies all fields
         present in the new format.

      c. **Match known patterns and infer unknown fields**: For fields
         that exist in other configurations, the system reuses the exact
         mapping. For fields not found anywhere, the service calls an
         LLM with context about the format type to suggest a mapping.

      d. **Generate and review the configuration**: The service outputs
         a draft configuration document with all mappings. The system
         flags fields below the confidence threshold for human review.
         Once approved, it inserts the document into MongoDB—the
         converter processes it up immediately.

Key Learnings
-------------

- **Aggregation pipelines for adaptive learning**: Build
  field-to-mapping lookups from all existing configurations in a single
  query by using ``$objectToArray`` and ``$unwind``. Known fields
  receive automatic mappings; unknown fields receive LLM suggestions
  constrained by format specifications.

- **Atlas Search for typo tolerance**: When exact database lookups fail,
  fuzzy search with edit distance handles misspellings (for example "HDFC
  Connaught" maps to "HDFC Connaught Place") without falling back to
  slower AI inference.

- **Configuration-driven conversion**: Store extraction patterns, field
  mappings, and output paths in MongoDB documents rather than
  application code. Adding a new payment format becomes a database
  insert, not a deployment.  

- **Human-in-the-loop compliance**: Use the LangGraph ``interrupt()``
  mechanism to pause agent workflows before applying changes, present
  proposed fixes to users, and resume only after explicit approval.

- **Supervisor multi-agent pattern**: Route research tasks—such as IFSC
  lookup and transliteration—to a Resolution Agent with database tools,
  and execution tasks—such as field updates—to an Execution Agent. This
  separation ensures focused expertise and cleaner audit trails.

- **Unified operational data layer**: Store canonical JSON, conversion
  configurations, and audit trails in MongoDB Atlas. This centralization
  eliminates data silos between payment rails and provides the complete
  transaction context agents need.

Authors 
-------

- Wei You Pan, MongoDB
- Kiran Tulsulkar, MongoDB
- Ainhoa Múgica, MongoDB
- Daniel Jamir, MongoDB

Learn More
----------

- :ref:`Unified Interface for RAG Applications
  <arch-center-is-rag-applications>`

- :ref:`Financial Crime Mitigation with MongoDB Atlas
  <arch-center-is-fincrime-mitigation>`

- :ref:`AI-Driven Interactive Banking
  <arch-center-is-interactive-banking>`