.. _streams-agg-pipeline-emit:
.. _atlas-sp-agg-emit:

===============================================
``$emit`` Aggregation Stage (Stream Processing)
===============================================

.. default-domain:: mongodb

.. meta::
   :keywords: atlas stream processing, $emit aggregation pipeline stage 
   :description: Learn how to use the $emit stage to output processed data
                 to streaming data platforms.

.. facet::
   :name: genre
   :values: reference

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

.. _atlas-sp-agg-emit-def:

Definition
----------

The ``$emit`` stage specifies a connection in the
:ref:`Connection Registry <atlas-sp-manage-connections>` to emit
messages to. The following connection types are supported:

- {+kafka+} broker
- MongoDB :manual:`time series collection </core/timeseries-collections>`
- {+aws+} Kinesis data stream
- {+aws+} |s3| bucket

.. _atlas-sp-agg-emit-placement:

Placement
---------

``$emit`` must be the last stage of any pipeline it appears in. You can
use only one ``$emit`` stage per pipeline.

.. _atlas-sp-agg-emit-syntax:

Syntax
------

.. _sp-emit-kafka:

Apache Kafka Broker
~~~~~~~~~~~~~~~~~~~

To write processed data to an {+kafka+} broker, use the ``$emit``
pipeline stage with the following prototype form:

.. code-block:: json

   {
     "$emit": {
       "connectionName": "<registered-connection>",
       "topic": "<target-topic>" | <expression>,
       "schemaRegistry": {
	 "connectionName": "<schema-registry-name>",
	 "valueSchema": {
	   type: "<schema-type>",
	   schema: <schema-name>,
	   options: {
	     subjectNameStrategy: "<topic-name-strategy>",
	     autoRegisterSchemas: true
	   }
	 }
       },       
       "config": {
         "acks": <number-of-acknowledgements>,
         "compression_type": "<compression-type>",
         "dateFormat": "default" | "ISO8601",
         "headers": "<expression>",
         "key": "<key-string>" | { key-document },
         "keyFormat": "<serialization-type>",
         "outputFormat": "basicJson" | "canonicalJson" | "relaxedJson",
         "tombstoneWhen": <expression>
       }
     }
   }

The ``$emit`` stage takes a document with the following fields:

.. list-table:: 
   :header-rows: 1
   :widths: 25 10 12 40

   * - Field 
     - Type 
     - Necessity 
     - Description

   * - ``connectionName`` 
     - string
     - Required 
     - Name, as it appears in the 
       :ref:`Connection Registry <atlas-sp-manage-connections>`, of the
       connection to ingest data from.

   * - ``topic``
     - string or expression
     - Required 
     - Name of the {+kafka+} topic to emit messages to.

   * - ``schemaRegistry``
     - document 
     - Optional
     - Document enabling the use of a `Schema Registry
       <https://docs.confluent.io/platform/current/schema-registry/index.html>`__
       to support writing to an `Avro-serialized
       <https://avro.apache.org/docs/>`__ source.

       To enable this feature, you must create a :ref:`Schema Registry
       connection <atlas-sp-add-connection>`.

   * - ``schemaRegistry.connectionName``
     - string 
     - Conditional
     - Name of the Schema Registry connection to use for Avro deserialization.

   * - ``schemaRegistry.valueSchema``
     - document | expression
     - Conditional
     - Document defining the properties of your serialization
       schema, or an expression that evaluates to such.

   * - ``schemaRegistry.valueSchema.type``
     - string
     - Conditional
     - The type of serialization for which to use the Schema
       Registry. {+atlas-sp+} currently supports ``"avro"``
       serialization through Schema Registry connections.

   * - ``schemaRegistry.valueSchema.schema``
     - document
     - Conditional
     - Document defining your `Schema Declaration
       <https://avro.apache.org/docs/1.12.0/specification/#schema-declaration>`__.

   * - ``schemaRegistry.valueSchema.options``
     - document
     - Optional
     - Document defining optional configuration parameters for your
       schema registry connection.

   * - ``schemaRegistry.valueSchema.options.autoRegisterSchemas``
     - boolean
     - Optional
     - Toggle determining whether or not to automatically register
       schemas when processing documents with unrecognized schemas. If
       set to false, documents with unrecognized schemas are sent to
       the :ref:`dead letter queue <atlas-sp-dlq>`.

       Defaults to ``true``.

   * - ``schemaRegistry.valueSchema.options.subjectNameStrategy``
     - string
     - Conditional
     - Method of determining the subject name of automatically
       registered schemas. Must be one of the following:

       - ``"TopicNameStrategy"``: Uses the Kafka ``{topic}`` name as
         the subject name.
       - ``"RecordNameStrategy"``: Uses the Avro record name as the
         subject name.
       - ``"TopicRecordNameStrategy"``: Uses a composite of the Kafka
         ``{topic}`` name and Avro record name as the subject name.

       Defaults to ``"TopicNameStrategy"``. You can set this parameter
       only if
       ``schemaRegistry.valueSchema.options.autoRegisterSchemas`` is
       set to ``true``.
	 

   * - ``config``
     - document 
     - Optional
     - Document containing fields that override various default 
       values.

   * - ``config.acks``
     - int
     - Optional
     - Number of acknowledgements required from the {+kafka+} {+cluster+} for a 
       successful ``$emit`` operation.

       The default value is ``all``. {+atlas-sp+} supports the following values:
     
       - ``-1``
       - ``0``
       - ``1``
       - ``all``

   * - ``config.compression_type``
     - string
     - Optional
     - Compression type for all data generated by the producer. The default
       is none (i.e no compression). Valid values are:

       - ``none``
       - ``gzip``
       - ``snappy``
       - ``lz4``
       - ``zstd``

       Compression is used for full batches of data, so the efficacy of batching
       impacts the compression ratio; more batching results in better compression.

   * - ``config.dateFormat``
     - string
     - Optional
     - Date format for the date value. Valid values are:

       - ``default`` - to use the default of the ``outputFormat``.
       - ``ISO8601`` - to convert dates to strings in the ``ISO8601``
         format, which includes millisecond precision
         (``YYYY-MM-DDTHH:mm:ss.sssZ``).

       For example:

       Consider the following input.

       .. code-block:: json

          { "flightTime" :  ISODate('2025-01-10T20:17:38.387Z') }

       If ``$emit.config.dateFormat`` is set to ``default``,
       output looks similar to the following:

       .. code-block:: shell 
          :copyable: false

          { "flightTime" :  {$date :"2025-01-10T20:17:38.387Z"}}

       If ``$emit.config.dateFormat`` is set to ``ISO8601``,
       output looks similar to the following:

       .. code-block:: shell
          :copyable: false

          { "flightTime" :  "2025-01-10T20:17:38.387Z" }

   * - ``config.headers``
     - expression
     - Optional
     - Headers to add to the output message. The expression must evaluate
       to either an object or an array.

       If the expression evaluates to an object, {+atlas-sp+}
       constructs a header from each key-value pair in that object, where
       the key is the header name, and the value is the header value.

       If the expression evaluates to an array, it must take the form of
       an array of key-value pair objects. For example:     
        
       .. code-block:: json
  
          [
            {k: "name1", v: ...},
            {k: "name2", v: ...},
            {k: "name3", v: ...} 
          ]

       {+atlas-sp+} constructs a header from each object in the array, 
       where the key is the header name, and the value is the header value.
       {+atlas-sp+} supports header values of the following types:

       - ``binData``
       - ``string``
       - ``object``
       - ``int``
       - ``long``
       - ``double``
       - ``null``

   * - ``config.key``
     - object | string | expression
     - Optional 
     - Expression that evaluates to a {+kafka+} message key.

       If you specify ``config.key``, you must specify
       ``config.keyFormat``.

   * - ``config.keyFormat``
     - string
     - Conditional
     - Data type used to serialize {+kafka+} key data. Must be one
       of the following values:

       - ``"binData"``
       - ``"string"``
       - ``"json"``
       - ``"int"``
       - ``"long"``

       Defaults to ``binData``. If you specify ``config.key``, you
       must specify ``config.keyFormat``. If the ``config.key`` of a
       document does not serialize successfully to the specified
       data type, {+atlas-sp+} sends it to your :ref:`dead letter
       queue <atlas-sp-dlq>`.

   * - ``config.outputFormat``
     - string
     - Optional 
     - JSON format to use when emitting messages to {+kafka+}. Must be one of the
       following values:

       - ``"basicJson"``
       - ``"canonicalJson"``
       - ``"relaxedJson"``

       Defaults to ``"relaxedJson"``.

       To learn more, see :ref:`Basic JSON
       <atlas-sp-agg-emit-basic-json>`.

   * - ``config.tombstoneWhen``
     - expression
     - Optional 
     - Expression that determines when to emit ``null`` to
       Kafka. The expression must evaluate to either boolean ``true``
       or ``false``. When the expression evaluates to ``true`` for a
       given document, {+atlas-sp+} emits a ``null`` in its place
       to your Kafka sink. When the expression evaluates to false,
       {+atlas-sp+} emits the document as it exists when it reaches the
       ``$emit`` stage.

       If the expression fails to evaluate to a boolean value, or
       can't be evaluated, {+atlas-sp+} writes the document to the
       :ref:`DLQ <atlas-sp-dlq>`.

       This setting can be used to enable topic compaction if you
       provide ``$emit.config.key`` and ``$emit.config.keyFormat``
       values. If you don't provide these values, {+atlas-sp+} still
       emits ``null`` when this expression evaluates to ``true``, but
       these don't trigger Kafka topic compaction.

.. _sp-emit-timeseries:

{+service+} Time Series Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To write processed data to an {+service+} time series collection,
use the ``$emit`` pipeline stage with the following prototype form:

.. code-block:: json

   {
     "$emit": {
       "connectionName": "<registered-connection>",
       "db": "<target-db>" | <expression>,
       "coll": "<target-coll>" | <expression>,
       "timeseries": {
         <options>
       }
     }
   }

The ``$emit`` stage takes a document with the following fields:

.. list-table:: 
   :header-rows: 1
   :widths: 25 10 12 40

   * - Field 
     - Type 
     - Necessity 
     - Description

   * - ``connectionName`` 
     - string
     - Required 
     - Name, as it appears in the 
       :ref:`Connection Registry <atlas-sp-manage-connections>`, of the
       connection to ingest data from.

   * - ``db``
     - string | expression
     - Required 
     - Name of, or expression that resolves to the {+service+} database that contains the target
       time series collection.

   * - ``coll``
     - string | expression
     - Required 
     - Name of, or expression that resolves to the {+service+} time series collection 
       to write to.

   * - ``timeseries``
     - document
     - Required 
     - Document defining the :manual:`time series fields
       </core/timeseries/timeseries-procedures/#time-series-field-reference>`
       for the collection.

.. note::

   The maximum size for documents within a time series collection is 4 MB. 
   To learn more, see :ref:`manual-timeseries-collection-limitations`.

.. _sp-emit-kinesis:

{+aws+} Kinesis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To write processed data to {+aws+} Kinesis, use the ``$emit``
pipeline stage with the following prototype form:

.. code-block:: json

   {
     "$emit": {
       "connectionName": "<registered-connection>",
       "stream": "<stream-name>",
       "region": "<aws-region>",
       "partitionKey": "<key>" | <field> | <expression>
       "config": {
         "outputFormat": "basicJson" | "canonicalJson" | "relaxedJson",
         "dateFormat": "default" | "ISO8601",
       }
     }
   }

The ``$emit`` stage takes a document with the following fields:

.. list-table:: 
   :header-rows: 1
   :widths: 25 10 12 40

   * - Field 
     - Type 
     - Necessity 
     - Description

   * - ``connectionName`` 
     - string
     - Required 
     - Name, as it appears in the 
       :ref:`Connection Registry <atlas-sp-manage-connections>`, of the
       connection to ingest data from.

   * - ``stream``
     - string
     - Required 
     - Name of the Kinesis data stream to which to connect.

   * - ``region``
     - string 
     - Optional
     - Region in which the Kinesis Data Stream operates. {+aws+}
       supports multiple streams with the same name, each in a
       distinct region. This parameter allows {+atlas-sp+} to
       distinguish between such streams.

   * - ``config``
     - document 
     - Optional
     - Document containing fields that override various default 
       values.

   * - ``config.outputFormat``
     - string
     - Optional 
     - JSON format to use when emitting messages to Kinesis. Must be one of the
       following values:

       - ``"basicJson"``
       - ``"canonicalJson"``
       - ``"relaxedJson"``

       Defaults to ``"relaxedJson"``.

       To learn more, see :ref:`Basic JSON
       <atlas-sp-agg-emit-basic-json>`.

   * - ``config.dateFormat``
     - string
     - Optional
     - Date format for the date value. Valid values are:

       - ``default`` - to use the default of the ``outputFormat``.
       - ``ISO8601`` - to convert dates to strings in the ``ISO8601``
         format, which includes millisecond precision
         (``YYYY-MM-DDTHH:mm:ss.sssZ``).

       For example:

       Consider the following input.

       .. code-block:: json

          { "flightTime" :  ISODate('2025-01-10T20:17:38.387Z') }

       If ``$emit.config.dateFormat`` is set to ``default``,
       output looks similar to the following:

       .. code-block:: shell 
          :copyable: false

          { "flightTime" :  {$date :"2025-01-10T20:17:38.387Z"}}

       If ``$emit.config.dateFormat`` is set to ``ISO8601``,
       output looks similar to the following:

       .. code-block:: shell
          :copyable: false

          { "flightTime" :  "2025-01-10T20:17:38.387Z" }

.. _sp-emit-s3:

{+aws+} S3
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To write processed data to an {+aws+} |s3| bucket sink connection, use
the ``$emit`` pipeline stage with the following prototype form:

.. code-block:: json

   {
     "$emit": {
       "connectionName": "<registered-connection>",
       "bucket": "<target-bucket>",
       "region": "<target-region>",
       "path": "<key-prefix>" | <expression>,
       "config": {
         "writeOptions": {
           "count": <doc-count>,
           "bytes": <threshold>,
           "interval": {
             "size": <unit-count>,
             "unit": "<time-denomination>"
           }
         },
         "delimiter": "<delimiter>",
         "outputFormat": "basicJson" | "canonicalJson" | "relaxedJson",
         "dateFormat": "default" | "ISO8601",
         "compression": "gzip" | "snappy",
         "compressionLevel": <level>
       }
     }
   }


The ``$emit`` stage takes a document with the following fields:

.. list-table:: 
   :header-rows: 1
   :widths: 30 10 10 40

   * - Field 
     - Type 
     - Necessity 
     - Description

   * - ``connectionName`` 
     - string
     - Required 
     - Name, as it appears in the 
       :ref:`Connection Registry <atlas-sp-manage-connections>`, of the
       connection to write data to.

   * - ``bucket``
     - string
     - Required 
     - Name of the |s3| bucket to which to write data.

   * - ``region``
     - string
     - Optional 
     - Name of the {+aws+} region in which the target bucket resides.
       If you host your {+spw+} in an {+aws+} region, this parameter
       defaults to that region. Otherwise, it defaults to the {+aws+}
       region nearest your {+spw+} host region.

   * - ``path``
     - string | expression
     - Required 
     - Prefix of the key of objects written to the |s3| bucket. Must
       either be a literal :aws:`prefix
       </AmazonS3/latest/userguide/using-prefixes.html>` string or an
       expression that evaluates to a string.

   * - ``config``
     - document
     - Optional 
     - Document containing additional parameters that override various
       default values.

   * - ``config.writeOptions``
     - document
     - Optional 
     - Document containing additional parameters governing write behavior.
       These parameters trigger write behavior according to which threshold is
       met first.

       For example, if the ingested documents reach the ``config.writeOptions.count``
       threshold without reaching the ``config.writeOptions.interval`` threshold,
       the stream processor still emits these documents to |s3| according to the
       ``config.writeOptions.count`` threshold.

   * - ``config.writeOptions.count``
     - integer
     - Optional 
     - Number of documents to group into each file written to |s3|.

   * - ``config.writeOptions.bytes``
     - integer
     - Optional
     - Specifies the minimum number of bytes that must accumulate
       before a file is written to |s3|. The byte count is determined by
       the size of the BSON documents ingested by the pipeline, not the size
       of the final output file.

   * - ``config.writeOptions.interval``
     - document
     - Optional
     - Specifies a timer for bulk writing documents as a combination of
       ``size`` and ``unit``.

       Defaults to 1 minute. You can't set the ``size`` to 0 for any
       ``unit``. The maximum interval is 7 days.

   * - ``config.writeOptions.interval.size``
     - integer
     - Conditional
     - The number of units specified by
       ``writeOptions.interval.unit`` after which the stream
       processor bulk writes documents to |s3|.

       Defaults to ``1``. You can't set a ``size`` of 0. If you define
       ``writeOptions.interval``, you must also define this parameter.

   * - ``config.writeOptions.interval.unit``
     - string
     - Conditional
     - The denomination of time in which to count the bulk write
       timer. This parameter supports the following values:

       - ``ms``
       - ``second``
       - ``minute``
       - ``hour``
       - ``day``

       Defaults to ``minute``. If you define
       ``writeOptions.interval``, you must also define this parameter.

   * - ``config.delimiter``
     - string
     - Optional 
     - Delimiter between each entry in the emitted file.

       Defaults to ``\n``.

   * - ``config.outputFormat``
     - string
     - Optional
     - Specifies the output format of the JSON written to |s3|.
       Must be one of the following values:

       - ``"basicJson"``
       - ``"canonicalJson"``
       - ``"relaxedJson"``

       Defaults to "``relaxedJson``".

       To learn more, see :ref:`Basic JSON
       <atlas-sp-agg-emit-basic-json>`.

   * - ``config.dateFormat``
     - string
     - Optional
     - Date format for the date value. Valid values are:

       - ``default`` - to use the default of the ``outputFormat``.
       - ``ISO8601`` - to convert dates to strings in the ``ISO8601``
         format, which includes millisecond precision
         (``YYYY-MM-DDTHH:mm:ss.sssZ``).

       For example, if you add the following record to the pipeline:
   
       .. code-block:: json

          { "flightTime" :  ISODate('2025-01-10T20:17:38.387Z') }

       then if ``$emit.config.dateFormat`` is set to ``default``,
       output looks similar to the following:

       .. code-block:: shell 
          :copyable: false

          { "flightTime" :  {$date :"2025-01-10T20:17:38.387Z"}}

       If ``$emit.config.dateFormat`` is set to ``ISO8601``,
       output looks similar to the following:

       .. code-block:: shell 
          :copyable: false

          { "flightTime" :  "2025-01-10T20:17:38.387Z" }

   * - ``config.compression``
     - string
     - Optional 
     - Name of the compression algorithm to use.
       Must be one of the following values:

       - "gzip"
       - "snappy"

   * - ``config.compressionLevel``
     - string
     - Conditional
     - Level of compression to apply to the emitted message. Supports
       values ``1-9`` inclusive; higher values mean more compression.

       Defaults to ``6``.

       This parameter is required for and limited to ``gzip``. If you
       set ``config.compression`` to ``snappy``, setting this parameter
       has no effect.

.. _atlas-sp-agg-emit-basic-json:

Basic JSON
~~~~~~~~~~

To ease ingestion of messages, the ``$emit`` stage supports writing
processed data to sinks in the Basic JSON format, which simplifies the
Extended (``canonicalJson``) and Relaxed Extended (``relaxedJson``) JSON
formats. Basic JSON does not use MongoDB's Extended JSON wrappers and
therefore does not preserve all BSON types.

You can specify the Basic JSON format by setting the
``config.outputFormat`` field to ``"basicJson"`` in your ``$emit``
stage.

The following table provides examples of these simplifications for all
affected fields.

.. list-table:: 
   :header-rows: 1
   :widths: 20 40 40

   * - Field Type
     - relaxedJson 
     - basicJson
   * - Binary
     - ``{ "binary": { "$binary": { "base64": "gf1UcxdHTJ2HQ/EGQrO7mQ==", "subType": "00" }}}``
     - ``{ "binary": "gf1UcxdHTJ2HQ/EGQrO7mQ=="}``

   * - Date
     - ``{ "date": { "$date": "2024-10-24T18:07:29.636Z"}}``
     - ``{ "date": 1729625275856}``

   * - Decimal
     - ``{ "decimal": { "$numberDecimal": "9.9" }}``
     - ``{ "decimal": "9.9" }``

   * - Timestamp
     - ``{ "timestamp": { "$timestamp": { "t": 1729793249, "i": 1 }}}``
     - ``{ "timestamp": 1729793249000}``
   * - ObjectId
     - ``{ "_id": { "$oid": "671a8ce1497407eff0e17cba" }}``
     - ``{ "_id": "6717fcbba18c8a8f74b6d977" }``

   * - Negative Infinity
     - ``{ "negInf": { "$numberDouble": "-Infinity" }}``
     - ``{ "negInf": "-Infinity" }``

   * - Positive Infinity
     - ``{ "posInf": { "$numberDouble": "Infinity" }}``
     - ``{ "posInf": "Infinity" }``

   * - Regular Expressions
     - ``{ "regex": { "$regularExpression": { "pattern": "ab+c", "options": "i" }}}``
     - ``{ "regex": { "pattern": "ab+c", "options": "i" }}``

   * - UUID
     - ``{ "uuid": { "$binary": { "base64": "Kat+fHk6RkuAmotUmsU7gA==", "subType": "04" }}}``
     - ``{ "uuid": "420b7ade-811a-4698-aa64-c8347c719cf1"}``

.. _atlas-sp-agg-emit-behavior:

Behavior
--------

You can only write to a single {+service+} time series collection per
stream processor. If you specify a collection that doesn't exist,
{+service+} creates the collection with the time series fields you
specified. You must specify an existing database.

You can use a :manual:`dynamic expression
</reference/operator/aggregation/#expression-operators>` as the value
of the ``topic``, ``db``, and ``coll`` fields to enable your stream processor to write to
different targets on a message-by-message basis. The
expression must evaluate to a string.

.. example::

   You have a stream of transaction events that generates messages of   
   the following form:

   .. code-block:: json

      {
        "customer": "Very Important Industries",
        "customerStatus": "VIP",
        "tenantId": 1,
        "transactionType": "subscription"
      }

      {
        "customer": "N. E. Buddy",
        "customerStatus": "employee",
        "tenantId": 5,
        "transactionType": "requisition"
      }

      {
        "customer": "Khan Traktor",
        "customerStatus": "contractor",
        "tenantId": 11,
        "transactionType": "billableHours"
      }

   To sort each of these into a distinct {+kafka+} topic, you can write  
   the following ``$emit`` stage:

   .. code-block:: json

      {
        "$emit": {
          "connectionName": "kafka1",
          "topic": "$customerStatus"
        }
      }

   This ``$emit`` stage:

   - Writes the ``Very Important Industries`` message to a topic named 
     ``VIP``.
   - Writes the ``N. E. Buddy`` message to a topic named ``employee``.
   - Writes the ``Khan Traktor`` message to a topic named 
     ``contractor``.

For more information on dynamic expressions, see :manual:`expression
operators </reference/operator/aggregation/#expression-operators>`.

If you specify a topic that doesn't already exist, {+kafka+}
automatically creates the topic when it receives the first message
that targets it.

If you specify a topic with a dynamic expression, but 
{+atlas-sp+} cannot evaluate the expression for a given message, 
{+atlas-sp+} sends that message to the :term:`dead letter queue` if configured 
and processes subsequent messages. If there is no :term:`dead letter queue` 
configured, then {+atlas-sp+} skips the message completely and processes 
subsequent messages.

Examples
~~~~~~~~

A streaming data source generates detailed weather reports from
various locations, conformant to the schema of the :ref:`Sample
Weather Dataset <sample-weather>`. The following aggregation has three
stages:

1. The :pipeline:`$source` stage establishes a connection with the
   {+kafka+} broker collecting these reports in a topic named
   ``my_weatherdata``, exposing each record as it is ingested to the
   subsequent aggregation stages. This stage also overrides the name
   of the timestamp field it projects, setting it to
   ``ingestionTime``.

#. The :pipeline:`$match` stage excludes documents that have an
   ``airTemperature.value`` of greater than or equal to ``30.0`` and
   passes along the documents with an ``airTemperature.value`` less
   than ``30.0`` to the next stage.

#. The :pipeline:`$addFields` stage enriches the stream with metadata.

#. The ``$emit`` stage writes the output to a topic named
   ``stream`` over the ``weatherStreamOutput`` Kafka broker
   connection.
 
.. code-block:: json
   :copyable: true

   {
     "$source": {
       "connectionName": "sample_weatherdata",
       "topic": "my_weatherdata",
       "tsFieldName": "ingestionTime"
     }
   },
   {
     "$match": {
       "airTemperature.value": {
         "$lt": 30
       }
     }
   },
   {
     "$addFields": {
       "processorMetadata": {
         "$meta": "stream"
       }
     }
   },
   {
     "$emit": {
       "connectionName": "weatherStreamOutput",
       "topic": "stream"
     }
   }

Documents in the ``stream`` topic take the following form:

.. code-block:: json
   :copyable: false

   {
     "st": "x+34700+119500",
     "position": {
       "type": "Point",
       "coordinates": [122.8, 116.1]
     },
     "elevation": 9999,
     "callLetters": "6ZCM",
     "qualityControlProcess": "V020",
     "dataSource": "4",
     "type": "SAO",
     "airTemperature": {
       "value": 6.7,
       "quality": "9"
     },
     "dewPoint": {
       "value": 14.1,
       "quality": "1"
     },
     "pressure": {
       "value": 1022.2,
       "quality": "1"
     },
     "wind": {
       "direction": {
         "angle": 200,
         "quality": "9"
       },
       "type": "C",
       "speed": {
         "rate": 35,
         "quality": "1"
       }
     },
     "visibility": {
       "distance": {
         "value": 700,
         "quality": "1"
       },
       "variability": {
         "value": "N",
         "quality": "1"
       }
     },
     "skyCondition": {
       "ceilingHeight": {
         "value": 1800,
         "quality": "9",
         "determination": "9"
       },
       "cavok": "N"
     },
     "sections": ["AA1", "AG1", "UG1", "SA1", "MW1"],
     "precipitationEstimatedObservation": {
       "discrepancy": "0",
       "estimatedWaterDepth": 999
     },
     "atmosphericPressureChange": {
       "tendency": {
         "code": "4",
         "quality": "1"
       },
       "quantity3Hours": {
         "value": 3.8,
         "quality": "1"
       },
       "quantity24Hours": {
         "value": 99.9,
         "quality": "9"
       }
     },
     "seaSurfaceTemperature": {
       "value": 9.7,
       "quality": "9"
     },
     "waveMeasurement": {
       "method": "M",
       "waves": {
         "period": 8,
         "height": 3,
         "quality": "9"
       },
       "seaState": {
         "code": "00",
         "quality": "9"
       }
     },
     "pastWeatherObservationManual": {
       "atmosphericCondition": {
         "value": "6",
         "quality": "1"
       },
       "period": {
         "value": 3,
         "quality": "1"
       }
     },
     "skyConditionObservation": {
       "totalCoverage": {
         "value": "02",
         "opaque": "99",
         "quality": "9"
       },
       "lowestCloudCoverage": {
         "value": "00",
         "quality": "9"
       },
       "lowCloudGenus": {
         "value": "00",
         "quality": "1"
       },
       "lowestCloudBaseHeight": {
         "value": 1750,
         "quality": "1"
       },
       "midCloudGenus": {
         "value": "99",
         "quality": "1"
       },
       "highCloudGenus": {
         "value": "00",
         "quality": "1"
       }
     },
     "presentWeatherObservationManual": {
       "condition": "52",
       "quality": "1"
     },
     "atmosphericPressureObservation": {
       "altimeterSetting": {
         "value": 1015.9,
         "quality": "9"
       },
       "stationPressure": {
         "value": 1026,
         "quality": "1"
       }
     },
     "skyCoverLayer": {
       "coverage": {
         "value": "08",
         "quality": "1"
       },
       "baseHeight": {
         "value": 2700,
         "quality": "9"
       },
       "cloudType": {
         "value": "99",
         "quality": "9"
       }
     },
     "liquidPrecipitation": {
       "period": 12,
       "depth": 20,
       "condition": "9",
       "quality": "9"
     },
     "extremeAirTemperature": {
       "period": 99.9,
       "code": "N",
       "value": -30.4,
       "quantity": "1"
     },
     "ingestionTime": {
       "$date": "2024-09-26T17:34:41.843Z"
     }
   }

.. note::

   The preceding is a representative example. Streaming data are
   not static, and each user sees distinct documents.
