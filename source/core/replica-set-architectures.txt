.. _replica-set-deployment-overview:
.. _replica-set-architecture:

====================================
Replica Set Deployment Architectures
====================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

The architecture of a :term:`replica set <replica set>` affects the
set's capacity and capability. This document provides strategies for
replica set deployments and describes common architectures.

The standard replica set deployment for production system is a
three-member replica set. These sets provide redundancy and fault
tolerance. Avoid complexity when possible, but let your application
requirements dictate the architecture.

Strategies
----------

Determine the Number of Members
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Add members in a replica set according to these strategies.

Maximum Number of Voting Members
````````````````````````````````

A replica set can have up to :limit:`50 members <Number of Members of a
Replica Set>`, but only :limit:`7 voting members <Number of Voting
Members of a Replica Set>`. If the replica set already has 7 voting
members, additional members must be :ref:`non-voting members
<replica-set-non-voting-members>`.

Deploy an Odd Number of Members
```````````````````````````````

Ensure that the replica set has an odd number of voting members. If you
have an *even* number of voting members, deploy an :ref:`arbiter
<replica-set-arbiters>` so that the set has an odd number of voting
members.

An :term:`arbiter <arbiter>` does not store a copy of the data and
requires fewer resources. As a result, you may run an arbiter on an
application server or other shared process. With no copy of the data,
it may be possible to place an arbiter into environments that you would
not place other members of the replica set. Consult your security
policies.

.. include:: /includes/extracts/arbiters-and-pvs-with-reference.rst

.. include:: /includes/admonition-multiple-arbiters.rst

.. _replica-set-architectures-consider-fault-tolerance:

Consider Fault Tolerance
````````````````````````

*Fault tolerance* for a replica set is the number of members that can
become unavailable and still leave enough members in the set to elect a
primary. In other words, it is the difference between the number of
members in the set and the majority of voting members needed to elect a
primary. Without a primary, a replica set cannot accept write
operations. Fault tolerance is an effect of replica set size, but the
relationship is not direct. See the following table:

.. list-table::
   :header-rows: 1
   :widths: 15 25 15

   * - Number of Members

     - Majority Required to Elect a New Primary

     - Fault Tolerance

   * - 3

     - 2

     - 1

   * - 4

     - 3

     - 1

   * - 5

     - 3

     - 2

   * - 6

     - 4

     - 2

Adding a member to the replica set does not *always* increase the
fault tolerance. However, in these cases, additional members can
provide support for dedicated functions, such as backups or reporting.

Use Hidden and Delayed Members for Dedicated Functions
``````````````````````````````````````````````````````

Add :ref:`hidden <replica-set-hidden-members>` or :ref:`delayed
<replica-set-delayed-members>` members to support dedicated functions,
such as backup or reporting.

Load Balance on Read-Heavy Deployments
``````````````````````````````````````

In a deployment with *very* high read traffic, you can improve read
throughput by distributing reads to secondary members. As your
deployment grows, add or move members to alternate data centers to
improve redundancy and availability.

Always ensure that the main facility is able to elect a primary.

Add Capacity Ahead of Demand
````````````````````````````

The existing members of a replica set must have spare capacity to
support adding a new member. Always add new members before the
current demand saturates the capacity of the set.

.. _determine-geographic-distribution:

Distribute Members Geographically
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To protect your data in case of a data center failure, keep at least
one member in an alternate data center. If possible, use an odd number
of data centers, and choose a distribution of members that maximizes
the likelihood that even with a loss of a data center, the remaining
replica set members can form a majority or at minimum, provide a copy
of your data.

To ensure that the members in your main data center be elected primary
before the members in the alternate data center, set the
:rsconf:`members[n].priority` of the members in the alternate data
center to be lower than that of the members in the primary data center.

For more information, see
:doc:`/core/replica-set-architecture-geographically-distributed`

Target Operations with Tag Sets
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use :ref:`replica set tag sets <replica-set-configuration-tag-sets>` to
target read operations to specific members or to customize write
concern to request acknowledgement from specific members.

.. seealso:: :doc:`/data-center-awareness` and
   :doc:`/core/workload-isolation`.

Use Journaling to Protect Against Power Failures
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

MongoDB enables :doc:`journaling </core/journaling>` by default.
Journaling protects against data loss in the event of service
interruptions, such as power failures and unexpected reboots.

Replica Set Naming
------------------

.. include:: /includes/fact-unique-replica-set-names.rst

Deployment Patterns
-------------------

The following documents describe common replica set deployment
patterns. Other patterns are possible and effective depending on
the application's requirements. If needed, combine features of each
architecture in your own deployment:

.. include:: /includes/toc/dfn-list-replica-set-architectures.rst

.. include:: /includes/toc/replica-set-architectures.rst
