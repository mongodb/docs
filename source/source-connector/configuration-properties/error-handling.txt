.. _source-configuration-error-handling:

========================================================
Error Handling and Resuming from Interruption Properties
========================================================

.. meta::
   :description: Configure error handling and resumption settings for the Kafka source connector, including error tolerance, logging, dead letter queue, and heartbeat messages.

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

Overview
--------

.. _source-configuration-error-handling-description-start:

Use the following configuration settings to specify how the {+source-connector+}
behaves when it encounters errors and to specify settings related to resuming
interrupted reads.

.. _source-configuration-error-handling-description-end:

Settings
--------

.. _source-configuration-error-handling-table-start:

.. list-table::
   :header-rows: 1
   :widths: 45 55

   * - Name
     - Description

   * - | **mongo.errors.tolerance**
     - | **Type:** string
       |
       | **Description:**
       | Whether to continue processing messages when the connector encounters
         an error.
       |
       | Set this to ``"none"`` if you want the connector to stop
         processing messages and report the issue if it encounters an
         error.
       |
       | Set this to ``"all"`` if you want the connector to continue
         processing messages and ignore any errors it encounters.
       |
       | :gold:`IMPORTANT:` This property overrides the 
        `errors.tolerance <https://docs.confluent.io/platform/current/installation/configuration/connect/source-connect-configs.html#errors-tolerance>`__
        Connect Framework property.
       |
       | **Default:** ``"none"``
       | **Accepted Values**: ``"none"`` or ``"all"``

   * - | **mongo.errors.log.enable**
     - | **Type:** boolean
       |
       | **Description:**
       | Whether the connector should report errors in the log file.
       |
       | Set this to ``true`` to log all errors the connector encounters.
       |
       | Set this to ``false`` to log errors that are not tolerated by the
         connector. You can specify which errors the connector should
         tolerate using the ``errors.tolerance`` or ``mongo.errors.tolerance``
         setting.
       |
       | :gold:`IMPORTANT:` This property overrides the 
         `errors.log.enable <https://docs.confluent.io/platform/current/installation/configuration/connect/source-connect-configs.html#errors-log-enable>`__
         Connect Framework property.
       |
       | **Default:** ``false``
       | **Accepted Values**: ``true`` or ``false``

   * - | **mongo.errors.deadletterqueue.topic.name**
     - | **Type:** string
       |
       | **Description:**
       | The name of topic to use as the dead letter queue.
       |
       | If you specify a value, the connector writes invalid messages to the
         dead letter queue topic as :manual:`extended JSON strings </reference/mongodb-extended-json/>`.
       |
       | If you leave this setting blank, the connector does not write
         invalid messages to any topic.

       | :gold:`IMPORTANT:` You must set ``errors.tolerance`` or ``mongo.errors.tolerance``
         setting to ``"all"`` to enable this property.
       |
       | **Default:** ``""``
       | **Accepted Values**: A valid Kafka topic name

   * - | **offset.partition.name**
     - | **Type:** string
       |
       | **Description:**
       | The custom offset partition name to use. You can use this option
         to instruct the connector to start a new change stream when an
         existing offset contains an invalid resume token. 
       |
       | If you leave this setting blank, the connector uses the default partition name
         based on the connection details. 
       |
       | To view a strategy for naming
         offset partitions, see :ref:`<troubleshoot-reset-stored-offsets>`.
       |
       | **Default:** ``""``
       | **Accepted Values**: A string. To learn more about naming a partition,
         see
         `SourceRecord <{+connector_kafka_version_docs+}/javadoc/org/apache/kafka/connect/source/SourceRecord.html>`__
         in the {+kafka+} API documentation.

   * - | **heartbeat.interval.ms**
     - | **Type:** long
       |
       | **Description:**
       | The number of milliseconds the connector waits between sending 
         heartbeat messages. The connector sends heartbeat messages when 
         source records are not published in the specified interval. This mechanism improves 
         resumability of the connector for low volume namespaces. 
       |
       | Heartbeat messages contain a ``postBatchResumeToken`` data field.
         The value of this field contains the MongoDB server oplog entry that
         the connector last read from the change stream.
       |
       | Set this to ``0`` to disable heartbeat messages.
       |
       | To learn more, see :ref:`Prevention <invalid-resume-token-prevention>`
         in the :ref:`Invalid Resume Token <kafka-troubleshoot-invalid-resume-token>`
         page.
       |
       | **Default**: ``0``
       | **Accepted Values**: An integer

   * - | **heartbeat.topic.name**
     - | **Type:** string
       |
       | **Description:**
       | The name of the topic on which the connector should publish
         heartbeat messages. You must provide a positive value in the
         ``heartbeat.interval.ms`` setting to enable this feature.
       |
       | **Default**: ``{+default-heartbeat-topic+}``
       | **Accepted Values**: A valid Kafka topic name

.. _source-configuration-error-handling-table-end:

.. _source-configuration-error-handling-smt:

Heartbeats with Single Message Transforms
-----------------------------------------

If you enable heartbeats and specify **Single Message Transforms (SMTs)** in your
{+kafka-connect+} deployment, you must exclude your heartbeat messages from
your SMTs. SMTs are a feature of {+kafka-connect+} that enables you to specify transformations on
the messages that pass through your source connector without having to deploy a
stream processing application.

To exclude heartbeat messages from your SMTs, you must create and apply a
**predicate** to your SMTs. Predicates are a feature of SMTs that
enables you to check if a message matches a conditional statement before
applying a transformation.

The following configuration defines the ``IsHeartbeat`` predicate which matches
heartbeat messages sent to the default heartbeat topic:

.. code-block:: properties

   predicates=IsHeartbeat
   predicates.IsHeartbeat.type=org.apache.kafka.connect.transforms.predicates.TopicNameMatches
   predicates.IsHeartbeat.pattern={+default-heartbeat-topic+}

The following configuration uses the preceding predicate to exclude heartbeat
messages from an ``ExtractField`` transformation:

.. code-block:: properties

   transforms=Extract
   transforms.Extract.type=org.apache.kafka.connect.transforms.ExtractField$Key
   transforms.Extract.field=<the field to extract from your {+kafka+} key>
   transforms.Extract.predicate=IsHeartbeat  
   transforms.Extract.negate=true

   # apply the default key schema as the extract transformation requires a struct object
   output.format.key=schema

If you do not exclude your heartbeat messages from the preceding transformation,
your connector raises the following error once it processes a heartbeat message:

.. code-block:: none

   ERROR WorkerSourceTask{id=mongo-source-0} Task threw an uncaught and unrecoverable exception. Task is being killed ...
   ...
   Only Struct objects supported for [field extraction], found: java.lang.String

To learn more about SMTs, see
`How to Use Single Message Transforms in Kafka Connect <https://www.confluent.io/blog/kafka-connect-single-message-transformation-tutorial-with-examples/>`__
from Confluent.

To learn more about predicates, see
`Filter (Apache Kafka) <https://docs.confluent.io/platform/current/connect/transforms/filter-ak.html#predicates>`__
from Confluent.

To learn more about the ``ExtractField`` transformation, see
`ExtractField <https://docs.confluent.io/platform/current/connect/transforms/extractfield.html>`__
from Confluent.

To learn more about the default key schema, see the 
:ref:`<kafka-source-apply-schemas-default-schema>` page.
