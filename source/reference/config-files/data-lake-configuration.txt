
.. _datalake-configuration-file:

======================================
{+data-lake-short+} Configuration File
======================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

.. _datalake-configuration-file-overview:

Overview 
--------

The {+adl+} configuration file uses the |json| format. It defines mappings 
between your {+data-lake-stores+} and {+data-lake-short+}. {+dl+} supports 
|s3| buckets and |service| clusters as {+data-lake-stores+}. Click on the 
tab to learn more about the {+dl+} configuration for that {+data-lake-store+}.

.. tabs:: 

   .. tab:: S3 
      :tabid: s3
      
      Consider a S3 bucket ``datacenter-alpha`` containing data 
      collected from a datacenter:

      .. code-block:: none
         :copyable: false

         |--metrics
            |--hardware

      The ``/metrics/hardware`` path stores JSON files with metrics derived
      from the datacenter hardware, where each filename is the UNIX timestamp
      in milliseconds of the 24 hour period covered by that file:

      .. code-block:: none
         :copyable: false

         /hardware/1564671291998.json

      The following configuration file:

      - Defines a {+data-lake-store+} on the ``datacenter-alpha`` S3 bucket
        in the ``us-east-1`` AWS region. The {+data-lake-store+}
        is specifically restricted to only datafiles in the ``metrics``
        folder path.

      - Maps files from the ``hardware`` folder to a MongoDB database
        ``datacenter-alpha-metrics`` and collection ``hardware``. The
        configuration mapping includes parsing logic for capturing the
        timestamp implied in the filename.

      .. code-block:: json

         {
           "stores" : [
             {
               "name" : "datacenter-alpha",
               "provider" : "s3",
               "region" : "us-east-1",
               "bucket" : "datacenter-alpha",
               "prefix" : "/metrics",
               "delimiter" : "/"
             }
           ],
           "databases" : [ 
             {
               "name" : "datacenter-alpha-metrics", 
               "collections" : [
                 {
                   "name" : "hardware",
                   "dataSources" : [
                     {
                       "storeName" : "datacenter-alpha",
                       "path" : "/hardware/{date date}"
                     }
                   ]
                 }
               ]
             }
           ]
         }

      {+data-lake+} parses the S3 bucket ``datacenter-alpha`` and processes 
      all files under ``/metrics/hardware/``. The ``collections`` uses the 
      :ref:`path parsing syntax <datalake-path-syntax>` to map the filename to 
      the ``date`` field, which is an ISO-8601 date, in each document. If a 
      matching ``date`` field does not exist in a document, it will be added.

      Users connected to the {+data-lake-short+} can use the MongoDB
      Query Language and supported aggregations to analyze data in the
      S3 bucket through the ``datacenter-metrics.hardware`` collection.

   .. tab:: Atlas Cluster 
      :tabid: atlas 

      .. note:: 

         To use |service| as a {+data-lake-store+}, {+dl+} requires ``M10`` or 
         higher cluster. 

      Consider a ``M10`` or higher |service| cluster named ``myDataCenter`` 
      containing data in the ``metrics.hardware`` collection. The ``metrics.hardware`` 
      collection contains JSON documents with metrics derived from the hardware in 
      a datacenter. The following configuration file:

      - Specifies the |service| cluster named ``myDataCenter`` in the specified 
        project as a {+data-lake-store+}.

      - Maps documents from the ``metrics.hardware`` collection in the 
        |service| cluster to the ``dataCenter.inventory`` collection 
        in the storage configuration.

      .. code-block:: json 

         {
           "stores" : [
             {
               "name" : "atlasClusterStore",
               "provider" : "atlas",
               "clusterName" : "myDataCenter",
               "projectID" : "5e2211c17a3e5a48f5497de3"
             }
           ],
           "databases" : [ 
             {
               "name" : "dataCenter", 
               "collections" : [
                 {
                   "name" : "inventory",
                   "dataSources" : [
                     {
                       "storeName" : "atlasClusterStore",
                       "database" : "metrics",
                       "collection" : "hardware"
                     }
                   ]
                 }
               ]
             }
           ]
         }

      {+data-lake+} maps all the documents in the ``metrics.hardware`` 
      collection to the ``dataCenter.inventory`` collection in the storage 
      configuration.

      Users connected to the {+data-lake-short+} can use the MongoDB
      Query Language and supported aggregations to analyze data in the
      |service| cluster through the ``dataCenter.inventory`` collection.  
      When you run queries, the query first goes to {+adl+}. Therefore, 
      if you run aggregation queries that are supported by your |service| 
      cluster but not by {+adl+}, the queries will fail. To learn 
      more about supported and unsupported commands in {+dl+}, see 
      :ref:`data-lake-mql-support`.

.. important:: 

   If the database in the storage configuration contains collections from 
   both |s3| and |service| {+data-lake-stores+}, the query results might 
   contain data from both the {+data-lake-stores+}.

.. _datalake-configuration-format:

Configuration File Format
-------------------------

The {+data-lake-short+} configuration file has the following format:

.. tabs:: 
   :hidden:

   .. tab:: S3  
      :tabid: s3

      .. code-block:: json

         {
           "stores" : [
             {
               "name" : "<string>",
               "provider": "<string>",
               "region" : "<string>", 
               "bucket" : "<string>",
               "prefix" : "<string>", 
               "includeTags": <boolean>, 
               "delimiter": "<string>"
             }
           ],
             "databases" : [
               {
                 "name" : "<string>",
                 "collections" : [
                   {
                     "name" : "<string>",
                     "dataSources" : [
                       {
                         "storeName" : "<string>",
                         "path" : "<string>",
                         "defaultFormat" : "<string>" 
                       }
                     ]
                   }
                 ],
                 "views" : [ 
                   {
                     "name" : "<string>", 
                     "source" : "<string>", 
                     "pipeline" : "<string>" 
                   }
                 ] 
               }
             ]
           }

   .. tab:: Atlas Cluster 
      :tabid: atlas 

      .. code-block:: json

         {
           "stores" : [
             {
               "name" : "<string>",
               "provider": "<string>",
               "clusterName": "<string>", 
               "projectId": "<string>"
             }
           ],
             "databases" : [
               {
                 "name" : "<string>",
                 "collections" : [
                   {
                     "name" : "<string>",
                     "dataSources" : [
                       {
                         "storeName" : "<string>",
                         "database" : "<string>",
                         "collection" : "<string>" 
                       }
                     ]
                   }
                 ],
                 "views" : [ 
                   {
                     "name" : "<string>", 
                     "source" : "<string>", 
                     "pipeline" : "<string>" 
                   }
                 ] 
               }
             ]
           }

:ref:`datalake-stores-reference`
  The ``stores`` object defines each {+data-lake-store+} associated
  to the {+data-lake-short+}. {+data-lake-short+} can only access
  {+data-lake-stores+} defined in the ``stores`` object.

:ref:`datalake-databases-reference`
  The ``databases`` object defines the mapping between each
  {+data-lake-store+} defined in ``stores`` and MongoDB collections 
  in the databases. 

.. _datalake-stores-reference:

``stores``
~~~~~~~~~~

.. tabs:: 
   :hidden:

   .. tab:: S3  
      :tabid: s3

      .. code-block:: json

         "stores" : [
           {
             "name" : "<string>",
             "provider" : "<string>",
             "region" : "<string>",
             "bucket" : "<string>",
             "prefix" : "<string>",
             "delimiter" : "<string>",
             "includeTags": <boolean>
           }
         ]

   .. tab:: Atlas Cluster
      :tabid: atlas

      .. code-block:: json 

         "stores" : [
           {
             "name" : "<string>",
             "provider" : "<string>",
             "clusterName" : "<string>",
             "projectId": "<string>"
           }
         ]

.. datalakeconf:: stores

   The ``stores`` object defines an array of {+data-lake-stores+} associated 
   with a {+data-lake-short+}. {+data-lake-store+} captures files in an S3 
   bucket or documents in |service| cluster. An {+adl+} can only access 
   {+data-lake-stores+} defined in the ``stores`` object.

.. datalakeconf:: stores.[n].name

   Name of the {+data-lake-store+}. The
   :datalakeconf:`databases.[n].collections.[n].dataSources.[n].storeName`
   field references this value as part of mapping configuration.

.. datalakeconf:: stores.[n].provider

   Defines where the data is stored. Value can be one of the following: 
   
   - ``s3`` for an |aws| |s3| bucket.
   - ``atlas`` for a collection in an |service| cluster.

.. tabs:: 
   :hidden:

   .. tab:: S3  
      :tabid: s3

      .. datalakeconf:: stores.[n].region

         Name of the |aws| region in which the S3 bucket is hosted. For
         a list of valid region names, see :atlas:`Amazon Web Services 
         (AWS) </reference/amazon-aws/#amazon-aws>`. 

      .. datalakeconf:: stores.[n].bucket

         Name of the |aws| S3 bucket. Must exactly match the name of an S3
         bucket which {+data-lake-short+} can access given the
         configured |aws| IAM credentials.

      .. datalakeconf:: stores.[n].prefix

         *Optional.* Prefix {+data-lake-short+} applies when searching for 
         files in the S3 bucket. 

         For example, consider an S3 bucket ``metrics`` with the following 
         structure:

         .. code-block:: text
            :copyable: false

            metrics
              |--hardware
              |--software
                 |--computed

         The {+data-lake-store+} prepends the value of ``prefix`` to the
         :datalakeconf:`databases.[n].collections.[n].dataSources.[n].path` 
         to create the full path for files to ingest. Setting the ``prefix``
         to ``/software`` restricts any :datalakeconf:`databases` 
         objects using the {+data-lake-store+} to only subpaths 
         ``/software``.

         If omitted, {+data-lake-short+} searches all files from the
         root of the S3 bucket.

      .. datalakeconf:: stores.[n].delimiter
   
         *Optional.* The delimiter that separates 
         :datalakeconf:`databases.[n].collections.[n].dataSources.[n].path` 
         segments in the {+data-lake-store+}. |data-lake| uses the delimiter 
         to efficiently traverse |s3| buckets with a hierarchical directory 
         structure. You can specify any character supported by the |s3| `object 
         keys <https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html>`__ 
         as the delimiter. For example, you can specify an underscore (``_``) or a 
         plus sign (``+``) or multiple characters such as double underscores (``__``) 
         as the delimiter.

         If omitted, defaults to ``"/"``.

      .. datalakeconf:: stores.[n].includeTags

         *Optional.* Determines whether or not to use |s3| tags on the files in 
         the given path as additional partition attributes. Valid values are 
         ``true`` and ``false``. 

         If omitted, defaults to ``false``.

         If set to ``true``, {+data-lake-short+} does the following:

         - Adds the |s3| tags as additional partition attributes. 

         - Adds new top level BSON elements that associates each tag to each document 
           for the tagged files.

         .. warning::

            If set to ``true``, {+data-lake-short+} processes the files for 
            additional partition attributes by making extra calls to |s3| to get the 
            tags. This behavior might impact performance.

   .. tab:: Atlas Cluster
      :tabid: atlas

      .. datalakeconf:: stores.[n].clusterName 

         Name of the |service| cluster on which the store is based. Note that the 
         cluster must be a M10 or higher cluster and must exist in the same 
         project as your {+dl+}. The ``source`` field on the data partition is the 
         name of the |service| cluster.

      .. datalakeconf:: stores.[n].projectID 

         Unique identifier of the project that contains the |service| cluster on 
         which the store is based.

.. _datalake-databases-reference:

``databases``
~~~~~~~~~~~~~

.. tabs:: 
   :hidden:

   .. tab:: S3  
      :tabid: s3

      .. code-block:: json

         "databases" : [
           {
             "name" : "<string>",
             "collections" : [
               {
                 "name" : "<string>",
                 "dataSources" : [
                   {
                     "storeName" : "<string>",
                     "defaultFormat" : "<string>",
                     "path" : "<string>"
                   }
                 ]
               }
             ], 
             "views" : [
               {
                 "name" : "<string>",
                 "source" : "<string>",
                 "pipeline" : "<string>"
               }
             ]
           }
         ]

   .. tab:: Atlas Cluster 
      :tabid: atlas

      .. code-block:: json

         "databases" : [
           {
             "name" : "<string>",
             "collections" : [
               {
                 "name" : "<string>",
                 "dataSources" : [
                   {
                     "storeName" : "<string>",
                     "database" : "<string>",
                     "collection" : "<string>"
                   }
                 ]
               }
             ]
           }
         ]

.. datalakeconf:: databases

   Array of objects where each object represents a database, its collections, 
   and, optionally, any :manual:`views </core/views/>` on the collections. Each 
   database can have multiple ``collections`` and ``views`` objects.

.. datalakeconf:: databases.[n].name

   Name of the database to which {+data-lake-short+} maps the
   data contained in the {+data-lake-store+}.  

.. datalakeconf:: databases.[n].collections

   Array of objects where each object represents a collection and data 
   sources that map to a :datalakeconf:`stores` {+data-lake-store+}.

.. datalakeconf:: databases.[n].collections.name

   Name of the collection to which {+data-lake-short+} maps the
   data contained in each
   :datalakeconf:`databases.[n].collections.[n].dataSources.[n].storeName`. 
   Each object in the array represents the mapping between the collection 
   and an object in the :datalakeconf:`stores` array. 

   .. tabs:: 
      :hidden:

      .. tab:: S3 
         :tabid: s3

         You can generate collection names dynamically from file paths by 
         specifying ``*`` for the collection name and the ``collectionName()`` 
         function in the 
         :datalakeconf:`~databases.[n].collections.[n].dataSources.[n].path`
         field. See :ref:`datalake-advanced-path-generate-collection` for examples. 

      .. tab:: Atlas Cluster 
         :tabid: atlas

         You can generate collection names dynamically by specifying ``*`` for the 
         collection name and omitting the 
         :datalakeconf:`~databases.[n].collections[n].dataSources.[n].collection` 
         field. 

.. datalakeconf:: databases.[n].collections.[n].dataSources 

   Array of objects where each object represents a :datalakeconf:`stores` 
   {+data-lake-store+} to map with the collection.

.. datalakeconf:: databases.[n].collections.[n].dataSources.[n].storeName

   Name of a {+data-lake-store+} to map to the ``<collection>``. 
   Must match the :datalakeconf:`~stores.[n].name` of an object in the 
   :datalakeconf:`stores` array. 

.. tabs:: 
   :hidden:

   .. tab:: S3 
      :tabid: s3

      .. datalakeconf:: databases.[n].collections.[n].dataSources.[n].path

         Controls how {+data-lake+} searches for and parses files in the
         :datalakeconf:`~databases.[n].collections.[n].dataSources.[n].storeName` 
         before mapping them to the ``<collection>``. {+data-lake-short+} prepends 
         the :datalakeconf:`stores.[n].prefix` to the ``path`` to build the full 
         path to search within. Specify ``/`` to capture all files and folders from 
         the ``prefix`` path.

         For example, consider an S3 bucket ``metrics`` with the following 
         structure:

         .. code-block:: text
            :copyable: false

            metrics
            |--hardware
            |--software
               |--computed

         A ``path`` of ``/`` directs {+data-lake-short+} to search
         all files and folders in the ``metrics`` bucket.

         A ``path`` of ``/hardware`` directs {+data-lake-short+} to
         search only that path for files to ingest.

         If the :datalakeconf:`~stores.[n].prefix` is ``software``,
         {+data-lake-short+} searches for files only in the path
         ``/software/computed``.

         Appending the ``*`` wildcard character to the path 
         directs {+data-lake-short+} to include all files and folders from 
         that point in the path. For example, ``/software/computed*``
         would match files like ``/software/computed-detailed``,
         ``/software/computedArchive``, and ``/software/computed/errors``.

         :datalakeconf:`~databases.[n].collections.[n].dataSources.[n].path`
         supports additional syntax for parsing filenames, including:

         - Generating document fields from filenames.
         - Using regular expressions to control field generation.
         - Setting boundaries for bucketing filenames by timestamp.
   
         See :ref:`datalake-path-syntax` for more information.

         .. include:: /includes/fact-path-delimiter.rst

      .. datalakeconf:: databases.[n].collections.[n].dataSources.[n].defaultFormat

         *Optional.* Specifies the default format {+data-lake-short+} assumes
         if it encounters a file without an extension while searching the
         :datalakeconf:`~databases.[n].collections.[n].dataSources.[n].storeName`.

         If omitted, {+data-lake-short+} attempts to detect the file type
         by processing a few bytes of the file.

         .. note::

            If your file format is ``CSV`` or ``TSV``, you must include a
            header row in your data. See :ref:`data-lake-csv-tsv-data` 
            for more information.

         The following values are valid for the ``defaultFormat`` field:

         ``.json, .json.gz, .bson, .bson.gz, .avro, .avro.gz,
         .tsv, .tsv.gz, .csv, .csv.gz, .parquet``

   .. tab:: Atlas Cluster  
      :tabid: atlas 

      .. datalakeconf:: databases.[n].collections.[n].dataSources.[n].database 

         Name of the database on the |service| cluster that contains the 
         collection. 

      .. datalakeconf:: databases.[n].collections[n].dataSources.[n].collection 

         Name of the collection in the |service| cluster on which the 
         {+dl+} {+data-lake-store+} is based. When creating a wildcard 
         collection, this must not be specified.

.. datalakeconf:: databases.[n].views 

   Array of objects where each object represents an :manual:`aggregation 
   pipeline </core/aggregation-pipeline/#id1>` on a collection. To learn 
   more about views, see :manual:`Views </core/views/>`.

.. datalakeconf:: databases.[n].views.[n].name 

   Name of the view. 

.. datalakeconf:: databases.[n].views.[n].source 

   Name of the source collection for the view.

.. datalakeconf:: databases.[n].views.[n].pipeline 
 
   :manual:`Aggregation pipeline stage(s) </core/aggregation-pipeline/#id1>` 
   to apply to the 
   :datalakeconf:`~databases.[n].views.[n].source` collection.

.. seealso::

   - :ref:`Configure Atlas Data Lake <config-datalake>` 
   - `Tutorial: Federated Queries and $out to S3 
     <https://developer.mongodb.com/how-to/atlas-data-lake-federated-queries-out-aws-s3>`__
