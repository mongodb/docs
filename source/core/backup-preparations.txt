===================
Backup Preparations
===================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

Before backing up your cluster or replica set, decide how to back up
the data and what data to back up. This page describes items you must
consider before starting a backup.

.. seealso::

   To learn how Backup works, see
   :ref:`mms-backup-functional-overview`.

.. cond:: onprem
   
   .. warning::

      .. include:: /includes/facts/fcv-backup-considerations.rst

.. _backup-configuration-options:

Backup Configuration Options
----------------------------

The backup and recovery requirements of a given system vary to meet
the cost, performance and data protection standards the system's
owner sets.

|mms| Enterprise Backup and Recovery supports five backup
architectures, each with its own strengths and trade-offs. Consider
the architecture that meets the data protection requirements for
your deployment before configuring and deploying your backup
architecture.

.. example::

  Consider a system whose requirements include low operational
  costs. The system's owners may have strict limits on what they
  can spend on storage for their backup and recovery configuration.
  They may accept a longer recovery time as a result.

  Conversely, consider a system whose requirements include a low
  :opsmgr:`Recovery Time Objective </reference/glossary/#std-term-Recovery-Time-Objective>`. The system's owners tolerate
  greater storage costs if it results in a backup and recovery
  configuration that fulfills the recovery requirements.

|mms| Enterprise Backup and recovery supports the following backup
architectures:

- A file system on a |san| with advanced features for filesystem
  backups, such as high availability, compression, or
  :term:`deduplication`
- A file system on one or more |nas| devices
- An `S3-compatible <https://aws.amazon.com/s3>`_ :term:`blockstore`
- MongoDB blockstore in a highly available configuration
- MongoDB blockstore in a standalone configuration

We provide the backup architecture recommendations as
guidance for developing your own data protection approaches.
Our recommendations don't cover or represent each scenario
or deployment.

Backup Method Features
~~~~~~~~~~~~~~~~~~~~~~

.. list-table::
   :header-rows: 1
   :widths: 25 15 15 15 15 15
   :stub-columns: 1

   * - Backup System Feature
     - File System on |san|
     - File System on |nas|
     - AWS S3 Blockstore
     - MongoDB :abbr:`HA (High Availability)` Blockstore
     - MongoDB Blockstore

   * - Snapshot Types
     - Complete ***** 
     - Complete ***** 
     - Many partial
     - Many partial
     - Many partial

   * - Backup Data Deduplication
     - If SAN supports
     - No
     - Yes
     - Yes
     - Yes

   * - Backup Data Compression
     - Yes
     - No
     - Yes
     - Yes
     - Yes

   * - Backup Data Replication
     - If SAN supports
     - No
     - No
     - Yes
     - No

   * - Backup Storage Cost
     - Higher
     - Medium
     - Lower
     - Higher
     - Lower

   * - Staff Time to Manage Backups
     - Medium
     - Medium
     - Lower
     - Higher
     - Medium

   * - Backup |rto|
     - Lower
     - Medium
     - Lower
     - Lower
     - Medium

***** To perform an incremental backup to a :term:`File System Store`,
the {+mdbagent+} slices each :manual:`storage engine
</core/storage-engines/>` file in the path specified for backup into
block(s) of data and transfers only changed block(s) to |onprem|.
|onprem| creates a new :manual:`snapshot
</reference/glossary/#std-term-snapshot>` from received block(s) and
copies the remaining unchanged block(s) from the previous backup
snapshot. Each incremental backup snapshot stored to a file system
saves network I/O. Each such backup snapshot contains a full copy of
all required files from a backed up MongoDB deployment and does not
:term:`de-duplicate <deduplication>` records.

.. note:: **When Do You Use a Particular Backup Method?**

   - To run backups frequently on large amounts of data and
     restore from backups, consider backing up to a file
     system on a |san|, an AWS |s3| snapshot store, or a MongoDB
     blockstore configured as a replica set or a
     :manual:`sharded cluster </reference/glossary/#std-term-sharded-cluster>`.

   - To restore data without relying on MongoDB
     database, consider backing up to an AWS |s3| snapshot store,
     one or more |nas| devices, or a file system on a |san| with
     advanced features for file system backups, such as high
     availability, compression, or :term:`deduplication`.

   - To minimize internal storage and maintenance
     costs, consider backing up to an AWS |s3| snapshot store or
     a MongoDB standalone blockstore.

     A MongoDB standalone blockstore offers limited resilience. If
     the disk fills, this blockstore may go offline. You can
     recover backup snapshots only after adding additional storage.

.. _limits-of-backup:

Backup Sizing Recommendation
----------------------------

When sizing the backup of your data, keep the :manual:`replica set </reference/glossary/#std-term-replica-set>` 
size to 2 TB or less of :manual:`uncompressed data 
</reference/command/dbStats/#mongodb-data-dbStats.dataSize>`. If 
your database increases beyond 2 TB of uncompressed data:

- Shard the database
- Keep each shard to 2 TB or less of uncompressed data

.. cond:: onprem

   .. include:: /includes/backup/fact-backup-with-blockstores.rst

These size recommendations are a best practice. They are not a
limitation of the MongoDB database or |mms|.

Backup and restore can use large amounts of CPU, memory, storage, 
and network bandwidth.

.. example::

   Your stated network throughput, such as 10 Gbps or 100 Gbps, is 
   a *theoretical* maximum. That value doesn't account for sharing 
   or throttling of network traffic.

   Consider the following scenario:

   - You want to back up a 2 TB database.
   - Your hosts support a 10 Gbps TCP connection from |mms| to its
     backup storage.
   - The network connection has very low packet loss and a low round
     trip delay time.

   A *full* backup of your data would take *more than 30 hours to
   complete*. [#]_

   This doesn't account for disk read and write speeds, which can be,
   at most, 3 Gbps reads and 1 Gbps writes for a single or mirrored
   NVMe storage device.

   The time required to complete each successive incremental backup
   depends on write load.

   If you :doc:`shard this database 
   </tutorial/convert-replica-set-to-sharded-cluster>`
   into 4 shards, each shard runs its backup separately. This results
   in a backup that takes less than 8 hours to complete.

   .. [#]
      These throughput figures were calculated using industry standard
      methods for measuring network throughput and assume no additional
      network compression.

.. _snapshot-frequency-and-retention:

Snapshot Frequency and Retention Policy
---------------------------------------

By default, |mms| takes a base snapshot of your data every 24 hours.

If desired, administrators can change the frequency of base snapshots
to 6, 8, 12, or 24 hours. |mms| creates snapshots automatically on a
schedule. You can also take on-demand snapshots.

|mms| retains snapshots for the time periods listed in the following
table. 

If you :ref:`terminate a deployment's backup <terminate-backup>`,
|mms| immediately deletes the snapshots that are within the dates
of the current retention policy.

If you :ref:`stop a deployment's backup <stop-backup>`, |mms| stops
taking new snapshots but retains existing snapshots until their listed
expiration date.

.. list-table::
   :widths: 30 30 40
   :header-rows: 1

   * - Snapshot
     - Default Retention Policy
     - Maximum Retention Policy
   * - Base snapshot
     - 2 days
     - 5 days (30 days if frequency is 24 hours)
   * - Daily snapshot
     - 0 days
     - 1 year
   * - Weekly snapshot
     - 2 weeks
     - 1 year
   * - Monthly snapshot
     - 1 month
     - 7 years

.. include:: /includes/extracts/backup-preparations-pricing.rst

You can change a backed-up deployment's schedule through its
:guilabel:`Edit Snapshot Schedule` menu option, available through the
:guilabel:`Backup` page. Administrators can change snapshot frequency
and retention through the
:doc:`snapshotSchedule resource in the API </reference/api/snapshot-schedule>`.

.. _changing-reference-time:

Changing the reference time changes the time of the next scheduled
snapshot. You can't make the next scheduled snapshot happen sooner than
the current next snapshot time. The current next snapshot time is the
current reference time plus the interval between snapshots.

To determine the time of the next snapshot, compare the current next
snapshot time to the new reference time:

- If the new reference time is before the current next snapshot time,
  the next snapshot still occurs after the current next snapshot time.
  The snapshot occurs at the new reference time plus the number of
  intervals needed to surpass the current next snapshot time. If this
  time has already passed when you make the change, the |mms| takes the
  next snapshot at the next occurrence of the new reference time. See
  the first two rows of the following table for examples.

- If the new reference time is after the current next snapshot time,
  |mms| takes the next snapshot at the next occurrence of the new
  reference time. See the third and fourth row of the following table
  for examples.

.. list-table::
   :widths: 20 20 20 20 20 20
   :header-rows: 1

   * - Time of Change

     - Current Reference Time

     - Interval Between Snapshots

     - Current Next Snapshot Time

     - New Reference Time

     - Time of Next Snapshot

   * - 08:00 UTC

     - 12:00 UTC

     - 6 hours

     - 12:00 UTC

     - 10:00 UTC

     - 16:00 UTC today

   * - 23:00 UTC

     - 12:00 UTC

     - 6 hours

     - 00:00 UTC

     - 10:00 UTC

     - 04:00 UTC tomorrow

   * - 08:00 UTC

     - 12:00 UTC

     - 6 hours

     - 12:00 UTC

     - 19:00 UTC

     - 19:00 UTC today

   * - 20:00 UTC

     - 12:00 UTC

     - 6 hours

     - 00:00 UTC

     - 19:00 UTC

     - 01:00 UTC tomorrow

If you change the schedule to save fewer snapshots, |mms| does
**not** delete existing snapshots to conform to the new schedule. To
delete unneeded snapshots, see :doc:`/tutorial/delete-backup-snapshots`.

Limits
~~~~~~

- |mms| does not backup deployments where the total number of
  collections on the deployment meets or exceeds ``100,000``.

- |mms| does not replicate index collection options.

Encryption
~~~~~~~~~~

.. include:: /includes/head-database-backup-deprecated.rst

.. seealso::

   :doc:`/tutorial/encrypt-snapshots`.

.. _4.2-backup-considerations:

Backup Considerations
---------------------

Consistency and Snapshots
~~~~~~~~~~~~~~~~~~~~~~~~~

|mms| maintains :manual:`causal consistency </core/read-isolation-consistency-recency/#std-label-sessions>` 
when taking snapshots except for size statistics reported by
:manual:`collStats </reference/command/collStats/#mongodb-dbcommand-dbcmd.collStats>` 
and ``db.[collection].count()``. Size statistics reported by
:manual:`collStats </reference/command/collStats/#mongodb-dbcommand-dbcmd.collStats>` 
and ``db.[collection].count()`` may be inccurate. |mms| coordinates the
time across all shards for sharded clusters. This ensures that snapshots
include all documents written to every shard and node as of the
scheduled snapshot time. 

.. important::

   .. include:: /includes/fact-backup-standalone-restriction.rst

.. include:: /release-notes/release-advisories/advisories-v4.2-backup.rst

Shared File System
++++++++++++++++++

.. include:: /includes/fact-shared-file-system-reqs.rst

Garbage Collection of Expired Snapshots
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

|mms| manages expired snapshots using groom jobs. These groom jobs act
differently depending upon which snapshot store contains the snapshots:

.. list-table::
   :widths: 30 70
   :header-rows: 1
   :stub-columns: 1

   * - Snapshot Store
     - How Groom Jobs Work

   * - MongoDB Blockstore
     - Uses additional disk space up to the amount of living blocks for
       each job.

   * - Filesystem Snapshot stores
     - Deletes expired snapshots

   * - S3 snapshot stores
     - May use additional disk space if |mms| creates a snapshot while
       the groom job runs. |mms| can run concurrent groom jobs on S3
       snapshot stores.

.. note::

   Snapshot jobs and groom jobs can't run concurrently. If you
   initiate a groom job while a snapshot job is running, the groom job
   fails, and |onprem| logs an error and automatically retries the
   groom job after the snapshot job completes. If you initiate a
   snapshot job while a groom job is running, the snapshot job fails,
   and |onprem| logs an error and retries the snapshot job after the
   groom job completes.

To learn more about groom jobs, see :ref:`Groom jobs <grooms-page>`.

.. _namespaces-filter:

Namespaces Filter
~~~~~~~~~~~~~~~~~

The namespaces filter lets you specify which databases and collections
to back up. You can apply a namespace filter to any database except
``admin`` and ``local`` and any collection that doesn't start with
``system``. 

You create either a :guilabel:`Deny List` of those to
exclude or a :guilabel:`Access List` of those to include. You make your
selections when starting a backup and can later edit them as needed. If
you change the filter in a way that adds data to your backup, a resync
is required.

Use the deny list to prevent backup of collections that contain logging
data, caches, or other ephemeral data. Excluding these kinds of
databases and collections will allow you to reduce backup time and
costs. Using a deny list is often preferable to using an access list as a
access list requires you to intentionally opt in to every namespace you
want backed up.

.. include:: /includes/facts/namespace-filter-version.rst

.. _considerations-backup-storage-engine:

Storage Engine
~~~~~~~~~~~~~~

To back up MongoDB clusters, use the
:ref:`WiredTiger storage engine <storage-wiredtiger>` storage engine.

If your current backing databases use MMAPv1, upgrade to WiredTiger:

- :manual:`Change Sharded Cluster to WiredTiger </tutorial/change-sharded-cluster-wiredtiger>`

- :manual:`Change Replica Set to WiredTiger </tutorial/change-replica-set-wiredtiger>`

With WiredTiger, |mms| limits backups to deployments with fewer than
100,000 files. Files includes collections and indexes.

MongoDB 4.2 removes MMAPv1 storage. To learn more about storage
engines, see :manual:`Storage </core/storage>` in the MongoDB manual.

Resyncing Production Deployments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For production deployments, :doc:`resync </tutorial/resync-backup>` all
backed up replica sets periodically, such as once a year. When you resync,
data is read from a secondary in each replica set. During resync,
no new snapshots are generated.

You may also want to resync your backup if you:

- Reduce the size of the data, such that the size of |mms|'s copy of the
  data on disk is also reduced.

- Create a :manual:`TTL index </core/index-ttl>`, which periodically
  deletes documents.

- :manual:`Drop a collection </reference/method/db.collection.drop>`
  (MMAPv1 only).

- Run a :manual:`sharded cluster </reference/glossary/#std-term-sharded-cluster>`, and move chunks
  from a particular shard.

- Switch storage engines, if you want |mms| to provide snapshots
  in the new storage engine format.

- :manual:`Build an Index on a replica set in a rolling fashion
  </tutorial/build-indexes-on-replica-sets>`.

Snapshots when Agent Can't Contact a ``mongod``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For :manual:`sharded clusters
</reference/glossary/#std-term-sharded-cluster>`, if the {+bagent+}
can't reach a :manual:`mongod
</reference/program/mongod/#mongodb-binary-bin.mongod>` process,
whether a shard or config server, then the agent can't insert a
synchronization :manual:`oplog </reference/glossary/#std-term-oplog>`
token. In these cases, |mms| doesn't create the snapshot and displays
a warning message.

.. _regional-backup-considerations:

Regional Backup Considerations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To enable :ref:`regional-backup` you must associate at least one
of the following with the :ref:`deployment region
<deployment-regions-interface>` that a replica set or shard targets:

- :ref:`manage-snapshot-blockstore`

- :ref:`manage-s3-snapshot-store`

- :ref:`manage-snapshot-filestore`

Additionally, you must associate one of each of the following items
with a deployment region:

- :ref:`oplog-stores-page`
- :opsmgr:`sync store </reference/glossary/#std-term-sync-store>`
  (unless you set ``mms.backup.noSyncState`` to ``true``)
- :opsmgr:`backup daemons
  </reference/glossary/#std-term-Backup-Daemon>`

If you add a shard to a sharded cluster after you enable regional
backup for that sharded cluster, you must assign a deployment region
to the new shard to continue the backup jobs for the existing
shards. Until you assign a deployment region to the new shard, the
entire sharded cluster backup job has a ``Misconfigured`` state and
doesn't generate new snapshots. A sharded cluster with a
``Misconfigured`` state continues to generate oplog entries.
