
.. _datalake-configuration-file:

=================================
{+data-lake-short+} Configuration
=================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

.. _datalake-configuration-file-overview:

Overview 
--------

The {+adl+} configuration is in |json| format. It contains mappings 
between your {+data-lake-stores+} and {+data-lake-short+}. {+dl+} 
supports |s3| buckets, |service| clusters, and publicly accessible 
|url|\s as {+data-lake-stores+}. You must define mappings in your 
{+dl+} to your |s3| bucket, |service| cluster, and |http| 
{+data-lake-stores+} to run queries against your data.

Example Configuration for Individual Data Stores 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Click on the tab below to learn more about the {+dl+} configuration for 
that data store provider.

.. tabs:: 

   .. tab:: S3  
      :tabid: s3 

      .. example::

         Consider a S3 bucket ``datacenter-alpha`` containing data 
         collected from a datacenter:

         .. code-block:: none
            :copyable: false

            |--metrics
              |--hardware

         The ``/metrics/hardware`` path stores JSON files with metrics 
         derived from the datacenter hardware, where each filename is 
         the UNIX timestamp in milliseconds of the 24 hour period 
         covered by that file:

         .. code-block:: none
            :copyable: false

            /hardware/1564671291998.json

         The following configuration:

         - Defines a {+data-lake-store+} on the ``datacenter-alpha`` S3 
           bucket in the ``us-east-1`` AWS region. The 
           {+data-lake-store+} is specifically restricted to only 
           datafiles in the ``metrics`` folder path.

         - Maps files from the ``hardware`` folder to a MongoDB database
           ``datacenter-alpha-metrics`` and collection ``hardware``. The
           configuration mapping includes parsing logic for capturing 
           the timestamp implied in the filename.

         .. code-block:: json

            {
              "stores" : [
                {
                  "name" : "datacenter-alpha",
                  "provider" : "s3",
                  "region" : "us-east-1",
                  "bucket" : "datacenter-alpha",
                  "additionalStorageClasses" : [
                    "STANDARD_IA"
                  ],
                  "prefix" : "/metrics",
                  "delimiter" : "/"
                }
              ],
              "databases" : [ 
                {
                  "name" : "datacenter-alpha-metrics", 
                  "collections" : [
                    {
                      "name" : "hardware",
                      "dataSources" : [
                        {
                          "storeName" : "datacenter-alpha",
                          "path" : "/hardware/{date date}"
                        }
                      ]
                    }
                  ]
                }
              ]
            }

         {+data-lake+} parses the S3 bucket ``datacenter-alpha`` and 
         processes all files under ``/metrics/hardware/``. The 
         ``collections`` uses the :ref:`path parsing syntax 
         <datalake-path-syntax>` to map the filename to the ``date`` 
         field, which is an ISO-8601 date, in each document. 
         If a matching ``date`` field does not exist in a document, it 
         will be added.

         Users connected to the {+data-lake-short+} can use the MongoDB 
         Query Language and supported aggregations to analyze data in 
         the |s3| bucket through the ``datacenter-metrics.hardware`` 
         collection.

      .. seealso:: 

         :ref:`query-s3`

   .. tab:: Atlas  
      :tabid: atlas 

      .. note:: 

         To use |service| as a {+data-lake-store+}, {+dl+} requires 
         ``M10`` or higher cluster.

      .. example:: 

         Consider a ``M10`` or higher |service| cluster named 
         ``myDataCenter`` containing data in the ``metrics.hardware`` 
         collection. The ``metrics.hardware`` collection contains JSON 
         documents with metrics derived from the hardware in a 
         datacenter. The following configuration:

         - Specifies the |service| cluster named ``myDataCenter`` in 
           the specified project as a {+data-lake-store+}.

         - Maps documents from the ``metrics.hardware`` collection in 
           the |service| cluster to the ``dataCenter.inventory`` 
           collection in the storage configuration.

         .. code-block:: json 

            {
              "stores" : [
                {
                  "name" : "atlasClusterStore",
                  "provider" : "atlas",
                  "clusterName" : "myDataCenter",
                  "projectId" : "5e2211c17a3e5a48f5497de3"
                }
              ],
              "databases" : [ 
                {
                  "name" : "dataCenter", 
                  "collections" : [
                    {
                      "name" : "inventory",
                      "dataSources" : [
                        {
                          "storeName" : "atlasClusterStore",
                          "database" : "metrics",
                          "collection" : "hardware"
                        }
                      ]
                    }
                  ]
                }
              ]
            }

         {+data-lake+} maps all the documents in the 
         ``metrics.hardware`` collection to the 
         ``dataCenter.inventory`` collection in the storage 
         configuration.

         Users connected to the {+data-lake-short+} can use the MongoDB 
         Query Language and supported aggregations to analyze data in 
         the |service| cluster through the ``dataCenter.inventory`` 
         collection. When you run queries, the query first goes to 
         {+adl+}. Therefore, if you run aggregation queries that are 
         supported by your |service| cluster but not by {+adl+}, the 
         queries will fail. To learn more about supported and 
         unsupported commands in {+dl+}, see 
         :ref:`data-lake-mql-support`.

      .. seealso:: 

         :ref:`query-atlas`

   .. tab:: HTTP  
      :tabid: http 

      .. include:: /includes/extracts/fact-http-beta-message.rst

      Consider |url|\s 
      ``https://www.datacenter-hardware.com/data.json``, 
      ``https://www.datacenter-software.com/data.json``, and
      ``https://www.datacenter-metrics.com/data.json`` containing data 
      collected from a datacenter. The following configuration:

      - Specifies the publicly accessible URLs that contain data in 
        files as a {+data-lake-store+}. 

      - Creates a partition for each |url|.

      .. code-block:: json 

         {
           "stores" : [
             {
               "name" : "httpStore",
               "provider" : "http",
               "allowInsecure" : false,
               "urls" : [
                 "https://www.datacenter-hardware.com/data.json",
                 "https://www.datacenter-software.com/data.json"
               ],
               "defaultFormat" : ".json"
             }
           ],
           "databases" : [ 
             {
               "name" : "dataCenter", 
               "collections" : [
                 {
                   "name" : "inventory",
                   "dataSources" : [
                     {
                       "storeName" : "httpStore",
                       "allowInsecure" : false,
                       "urls" : [
                         "https://www.datacenter-metrics.com/data"
                       ],
                       "defaultFormat" : ".json"
                     }
                   ]
                 }
               ]
             }
           ]
         }

      .. seealso:: 

         :ref:`query-http`

.. _federated-config-eg:

Example Configuration for Running Federated Queries 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can define mappings between your |s3|, |service| cluster, and 
|http| {+data-lake-stores+} and {+data-lake-short+} in the storage 
configuration to run federated queries against your data. 

.. example::

   For the preceding sample |s3|, |service| cluster, and |http| 
   {+data-lake-stores+}, the {+dl+} configuration for federated queries 
   resembles the following:

   .. code-block:: json

      {
        "stores" : [
          {
            "name" : "datacenter-alpha",
            "provider" : "s3",
            "region" : "us-east-1",
            "bucket" : "datacenter-alpha",
            "additionalStorageClasses" : [
              "STANDARD_IA"
            ],
            "prefix" : "/metrics",
            "delimiter" : "/"
          },
          {
            "name" : "atlasClusterStore",
            "provider" : "atlas",
            "clusterName" : "myDataCenter",
            "projectId" : "5e2211c17a3e5a48f5497de3"
          },
          {
            "name" : "httpStore",
            "provider" : "http",
            "allowInsecure" : false,
            "urls" [
              "https://www.datacenter-hardware.com/data.json",
              "https://www.datacenter-software.com/data.json"
            ],
            "defaultFormat" : ".json"
          }
        ],
        "databases" : [ 
          {
            "name" : "datacenter-metrics", 
            "collections" : [
              {
                "name" : "inventory",
                "dataSources" : [
                  {
                    "storeName" : "datacenter-alpha",
                    "path" : "/hardware/{date date}"
                  },
                  {
                    "storeName" : "atlasClusterStore",
                    "database" : "metrics",
                    "collection" : "hardware"
                  },
                  {
                    "storeName" : "httpStore",
                    "allowInsecure" : false,
                    "urls": [
                      "https://www.datacenter-metrics.com/data.json"
                    ],
                    "defaultFormat" : ".json"
                  }
                ]
              }
            ]
          }
        ]
      }

.. important:: 

   If the database in the storage configuration contains collections 
   from |s3|, |service|, and |http| {+data-lake-stores+}, the query 
   results might contain data from all the {+data-lake-stores+}.

.. seealso:: 

   :ref:`federated-queries`

.. _datalake-configuration-format:

Configuration Format
--------------------

The {+data-lake-short+} configuration has the following format:

.. tabs:: 
   :hidden:

   .. tab:: S3  
      :tabid: s3

      .. literalinclude:: /includes/s3-config-format.json
         :language: json
         :linenos:

   .. tab:: Atlas Cluster 
      :tabid: atlas 

      .. literalinclude:: /includes/atlas-config-format.json
         :language: json
         :linenos:

   .. tab:: HTTP 
      :tabid: http 

      .. literalinclude:: /includes/http-config-format.json
         :language: json
         :linenos:

:ref:`datalake-stores-reference`
  The ``stores`` object defines each {+data-lake-store+} associated
  with the {+data-lake-short+}. The {+data-lake-store+} captures files 
  in an S3 bucket, documents in |service| cluster, or files stored at 
  publicly accessible |url|\s. {+data-lake-short+} can only access
  {+data-lake-stores+} defined in the ``stores`` object.

:ref:`datalake-databases-reference`
  The ``databases`` object defines the mapping between each
  {+data-lake-store+} defined in ``stores`` and MongoDB collections 
  in the databases. 

.. _datalake-stores-reference:

``stores``
~~~~~~~~~~

.. tabs:: 
   :hidden:

   .. tab:: S3  
      :tabid: s3

      .. literalinclude:: /includes/s3-stores-config-format.json
         :language: json
         :linenos:

   .. tab:: Atlas Cluster
      :tabid: atlas

      .. literalinclude:: /includes/atlas-stores-config-format.json
         :language: json
         :linenos:

   .. tab:: HTTP
      :tabid: http

      .. literalinclude:: /includes/http-stores-config-format.json
         :language: json
         :linenos:

.. datalakeconf:: stores

   Array of objects where each object represents a {+data-lake-store+} 
   to associate with the {+data-lake-short+}. The {+data-lake-store+} 
   captures files in an S3 bucket, documents in |service| cluster, or 
   files stored at publicly accessible |url|\s. A {+dl+} can only 
   access {+data-lake-stores+} defined in the ``stores`` object.

.. datalakeconf:: stores.[n].name

   Name of the {+data-lake-store+}. The
   :datalakeconf:`databases.[n].collections.[n].dataSources.[n].storeName`
   field references this value as part of mapping configuration.

.. datalakeconf:: stores.[n].provider

   Defines where the data is stored. Value can be one of the following: 
   
   - ``s3`` for an |aws| |s3| bucket.
   - ``atlas`` for a collection in an |service| cluster.
   - ``http`` for data in files hosted at publicly accessible |url|\s.

.. tabs:: 
   :hidden:

   .. tab:: S3  
      :tabid: s3

      .. datalakeconf:: stores.[n].region

         Name of the |aws| region in which the S3 bucket is hosted. For
         a list of valid region names, see :atlas:`Amazon Web Services 
         (AWS) </reference/amazon-aws/#amazon-aws>`. 

      .. datalakeconf:: stores.[n].bucket

         Name of the |aws| S3 bucket. Must exactly match the name of an 
         |s3| bucket which {+data-lake-short+} can access with the
         configured |aws| IAM credentials.

      .. datalakeconf:: stores.[n].additionalStorageClasses

         *Optional.* Array of |aws| |s3| `storage classes 
         <https://aws.amazon.com/s3/storage-classes/>`__. {+adl+} will 
         include the files in these storage classes in the query 
         results. Valid values are: 

         - ``INTELLIGENT_TIERING`` to include files in the `Intelligent 
           Tiering <https://aws.amazon.com/s3/storage-classes/#Unknown_or_changing_access>`__ 
           storage class
         - ``STANDARD_IA`` to include files in the `Standard-Infrequent 
           Access <https://aws.amazon.com/s3/storage-classes/#Infrequent_access>`__ 
           storage class

         .. note:: 

            Files in the `Standard 
            <https://aws.amazon.com/s3/storage-classes/#General_purpose>`__ 
            storage class are supported by default.

      .. datalakeconf:: stores.[n].prefix

         *Optional.* Prefix {+data-lake-short+} applies when searching 
         for files in the S3 bucket. 

         For example, consider an S3 bucket ``metrics`` with the 
         following structure:

         .. code-block:: text
            :copyable: false

            metrics
              |--hardware
              |--software
                 |--computed

         The {+data-lake-store+} prepends the value of ``prefix`` to the
         :datalakeconf:`databases.[n].collections.[n].dataSources.[n].path` 
         to create the full path for files to ingest. Setting the 
         ``prefix`` to ``/software`` restricts any 
         :datalakeconf:`databases` objects using the 
         {+data-lake-store+} to only subpaths ``/software``.

         If omitted, {+data-lake-short+} searches all files from the
         root of the S3 bucket.

      .. datalakeconf:: stores.[n].delimiter
   
         *Optional.* The delimiter that separates 
         :datalakeconf:`databases.[n].collections.[n].dataSources.[n].path` 
         segments in the {+data-lake-store+}. |data-lake| uses the 
         delimiter to efficiently traverse |s3| buckets with a 
         hierarchical directory structure. You can specify any 
         character supported by the |s3| `object keys <https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html>`__ 
         as the delimiter. For example, you can specify an underscore 
         (``_``) or a plus sign (``+``) or multiple characters such as 
         double underscores (``__``) as the delimiter.

         If omitted, defaults to ``"/"``.

      .. datalakeconf:: stores.[n].includeTags

         *Optional.* Determines whether or not to use |s3| tags on the 
         files in the given path as additional partition attributes. 
         Valid values are ``true`` and ``false``. 

         If omitted, defaults to ``false``.

         If set to ``true``, {+data-lake-short+} does the following:

         - Adds the |s3| tags as additional partition attributes. 

         - Adds new top level BSON elements that associate each tag to 
           each document for the tagged files.

         .. warning::

            If set to ``true``, {+data-lake-short+} processes the files 
            for additional partition attributes by making extra calls 
            to |s3| to get the tags. This behavior might impact 
            performance.

   .. tab:: Atlas Cluster
      :tabid: atlas

      .. datalakeconf:: stores.[n].clusterName 

         Name of the |service| cluster on which the store is based. 
         Note that the cluster must be a M10 or higher cluster and must 
         exist in the same project as your {+dl+}. The ``source`` field 
         on the data partition is the name of the |service| cluster.

      .. datalakeconf:: stores.[n].projectId 

         Unique identifier of the project that contains the |service| 
         cluster on which the store is based.

   .. tab:: HTTP  
      :tabid: http

      .. datalakeconf:: stores.[n].allowInsecure 

         .. include:: /includes/extracts/param-allow-insecure.rst

      .. datalakeconf:: stores.[n].urls

         *Optional.* Comma-separated list of publicly accessible 
         |http| |url|\s where data is stored. You can't specify |url|\s 
         that require authentication. If empty or omitted, the 
         :ref:`storageGenerateConfig <datalake-storagegenconfig>` 
         command will not generate any virtual {+adl+} databases or 
         collections that reference the {+data-lake-store+}.

      .. datalakeconf:: stores.[n].defaultFormat
      
         .. include:: /includes/extracts/param-default-format.rst

         The specified format only applies to the |url|\s specified in 
         the :datalakeconf:`stores` object.

         .. seealso:: 

            :ref:`data-lake-data-formats`

.. _datalake-databases-reference:

``databases``
~~~~~~~~~~~~~

.. tabs:: 
   :hidden:

   .. tab:: S3  
      :tabid: s3

      .. literalinclude:: /includes/s3-databases-config-format.json
         :language: json
         :linenos:

   .. tab:: Atlas Cluster 
      :tabid: atlas

      .. literalinclude:: /includes/atlas-databases-config-format.json
         :language: json
         :linenos:

   .. tab:: HTTP 
      :tabid: http

      .. literalinclude:: /includes/http-databases-config-format.json
         :language: json
         :linenos:

.. datalakeconf:: databases

   Array of objects where each object represents a database, its 
   collections, and, optionally, any :manual:`views </core/views/>` on 
   the collections. Each database can have multiple ``collections`` and 
   ``views`` objects.

.. datalakeconf:: databases.[n].name

   Name of the database to which {+data-lake-short+} maps the
   data contained in the {+data-lake-store+}.  

.. datalakeconf:: databases.[n].collections

   Array of objects where each object represents a collection and data 
   sources that map to a :datalakeconf:`stores` {+data-lake-store+}.

.. datalakeconf:: databases.[n].collections.name

   Name of the collection to which {+data-lake-short+} maps the
   data contained in each
   :datalakeconf:`databases.[n].collections.[n].dataSources.[n].storeName`. 
   Each object in the array represents the mapping between the 
   collection and an object in the :datalakeconf:`stores` array. 

   .. tabs:: 
      :hidden:

      .. tab:: S3 
         :tabid: s3

         You can generate collection names dynamically from file paths 
         by specifying ``*`` for the collection name and the 
         ``collectionName()`` function in the 
         :datalakeconf:`~databases.[n].collections.[n].dataSources.[n].path`
         field. See :ref:`datalake-advanced-path-generate-collection` 
         for examples. 

      .. tab:: Atlas Cluster 
         :tabid: atlas

         You can generate collection names dynamically by specifying 
         ``*`` for the collection name and omitting the 
         :datalakeconf:`~databases.[n].collections.[n].dataSources.[n].collection` field. 

      .. tab:: HTTP 
         :tabid: http
         
         You cannot generate wildcard ``*`` collections.

.. datalakeconf:: databases.[n].collections.[n].dataSources 

   Array of objects where each object represents a 
   :datalakeconf:`stores` {+data-lake-store+} to map with the 
   collection.

.. datalakeconf:: databases.[n].collections.[n].dataSources.[n].storeName

   Name of a {+data-lake-store+} to map to the ``<collection>``. 
   Must match the :datalakeconf:`~stores.[n].name` of an object in the 
   :datalakeconf:`stores` array. 

.. tabs:: 
   :hidden:

   .. tab:: S3 
      :tabid: s3

      .. datalakeconf:: databases.[n].collections.[n].dataSources.[n].path

         Controls how {+data-lake+} searches for and parses files in the
         :datalakeconf:`~databases.[n].collections.[n].dataSources.[n].storeName` 
         before mapping them to the ``<collection>``. {+dl+} prepends 
         the :datalakeconf:`stores.[n].prefix` to the ``path`` to 
         build the full path to search within. Specify ``/`` to capture 
         all files and folders from the ``prefix`` path.

         For example, consider an S3 bucket ``metrics`` with the 
         following structure:

         .. code-block:: text
            :copyable: false

            metrics
            |--hardware
            |--software
               |--computed

         A ``path`` of ``/`` directs {+data-lake-short+} to search
         all files and folders in the ``metrics`` bucket.

         A ``path`` of ``/hardware`` directs {+data-lake-short+} to
         search only that path for files to ingest.

         If the :datalakeconf:`~stores.[n].prefix` is ``software``,
         {+data-lake-short+} searches for files only in the path
         ``/software/computed``.

         Appending the ``*`` wildcard character to the path 
         directs {+data-lake-short+} to include all files and folders 
         from that point in the path. For example, 
         ``/software/computed*`` would match files like 
         ``/software/computed-detailed``, 
         ``/software/computedArchive``, and 
         ``/software/computed/errors``.

         :datalakeconf:`~databases.[n].collections.[n].dataSources.[n].path`
         supports additional syntax for parsing filenames, including:

         - Generating document fields from filenames.
         - Using regular expressions to control field generation.
         - Setting boundaries for bucketing filenames by timestamp.
   
         See :ref:`datalake-path-syntax` for more information.

         .. include:: /includes/fact-path-delimiter.rst

      .. datalakeconf:: databases.[n].collections.[n].dataSources.[n].defaultFormat
      
         .. include:: /includes/extracts/param-default-format.rst

         .. seealso:: 

            :ref:`data-lake-data-formats`

      .. datalakeconf:: databases.[n].maxWildcardCollections 

         .. include:: /includes/extracts/param-max-wildcard-collections.rst 

   .. tab:: Atlas Cluster  
      :tabid: atlas 

      .. datalakeconf:: databases.[n].collections.[n].dataSources.[n].database 

         Name of the database on the |service| cluster that contains 
         the collection. 

      .. datalakeconf:: databases.[n].collections.[n].dataSources.[n].collection 

         Name of the collection in the |service| cluster on which the 
         {+dl+} {+data-lake-store+} is based. When creating a wildcard 
         collection, this must not be specified.

   .. tab:: HTTP  
      :tabid: http 

      .. datalakeconf:: databases.[n].collections.[n].dataSources.[n].allowInsecure 

         .. include:: /includes/extracts/param-allow-insecure.rst

      .. datalakeconf:: databases.[n].collections.[n].dataSources.[n].urls 

         *Optional*. Comma-separated list of publicly accessible 
         |url|\s where the data is stored. {+dl+} creates a partition 
         for each |url|. You can specify |url|\s that are not in the 
         :datalakeconf:`~stores.[n].urls`; however, the collection will 
         contain a union of data from |url|\s in both the 
         :datalakeconf:`~stores.[n].urls` and 
         :datalakeconf:`~databases.[n].collections.[n].dataSources.[n].urls`. 
         If omitted, {+dl+} uses the :datalakeconf:`~stores.[n].urls` 
         in the specified :datalakeconf:`~databases.[n].collections.[n].dataSources.[n].storeName`.

      .. datalakeconf:: databases.[n].collections.[n].dataSources.[n].defaultFormat
      
         .. include:: /includes/extracts/param-default-format.rst

         The specified format only applies to the |url|\s specified in 
         the :datalakeconf:`databases.[n].collections.[n].dataSources` 
         object.

         .. seealso:: 

            :ref:`data-lake-data-formats`

.. datalakeconf:: databases.[n].views 

   Array of objects where each object represents an 
   :manual:`aggregation pipeline </core/aggregation-pipeline/#id1>` on 
   a collection. To learn more about views, see :manual:`Views 
   </core/views/>`.

.. datalakeconf:: databases.[n].views.[n].name 

   Name of the view. 

.. datalakeconf:: databases.[n].views.[n].source 

   Name of the source collection for the view.

.. datalakeconf:: databases.[n].views.[n].pipeline 
 
   :manual:`Aggregation pipeline stage(s) 
   </core/aggregation-pipeline/#id1>` to apply to the 
   :datalakeconf:`~databases.[n].views.[n].source` collection.

.. seealso::

   - :ref:`Configure Atlas Data Lake <config-datalake>` 
   - `Tutorial: Federated Queries and $out to S3 
     <https://developer.mongodb.com/how-to/atlas-data-lake-federated-queries-out-aws-s3>`__
     