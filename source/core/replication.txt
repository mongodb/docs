========================
Replication Fundamentals
========================

.. default-domain:: mongodb

A MongoDB :term:`replica set` is a cluster of :program:`mongod` instances that
replicate amongst one another and ensure automated failover. Most replica
sets consists of two or more :program:`mongod` instances with at most one
of these designated as the primary and the rest as secondary
members. Clients direct all writes to the primary, while
the secondary members replicate from the primary asynchronously.

Database replication with MongoDB adds redundancy, helps to
ensure high availability, simplifies certain administrative tasks
such as backups, and may increase read capacity. Most production
deployments use replication.

If you're familiar with other database systems, you may think about
replica sets as a more sophisticated form of traditional master-slave
replication. [#master-slave]_ In master-slave replication, a
:term:`master` node accepts writes while one or more :term:`slave`
nodes replicate those write operations and thus maintain data sets
identical to the master. For MongoDB deployments, the member that accepts write
operations is the **primary**, and the replicating members are **secondaries**.

MongoDB's replica sets provide automated failover. If a
:term:`primary` fails, the remaining members will automatically
try to elect a new primary.

A replica set can have up to 12 members, but only 7 members can have
votes. For information regarding non-voting members, see
:ref:`non-voting members <replica-set-non-voting-members>`

.. seealso:: :ref:`Replica Set Implementation Details
   <replica-set-implementation>` and the :doc:`/replication` index for a
   list of all documents in this manual that contain information related
   to the operation and use of MongoDB's replica sets.

.. [#master-slave] MongoDB also provides conventional master/slave
   replication. Master/slave replication operates by way of the same
   mechanism as replica sets, but lacks the automatic failover
   capabilities. While replica sets are the recommended solution for
   production, a replica set can support only 12 members in total.
   If your deployment requires more than 11 :term:`slave` members, you'll
   need to use master/slave replication.

.. index:: configuration; replica set members

Member Configurations
---------------------

All replica sets at most use a single :term:`primary` and one or more
:term:`secondary` members. You can configure secondary members in a
variety of ways, as listed here. For details, see
:ref:`replica-set-member-configurations` in the
:doc:`/administration/replica-sets` document.

You can configure a member as any of the following:

- **Secondary-Only**: These members cannot become primary. See
  :ref:`replica-set-secondary-only-members`.

- **Hidden**: These members are invisible to client applications. See
  :ref:`replica-set-hidden-members`.

- **Delayed**: These members apply operations from the primary's :term:`oplog` with
  a specified delay. See :ref:`replica-set-delayed-members`.

- **Arbiters**: These members do not hold data and exist solely to
  participate in :ref:`elections <replica-set-elections>`. See :ref:`replica-set-arbiters`.

- **Non-Voting**: These members cannot vote in elections. See
  :ref:`replica-set-non-voting-members`.

.. _replica-set-failover:

Failover
--------

Replica sets feature automated failover. If the :term:`primary`
goes offline or becomes unresponsive and a majority of the original
set members can still connect to each other, the set will elect a new
primary. For details, see :ref:`replica-set-failover-administration`.

.. index:: replica set; elections
.. index:: failover; elections
.. _replica-set-elections:

Elections
~~~~~~~~~

When any failover occurs, an election takes place to decide which
member should become primary.

Elections provide a mechanism for the members of a :term:`replica set`
to autonomously select a new :term:`primary` without
administrator intervention. The election allows replica sets to
recover from failover situations very quickly and robustly.

Whenever the primary becomes unreachable, the secondary members
trigger an election. The first member to
receive votes from a majority of the set will become primary. The most
important feature of replica set elections is that a majority of the
original number of members in the replica set must be present for
election to succeed. If you have a three-member replica set, the set can
elect a primary when two or three members can connect to each other. If
two members in the replica go offline, then the remaining member will
remain a secondary.

.. note::

   When the current :term:`primary` steps down and triggers an
   election, the :program:`mongod` instances will close all client
   connections. This ensures that the clients maintain an accurate
   view of the :term:`replica set` and helps prevent :term:`rollbacks
   <rollback>`.

.. seealso:: The :ref:`replica-set-election-internals` section in the
   :doc:`/core/replication-internals` document, as well as the
   :ref:`replica-set-failover-administration` section in the
   :doc:`/administration/replica-sets` document.

.. index:: replica set; priority
.. _replica-set-node-priority:
.. _replica-set-member-priority:

Member Priority
~~~~~~~~~~~~~~~

In a replica set, every member has a "priority," that helps determine
eligibility for :ref:`election <replica-set-elections>` to
:term:`primary`. By default, all members have a priority of ``1``,
unless you modify the :data:`members[n].priority` value. All members
have a single vote in elections.

.. warning::

   Always configure the :data:`members[n].priority` value to control
   which members will become primary. Do not configure
   :data:`members[n].votes` except to permit more than 7 secondary
   members.

.. seealso:: :ref:`Member Priority Configuration <replica-set-node-priority-configuration>`

.. index:: pair: replica set; consistency
.. _replica-set-consistency:

Consistency
-----------

In MongoDB, all read operations issued to the primary of a
replica set are :term:`consistent <strict consistency>`, with the last
write operation.

If clients configure the :term:`read preference` to permit allow secondary reads,
read operations cannot return from :term:`secondary` members that have not
replicated more recent updates or operations. In these situations the
query results may reflect a previous state.

This behavior is sometimes characterized as :term:`eventual
consistency` because the secondary member's state will *eventually*
reflect the primary's state and MongoDB cannot guarantee :term:`strict
consistency` for read operations from secondary members.

There is no way to guarantee consistency for reads from *secondary
members,* except by configuring the :term:`client` and :term:`driver` to
ensure that write operations succeed on all members before completing
successfully.

This section provides an overview of the concepts that underpin
database consistency and the MongoDB mechanisms to ensure that users
have access to consistent data.

.. index:: rollbacks
   single: replica set; rollbacks
   single: consistency; rollbacks

.. _replica-set-rollbacks:

Rollbacks
~~~~~~~~~

In some :term:`failover` situations :term:`primaries <primary>` will have
accepted write operations that have *not* replicated to the
:term:`secondaries <secondary>` after a failover occurs. This case is
rare and typically occurs as a result of a network partition with
replication lag. When this member (the former primary) rejoins the
:term:`replica set` and attempts to continue replication as a
secondary the former primary must revert these operations or "roll
back" these operations to maintain database consistency across the
replica set.

MongoDB writes the rollback data to a :term:`BSON` file in the
database's :setting:`dbpath` directory. Use :doc:`bsondump
</reference/bsondump>` to read the contents of these rollback files
and then manually apply the changes to the new primary. There is no
way for MongoDB to appropriately and fairly handle rollback situations
without manual intervention. Even after the member completes the
rollback and returns to secondary status, administrators will need to
apply or decide to ignore the rollback data.

The best strategy for avoiding all rollbacks is to ensure :ref:`write
propagation <replica-set-write-concern>` to all or some of the
members in the set. Using
these kinds of policies prevents situations
that might create rollbacks.

.. warning::

   A :program:`mongod` instance will not rollback more than 300
   megabytes of data. If your system needs to rollback more than 300
   MB, you will need to manually intervene to recover this data.

Application Concerns
~~~~~~~~~~~~~~~~~~~~

Client applications are indifferent to the configuration and operation
of replica sets. While specific configuration depends to some extent
on the client :doc:`drivers </applications/drivers>`, there is often
minimal or no difference between applications using
:term:`replica sets <replica set>` or standalone instances.

There are two major concepts that *are* important to consider when
working with replica sets:

1. :ref:`Write Concern <replica-set-write-concern>`.

   By default, MongoDB clients receive no response from the server to
   confirm successful write operations. Most drivers provide a
   configurable "safe mode," where the server will return a response
   for all write operations using :dbcommand:`getLastError`. For
   replica sets, :term:`write concern` is configurable to ensure that
   secondary members of the set have replicated operations before the
   write returns.

2. :ref:`Read Preference <replica-set-read-preference>`

   By default, read operations issued against a replica set return
   results from the :term:`primary`. Users may
   configure :term:`read preference` on a per-connection basis to
   prefer that read operations return on the :term:`secondary`
   members.

:term:`Read preference` and :term:`write concern` have particular
:ref:`consistency <replica-set-consistency>` implications.

.. seealso:: :doc:`/applications/replication`,
   :ref:`replica-set-write-concern`, and
   :ref:`replica-set-read-preference`.

Administration and Operations
-----------------------------

This section provides a brief overview of relevant concerns for
administrators of replica set deployments.

.. seealso::

   - :doc:`/administration/replica-sets`
   - :doc:`/administration/replication-architectures`

.. index:: replica set; oplog
.. _replica-set-oplog-sizing:

Oplog
~~~~~
..
  Actual oplog sizing as of 2012-07-02:

  32 bit systems = ~48 megabytes
  64 bit = larger of 5% of disk or ~1 gigabyte
  64 bit OS X = ~183 megabytes

The :term:`oplog` (operations log) is a special :term:`capped
collection` that keeps a rolling record of all operations that modify
that data stored in your databases. MongoDB applies database operations
on the :term:`primary` and then records the operations on the primary's
oplog. The :term:`secondary` members then replicate this log and apply
the operations to themselves in an asynchronous process. All replica set
members contain a copy of the oplog, allowing them to maintain the
current state of the database. Operations in the oplog are :term:`idempotent`.

By default, the size of the oplog is as follows:

- For 64-bit Linux, Solaris, and FreeBSD systems, MongoDB will
  allocate 5% of the available free disk space to the oplog.

  If this amount is smaller than a gigabyte, then MongoDB will
  allocate 1 gigabyte of space.

- For 64-bit OS X systems, MongoDB allocates 183 megabytes of space to
  the oplog.

- For 32-bit systems, MongoDB allocates about 48 megabytes of space to
  the oplog.

Before oplog creation, you can specify the size of your oplog with the
:setting:`oplogSize` option. After you start a replica set member for
the first time, you can only change the size of the oplog by using the
:doc:`/tutorial/change-oplog-size` tutorial.

In most cases, the default oplog size is sufficient. For example, if an
oplog that is 5% of free disk space fills up in 24 hours of operations, then
secondaries can stop copying entries from the oplog for 24 hours before
they require full resyncing. However, most replica sets have much
lower operation volumes, and their oplogs can hold a much larger
number of operations.

The following factors affect how MongoDB uses space in the oplog:

- Update operations that affect multiple documents at once.

  The oplog must translate multi-updates into individual operations,
  in order to maintain :term:`idempotency <idempotent>`. This can use
  a great deal of oplog space without a corresponding increase
  in disk utilization.

- If you delete roughly the same amount of data as you insert.

  In this situation the database will not grow significantly in disk
  utilization, but the size of the operation log can be quite large.

- If a significant portion of your workload entails in-place updates.

  In-place updates create a large number of operations but do not
  change the quantity data on disk.

If you can predict your replica set's workload to resemble one
of the above patterns, then you may want to consider creating an oplog
that is larger than the default. Conversely, if the predominance of
activity of your MongoDB-based application are reads and you are
writing a small amount of data, you may find that you need a much
smaller oplog.

.. seealso:: The :ref:`replica-set-oplog` topic in
   the :doc:`/core/replication-internals` document.

Deployment
~~~~~~~~~~

Without replication, a standalone MongoDB instance represents a single
point of failure and any disruption of the MongoDB system will render
the database unusable and potentially unrecoverable. Replication
increase the reliability of the database instance, and replica sets
are capable of distributing reads to :term:`secondary` members depending
on :term:`read preference`. For database work loads dominated by read
operations, (i.e. "read heavy") replica sets can greatly increase the
capability of the database system.

The minimum requirements for a replica set include two members with
data, for a :term:`primary` and a :term:`secondary`, and an :ref:`arbiters
<replica-set-arbiters>`. In most circumstances, however, you will want
to deploy three data members.

For those deployments that rely heavily on distributing reads to
secondary instances, add additional members to the set as load
increases. As your deployment grows, consider adding or moving
replica set members to secondary data centers or to geographically
distinct locations for additional redundancy. While many architectures
are possible, always ensure that the quorum of members required to elect
a primary remains in your main facility.

Depending on your operational requirements, you may consider adding
members configured for a specific purpose including, a :term:`delayed
member` to help provide protection against human errors and change
control, a :term:`hidden member` to provide an isolated member for
reporting and monitoring, and/or a :ref:`secondary only member
<replica-set-secondary-only-members>` for dedicated backups.

The process of establishing a new replica set member can be resource
intensive on existing members. As a result, deploy new members to
existing replica sets significantly before current demand saturates
the existing members.

.. note::

   :term:`Journaling <journal>`, provides single-instance write
   durability. The journaling greatly improves the reliability and
   durability of a database. Unless MongoDB runs with journaling, when
   a MongoDB instance terminates ungracefully, the database can end in
   a corrupt and unrecoverable state.

   You should assume that a database, running without journaling, that
   suffers a crash or unclean shutdown is in corrupt or inconsistent
   state.

   **Use journaling**, however, do not forego proper replication
   because of journaling.

   64-bit versions of MongoDB after version 2.0 have journaling
   enabled by default.

.. index:: pair: replica set; security

Security
~~~~~~~~

In most cases, :term:`replica set` administrators do not have to keep
additional considerations in mind beyond the normal security
precautions that all MongoDB administrators must take. However, ensure
that:

- Your network configuration will allow every member of the replica
  set to contact every other member of the replica set.

- If you use MongoDB's authentication system to limit access to your
  infrastructure, ensure that you configure a
  :setting:`keyFile` on all members to permit authentication.

.. seealso:: :ref:`Replica Set Security <replica-set-security>`

.. _replica-set-deployment-overview:
.. _replica-set-architecture:

Architectures
~~~~~~~~~~~~~

The architecture and design of the replica set deployment can have a
great impact on the set's capacity and capability. This section
provides a general overview of best practices for replica set
architectures.

This document provides an overview of the *complete* functionality of
replica sets, which highlights the flexibility of the replica set and
its configuration. However, for most production deployments a
conventional 3-member replica set with :data:`members[n].priority`
values of ``1`` are sufficient.

While the additional flexibility discussed is below helpful for
managing a variety of operational complexities, it always makes sense
to let those complex requirements dictate complex architectures,
rather than add unnecessary complexity to your deployment.

Consider the following factors when developing an architecture for
your replica set:

- Ensure that the members of the replica set will always be able to
  elect a primary. Run an odd number of members or run an arbiter
  on one of your application servers if you have an even number of
  members.

- With geographically distributed members, be aware of where the
  "quorum" of members will be in case of likely network partitions,
  attempt to ensure that the set can elect a primary among the members in
  the primary data center.

- Consider including a :ref:`hidden <replica-set-hidden-members>`
  or :ref:`delayed member <replica-set-delayed-members>` in your replica
  set to support dedicated functionality, like backups, reporting, and
  testing.

- Consider keeping one or two members of the set in an off-site data
  center, but make sure to configure the :ref:`priority
  <replica-set-node-priority>` to prevent it from
  becoming :term:`primary`.

.. seealso:: :doc:`/administration/replication-architectures` for
   more information regarding replica set architectures.
