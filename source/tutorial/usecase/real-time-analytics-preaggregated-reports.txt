===========================================
Real Time Analytics: Pre-Aggregated Reports
===========================================

Problem
=======

You have one or more servers generating events for which you want
real-time statistical information in a MongoDB collection.

Solution overview
=================

This solution assumes the following:

-  There is no need to retain transactional event data in MongoDB, or
   that retention is handled outside the scope of this use case
-  You need statistical data to be up-to-the minute (or up-to-the-second,
   if possible.)
-  The queries to retrieve time series of statistical data need to be as
   fast as possible.

The general approach is to use upserts and increments to generate the
statistics and simple range-based queries and filters to draw the time
series charts of the aggregated data.

To help anchor this solution, it will examine a simple scenario where you
want to count the number of hits to a collection of web site at various
levels of time-granularity (by minute, hour, day, week, and month) as
well as by path. It is assumed that either you have some code that can
run as part of your web app when it is rendering the page, or you have
some set of logfile post-processors that can run in order to integrate
the statistics.

Schema design
=============

There are two important considerations when designing the schema for a
real-time analytics system: the ease & speed of updates and the ease &
speed of queries. In particular, you want to avoid the following
performance-killing circumstances:

-  documents changing in size significantly, causing reallocations on
   disk
-  queries that require large numbers of disk seeks to be satisfied
-  document structures that make accessing a particular field slow

One approach you *could* use to make updates easier would be to keep your
hit counts in individual documents, one document per
minute/hour/day/etc. This approach, however, requires us to query
several documents for nontrivial time range queries, slowing down our
queries significantly. In order to keep your queries fast, you will
instead use somewhat more complex documents, keeping several aggregate
values in each document.

In order to illustrate some of the other issues you might encounter, here are
several schema designs that you might try as well as discussion of
the problems with them.

Design 0: one document per page/day
-----------------------------------

The initial approach will be to simply put all the statistics in which
you're interested into a single document per page:

.. code-block:: javascript

    {
        _id: "20101010/site-1/apache_pb.gif",
        metadata: {
            date: ISODate("2000-10-10T00:00:00Z"),
            site: "site-1",
            page: "/apache_pb.gif" },
        daily: 5468426,
        hourly: {
            "0": 227850,
            "1": 210231,
            ...
            "23": 20457 },
        minute: {
            "0": 3612,
            "1": 3241,
            ...
            "1439": 2819 }
    }

This approach has a couple of advantages: a) it only requires a single
update per hit to the website, b) intra-day reports for a single page
require fetching only a single document. There are, however, significant
problems with this approach. The biggest problem is that, as you upsert
data into the 'hy' and 'mn' properties, the document grows. Although
MongoDB attempts to pad the space required for documents, it will still
end up needing to reallocate these documents multiple times throughout
the day, copying the documents to areas with more space.

Design #0.5: Preallocate documents
----------------------------------

In order to mitigate the repeated copying of documents, you can tweak your
approach slightly by adding a process which will preallocate a document
with initial zeros during the previous day. In order to avoid a
situation where you preallocate documents *en masse* at midnight, you will
(with a low probability) randomly upsert the next day's document each
time you update the current day's statistics. This requires some tuning;
you'd like to have almost all the documents preallocated by the end of
the day, without spending much time on extraneous upserts (preallocating
a document that's already there). A reasonable first guess would be to
look at your average number of hits per day (call it *hits* ) and
preallocate with a probability of *1/hits* .

Preallocating helps us mainly by ensuring that all the various 'buckets'
are initialized with 0 hits. Once the document is initialized, then, it
will never dynamically grow, meaning a) there is no need to perform the
reallocations that could slow us down in design #0 and b) MongoDB
doesn't need to pad the records, leading to a more compact
representation and better usage of your memory.

Design #1: Add intra-document hierarchy
---------------------------------------

One thing to be aware of with BSON is that documents are stored as a
sequence of (key, value) pairs, *not* as a hash table. What this means
for us is that writing to stats.mn.0 is *much* faster than writing to
stats.mn.1439.

.. figure:: img/rta-preagg1.png
   :align: center
   :alt:

   In order to update the value in cell #1349, MongoDB must skip over all 1349
   entries before it.

In order to speed this up, you can introduce some intra-document
hierarchy. In particular, you can split the 'mn' field up into 24 hourly
fields:

.. code-block:: javascript

    {
        _id: "20101010/site-1/apache_pb.gif",
        metadata: {
            date: ISODate("2000-10-10T00:00:00Z"),
            site: "site-1",
            page: "/apache_pb.gif" },
        daily: 5468426,
        hourly: {
            "0": 227850,
            "1": 210231,
            ...
            "23": 20457 },
        minute: {
            "0": {
                "0": 3612,
                "1": 3241,
                ...
                "59": 2130 },
            "1": {
            "60": ... ,
            },
            ...
            "23": {
                ...
                "1439": 2819 }
        }
    }

This allows MongoDB to "skip forward" when updating the minute
statistics later in the day, making your performance more uniform and
generally faster.

.. figure:: img/rta-preagg2.png
   :align: center
   :alt:

   To update the value in cell #1349, MongoDB first skips the first 23 hours and
   then skips 59 minutes for only 82 skips as opposed to 1439 skips in the
   previous schema.

Design #2: Create separate documents for different granularities
----------------------------------------------------------------

Design #1 is certainly a reasonable design for storing intraday
statistics, but what happens when you want to draw a historical chart
over a month or two? In that case, you need to fetch 30+ individual
documents containing or daily statistics. A better approach would be to
store daily statistics in a separate document, aggregated to the month.
This does introduce a second upsert to the statistics generation side of
your system, but the reduction in disk seeks on the query side should
more than make up for it. At this point, your document structure is as
follows:

Daily Statistics
~~~~~~~~~~~~~~~~

.. code-block:: javascript

    {
        _id: "20101010/site-1/apache_pb.gif",
        metadata: {
            date: ISODate("2000-10-10T00:00:00Z"),
            site: "site-1",
            page: "/apache_pb.gif" },
        hourly: {
            "0": 227850,
            "1": 210231,
            ...
            "23": 20457 },
        minute: {
            "0": {
                "0": 3612,
                "1": 3241,
                ...
                "59": 2130 },
            "1": {
                "0": ...,
            },
            ...
            "23": {
                "59": 2819 }
        }
    }

Monthly Statistics
~~~~~~~~~~~~~~~~~~

.. code-block: javascript::

    {
        _id: "201010/site-1/apache_pb.gif",
        metadata: {
            date: ISODate("2000-10-00T00:00:00Z"),
            site: "site-1",
            page: "/apache_pb.gif" },
        daily: {
            "1": 5445326,
            "2": 5214121,
            ... }
    }

Operations
==========

In this system, you want balance between read performance and write
(upsert) performance. This section will describe each of the major
operations you perform, using the Python programming language and the
pymongo MongoDB driver. These operations would be similar in other
languages as well.

Log a hit to a page
-------------------

Logging a hit to a page in your website is the main 'write' activity in
your system. In order to maximize performance, you will be doing in-place
updates with the upsert operation:

.. code-block:: python

    from datetime import datetime, time


    def log_hit(db, dt_utc, site, page):


        # Update daily stats doc
        id_daily = dt_utc.strftime('%Y%m%d/') + site + page
        hour = dt_utc.hour
        minute = dt_utc.minute


        # Get a datetime that only includes date info
        d = datetime.combine(dt_utc.date(), time.min)
        query = {
            '_id': id_daily,
            'metadata': { 'date': d, 'site': site, 'page': page } }
        update = { '$inc': {
                'hourly.%d' % (hour,): 1,
                'minute.%d.%d' % (hour,minute): 1 } }
        db.stats.daily.update(query, update, upsert=True)


        # Update monthly stats document
        id_monthly = dt_utc.strftime('%Y%m/') + site + page
        day_of_month = dt_utc.day
        query = {
            '_id': id_monthly,
            'metadata': {
                'date': d.replace(day=1),
                'site': site,
                'page': page } }
        update = { '$inc': {
                'daily.%d' % day_of_month: 1} }
        db.stats.monthly.update(query, update, upsert=True)

Since you're using the upsert operation, this function will perform
correctly whether the document is already present or not, which is
important, as your preallocation (the next operation) will only
preallocate documents with a high probability. Note however, that
without preallocation, you end up with a dynamically growing document,
slowing down your upserts significantly as documents are moved in order
to grow them.

Preallocate
-----------

In order to keep your documents from growing, you can preallocate them
before they are needed. When preallocating, you set all the statistics to
zero for all time periods so that later, the document doesn't need to
grow to accomodate the upserts. Here, you add this preallocation as its
own function:

.. code-block:: python

    def preallocate(db, dt_utc, site, page):


        # Get id values
        id_daily = dt_utc.strftime('%Y%m%d/') + site + page
        id_monthly = dt_utc.strftime('%Y%m/') + site + page


        # Get daily metadata
        daily_metadata = {
            'date': datetime.combine(dt_utc.date(), time.min),
            'site': site,
            'page': page }
        # Get monthly metadata
        monthly_metadata = {
            'date': daily_m['d'].replace(day=1),
            'site': site,
            'page': page }


        # Initial zeros for statistics
        hourly = dict((str(i), 0) for i in range(24))
        minute = dict(
            (str(i), dict((str(j), 0) for j in range(60)))
            for i in range(24))
        daily = dict((str(i), 0) for i in range(1, 32))


        # Perform upserts, setting metadata
        db.stats.daily.update(
            {
                '_id': id_daily,
                'hourly': hourly,
                'minute': minute},
            { '$set': { 'metadata': daily_metadata }},
            upsert=True)
        db.stats.monthly.update(
            {
                '_id': id_monthly,
                'daily': daily },
            { '$set': { 'm': monthly_metadata }},
            upsert=True)

In this case, note that you went ahead and preallocated the monthly
document while you were preallocating the daily document. While you could
have split this into its own function and preallocated monthly documents
less frequently that daily documents, the performance difference is
negligible, so you opted to simply combine monthly preallocation with
daily preallocation.

The next question you must answer is when you should preallocate. You would
like to have a high likelihood of the document being preallocated before
it is needed, but you don't want to preallocate all at once (say at
midnight) to ensure you don't create a spike in activity and a
corresponding increase in latency. Your solution here is to
probabilistically preallocate each time you log a hit, with a probability
tuned to make preallocation likely without performing too many
unnecessary calls to preallocate:

.. code-block:: python

    from random import random
    from datetime import datetime, timedelta, time


    # Example probability based on 500k hits per day per page
    prob_preallocate = 1.0 / 500000


    def log_hit(db, dt_utc, site, page):
        if random.random() < prob_preallocate:
            preallocate(db, dt_utc + timedelta(days=1), site_page)
        # Update daily stats doc
        â€¦

Now with a high probability, you will preallocate each document before
it's used, preventing the midnight spike as well as eliminating the
movement of dynamically growing documents.

Get data for a real-time chart
------------------------------

One chart that you may be interested in seeing would be the number of
hits to a particular page over the last hour. In that case, your query is
fairly straightforward:

.. code-block:: python

    >>>``db.stats.daily.find_one(
    ...     {'metadata': {'date':dt, 'site':'site-1', 'page':'/foo.gif'}},
    ...     { 'minute': 1 })

Likewise, you can get the number of hits to a page over the last day,
with hourly granularity:

.. code-block:: python

    >>> db.stats.daily.find_one(
    ...     {'metadata': {'date':dt, 'site':'site-1', 'page':'/foo.gif'}},
    ...     { 'hy': 1 })

If you want a few days' worth of hourly data, you can get it using the
following query:

.. code-block:: python

    >>> db.stats.daily.find(
    ...     {
    ...         'metadata.date': { '$gte': dt1, '$lte': dt2 },
    ...         'metadata.site': 'site-1',
    ...         'metadata.page': '/foo.gif'},
    ...     { 'metadata.date': 1, 'hourly': 1 } },
    ...     sort=[('metadata.date', 1)])

In this case, you are retrieving the date along with the statistics since
it's possible (though highly unlikely) that you could have a gap of one
day where a) you didn't happen to preallocate that day and b) there were
no hits to the document on that day.

Index support
~~~~~~~~~~~~~

These operations would benefit significantly from indexes on the
metadata of the daily statistics:

.. code-block:: python

    >>> db.stats.daily.ensure_index([
    ...     ('metadata.site', 1),
    ...     ('metadata.page', 1),
    ...     ('metadata.date', 1)])

Note in particular that you indexed on the page first, date second. This
allows us to perform the third query above (a single page over a range
of days) quite efficiently. Having any compound index on page and date,
of course, allows us to look up a single day's statistics efficiently.

Get data for a historical chart
-------------------------------

In order to retrieve daily data for a single month, you can perform the
following query:

.. code-block:: python

    >>> db.stats.monthly.find_one(
    ...     {'metadata':
    ...         {'date':dt,
    ...         'site': 'site-1',
    ...         'page':'/foo.gif'}},
    ...     { 'daily': 1 })

If you want several months' worth of daily data, of course, you can do the
same trick as above:

.. code-block:: python

    >>> db.stats.monthly.find(
    ...     {
    ...         'metadata.date': { '$gte': dt1, '$lte': dt2 },
    ...         'metadata.site': 'site-1',
    ...         'metadata.page': '/foo.gif'},
    ...     { 'metadata.date': 1, 'hourly': 1 } },
    ...     sort=[('metadata.date', 1)])

Index support
~~~~~~~~~~~~~

Once again, these operations would benefit significantly from indexes on
the metadata of the monthly statistics:

.. code-block:: python

    >>> db.stats.monthly.ensure_index([
    ...     ('metadata.site', 1),
    ...     ('metadata.page', 1),
    ...     ('metadata.date', 1)])

The order of your index is once again designed to efficiently support
range queries for a single page over several months, as above.

Sharding
========

Your performance in this system will be limited by the number of shards
in your cluster as well as the choice of your shard key. Your ideal shard
key will balance upserts beteen your shards evenly while routing any
individual query to a single shard (or a small number of shards). A
reasonable shard key for us would thus be ('metadata.site',
'metadata.page'), the site-page combination for which you are calculating
statistics:

.. code-block:: python

    >>> db.command('shardcollection', 'stats.daily', {
    ...     key : { 'metadata.site': 1, 'metadata.page' : 1 } })
    { "collectionsharded" : "stats.daily", "ok" : 1 }
    >>> db.command('shardcollection', 'stats.monthly', {
    ...     key : { 'metadata.site': 1, 'metadata.page' : 1 } })
    { "collectionsharded" : "stats.monthly", "ok" : 1 }

One downside to using ('metadata.site', 'metadata.page') as your shard
key is that, if one page dominates all your traffic, all updates to that
page will go to a single shard. The problem, however, is largely
unavoidable, since all update for a single page are going to a single
*document.*

You also have the problem using only ('metadata.site', 'metadata.page')
shard key that, if a high percentage of your queries go to the same page,
these will all be handled by the same shard. A (slightly) better shard
key would the include the date as well as the site/page so that you could
serve different historical ranges with different shards:

.. code-block:: python

    >>> db.command('shardcollection', 'stats.daily', {
    ...     key:{'metadata.site':1,'metadata.page':1,'metadata.date':1}})
    { "collectionsharded" : "stats.daily", "ok" : 1 }
    >>> db.command('shardcollection', 'stats.monthly', {
    ...     key:{'metadata.site':1,'metadata.page':1,'metadata.date':1}})
    { "collectionsharded" : "stats.monthly", "ok" : 1 }

It is worth noting in this discussion of sharding that, depending on the
number of sites/pages you are tracking and the number of hits per page,
you're talking about a fairly small set of data with modest performance
requirements, so sharding may be overkill. In the case of the MongoDB
Monitoring Service (MMS), a single shard is able to keep up with the
totality of traffic generated by all the customers using this (free)
service.
