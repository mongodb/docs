.. index:: balancing; configure

==========================================================
Configure Behavior of Balancer Process in Sharded Clusters
==========================================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

The balancer is a process that runs on *one* of the :binary:`~bin.mongos`
instances in a cluster and ensures that :term:`chunks <chunk>` are
evenly distributed throughout a sharded cluster. In most deployments,
the default balancer configuration is sufficient for normal
operation. However, administrators might need to modify balancer
behavior depending on application or operational requirements. If you
encounter a situation where you need to modify the behavior of the
balancer, use the procedures described in this document.

For conceptual information about the balancer, see
:ref:`sharding-balancing` and :ref:`sharding-balancing-internals`.

.. _sharded-cluster-config-balancing-window:

Schedule a Window of Time for Balancing to Occur
------------------------------------------------

You can schedule a window of time during which the balancer can
migrate chunks, as described in the following procedures:

- :ref:`sharding-schedule-balancing-window`

- :ref:`sharding-balancing-remove-window`.

The :binary:`~bin.mongos` instances use their own local timezones when
respecting balancer window.

.. _sharded-cluster-config-default-chunk-size:

Configure Default Chunk Size
----------------------------

The default chunk size for a sharded cluster is 64 megabytes. In most
situations, the default size is appropriate for splitting and migrating
chunks. For information on how chunk size affects deployments, see
details, see :ref:`sharding-chunk-size`.

Changing the default chunk size affects chunks that are processes during
migrations and auto-splits but does not retroactively affect all chunks.

To configure default chunk size, see :doc:`modify-chunk-size-in-sharded-cluster`.

.. _sharded-cluster-config-max-shard-size:

Change the Maximum Storage Size for a Given Shard
-------------------------------------------------

The ``maxSize`` field in the :data:`~config.shards` collection in the
:ref:`config database <config-database>` sets the maximum size for a
shard, allowing you to control whether the balancer will migrate chunks
to a shard. If :data:`~serverStatus.mem.mapped` size [#local-limitation]_ is above a shard's
``maxSize``, the balancer will not move chunks to the shard. Also, the
balancer will not move chunks off an overloaded shard. This must happen
manually. The ``maxSize`` value only affects the balancer's selection of
destination shards.

By default, ``maxSize`` is not specified, allowing shards to consume the
total amount of available space on their machines if necessary.

You can set ``maxSize`` both when adding a shard and once a shard is
running.

To set ``maxSize`` when adding a shard, set the :dbcommand:`addShard`
command's ``maxSize`` parameter to the maximum size in megabytes. For
example, the following command run in the :binary:`~bin.mongo` shell adds a
shard with a maximum size of 125 megabytes:

.. code-block:: javascript

   db.runCommand( { addshard : "example.net:34008", maxSize : 125 } )

To set ``maxSize`` on an existing shard, insert or update the
``maxSize`` field in the :data:`~config.shards` collection in the
:ref:`config database <config-database>`. Set the ``maxSize`` in
megabytes.

.. example::

   Assume you have the following shard without a ``maxSize`` field:

   .. code-block:: javascript

      { "_id" : "shard0000", "host" : "example.net:34001" }

   Run the following sequence of commands in the :binary:`~bin.mongo` shell
   to insert a ``maxSize`` of 125 megabytes:

   .. code-block:: javascript

      use config
      db.shards.update( { _id : "shard0000" }, { $set : { maxSize : 125 } } )

   To later increase the ``maxSize`` setting to 250 megabytes, run the
   following:

   .. code-block:: javascript

      use config
      db.shards.update( { _id : "shard0000" }, { $set : { maxSize : 250 } } )

.. [#local-limitation] This value includes the mapped size of all data
   files including the``local`` and ``admin`` databases. Account for
   this when setting ``maxSize``.

.. index:: balancing; secondary throttle
.. index:: secondary throttle
.. _sharded-cluster-config-secondary-throttle:

Change Replication Behavior for Chunk Migration
-----------------------------------------------

Secondary Throttle
~~~~~~~~~~~~~~~~~~

.. versionchanged:: 3.0.0
   The balancer configuration document added configurable
   ``writeConcern`` to control the semantics of the
   ``_secondaryThrottle`` option.

The ``_secondaryThrottle`` parameter of the balancer and the
:dbcommand:`moveChunk` command affects the replication behavior during
:ref:`chunk migration <chunk-migration-replication>`. By default,
``_secondaryThrottle`` is ``true``, which means each document move
during chunk migration propagates to at least one secondary before the
balancer proceeds with the next document: this is equivalent to a write
concern of ``{ w: 2 }``.

You can also configure the ``writeConcern`` for the
``_secondaryThrottle`` operation, to configure how migrations will
wait for replication to complete.  For more information on the
replication behavior during various steps of chunk migration,
see :ref:`chunk-migration-replication`.

To change the balancer's ``_secondaryThrottle`` and ``writeConcern``
values, connect to a :binary:`~bin.mongos` instance and directly update
the ``_secondaryThrottle`` value in the :data:`~config.settings`
collection of the :ref:`config database <config-database>`. For
example, from a :binary:`~bin.mongo` shell connected to a
:binary:`~bin.mongos`, issue the following command:

.. code-block:: javascript

   use config
   db.settings.update(
      { "_id" : "balancer" },
      { $set : { "_secondaryThrottle" : false ,
                 "writeConcern": { "w": "majority" } } },
      { upsert : true }
   )

The effects of changing the ``_secondaryThrottle`` and
``writeConcern`` value may not be
immediate. To ensure an immediate effect, stop and restart the balancer
to enable the selected value of ``_secondaryThrottle``. See
:doc:`/tutorial/manage-sharded-cluster-balancer` for details.

.. _wait-for-delete-setting:

Wait for Delete
~~~~~~~~~~~~~~~

The ``_waitForDelete`` setting of the balancer and the
:dbcommand:`moveChunk` command affects how the balancer migrates
multiple chunks from a shard. By default, the balancer does not wait
for the on-going migration's delete phase to complete before starting
the next chunk migration. To have the delete phase **block** the start
of the next chunk migration, you can set the ``_waitForDelete`` to
true.

For details on chunk migration, see :ref:`sharding-chunk-migration`.
For details on the chunk migration queuing behavior, see
:ref:`chunk-migration-queuing`.

The ``_waitForDelete`` is generally for internal testing purposes. To
change the balancer's ``_waitForDelete`` value:

#. Connect to a :binary:`~bin.mongos` instance.

#. Update the ``_waitForDelete`` value in the :data:`~config.settings`
   collection of the :ref:`config database <config-database>`. For
   example:

   .. code-block:: javascript

      use config
      db.settings.update(
         { "_id" : "balancer" },
         { $set : { "_waitForDelete" : true } },
         { upsert : true }
      )

Once set to ``true``, to revert to the default behavior:

#. Connect to a :binary:`~bin.mongos` instance.

#. Update or unset the ``_waitForDelete`` field in the
   :data:`~config.settings` collection of the :ref:`config database
   <config-database>`:

   .. code-block:: javascript

      use config
      db.settings.update(
         { "_id" : "balancer", "_waitForDelete": true },
         { $unset : { "_waitForDelete" : "" } }
      )
