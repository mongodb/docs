.. _stream-aggregation:

======================================================
Supported Aggregation Pipeline Stages
======================================================

.. default-domain:: mongodb

.. meta::
   :keywords: atlas stream processing, $source aggregation pipeline stage, $tumblingWindow aggregation pipeline stage, $hoppingWindow aggregation pipeline stage, $merge aggregation pipeline stage, $emit aggregation pipeline stage, $validate aggregation pipeline stage, 
   :description: Learn how to use the extensions to the aggregation pipeline syntax provided by Atlas Stream Processing.

.. facet::
   :name: genre
   :values: reference

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

.. include:: includes/atlas-sp/public-preview.rst

{+atlas-sp+} extends the :manual:`aggregation pipeline 
</aggregation>` with stages for processing continuous data streams.
These stages combine with existing aggregation stages built in 
to the default :binary:`mongod <bin.mongod>` process, enabling you to 
perform many of the same operations on continuous data as you can 
perform on data-at-rest.

The following table lists the aggregation pipeline stages unique to
{+atlas-sp+} and those which exist in modified form in {+atlas-sp+}.

.. list-table::
   :header-rows: 1
   :widths: 40 60
   
   * - Aggregation Pipeline Stage
     - Purpose 

   * - :pipeline:`$source`
     - Specifies a streaming data source to consume messages from.

   * - :pipeline:`$validate`
     - Validates the documents of a stream against a user-defined 
       schema.

   * - :ref:`$lookup <stream-agg-pipeline-lookup>`
     - Performs a left outer join to a specified collection to filter
       in documents from the "joined" collection for processing.
     
       This version of the existing :pipeline:`$lookup` stage requires
       that you specify a {+service+} collection in the 
       :ref:`Connection Registry <manage-spi-connection-add>` as the
       value for the ``from`` field.

   * - :pipeline:`$hoppingWindow`
     - Assigns documents from a stream to 
       :ref:`windows <atlas-sp-windows>` with user-defined
       durations and intervals between start times.

   * - :pipeline:`$tumblingWindow`
     - Assigns documents from a stream to non-overlapping, continuous 
       :ref:`windows <atlas-sp-windows>` with user-defined
       durations.

   * - :pipeline:`$emit`
     - Specifies a stream in the connection registry to emit messages 
       to.

   * - :ref:`$merge <stream-agg-pipeline-merge>`
     - A version of the existing :ref:`adf-merge-stage` stage where 
       the value of the ``connectionName`` field must always be the 
       name of a remote collection in the 
       :ref:`Connection Registry <manage-spi-connection-add>`.

.. The following can be uncommented once this stage is supported.
   * - :pipeline:`$validatedSource`
     - Similar to :pipeline:`$validate`, but is bound to a specific
       source, allowing you to define one stage in place of both
       :pipeline:`$source` and :pipeline:`$validate`.

You can also use the following stages supported by all 
``mongod`` processes in your streaming data pipelines:

.. list-table::
   :header-rows: 1
   :widths: 40 60

   * - Aggregation Pipeline Stage
     - Use Conditions

   * - :pipeline:`$addFields`
     - Anywhere

   * - :pipeline:`$match`
     - Anywhere

   * - :pipeline:`$project`
     - Anywhere

   * - :pipeline:`$redact`
     - Anywhere

   * - :pipeline:`$replaceRoot`
     - Anywhere

   * - :pipeline:`$replaceWith`
     - Anywhere

   * - :pipeline:`$set`
     - Anywhere

   * - :pipeline:`$unset`
     - Anywhere

   * - :pipeline:`$unwind`
     - Anywhere

   * - :pipeline:`$group`
     - Only within :pipeline:`$hoppingWindow` or
       :pipeline:`$tumblingWindow` stages.

   * - :pipeline:`$sort`
     - Only within :pipeline:`$hoppingWindow` or
       :pipeline:`$tumblingWindow` stages.

   * - :pipeline:`$limit`
     - Only within :pipeline:`$hoppingWindow` or
       :pipeline:`$tumblingWindow` stages.

   * - :pipeline:`$count`
     - Only within :pipeline:`$hoppingWindow` or
       :pipeline:`$tumblingWindow` stages.

.. _streams-agg-pipeline-source:

``$source``
-----------

Definition
~~~~~~~~~~

The :pipeline:`$source` stage specifies a connection in the 
:ref:`Connection Registry <manage-spi-connection-add>` to stream data
from. The following connection types are supported:

- An {+kafka+} broker
- The change stream for a specific MongoDB collection within a database
- The change stream for an entire MongoDB database

.. note:: 

   You can't use |service| serverless instances as a :pipeline:`$source`.

.. pipeline:: $source

   The ``$source`` pipeline stage has the following prototype form:

   .. code-block:: json

      {
        "$source": {
	        "connectionName": "<registered-connection>",
	        "topic" : "<source-topic>" | "db" : "<source-db>",
          "coll" : ["<source-coll>",...],
          "config": { 
            "autoOffsetReset": "<start-event>",
            "startAfter": <start-token> | "startAtOperationTime": <timestamp>,
            "fullDocument": "<full-doc-condition>",
            "fullDocumentOnly": <boolean>
            "fullDocumentBeforeChange": "<before-change-condition>",
            
          },
          "tsFieldOverride": "<timestamp>" 
          "timeField": { 
            $toDate | $dateFromString: <expression>
          }
        }
      }

Fields
~~~~~~

The ``$source`` stage takes a document with the following fields: 

.. list-table:: 
   :header-rows: 1
   :widths: 16 17 17 60

   * - Field 
     - Type 
     - Necessity 
     - Description

   * - ``coll``
     - string or array of strings
     - Conditional
     - Name of one or more MongoDB collections hosted on the |service|
       instance specified by ``connectionName``. The change stream of
       these collections act as the streaming data source. This field is 
       necessary for and limited to connections to MongoDB collections.

   * - ``config``
     - document 
     - Optional
     - Document containing fields that override various default 
       values.  

   * - ``config.auto_offset_reset`` 
     - string 
     - Conditional
     - Specifies which event in the {+kafka+} source topic to begin 
       ingestion with. ``auto_offset_reset`` takes the following values: 
       
       - ``end``, ``latest``, or ``largest`` - to begin ingestion from
         the latest event in the topic at the time the aggregation is
         initialized.
       - ``earliest``, ``beginning``, or ``smallest`` - to begin
         ingestion from the earliest event in the topic. 
         
       This field only applies to {+kafka+} sources.

       Defaults to ``latest``.

   * - ``config.group_id`` 
     - string
     - Conditional 
     - ID of the kafka consumer group to associate with the stream
       processor. If omitted, {+atlas-sp+} associates the {+spi+} with
       an auto-generated ID in the following format:  

       .. code-block:: sh 
          :copyable: false 

          asp-${streamProcessorId}-consumer

       {+atlas-sp+} commits partition offsets to the {+kafka+} broker for the
       specified consumer group ID after a checkpoint is committed. It
       commits an offset when messages up through that offset are durably
       recorded in a checkpoint. This allows you to track the offset lag
       and progress of the stream processor directly from the Kafka
       broker consumer group metadata. 

       This field applies only to {+kafka+} sources.

   * - ``config.startAfter`` 
     - resumeToken
     - Conditional
     - The change event after which the source begins reporting.
       This takes the form of a :manual:`resume token </changeStreams/#resume-tokens-from-change-events>`.

       You can use only one of either ``config.startAfter`` or ``config.StartAtOperationTime``.

   * - ``config.startAtOperationTime`` 
     - timestamp
     - Conditional
     - The operation time after which the source should begin reporting.

       You can use only one of either ``config.startAfter`` or ``config.StartAtOperationTime``.

   * - ``config.fullDocument``
     - string
     - Conditional
     - Setting that controls whether a change stream source should 
       return a full document, or only the changes when an update 
       occurs. Must be one of the following:

       - ``updateLookup`` - Returns only changes on update.
       - ``required`` - Must return a full document. If a full document
         is unavailable, returns nothing.
       - ``whenAvailable`` - Returns a full document whenever one is 
         available, otherwise returns changes.
      
       If you do not specify a value for fullDocument, it defaults to
       ``updateLookup``.

       This field applies only to connections to MongoDB change streams.
       
       To use this field with a collection change stream, you must
       enable change stream :ref:`Pre- and Post-Images <collMod-change-stream-pre-and-post-images>`
       on that collection.

       To use this field with a database change stream, you must 
       enable change stream pre- and post-images on every collection 
       in that database.

   * - ``config.fullDocumentOnly``
     - boolean
     - Conditional
     - Setting that controls whether a change stream source returns 
       the entire change event document including all metadata, or 
       only the contents of ``fullDocument``. If set to ``true``, the 
       source returns only the contents of ``fullDocument``.

       This field applies only to connections to MongoDB change streams.
       
       To use this field with a collection change stream, you must
       enable change stream :ref:`Pre- and Post-Images <collMod-change-stream-pre-and-post-images>`
       on that collection.

       To use this field with a database change stream, you must 
       enable change stream Pre- and Post-Images on every collection 
       in that database.

   * - ``config.fullDocumentBeforeChange``
     - string
     - Optional
     - Specifies whether a change stream source should include the
       full document in its original "before changes" state
       in the output. Must be one of the following:

       - ``off`` - Omits the ``fullDocumentBeforeChange`` field.
       - ``required`` - Must return a full document in its before
         changes state. If a full document in its before changes state
         is unavailable, the stream processor fails.
       - ``whenAvailable`` - Returns a full document in its before
         changes state whenever one is 
         available, otherwise omits the ``fullDocumentBeforeChange`` field.
      
       If you do not specify a value for ``fullDocumentBeforeChange``,
       it defaults to ``off``.

       This field is limited to connections to MongoDB change streams.
       
       To use this field with a collection change stream, you must
       enable change stream :ref:`Pre- and Post-Images <collMod-change-stream-pre-and-post-images>`
       on that collection.

       To use this field with a database change stream, you must 
       enable change stream Pre- and Post-Images on every collection 
       in that database.

   * - ``connectionName`` 
     - string
     - Required 
     - Label, that identifies the connection in the
       :ref:`Connection Registry <manage-spi-connection-add>`, to 
       ingest data from.

   * - ``db``
     - string
     - Conditional
     - Name of a MongoDB database hosted on the |service| instance
       specified by ``connectionName``. The change stream of this 
       database acts as the streaming data source. This field is
       necessary for and limited to connections to MongoDB databases or
       collections.
  
   * - ``partitionIdleTimeout``
     - document
     - Optional
     - Document specifying the amount of time that a partition is
       allowed to be idle before it is ignored in watermark calculations. This field is limited to connections to {+kafka+} brokers.

   * - ``timeField``
     - document
     - Optional 
     - Document that defines an authoritative timestamp for incoming
       messages.
       
       If you use ``timeField``, you must define it as one of the
       following:

       - a ``$toDate`` expression that takes a source message field as 
         an argument
       - a ``$dateFromString`` expression that takes a source message 
         field as an argument.

       If you do not declare a ``timeField``, {+atlas-sp+} creates a
       timestamp from the message timestamp provided by the source.

   * - ``topic``
     - string
     - Conditional 
     - Name of the {+kafka+} topic to stream messages from. This
       field is necessary for and limited to connections to {+kafka+}
       brokers.

   * - ``tsFieldOverride``
     - string 
     - Optional
     - Name that overrides the name of default timestamp fields
       declared by the source.

       {+atlas-sp+} pipelines internally add a field to incoming 
       messages called ``_ts`` to store checkpointing information.
       Sources of streaming data might also use a field named ``_ts`` 
       to store the timestamps of each message. To prevent a conflict 
       between these fields, use ``tsFieldOverride`` to rename any
       source-provided field named ``_ts`` before additional processing
       takes place.

Behavior
~~~~~~~~

:pipeline:`$source` must be the first stage of any pipeline it appears 
in. You can use only one :pipeline:`$source` stage per pipeline.

.. _stream-agg-pipeline-validate:

``$validate``
-------------

Definition
~~~~~~~~~~

The :pipeline:`$validate` stage checks streaming documents for 
conformity to a schema of expected ranges, values, or datatypes.

.. pipeline:: $validate

   A ``$validate`` pipeline stage has the following prototype form:

   .. code-block:: json

      {
        "$validate": {
          "validator": { <filter> },
          "validationAction" : "discard" | "dlq"
        }  
      }

Fields
~~~~~~

The ``$validate`` stage takes a document with the following fields: 

.. list-table:: 
   :header-rows: 1
   :widths: 10 15 15 70

   * - Field 
     - Type 
     - Necessity 
     - Description

   * - ``validationAction`` 
     - string
     - Optional 
     - Specifies the action to take when a message violates the
       user-defined schema. You can specify one of the following 
       values:

       - ``discard``: Discards the message. If you do not specify a 
         value for ``validationAction``, this is the default behavior.
       - ``dlq``: Logs the violation to the collection defined in your
         Stream Processor configuration and performs a best-effort 
         discard without transactional guarantees.

   * - ``validator``
     - document
     - Required
     - Document of expressions used to validate incoming messages
       against a user-defined schema. You can use all but the
       following :ref:`query operators <query-selectors>` to define 
       validation expressions:

       - ``$near``
       - ``$nearSphere``
       - ``$text``
       - ``$where``

Behavior
~~~~~~~~

You can use ``$validate`` at any point in a pipeline after the 
``$source`` stage, and before the ``$emit`` or ``$merge`` stage.

If you specify either the ``discard`` or ``dlq`` options for the
``validationAction`` field, {+atlas-sp+} logs messages which fail
validation in the following format:

.. code-block:: json

   {
    "t": <datetime>,
    "s": "<severity-level>",
    "c": "streams-<job-name>",
    "ctx": "<processed-pipeline>",
    "msg": "<message-body>",
    "attrs": {
      <result-of-logAttributes-evaluation>
    },
    "tags": <array-of-strings>,
    "truncated": {
      <truncation-description>
    },
    "size": <size-of-entry>
  }

The following table describes the log entry fields:

.. list-table:: 
   :header-rows: 1
   :widths: 15 15 70

   * - Field 
     - Type 
     - Description

   * - ``attrs``
     - document 
     - Document containing the results of evaluating the 
       ``logAttributes`` field in the ``$validate`` definition. The
       result is a list of fields.

   * - ``c``
     - string 
     - Name of the specific stream processing job in which the 
       failure occurred. 

   * - ``ctx`` 
     - string
     - Name of the streaming data pipeline being processed.

   * - ``msg``
     - string
     - Body of the message that failed validation. 
     
.. Add the following back to the above field description when
   $validatedSource is added:
   If the log is generated by a :pipeline:`$validatedSource` stage, it
   includes additional metadata from the source stream.

   * - ``s`` 
     - string
     - Severity level of the validation failure.

   * - ``size`` 
     - integer
     - Original size of the message in bytes. Included only if
       the message was truncated on ingestion.

   * - ``t`` 
     - datetime
     - Timestamp of the validation failure event.

   * - ``tags`` 
     - array
     - Array of strings where each string is a tag on the message.

   * - ``truncated`` 
     - document
     - Document containing information about the truncation of the
       message.

{+atlas-sp+} supports only 
`JSON Schema Draft 4 <https://json-schema.org/specification-links.html#draft-4>`__
or earlier.

Validator Example
`````````````````

The following document shows an example validator expression using 
:manual:`$and </reference/operator/query/and/>` to perform a logical 
AND operation:

.. code-block:: json

  { 
    $validate: {
      validator: {
        $and: [{
          $expr: {
            $ne: [
              "$Racer_Name",
              "Pace Car"
            ]
          }
        },
        {
          $jsonSchema: {
            required: [ "Racer_Num", "Racer_Name", "lap", "Corner_Num", "timestamp" ],
            properties: {
              Racer_Num: {
                bsonType: "int",
                description: "'Racer_Num' is the integer number of the race car and is required"
              },
              Racer_Name: {
                bsonType: "string",
                description: "'Racer_Name' must be a string and is required"
              },
              lap: {
                bsonType: "int",
                minimum: 1,
                description: "'lap' must be a int and is required"
              },
              Corner_Num: {
                bsonType: "int",
                minimum: 1,
                maximum: 4,
                description: "'Corner_Num' must be a int between 1 and 4 and is required"
              },
              timestamp: {
                bsonType: "string",
                pattern: "^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{6}$",
                description: "'timestamp' must be a string matching iso date pattern and is required"
              }
            }
          }
        }]
      }, validationAction : "dlq"
    }
  }

.. Commit 2de43ac of PR 4592 for DOCSP-30690 contains a description of
   the $validatedSource aggregation stage. At the time of this comment
   (26th of June, 2023) it is not slated for private preview and has
   been removed. Refer to the aforementioned commit for the text rather
   than rewriting the whole section when it's implemented.

.. _stream-agg-pipeline-lookup:

``$lookup``
-----------

Definition
~~~~~~~~~~

The :ref:`$lookup <stream-agg-pipeline-lookup>` performs a left outer
join of the stream of messages from your :pipeline:`$source` to an
{+service+} collection in your :ref:`Connection Registry <manage-spi-connection-add>`.

Depending on your use case, a ``$lookup`` pipeline stage uses one of 
the following three syntaxes:

- :manual:`Equality Match with a Single Join Collection 
  </reference/operator/aggregation/lookup/#equality-match-with-a-single-join-collection>`
- :manual:`Join Conditions and Subqueries on a Joined Collection
  </reference/operator/aggregation/lookup/#join-conditions-and-subqueries-on-a-joined-collection>`
- :manual:`Correlated Subqueries Using Concise Syntax
  </reference/operator/aggregation/lookup/#correlated-subqueries-using-concise-syntax>`

To learn more, see :manual:`$lookup Syntax 
</reference/operator/aggregation/lookup/#syntax>` .

The following prototype form illustrates all available fields:

.. code-block:: json

  {
    "$lookup": {
      "from": {
        "connectionName": "<registered-atlas-connection>",
        "db": "<registered-database-name>",
        "coll": "<atlas-collection-name>"
      },
      "localField": "<field-in-source-messages>",
      "foreignField": "<field-in-from-collection>",
      "let": { 
        <var_1>: <expression>, 
        <var_2>: <expression>, 
        …, 
        <var_n>: <expression> 
      },
      "pipeline": [ 
        <pipeline to run> 
      ],
      "as": "<output-array-field>"
    }  
  } 

Fields
~~~~~~

The ``$lookup`` stage takes a document with the following fields:

.. list-table:: 
   :header-rows: 1
   :widths: 15 15 15 65

   * - Field 
     - Type 
     - Necessity 
     - Description

   * - from
     - document 
     - Required 
     - Document that specifies a collection in an {+service+} database
       to join to messages from your :pipeline:`$source`. You must 
       specify a collection from your Connection Registry, and you must
       identify it with the following fields:

       - ``connectionName``: Name of the connection in your Connection 
         Registry.
       - ``db``: Name of the {+service+} database that contains the 
         collection you want to join.
       - ``coll``: Name of the collection you want to join.

   * - localField
     - string
     - Conditional
     - Field from your ``$source`` messages to join on.

       This field is part of the following syntaxes:

       - :manual:`Equality Match with a Single Join Collection 
         </reference/operator/aggregation/lookup/#equality-match-with-a-single-join-collection>`
       - :manual:`Correlated Subqueries Using Concise Syntax
         </reference/operator/aggregation/lookup/#correlated-subqueries-using-concise-syntax>`

   * - foreignField
     - string
     - Conditional
     - Field from documents in the ``from`` collection to join on.

       This field is part of the following syntaxes:

       - :manual:`Equality Match with a Single Join Collection 
         </reference/operator/aggregation/lookup/#equality-match-with-a-single-join-collection>`
       - :manual:`Correlated Subqueries Using Concise Syntax
         </reference/operator/aggregation/lookup/#correlated-subqueries-using-concise-syntax>`

   * - let
     - document
     - Conditional
     - Specifies the variables to use in the 
       :ref:`pipeline <lookup-join-pipeline>` stages. To learn more,
       see :ref:`let <lookup-join-let>`.

       This field is part of the following syntaxes:

       - :manual:`Join Conditions and Subqueries on a Joined Collection
         </reference/operator/aggregation/lookup/#join-conditions-and-subqueries-on-a-joined-collection>`
       - :manual:`Correlated Subqueries Using Concise Syntax
         </reference/operator/aggregation/lookup/#correlated-subqueries-using-concise-syntax>`

   * - pipeline
     - document
     - Conditional
     - Specifies the ``pipeline`` to run on the joined collection. To
       learn more, see :ref:`pipeline <lookup-join-pipeline>`.

       This field is part of the following syntaxes:

       - :manual:`Join Conditions and Subqueries on a Joined Collection
         </reference/operator/aggregation/lookup/#join-conditions-and-subqueries-on-a-joined-collection>`
       - :manual:`Correlated Subqueries Using Concise Syntax
         </reference/operator/aggregation/lookup/#correlated-subqueries-using-concise-syntax>`

   * - as
     - string
     - Required
     - Name of the new array field to add to the input documents. This
       new array field contains the matching documents from the 
       ``from`` collection. If the specified name already exists as a 
       field in the input document, that field is overwritten.

Behavior
~~~~~~~~

The {+atlas-sp+} version of :ref:`$lookup <stream-agg-pipeline-lookup>` 
performs a left outer join of messages from your ``$source`` and the 
documents in a specified {+service+} collection. This version behaves 
similarly to the :pipeline:`$lookup` stage available in a standard 
MongoDB database. However, this version requires that you specify an 
{+service+} collection from your 
:ref:`Connection Registry <manage-spi-connection-add>` as the value for
the ``from`` field.

The :ref:`pipeline <lookup-join-pipeline>` can contain a nested 
:pipeline:`$lookup` stage. If you include a nested :pipeline:`$lookup` 
stage in your pipeline, you must use the standard ``from`` syntax to
specify a collection in the same remote {+service+} connection as the
outer :pipeline:`$lookup` stage.

.. example::

   .. code-block:: json

      $lookup : {
        from: {connectionName: "dbsrv1", db: "db1", coll: "coll1"},
          …,
        pipeline: [
          …,
          {
            $lookup: {
              from: "coll2",
              …,
            }
          },
          …,
        ]
      }

If your pipeline has both :pipeline:`$lookup` and :pipeline:`$merge`
on the same collection, {+atlas-sp+} results might vary if you
try to maintain an incremental view. {+atlas-sp+} processes
multiple source messages simultaneously and then merges them all
together. If multiple messages have the same ID, which both
:pipeline:`$lookup` and :pipeline:`$merge` use, {+atlas-sp+} might 
return results that haven't yet materialized. 

.. example:: 

   Consider the following input stream:

   .. code-block:: shell
      :copyable: false 

      { _id: 1, count: 2 }
      { _id: 1, count: 3 }

   Suppose your query contains the following inside the pipeline: 

   .. code-block:: js
      :copyable: false 

      {
        ...,
        pipeline: [
          { $lookup on _id == foreignDoc._id from collection A }
          { $project: { _id: 1, count: $count + $foreignDoc.count } }
          { $merge: { into collection A } }
        ]
      }

   If you are trying to maintain an incremental view, you might expect a
   result similar to the following:

   .. code-block:: shell
      :copyable: false 

      { _id: 1, count: 5 } 

   However, {+atlas-sp+} might return a count of ``5`` or ``3``
   depending on whether {+atlas-sp+} has processed the documents.

For more information, see :pipeline:`$lookup`.

.. warning::

   Using ``$lookup`` to enrich a stream can reduce stream processing
   speed.

.. _stream-agg-pipeline-hoppingWindow:

``$hoppingWindow``
------------------

Definition
~~~~~~~~~~

The :pipeline:`$hoppingWindow` stage specifies a 
:ref:`hopping window <hopping-windows>` for aggregation of data.
{+atlas-sp+} windows are stateful, can be recovered if interrupted,
and have mechanisms for processing late-arriving data. You must apply
all other aggregation queries to your streaming data within this window
stage.

.. pipeline:: $hoppingWindow

   A ``$hoppingWindow`` pipeline stage has the following prototype 
   form:

   .. code-block:: json

      {
        "$hoppingWindow": {
          "idleTimeout": {
            "size": <number>,
            "unit": <string>
          },
          "interval": { 
            "size": <number>,
            "unit": <string>
          },
          "hopSize": {
            "size": <number>,
            "unit": <string>
          },
          "allowedLateness": {
            size: <int> 
            unit: "<unit-of-time>" 
          },
          "pipeline" : [ 
            <aggregation-stage-array> 
          ]
        }  
      }

Fields
~~~~~~

The ``$hoppingWindow`` stage takes a document with the following 
fields: 

.. list-table:: 
   :header-rows: 1
   :widths: 10 15 15 70

   * - Field 
     - Type 
     - Necessity 
     - Description

   * - ``allowedLateness``
     - document 
     - Optional
     - Document that specifies how long to keep 
       :ref:`windows <atlas-sp-windows>` generated from the 
       source open to accept late-arriving data after processing
       documents for window end time. If omitted, defaults to 3
       seconds. 

   * - ``hopSize``
     - document 
     - Required
     - Document that specifies the length of the 
       :ref:`hop <hopping-windows>` between window start times as a 
       combination of a ``size`` and a ``unit`` of time where: 
       
       - The value of ``size`` must be a non-zero positive integer. 
       - The value of ``unit`` can be one of the following:

         - ``"ms"`` (millisecond)
         - ``"second"``
         - ``"minute"``
         - ``"hour"``
         - ``"day"``
       
       For example, a ``size`` of ``10`` and a ``unit`` of ``second``
       defines a 10-second hop between window start times.

   * - ``idleTimeout`` 
     - document 
     - Optional 
     - Document specifying how long to wait before closing windows if 
       ``$source`` is idle. Define this setting as a combination of a
       ``size`` and a ``unit`` of time where: 

       - The value of ``size`` must be a non-zero positive integer. 
       - The value of ``unit`` can be one of the following:

         - ``"ms"`` (millisecond)
         - ``"second"``
         - ``"minute"``
         - ``"hour"``
         - ``"day"``

       If you set ``idleTimeout``, {+atlas-sp+} closes open windows only
       if ``$source`` is idle for longer than the greater of either the 
       remaining window time or ``idleTimeout`` time. The idle timer 
       starts as soon as ``$source`` goes idle. 
       
       For example, consider a 12:00 pm to 1:00 pm window and
       ``idleTimeout`` time of 2 hours. If the last event is at 12:02 pm 
       after which ``$source`` goes idle, the remaining window time is
       58 minutes. {+atlas-sp+} closes the window after 2 hours of
       idleness at 2:02 pm, which is longer than the remaining window
       time and the ``idleTimeout`` time. If the ``idleTimeout`` time is
       set to only 10 minutes, {+atlas-sp+} closes the window after 58
       minutes of idleness at 1:00 pm, which is longer than
       ``idleTimeout`` time and the remaining window time.  
 
   * - ``interval``
     - document 
     - Required
     - Document specifying the interval of a 
       :ref:`hopping window <hopping-windows>` as a combination of
       a size and a unit of time where: 
       
       - The value of ``size`` must be a non-zero positive integer. 
       - The value of ``unit`` can be one of the following:

         - ``"ms"`` (millisecond)
         - ``"second"``
         - ``"minute"``
         - ``"hour"``
         - ``"day"``

       For example, a ``size`` of ``20`` and a ``unit`` of ``second``
       sets each window to remain open for 20 seconds.

   * - ``pipeline`` 
     - array
     - Required 
     - Nested aggregation pipeline evaluated against the messages
       within the window.

Behavior
~~~~~~~~

{+atlas-sp+} supports only one :ref:`window <atlas-sp-windows>` 
stage per pipeline.

Support for certain aggregation stages might be limited or unavailable
within windows. To learn more, see :ref:`atlas-sp-aggregation-support`.

In the event of a service interruption, you can resume the internal
pipeline of a window from its state at the point of interruption. To
learn more, see :ref:`Checkpoints <atlas-sp-checkpointing>`.

.. _stream-agg-pipeline-tumblingWindow:

``$tumblingWindow``
-------------------

Definition
~~~~~~~~~~

The :pipeline:`$tumblingWindow` stage specifies a 
:ref:`tumbling window <tumbling-windows>` for aggregation of data.
{+atlas-sp+} windows are stateful, can be recovered if interrupted,
and have mechanisms for processing late-arriving data. You must apply
all other aggregation queries to your streaming data within this window
stage.

.. pipeline:: $tumblingWindow

   A ``$tumblingWindow`` pipeline stage has the following prototype 
   form:

   .. code-block:: json

      {
        "$tumblingWindow": {
          "idleTimeout": {
            "size": <number>,
            "unit": <string>
          },
          "interval": { 
            "size": <number>,
            "unit": <string>
          },
          "offset": {
            "offsetFromUtc": <number>, 
            "unit": <string>
          },
          "allowedLateness": {
            size: <int> 
            unit: "<unit-of-time>" 
          },
          "pipeline" : [ 
            <aggregation-stage-array> 
          ]
        }  
      }

Fields
~~~~~~

The ``$tumblingWindow`` stage takes a document with the following 
fields: 

.. list-table:: 
   :header-rows: 1
   :widths: 10 15 15 70

   * - Field 
     - Type 
     - Necessity 
     - Description

   * - ``allowedLateness``
     - document 
     - Optional
     - Document that specifies how long to keep 
       :ref:`windows <atlas-sp-windows>` generated from the
       source open to accept late-arriving data after processing
       documents for window end time. If omitted, defaults to 3
       seconds.
     
   * - ``idleTimeout`` 
     - document     
     - Optional 
     - Document specifying how long to wait before closing windows if 
       ``$source`` is idle. Define this setting as a combination of a
       ``size`` and a ``unit`` of time where:  

       - The value of ``size`` must be a non-zero positive integer. 
       - The value of ``unit`` can be one of the following:

         - ``"ms"`` (millisecond)
         - ``"second"``
         - ``"minute"``
         - ``"hour"``
         - ``"day"``

       If you set ``idleTimeout``, {+atlas-sp+} closes open windows only
       if ``$source`` is idle for longer than the greater of either the 
       remaining window time or ``idleTimeout`` time. The idle timer 
       starts as soon as ``$source`` goes idle. 
       
       For example, consider a 12:00 pm to 1:00 pm window and
       ``idleTimeout`` time of 2 hours. If the last event is at 12:02 pm 
       after which ``$source`` goes idle, the remaining window time is
       58 minutes. {+atlas-sp+} closes the window after 2 hours of
       idleness at 2:02 pm, which is longer than the remaining window
       time and the ``idleTimeout`` time. If the ``idleTimeout`` time is
       set to only 10 minutes, {+atlas-sp+} closes the window after 58
       minutes of idleness at 1:00 pm, which is longer than
       ``idleTimeout`` time and the remaining window time.

   * - ``interval``
     - document 
     - Required
     - Document specifying the interval of a 
       :ref:`hopping window <hopping-windows>` as a combination of
       a size and a unit of time where: 
       
       - The value of ``size`` must be a non-zero positive integer. 
       - The value of ``unit`` must be one of the following:

         - ``"ms"`` (millisecond)
         - ``"second"``
         - ``"minute"``
         - ``"hour"``
         - ``"day"``

       For example, a ``size`` of ``20`` and a ``unit`` of ``second``
       sets each window to remain open for 20 seconds.

   * - ``offset``
     - document
     - Optional
     - Document specifying a time offset for window boundaries relative 
       to UTC. The document is a combination of the size field 
       ``offsetFromUtc`` and a unit of time where:
       
       - The value of ``offsetFromUtc`` must be a non-zero positive integer. 
       - The value of ``unit`` must be one of the following:

         - ``"ms"`` (millisecond)
         - ``"second"``
         - ``"minute"``
         - ``"hour"``
       
       For example, an ``offsetFromUtc`` of ``8`` and a ``unit`` of 
       ``hour`` generates boundaries that are shifted eight hours
       ahead of UTC. If you do not specify an offset, the window 
       boundaries align with UTC.

   * - ``pipeline`` 
     - array
     - Required 
     - Nested aggregation pipeline evaluated against the messages
       within the window.

Behavior
~~~~~~~~

{+atlas-sp+} supports only one :ref:`window <atlas-sp-windows>` stage 
per pipeline.

Support for certain aggregation stages might be limited or unavailable
within windows. To learn more, see :ref:`atlas-sp-aggregation-support`.

In the event of a service interruption, you can resume the internal
pipeline of a window from its state at the point of interruption. To
learn more, see :ref:`Checkpoints <atlas-sp-checkpointing>`.

.. _stream-agg-pipeline-emit:

``$emit``
-----------

Definition
~~~~~~~~~~

The :pipeline:`$emit` stage specifies a connection in the 
:ref:`Connection Registry <manage-spi-connection-add>` to emit messages
to. The connection must be an {+kafka+} broker.

.. pipeline:: $emit

   An ``$emit`` pipeline stage has the following prototype form:

   .. code-block:: json

      {
        "$emit": {
          "connectionName": "<registered-connection>",
          "topic" : "<target-topic>" | <expression>,
          }
        }  
      }

Fields
~~~~~~

The ``$emit`` stage takes a document with the following fields: 

.. list-table:: 
   :header-rows: 1
   :widths: 10 15 15 70

   * - Field 
     - Type 
     - Necessity 
     - Description

   * - ``connectionName`` 
     - string
     - Required 
     - The logical name, as it appears in the 
       :ref:`Connection Registry <manage-spi-connection-add>`, of the
       connection to ingest data from.

   * - ``topic``
     - string | expression
     - Conditional 
     - The name of the {+kafka+} topic to emit messages to.

Behavior
~~~~~~~~

``$emit`` must be the last stage of any pipeline it appears in. You can
use only one ``$emit`` stage per pipeline.

You can use a :manual:`dynamic expression </reference/operator/aggregation/#expression-operators>`
as the value of the ``topic`` field to enable your stream processor to 
write to different target {+kafka+} topics on a message-by-message 
basis.

.. example::

   You have a stream of transaction events that generates messages of 
   the following form:

   .. code-block:: json

      { 
        "customer": "Very Important Industries",
        "customerStatus": "VIP",
        "tenantId": 1,
        "transactionType": "subscription"
      }

      { 
        "customer": "N. E. Buddy",
        "customerStatus": "employee",
        "tenantId": 5,
        "transactionType": "requisition"
      }

      { 
        "customer": "Khan Traktor",
        "customerStatus": "contractor",
        "tenantId": 11,
        "transactionType": "billableHours"
      }

   To sort each of these into a distinct {+kafka+} topic, you can write
   the following ``$emit`` stage:

   .. code-block:: json

      $emit: {
        connectionName: "kafka1",
        topic: "$customerStatus"
      }

   This ``$emit`` stage:

   - Writes the ``Very Important Industries`` message to a topic named 
     ``VIP``.
   - Writes the ``N. E. Buddy`` message to a topic named ``employee``.
   - Writes the ``Khan Traktor`` message to a topic named 
     ``contractor``.

You can use only dynamic expressions that evaluate to strings. For 
more information on dynamic expressions, see 
:manual:`expression operators
</reference/operator/aggregation/#expression-operators>`. 

If you specify a topic that doesn't already exist, {+kafka+}
automatically creates the topic when it receives the first message
that targets it.

If you specify a topic with a dynamic expression, but 
{+atlas-sp+} cannot evaluate the expression for a given message, 
{+atlas-sp+} skips that message and continues processing subsequent 
messages.


.. _stream-agg-pipeline-merge:

``$merge``
-----------

Definition
~~~~~~~~~~

The :ref:`$merge <stream-agg-pipeline-merge>` stage specifies a 
connection in the :ref:`Connection Registry <manage-spi-connection-add>` 
to write messages to. The connection must be an {+service+} connection.

A ``$merge`` pipeline stage has the following prototype form:

.. code-block:: json

  {
    "$merge": {
      "into": {
        "connectionName": "<registered-atlas-connection>",
        "db": "<registered-database-name>" | <expression>,
        "coll": "<atlas-collection-name>" | <expression>
      },
      "on": "<identifier field>" | [ "<identifier field1>", ...],
      "whenMatched": "replace | keepExisting | merge",
      "whenNotMatched": "insert | discard"
    }  
  }

Fields
~~~~~~

The {+atlas-sp+} version of :ref:`$merge <stream-agg-pipeline-merge>` 
uses most of the same fields as the {+adf+} version. However, because 
{+atlas-sp+} only supports merging into an {+service+} connection, the 
syntax of ``into`` is simplified. For more information, see 
:ref:`this description <adf-merge-fields>` of the {+adf+} ``$merge``
fields.

Behavior
~~~~~~~~

``$merge`` must be the last stage of any pipeline it appears in. You can
use only one ``$merge`` stage per pipeline.

The ``on`` field has special requirements for ``$merge`` against
sharded collections. To learn more, see 
:manual:`$merge syntax </reference/operator/aggregation/merge/#merge-on>`.

You can use a :manual:`dynamic expression 
</reference/operator/aggregation/#expression-operators>` as
the value of the following fields: 

- ``into.db``
- ``into.coll``

This enables your stream processor to write messages to different 
target {+service+} collections on a message-by-message basis.

.. example::

   You have a stream of transaction events that generates messages of 
   the following form:

   .. code-block:: json

      { 
        "customer": "Very Important Industries",
        "customerStatus": "VIP",
        "tenantId": 1,
        "transactionType": "subscription"
      }

      { 
        "customer": "N. E. Buddy",
        "customerStatus": "employee",
        "tenantId": 5,
        "transactionType": "requisition"
      }

      { 
        "customer": "Khan Traktor",
        "customerStatus": "contractor",
        "tenantId": 11,
        "transactionType": "billableHours"
      }

   To sort each of these into a distinct {+service+} database and 
   collection, you can write the following ``$merge`` stage:

   .. code-block:: json

      $merge: {
        into: {
          connectionName: "db1",
          db: "$customerStatus",
          coll: "$transactionType"
      }

   This ``$merge`` stage:

   - Writes the ``Very Important Industries`` message to a {+service+} 
     collection named ``VIP.subscription``. 
   - Writes the ``N. E. Buddy`` message to a {+service+} 
     collection named ``employee.requisition``. 
   - Writes the ``Khan Traktor`` message to a {+service+} 
     collection named ``contractor.billableHours``. 

You can only use dynamic expressions that evaluate to strings. For more 
information on dynamic expressions, see :manual:`expression operators 
</reference/operator/aggregation/#expression-operators>`.

If you specify a database or collection with a dynamic expression, but 
{+atlas-sp+} cannot evaluate the expression for a given message, 
{+atlas-sp+} skips that message and continues processing subsequent 
messages.
