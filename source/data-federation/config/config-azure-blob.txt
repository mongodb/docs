.. _adf-configuration-file-azure:

==================
Azure Blob Storage  
==================

.. meta::
   :description: Configure Azure Blob Storage containers as federated database instance stores in Atlas Data Federation to run queries on your data.

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

.. _aws-configuration-file-overview:

{+adf+} supports |azure| Blob Storage containers as {+fdi+} stores. You
must define mappings in your {+fdi+} to your {+az-bs+} containers to run queries
against your data. 

.. note:: 

   While we refer to blobs as files and delimiter-separated prefixes as
   directories in this page, these blob storage services are not actually
   file systems and don't have the same behaviors in all cases as files
   on a hard drive.

Example Configuration for {+az-bs+} Data Store
-------------------------------------------------------

.. example::

   Consider {+az-bs+} container ``datacenter-alpha`` containing data 
   collected from a datacenter:

   .. code-block:: none
      :copyable: false

      |--metrics
        |--hardware

   The ``/metrics/hardware`` path stores JSON files with metrics 
   derived from the datacenter hardware, where each filename is 
   the UNIX timestamp in milliseconds of the 24 hour period 
   covered by that file:

   .. code-block:: none
      :copyable: false

      /hardware/1564671291998.json

   The following configuration:

   - Defines a {+fdi+} store on the ``datacenter-alpha`` {+az-bs+}
     container in the ``eastus2`` |azure| region. The {+fdi+} store is
     specifically restricted to include only data files in the
     ``metrics`` directory path. 

   - Maps files from the ``hardware`` directory to a MongoDB database 
     ``datacenter-alpha-metrics`` and collection ``hardware``. The 
     configuration mapping includes parsing logic for capturing the 
     timestamp implied in the filename.

   .. code-block:: json

      {
        "stores" : [
          {
            "name" : "datacenter",
            "provider" : "azure",
            "region" : "eastus2",
            "containerName" : "datacenter-alpha",
            "serviceURL" : "https://mystorageaccount.blob.core.windows.net/"
          }
        ],
        "databases" : [ 
          {
            "name" : "datacenter-alpha-metrics", 
            "collections" : [
              {
                "name" : "hardware",
                "dataSources" : [
                  {
                    "storeName" : "datacenter",
                    "path" : "/hardware/{date date}"
                  }
                ]
              }
            ]
          }
        ]
      }

   {+adf+} parses the {+az-bs+} container ``datacenter-alpha`` and
   processes all files under ``/metrics/hardware/``. The ``collections``
   uses the :ref:`path parsing syntax <adf-path-syntax>` to map the
   filename to the ``date`` field, which is an ISO-8601 date, in each
   document. If a matching ``date`` field does not exist in a document,
   {+adf+} adds it.

   Users connected to the {+fdi+} can use the MongoDB Query Language 
   and supported aggregations to analyze data in the {+az-bs+} container   
   through the ``datacenter-alpha-metrics.hardware`` collection.

.. _adf-azure-configuration-format:

Configuration Format
--------------------

The {+fdi+} configuration has the following format:

.. literalinclude:: /includes/data-federation/azure-config-format.json
   :language: json
   :linenos:


:ref:`adf-azure-stores-reference`
  The ``stores`` object defines each data store associated with the 
  {+fdi+}. The {+fdi+} store captures files in an |aws| |s3| bucket or   
  {+az-bs+} container, documents in an |service| {+cluster+}, or files
  stored at publicly accessible |url|\s. {+df+} can only access data
  stores defined in the ``stores`` object. 

:ref:`adf-azure-databases-reference`
  The ``databases`` object defines the mapping between each
  {+fdi+} store defined in ``stores`` and MongoDB collections 
  in the databases. 

.. _adf-azure-stores-reference:

``stores``
~~~~~~~~~~

.. literalinclude:: /includes/data-federation/azure-stores-config-format.json
   :language: json
   :linenos:

.. datalakeconf-azure:: stores

   Array of objects where each object represents a data store to 
   associate with the {+fdi+}. The {+fdi+} store captures: 
   
   - Files in an {+az-bs+} container
   - Documents in an |service| cluster
   - Files stored at publicly accessible |url|\s. 
   
   {+adf+} can only access data stores 
   defined in the ``stores`` object.

.. datalakeconf-azure:: stores.[n].name

   Name of the {+fdi+} store. The
   :datalakeconf-azure:`databases.[n].collections.[n].dataSources.[n].storeName` 
   field references this value as part of mapping configuration.

.. datalakeconf-azure:: stores.[n].provider

   Defines where the data is stored. Value must be ``azure`` for an
   {+az-bs+} container. 

.. datalakeconf-azure:: stores.[n].region

   Name of the |azure| region in which the data is stored. 

.. datalakeconf-azure:: stores.[n].serviceURL

   URL of the {+az-bs+} account that contains your blob containers. The
   ``serviceURL`` must be in the following format:

   .. code-block:: shell 
      :copyable: false 

      https://<storage-account-name>.blob.core.windows.net/

   where ``storage-account-name`` is the name of your {+az-bs+} account. 

.. datalakeconf-azure:: stores.[n].containerName

   Name of the {+az-bs+} container that contains the files. 

.. datalakeconf-azure:: stores.[n].prefix

   *Optional.* Prefix {+adf+} applies when searching for files in the 
   {+az-bs+}. 

   For example, consider an an {+az-bs+} container ``metrics`` with
   the following structure:

   .. code-block:: text
      :copyable: false

      metrics
        |--hardware
        |--software
          |--computed

   The {+fdi+} store prepends the value of ``prefix`` to the
   :datalakeconf-azure:`databases.[n].collections.[n].dataSources.[n].path` 
   to create the full path for files to ingest. Setting the ``prefix`` 
   to ``/software`` restricts any :datalakeconf-azure:`databases` objects 
   using the {+fdi+} store to only subpaths ``/software``.

   If omitted, {+adf+} searches all files from the root of the
   {+az-bs+} container. 

.. datalakeconf-azure:: stores.[n].delimiter
   
   *Optional.* The delimiter that separates 
   :datalakeconf-azure:`databases.[n].collections.[n].dataSources.[n].path` 
   segments in the {+fdi+} store. {+df+} uses the delimiter to 
   efficiently traverse {+az-bs+} containers with a hierarchical 
   directory structure. 
   
   If omitted, defaults to ``"/"``.

.. datalakeconf-azure:: stores.[n].public 

   *Optional.* Specifies whether the {+az-bs+} container is public. 

   If set to ``true``, {+adf+} doesn't use the configured |azure|
   Service Principal to access your {+az-bs+}. If set to ``false``, the
   configured Service Principal must include permissions to access the
   blob container, even if that blob container is public.

   If omitted, defaults to ``false``.

.. _adf-azure-databases-reference:

``databases``
~~~~~~~~~~~~~

.. literalinclude:: /includes/data-federation/azure-databases-config-format.json
   :language: json
   :linenos:

.. datalakeconf-azure:: databases

   Array of objects where each object represents a database, its 
   collections, and, optionally, any :manual:`views </core/views/>` on 
   the collections. Each database can have multiple ``collections`` and 
   ``views`` objects.

.. datalakeconf-azure:: databases.[n].name

   Name of the database to which {+adf+} maps the data contained in the 
   data store.

.. datalakeconf-azure:: databases.[n].collections

   Array of objects where each object represents a collection and data 
   sources that map to a :datalakeconf-azure:`stores` {+fdi+} store. 

.. datalakeconf-azure:: databases.[n].collections.[n].name

   Name of the collection to which {+adf+} maps the data contained in 
   each :datalakeconf-azure:`databases.[n].collections.[n].dataSources.[n].storeName`. 
   Each object in the array represents the mapping between the 
   collection and an object in the :datalakeconf-azure:`stores` array. 

   You can generate collection names dynamically from file paths by 
   specifying ``*`` for the collection name and the 
   ``collectionName()`` function in the 
   :datalakeconf-azure:`~databases.[n].collections.[n].dataSources.[n].path`
   field.

.. datalakeconf-azure:: databases.[n].collections.[n].dataSources 

   Array of objects where each object represents a 
   :datalakeconf-azure:`stores` {+fdi+} store to map with the 
   collection.

.. datalakeconf-azure:: databases.[n].collections.[n].dataSources.[n].storeName

   Name of a {+fdi+} store to map to the ``<collection>``. 
   Must match the :datalakeconf-azure:`~stores.[n].name` of an object in 
   the :datalakeconf-azure:`stores` array. 

.. datalakeconf-azure:: databases.[n].collections.[n].dataSources.[n].path


   Controls how {+adf+} searches for and parses files in the
   :datalakeconf-azure:`~databases.[n].collections.[n].dataSources.[n].storeName` 
   before mapping them to the ``<collection>``. {+adf+} prepends the 
   :datalakeconf-azure:`stores.[n].prefix` to the ``path`` to build the 
   full path to search within. Specify ``/`` to capture all files and 
   directories from the ``prefix`` path.

   For example, consider an {+az-bs+} container ``metrics`` with the
   following structure:

   .. code-block:: text
      :copyable: false

      metrics
      |--hardware
      |--software
        |--computed

   A ``path`` of ``/`` directs {+adf+} to search all files and directories
   in the ``metrics`` bucket.

   A ``path`` of ``/hardware`` directs {+adf+} to search only that path 
   for files to ingest.

   If the :datalakeconf-azure:`~stores.[n].prefix` is ``software``, {+adf+} 
   searches for files only in the path ``/software/computed``.

   Appending the ``*`` wildcard character to the path directs {+adf+} 
   to include all files and directories from that point in the path. For 
   example, ``/software/computed*`` would match files like 
   ``/software/computed-detailed``, ``/software/computedArchive``, and 
   ``/software/computed/errors``.

   :datalakeconf-azure:`~databases.[n].collections.[n].dataSources.[n].path`
   supports additional syntax for parsing filenames, including:

   - Generating document fields from filenames.
   - Using regular expressions to control field generation.
   - Setting boundaries for bucketing filenames by timestamp.
   
   See :ref:`adf-path-syntax` for more information.

   .. include:: /includes/data-federation/fact-path-delimiter.rst

.. datalakeconf-azure:: databases.[n].collections.[n].dataSources.[n].defaultFormat
      
   .. include:: /includes/extracts/param-default-format.rst

   .. seealso:: 

      :ref:`adf-data-formats`

.. datalakeconf-azure:: databases.[n].collections.[n].dataSources.[n].provenanceFieldName

   Name for the field that includes the provenance of the documents in
   the results. If you specify this setting in the storage
   configuration, {+adf+} returns the following fields for each document
   in the result:  

   .. list-table:: 
      :widths: 20 80
      :header-rows: 1

      * - Field Name 
        - Description
         
      * - ``provider`` 
        - Provider (:datalakeconf-azure:`stores.[n].provider`) in the
          {+fdi+} storage configuration.

      * - ``region``
        - |azure| region (:datalakeconf-azure:`stores.[n].region`).

      * - ``serviceURL``
        - |url| of the {+az-bs+} account that contains your blob
          containers (:datalakeconf-azure:`stores.[n].serviceURL`). 
         
      * - ``containerName`` 
        - Name of the {+az-bs+} container (:datalakeconf-azure:`stores.[n].containerName`)
         
      * - ``key`` 
        - Path
          (:datalakeconf-azure:`databases.[n].collections.[n].dataSources.[n].path`) 
          to the file.

   You can't configure this setting using the Visual Editor in the 
   |service| UI.

.. datalakeconf-azure:: databases.[n].collections.[n].dataSources.[n].omitAttributes

   *Optional.* Flag that specifies whether to omit the attributes (key and value
   pairs) that {+adf+} adds to the collection. You can specify one of the
   following values: 

   - ``false`` - to add the attributes 
   - ``true`` - to omit the attributes

   If omitted, defaults to ``false`` and {+adf+} adds the attributes. 

   .. example:: 

      Consider a file named ``/employees/949-555-0195.json`` for which
      you configure the :datalakeconf-aws:`~databases.[n].collections.[n].dataSources.[n].path`
      ``/employees/{phone string}``. {+adf+} adds the attribute ``phone:
      949-555-0195`` to the document if you set ``omitAttributes`` to
      ``false``. If you set ``omitAttributes`` to ``true``, {+adf+}
      doesn't add the attribute to the document in the virtual collection.

.. datalakeconf-azure:: databases.[n].maxWildcardCollections 

   .. include:: /includes/extracts/param-max-wildcard-collections.rst 

.. datalakeconf-azure:: databases.[n].views 

   Array of objects where each object represents an 
   :manual:`aggregation pipeline </core/aggregation-pipeline/#id1>` on 
   a collection. To learn more about views, see :manual:`Views 
   </core/views/>`.

.. datalakeconf-azure:: databases.[n].views.[n].name 

   Label that identifies the view. 

.. datalakeconf-azure:: databases.[n].views.[n].source 

   Name of the source collection for the view. If you want to create a
   view with a :ref:`adf-sql-stage` stage, you must omit this field
   as the SQL statement will specify the source collection.

.. datalakeconf-azure:: databases.[n].views.[n].pipeline 
 
   :manual:`Aggregation pipeline stage(s) 
   </core/aggregation-pipeline/#id1>` to apply to the 
   :datalakeconf-azure:`~databases.[n].views.[n].source` collection. You 
   can also create views using the :ref:`adf-sql-stage` stage.

.. seealso::

   - `Tutorial: Federated Queries and $out to S3 
     <https://www.mongodb.com/developer/products/atlas/atlas-data-lake-federated-queries-out-aws-s3/>`__

.. toctree::
   :titlesonly:
   :hidden:

   Deploy </data-federation/deployment/deploy-azure>
