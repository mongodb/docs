# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008-2016
# This file is distributed under the same license as the mongodb-manual
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: mongodb-manual 3.4\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-03-19 11:30-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../source/core/sharding-balancer-administration.txt:6
msgid "Sharded Cluster Balancer"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:17
msgid ""
"The MongoDB balancer is a background process that monitors the number of "
":term:`chunks <chunk>` on each :term:`shard`. When the number of chunks "
"on a given shard reaches specific :ref:`migration thresholds <sharding-"
"migration-thresholds>`, the balancer attempts to automatically migrate "
"chunks between shards and reach an equal number of chunks per shard."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:23
msgid ""
"The balancing procedure for :term:`sharded clusters <sharded cluster>` is"
" entirely transparent to the user and application layer, though there may"
" be some performance impact while the procedure takes place."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:29
msgid ""
"Starting in MongoDB 3.4, the balancer runs on the primary of the config "
"server replica set (CSRS):"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:32
msgid ""
"In version 3.4, when a balancer process is active, the primary of the "
"config server replica set acquires a \"balancer lock\" by modifying a "
"``_id: \"balancer\"`` document in the :data:`~config.locks` collection in"
" the :ref:`config-database`. This \"balancer lock\" is never released."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:38
msgid "Starting in version 3.6, the balancer no longer takes a \"lock\"."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:47
msgid "Cluster Balancer"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:49
msgid ""
"The :term:`balancer` process is responsible for redistributing the chunks"
" of a sharded collection evenly among the shards for every sharded "
"collection. By default, the balancer process is always enabled."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:55
msgid ""
"The balancer runs on the primary of the config server replica set. When a"
" balancer process is active, the primary of the config server replica set"
" acquires a \"balancer lock\" by modifying a document in the ``lock`` "
"collection in the :ref:`config-database`. This \"balancer lock\" is never"
" released."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:61
msgid ""
"To address uneven chunk distribution for a sharded collection, the "
"balancer :doc:`migrates chunks </core/sharding-balancer-administration>` "
"from shards with more chunks to shards with a fewer number of chunks. The"
" balancer migrates the chunks until there is an even distribution of "
"chunks for the collection across the shards. For details about chunk "
"migration, see :ref:`chunk-migration-procedure`."
msgstr ""

#: ../source/includes/fact-archiveMovedChunks.rst:3
msgid ""
"Chunk migrations can have an impact on disk space. Starting in MongoDB "
"2.6, the source shard automatically archives the migrated documents by "
"default. For details, see :ref:`moveChunk-directory`."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:70
msgid ""
"Chunk migrations carry some overhead in terms of bandwidth and workload, "
"both of which can impact database performance. The :term:`balancer` "
"attempts to minimize the impact by:"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:74
msgid ""
"Restricting a shard to at most one migration at any given time; i.e. a "
"shard cannot participate in multiple chunk migrations at the same time. "
"To migrate multiple chunks from a shard, the balancer migrates the chunks"
" one at a time."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:81
msgid ""
"Starting in MongoDB 3.4, MongoDB can perform parallel chunk migrations. "
"Observing the restriction that a shard can participate in at most one "
"migration at a time, for a sharded cluster with *n* shards, MongoDB can "
"perform at most *n/2* (rounded down) simultaneous chunk migrations."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:87
msgid "See also :ref:`chunk-migration-queuing`."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:89
msgid ""
"Starting a balancing round **only** when the difference in the number of "
"chunks between the shard with the greatest number of chunks for a sharded"
" collection and the shard with the lowest number of chunks for that "
"collection reaches the :ref:`migration threshold <sharding-migration-"
"thresholds>`."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:95
msgid ""
"You may disable the balancer temporarily for maintenance. See :ref"
":`sharding-balancing-disable-temporally` for details."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:98
msgid ""
"You can also limit the window during which the balancer runs to prevent "
"it from impacting production traffic. See :ref:`Schedule the Balancing "
"Window <sharding-schedule-balancing-window>` for details."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:104
msgid ""
"The specification of the balancing window is relative to the local time "
"zone of the primary of the config server replica set."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:107
msgid ":doc:`/tutorial/manage-sharded-cluster-balancer`."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:110
msgid "Adding and Removing Shards from the Cluster"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:112
msgid ""
"Adding a shard to a cluster creates an imbalance, since the new shard has"
" no chunks. While MongoDB begins migrating data to the new shard "
"immediately, it can take some time before the cluster balances. See the "
":doc:`/tutorial/add-shards-to-shard-cluster` tutorial for instructions on"
" adding a shard to a cluster."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:118
msgid ""
"Removing a shard from a cluster creates a similar imbalance, since chunks"
" residing on that shard must be redistributed throughout the cluster. "
"While MongoDB begins draining a removed shard immediately, it can take "
"some time before the cluster balances. *Do not* shutdown the servers "
"associated to the removed shard during this process."
msgstr ""

#: ../source/includes/fact-remove-shard-balance-order.rst:1
msgid ""
"When you remove a shard in a cluster with an uneven chunk distribution, "
"the balancer first removes the chunks from the draining shard and then "
"balances the remaining uneven chunk distribution."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:126
msgid ""
"See the :doc:`/tutorial/remove-shards-from-cluster` tutorial for "
"instructions on safely removing a shard from a cluster."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:132
msgid "Chunk Migration Procedure"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:134
msgid "All chunk migrations use the following procedure:"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:136
msgid ""
"The balancer process sends the :dbcommand:`moveChunk` command to the "
"source shard."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:139
msgid ""
"The source starts the move with an internal :dbcommand:`moveChunk` "
"command. During the migration process, operations to the chunk route to "
"the source shard. The source shard is responsible for incoming write "
"operations for the chunk."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:144
msgid ""
"The destination shard builds any indexes required by the source that do "
"not exist on the destination."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:147
msgid ""
"The destination shard begins requesting documents in the chunk and starts"
" receiving copies of the data. See also :ref:`chunk-migration-"
"replication`."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:151
msgid ""
"After receiving the final document in the chunk, the destination shard "
"starts a synchronization process to ensure that it has the changes to the"
" migrated documents that occurred during the migration."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:155
msgid ""
"When fully synchronized, the source shard connects to the :term:`config "
"database` and updates the cluster metadata with the new location for the "
"chunk."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:159
msgid ""
"After the source shard completes the update of the metadata, and once "
"there are no open cursors on the chunk, the source shard deletes its copy"
" of the documents."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:165
msgid ""
"If the balancer needs to perform additional chunk migrations from the "
"source shard, the balancer can start the next chunk migration without "
"waiting for the current migration process to finish this deletion step. "
"See :ref:`chunk-migration-queuing`."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:172
msgid ":ref:`moveChunk-directory`."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:174
msgid ""
"The migration process ensures consistency and maximizes the availability "
"of chunks during balancing."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:180
msgid "Migration Thresholds"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:182
msgid ""
"To minimize the impact of balancing on the cluster, the :term:`balancer` "
"only begins balancing after the distribution of chunks for a sharded "
"collection has reached certain thresholds. The thresholds apply to the "
"difference in number of :term:`chunks <chunk>` between the shard with the"
" most chunks for the collection and the shard with the fewest chunks for "
"that collection. The balancer has the following thresholds:"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:193
msgid "Number of Chunks"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:194
msgid "Migration Threshold"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:196
msgid "Fewer than 20"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:197
msgid "2"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:199
msgid "20-79"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:200
msgid "4"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:202
msgid "80 and greater"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:203
msgid "8"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:205
msgid ""
"The balancer stops running on the target collection when the difference "
"between the number of chunks on any two shards for that collection is "
"*less than two*, or a chunk migration fails."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:213
msgid "Asynchronous Chunk Migration Cleanup"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:215
msgid ""
"To migrate multiple chunks from a shard, the balancer migrates the chunks"
" one at a time. However, the balancer does not wait for the current "
"migration's delete phase to complete before starting the next chunk "
"migration. See :ref:`sharding-chunk-migration` for the chunk migration "
"process and the delete phase."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:221
msgid ""
"This queuing behavior allows shards to unload chunks more quickly in "
"cases of heavily imbalanced cluster, such as when performing initial data"
" loads without pre-splitting and when adding new shards."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:225
msgid ""
"This behavior also affects the :dbcommand:`moveChunk` command, and "
"migration scripts that use the :dbcommand:`moveChunk` command may proceed"
" more quickly."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:229
msgid ""
"In some cases, the delete phases may persist longer. If multiple delete "
"phases are queued but not yet complete, a crash of the replica set's "
"primary can orphan data from multiple migrations."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:233
msgid ""
"The ``_waitForDelete``, available as a setting for the balancer as well "
"as the :dbcommand:`moveChunk` command, can alter the behavior so that the"
" delete phase of the current migration blocks the start of the next chunk"
" migration. The ``_waitForDelete`` is generally for internal testing "
"purposes. For more information, see :ref:`wait-for-delete-setting`."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:243
msgid "Chunk Migration and Replication"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:247
msgid ""
"During chunk migration, the ``_secondaryThrottle`` value determines when "
"the migration proceeds with next document in the chunk."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:250
msgid "In the :data:`config.settings` collection:"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:252
msgid ""
"If the ``_secondaryThrottle`` setting for the balancer is set to a write "
"concern, each document move during chunk migration must receive the "
"requested acknowledgement before proceeding with the next document."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:257
msgid ""
"If the ``_secondaryThrottle`` setting for the balancer is set to "
"``true``, each document move during chunk migration must receive "
"acknowledgement from at least one secondary before the migration proceeds"
" with the next document in the chunk. This is equivalent to a write "
"concern of :writeconcern:`{ w: 2 } <\\<number\\>>`."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:263
msgid ""
"If the ``_secondaryThrottle`` setting is unset, the migration process "
"does not wait for replication to a secondary and instead continues with "
"the next document."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:267
msgid ""
"Default behavior for :ref:`WiredTiger <storage-wiredtiger>` starting in "
"MongoDB 3.4."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:270
msgid ""
"For the :dbcommand:`moveChunk` command, you can use the command's "
"``_secondaryThrottle`` and ``writeConcern`` options to specify the "
"behavior during the command. For details, see :dbcommand:`moveChunk` "
"command."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:275
msgid ""
"Independent of any ``_secondaryThrottle`` setting, certain phases of the "
"chunk migration have the following replication policy:"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:278
msgid ""
"MongoDB briefly pauses all application reads and writes to the collection"
" being migrated, on the source shard, before updating the config servers "
"with the new location for the chunk, and resumes the application reads "
"and writes after the update. The chunk move requires all writes to be "
"acknowledged by majority of the members of the replica set both before "
"and after committing the chunk move to config servers."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:286
msgid ""
"When an outgoing chunk migration finishes and cleanup occurs, all writes "
"must be replicated to a majority of servers before further cleanup (from "
"other outgoing migrations) or new incoming migrations can proceed."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:291
msgid ""
"To update the ``_secondaryThrottle`` setting in the "
":data:`config.settings` collection, see :ref:`sharded-cluster-config-"
"secondary-throttle` for an example."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:298
msgid "Maximum Number of Documents Per Chunk to Migrate"
msgstr ""

#: ../source/includes/limits-sharding-maximum-documents-chunk.rst:3
msgid ""
"MongoDB cannot move a chunk if the number of documents in the chunk is "
"greater than 1.3 times the result of dividing the configured :ref:`chunk "
"size<sharding-chunk-size>` by the average document size. "
":method:`db.collection.stats()` includes the ``avgObjSize`` field, which "
"represents the average document size in the collection."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:305
msgid "Shard Size"
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:307
msgid ""
"By default, MongoDB attempts to fill all available disk space with data "
"on every shard as the data set grows. To ensure that the cluster always "
"has the capacity to handle data growth, monitor disk usage as well as "
"other performance metrics."
msgstr ""

#: ../source/core/sharding-balancer-administration.txt:312
msgid ""
"See the :ref:`sharded-cluster-config-max-shard-size` tutorial for "
"instructions on setting the maximum size for a shard."
msgstr ""

# 56c3c2f87d6a47a6b69177609ce5fd65
#~ msgid "On this page"
#~ msgstr ""

# b6eac23b08304bb78dbe62bf48823789
#~ msgid "The balancer runs on the primary of the config server replica set."
#~ msgstr ""

# d71061b8e66c4d5390bd2fa56fc15286
#~ msgid ""
#~ "When a balancer process is active, "
#~ "config server acquires a \"lock\" by "
#~ "modifying a document in the ``lock`` "
#~ "collection in the :ref:`config-database`. "
#~ "This \"balancer\" lock is never "
#~ "released."
#~ msgstr ""

# 73e38f213fa44507981644da16baf6ec
#~ msgid ""
#~ "The source shard automatically archives "
#~ "the migrated documents by default. For"
#~ " more information, see :ref:`moveChunk-"
#~ "directory`."
#~ msgstr ""

# 6e88d01549804eab90c3514a5df08c19
#~ msgid ""
#~ "During chunk migration, the "
#~ "``_secondaryThrottle`` value determines when "
#~ "the balancer proceeds with the next "
#~ "document in the chunk:"
#~ msgstr ""

# e5b13cfb057d4cbf81fc4a5063d6bdf6
#~ msgid ""
#~ "If ``true``, then by default, each "
#~ "document move during chunk migration "
#~ "propagates to at least one secondary "
#~ "before the balancer proceeds with the"
#~ " next document. This is equivalent to"
#~ " a write concern of :writeconcern:`{ "
#~ "w: 2 } <\\<number\\>>`."
#~ msgstr ""

# e3110f12aa7b4486a0f1816141a0da75
#~ msgid ""
#~ "The ``writeConcern`` field in the "
#~ "balancer configuration document allows you "
#~ "to specify a different :doc:`write "
#~ "concern </reference/write-concern>` semantics "
#~ "with the ``_secondaryThrottle`` option. For"
#~ " an example, see :ref:`sharded-cluster-"
#~ "config-secondary-throttle`."
#~ msgstr ""

# 1326ef103e9e40b09b011c65d248475a
#~ msgid ""
#~ "If ``false``, the balancer does not "
#~ "wait for replication to a secondary "
#~ "and instead continues with the next "
#~ "document."
#~ msgstr ""

# d2cfab46eea24423bb6db873c05f324a
#~ msgid ""
#~ "Starting in MongoDB 3.4, for "
#~ ":ref:`WiredTiger <storage-wiredtiger>`, the "
#~ "default value ``_secondaryThrottle`` is "
#~ "``false`` for all chunk migrations."
#~ msgstr ""

# 707f736079a5478e95b9743b132c54fe
#~ msgid "The default value remains ``true`` for :ref:`MMAPv1 <storage-mmapv1>`."
#~ msgstr ""

# 09c0705518b34f75ab0cb63aaf696640
#~ msgid ""
#~ "To update the ``_secondaryThrottle`` parameter"
#~ " for the balancer, see :ref:`sharded-"
#~ "cluster-config-secondary-throttle` for an"
#~ " example."
#~ msgstr ""

# 7783edf9ee904001804cdae1bd5fa0fa
#~ msgid ""
#~ "Independent of the ``secondaryThrottle`` "
#~ "setting, certain phases of the chunk "
#~ "migration have the following replication "
#~ "policy:"
#~ msgstr ""

# 4cec5d3a3db04cb092d88efa14fcacdd
#~ msgid ""
#~ "MongoDB briefly pauses all application "
#~ "writes to the source shard before "
#~ "updating the config servers with the "
#~ "new location for the chunk, and "
#~ "resumes the application writes after the"
#~ " update. The chunk move requires all"
#~ " writes to be acknowledged by "
#~ "majority of the members of the "
#~ "replica set both before and after "
#~ "committing the chunk move to config "
#~ "servers."
#~ msgstr ""

# a54dc9d4df4b4e179ee0e7ce784ee462
#~ msgid ""
#~ "MongoDB cannot move a chunk if the"
#~ " number of documents in the chunk "
#~ "exceeds either 250000 documents or 1.3"
#~ " times the result of dividing the "
#~ "configured :ref:`chunk size<sharding-chunk-"
#~ "size>` by the average document size. "
#~ ":method:`db.collection.stats()` includes the "
#~ "``avgObjSize`` field, which represents the "
#~ "average document size in the collection."
#~ msgstr ""

# 5e58809b68514d59ad1d8642a2df3b04
# 00d1754e494f4725a17245999e2d5b52
#~ msgid "balancing"
#~ msgstr ""

# 5e58809b68514d59ad1d8642a2df3b04
#~ msgid "migration"
#~ msgstr ""

# 00d1754e494f4725a17245999e2d5b52
#~ msgid "internals"
#~ msgstr ""

