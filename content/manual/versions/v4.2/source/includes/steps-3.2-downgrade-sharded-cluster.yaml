title: Disable the Balancer.
level: 4
stepnum: 1
ref: disable-balancer
content: |
  Turn off the :ref:`balancer <sharding-balancing-internals>` in the
  sharded cluster, as described in
  :ref:`sharding-balancing-disable-temporarily`.
---
title: "Check the ``minOpTimeUpdaters`` value."
level: 4
ref: 3.2-downgrade-minoptimerecovery
content: |
    If the sharded cluster uses :doc:`CSRS
    </core/sharded-cluster-config-servers>`, for *each* shard, check
    the ``minOpTimeUpdaters`` value to see if it is zero. A
    ``minOpTimeUpdaters`` value of zero indicates that there are no
    migrations in progress. A non-zero value indicates either that a
    migration is in progress or that a previously completed migration
    has somehow failed to clear the ``minOpTimeUpdaters`` value and
    should be cleared.

    To check the value, for each shard, connect to the primary member
    (or if a shard is a standalone, connect to the standalone) and
    query the ``system.version`` collection in the ``admin`` database
    for the ``minOpTimeRecovery`` document:

    .. code-block:: javascript

       use admin
       db.system.version.findOne( { _id: "minOpTimeRecovery" }, { minOpTimeUpdaters: 1 } )

    If ``minOpTimeUpdaters`` is non-zero, clear the value by stepping
    down the current primary. The value is cleared when a new primary
    gets elected.

    .. code-block:: javascript

       rs.stepDown()

    If the shard is a standalone, restart the shard to clear the value.
---
title: "Prepare CSRS Config Servers for downgrade."
level: 4
ref: 3.2-downgrade-csrs
content: |
    If the sharded cluster uses :doc:`CSRS
    </core/sharded-cluster-config-servers>`:

    a. :ref:`Remove secondary members from the replica set
       <remove-member-using-reconfig>` to have only a primary and two
       secondaries and only the primary can vote and be eligible to be
       primary; i.e. the other two members have ``0`` for
       :rsconf:`~members[n].votes` and :rsconf:`~members[n].priority`.

       Connect a :binary:`~bin.mongo` shell to the primary and run:

       .. code-block:: javascript

          rs.reconfig(
             {
                "_id" : <name>,
                "configsvr" : true,
                "protocolVersion" : NumberLong(1),
                "members" : [
                   {
                      "_id" : 0,
                      "host" : "<host1>:<port1>",
                      "priority" : 1,
                      "votes" : 1
                   },
                   {
                      "_id" : 1,
                      "host" : "<host2>:<port2>",
                      "priority" : 0,
                      "votes" : 0
                   },
                   {
                      "_id" : 2,
                      "host" : "<host3>:<port3>",
                      "priority" : 0,
                      "votes" : 0
                   }
                ]
             }
          )

    #. Step down the primary using :dbcommand:`replSetStepDown` against the ``admin`` database.
       Ensure enough time for the secondaries to catch up.

       Connect a :binary:`~bin.mongo` shell to the primary and run:

       .. code-block:: javascript

          db.adminCommand( { replSetStepDown: 360, secondaryCatchUpPeriodSecs: 300 })

    #. Shut down all members of the config server replica set,
       the :binary:`~bin.mongos` instances, and the shards.

    #. If you are rolling back to MMAPv1:

       a. Start a :doc:`CSRS </core/sharded-cluster-config-servers>` member as
          a standalone; i.e. without the :option:`--replSet <mongod --replSet>`
          or, if using a configuration file, :setting:`replication.replSetName`.

       #. Run :binary:`~bin.mongodump` to dump the ``config`` database, then
          shutdown the :doc:`CSRS </core/sharded-cluster-config-servers>`
          member.

          .. code-block:: sh

             mongodump --db "config"

          Include all other options as required by your deployment.

       #. Create a data directory for the new :binary:`~bin.mongod` instance
          that will run with the MMAPv1 storage engine. ``mongod`` must have
          read and write permissions for the data directory.

          ``mongod`` with MMAPv1 will not start with data files created with
          a different storage engine.       

       #. Restart the mongod as a MMAPv1 standalone i.e. with
          :option:`--storageEngine mmapv1 <mongod --storageEngine>` and without
          the :option:`--replSet <mongod --replSet>` or, if using a
          configuration file, :setting:`replication.replSetName`.

       #. Use :option:`mongorestore --drop` to restore the ``config`` dump to
          the new MMAPv1 ``mongod``.

          .. code-block:: sh

             mongorestore --db="config" --drop /path/to/dump

       #. Repeat for each member of the CSRS.

       Optionally, once the sharded cluster is online and working as expected,
       delete the WiredTiger data directories.

    #. Restart each config server as standalone 3.2 :binary:`~bin.mongod`;
       i.e. without the :option:`--replSet <mongod --replSet>` or, if using a configuration
       file, :setting:`replication.replSetName`.

       .. code-block:: sh

          mongod --configsvr --dbpath <path> --port <port> --storageEngine <storageEngine>

---
title: Update the protocolVersion for each shard.
level: 4
ref: 3.2-downgrade-protocolVersion
pre: |
  Restart each replica set shard and update the protocolVersion.
action:
  pre: |
    Connect a :binary:`~bin.mongo` shell to the current primary and
    downgrade the replication protocol:
  language: javascript
  code: |
    cfg = rs.conf();
    cfg.protocolVersion=0;
    rs.reconfig(cfg);
---
title: "Downgrade the :binary:`~bin.mongos` instances."
level: 4
ref: downgrade-mongos
content: |

   .. important::

      As the config servers changed from a replica set to three
      mirrored :binary:`~bin.mongod` instances, update the
      :option:`--configdb <mongos --configdb>` setting. All :binary:`~bin.mongos` must use the
      same :option:`--configdb <mongos --configdb>` string.

   Downgrade the binaries and restart.
---
title: "Downgrade Config Servers."
level: 4
ref: 3.2-downgrade-config-servers
content: |
    Downgrade the binaries and restart. Downgrade in reverse order of
    their listing in the :option:`--configdb <mongos --configdb>` option for
    :binary:`~bin.mongos`.

    If your :binary:`~bin.mongod` instance is using the :doc:`WiredTiger
    </core/wiredtiger>` storage engine, you must include the
    :option:`--storageEngine <mongod --storageEngine>` option (or :setting:`storage.engine` if
    using the configuration file) with the 3.0 binary.

    .. code-block:: sh

       mongod --configsvr --dbpath <path> --port <port> --storageEngine <storageEngine>

---
title: Downgrade each shard, one at a time.
level: 4
ref: 3.2-downgrade-shard
content: |

  For each shard, remove the ``minOpTimeRecovery`` document from the
  ``admin.system.version`` collection using the following remove
  operation. If the shard is a replica set, issue the remove operation
  on the primary of the replica set for each shard:

  .. code-block:: javascript

     use admin
     db.system.version.remove(
        { _id: "minOpTimeRecovery" },
        { writeConcern: { w: "majority", wtimeout: 30000 } }
     )

  .. note::
     
     If the cluster is running with authentication enabled, you must have a
     user with the proper privileges to remove the ``minOpTimeRecovery``
     document from the ``admin.system.version`` collection. The following
     operation creates a ``downgrade`` user on the ``admin`` database with the
     proper privileges:

     .. code-block:: javascript
     
        use admin;

        db.createRole({
          role: "downgrade_csrs",
          privileges: [
             { resource: { db: "admin", collection: "system.version"}, actions: [ "remove" ] },
          ],
          roles: [  ]
        });
     
        db.createUser({
          user: "downgrade",
          roles: [
            { role: "downgrade_csrs", db: "admin" }
          ]
        });

  For each replica set shard, downgrade the :binary:`~bin.mongod` binaries
  and restart. If your :binary:`~bin.mongod` instance is using the
  :doc:`WiredTiger </core/wiredtiger>` storage engine, you must include
  the :option:`--storageEngine <mongod --storageEngine>` option (or :setting:`storage.engine` if
  using the configuration file) with the 3.0 binary.

  #. Downgrade the :binary:`~bin.mongod` secondaries *before* downgrading
     the primary.

  #. To downgrade the primary, run :dbcommand:`replSetStepDown` and then downgrade.

  For details on downgrading a replica set, see :ref:`3.2-downgrade-replica-set`.

  Optionally, drop the ``local`` database from the SCCC members if it exists.

---
title: "Re-enable the balancer."
level: 4
ref: reenable-balancer
content: |
  Once the downgrade of sharded cluster components is
  complete, :ref:`re-enable the balancer <sharding-balancing-enable>`.
...
