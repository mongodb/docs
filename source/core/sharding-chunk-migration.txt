.. index:: balancing; migration

=============================
Chunk Migration Across Shards
=============================

.. default-domain:: mongodb

Chunk migration moves the chunks of a sharded collection from one shard
to another and is part of the :doc:`balancer
</core/sharding-balancing>` process.

.. include:: /images/sharding-migrating.rst

.. _sharding-chunk-migration:

Chunk Migration
---------------

MongoDB migrates chunks in a :term:`sharded cluster` to distribute the
chunks of a sharded collection evenly among shards. Migrations may be
either:

- Manual. Only use manual migration in limited cases, such as
  to distribute data during bulk inserts. See :doc:`Migrating Chunks
  Manually </tutorial/migrate-chunks-in-sharded-cluster>` for more details.

- Automatic. The :doc:`balancer </core/sharding-balancing>` process
  automatically migrates chunks when there is an uneven distribution of
  a sharded collection's chunks across the shards. See :ref:`Migration
  Thresholds <sharding-migration-thresholds>` for more details.

All chunk migrations use the following procedure:

#. The balancer process sends the :dbcommand:`moveChunk` command to
   the source shard.

#. The source starts the move with an internal :dbcommand:`moveChunk`
   command. During the migration process, operations to the chunk
   route to the source shard. The source shard is responsible for
   incoming write operations for the chunk.

#. The destination shard begins requesting documents in the chunk and
   starts receiving copies of the data.

#. After receiving the final document in the chunk, the destination
   shard starts a synchronization process to ensure that it has the
   changes to the migrated documents that occurred during the migration.

#. When fully synchronized, the destination shard connects to the
   :term:`config database` and updates the cluster metadata with the new
   location for the chunk.

#. After the destination shard completes the update of the metadata,
   and once there are no open cursors on the chunk, the source shard
   deletes its copy of the documents. 

   .. versionchanged:: 2.4 

      If the balancer needs to perform additional chunk migrations from
      the source shard, the balancer can start the next chunk
      migration without waiting for the current
      migration process to finish this deletion step. See
      :ref:`chunk-migration-queuing`.

The migration process ensures consistency and maximizes the availability of
chunks during balancing.

.. _chunk-migration-queuing:

Chunk Migration Queuing
~~~~~~~~~~~~~~~~~~~~~~~

.. versionchanged:: 2.4

To migrate multiple chunks from a shard, the balancer migrates the
chunks one at a time. However, the balancer does not wait for the
current migration's delete phase to complete before starting the next
chunk migration. See :ref:`sharding-chunk-migration` for the chunk
migration process and the delete phase.

This queuing behavior allows shards to unload chunks more quickly in
cases of heavily imbalanced cluster, such as when performing initial
data loads without pre-splitting and when adding new shards.

This behavior also affect the :dbcommand:`moveChunk` command, and
migration scripts that use the :dbcommand:`moveChunk` command may
proceed more quickly.

In some cases, the delete phases may persist longer. If multiple delete
phases are queued but not yet complete, a crash of the replica set's
primary can orphan data from multiple migrations.

.. TODO will rename this reference to chunk-migration-replication
   but need to make change in both 2.6 and 2.4 version of doc, so will
   rename together afterwards.

.. _chunk-migration-write-concern:

Chunk Migration and Replication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Internal operations like chunk migration don't fit exactly into the definitions
of write concern established for user operations. The following explanations
should be taken as approximations.

Write concern for chunk migration depends on whether the balancer or
the :dbcommand:`moveChunk` command operate with ``_secondaryThrottle``
set to ``true`` or ``false``.

Before 2.2.1, chunk migration writes did not wait to replicate to
secondaries before starting the next chunk migration (the equivalent
of ``w: 1``). After all relevant chunks had been written to the
destination shard, the balancer would wait for replication to
a majority of members (the equivalent of ``w: majority``) before
deleting the chunks from the source shard.

This had the effect of creating a replication bottleneck at the end
of migration.

.. versionadded:: 2.2.1

   ``_secondaryThrottle`` became an option to the balancer and to the
   :dbcommand:`moveChunk` command. ``_secondaryThrottle`` requires the
   balancer to have a ``{ w: 2 }`` write concern on each delete and
   insert operation. This causes the balancer to wait for each chunk
   migration to replicate to at least one secondary before initiating
   a new chunk migration.

   As in the case before 2.2.1, after all relevant chunks are written
   to the destination shard, the balancer waits for replication to
   a majority of members (the equivalent of ``w: majority``) before
   deleting the chunks from the source shard.

.. versionchanged:: 2.4
   ``_secondaryThrottle`` became the default mode for all balancer and
   :dbcommand:`moveChunk` operations. The balancer does not require
   ``_secondaryThrottle`` to be ``true``. To revert to previous
   behavior, set ``_secondaryThrottle`` to ``false``.

.. BACKGROUND NOTES
   Specifically, secondary throttle affects the first and fourth
   phases (informal phases) of chunk migration. Migration can happen during
   the second and third phases (the "steady state"):
   1) copies the documents in the chunk from shardA to shardB
   2) continues to copy over ongoing changes that occurred during the initial copy step,
      as well as current changes to that chunk range
   3) Stop writes, allow shardB to get final changes, commit migration to config server
   4) cleanup now-inactive data on shardA in chunk range (once all cursors are done)

When you :ref:`change the value
<sharded-cluster-config-secondary-throttle>` of ``_secondaryThrottle``,
the balancer may not immediately require the new write concern on
delete and insert operations. To ensure that ``_secondaryThrottle``
takes effect immediately, stop the balancer and restart it with the
selected value of ``_secondaryThrottle``.
