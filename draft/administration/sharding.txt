.. index:: administration; sharding
.. _sharding-administration:

============================
Shard Cluster Administration
============================

.. default-domain:: mongodb

This document provides a collection of basic operations and procedures
for administering :term:`shard clusters <shard cluster>`. For a full
introduction to sharding in MongoDB see ":doc:`/core/sharding`," and
for a complete overview of all sharding documentation in the MongoDB
Manual, see ":doc:`/sharding`." The ":doc:`/administration/sharding-architectures`"
document provides an overview of deployment possibilities 'that you
may find helpful as you plan to deploy a shard cluster. Finally the
":doc:`/core/sharding-internals`" document provides a more detailed
introduction to sharding that you may find useful when troubleshooting
issues or understanding your cluster's behavior.

.. contents:: Sharding Procedures:
   :backlinks: none
   :local:

.. _sharding-procedure-setup:

Setup
-----

If you have an existing replica set, you can use then
":doc:`/tutorials/convert-replica-set-to-replicated-shard-cluster`"
tutorial as a guide. If you're deploying a :term:`shard
cluster` from scratch, use the following procedure as a starting point:

#. Provision the required hardware.

   The ":ref:`sharding-requirements`" describes what you'll need to get started.

#. On all three (3) config server instances, issue the following
   command to start the :program:`mongod` process:

   .. code-block:: sh

      mongod --configsvr

   This starts a :program:`mongod` instance running on TCP port
   ``27018``, with the data stored in the ``/data/configdb`` path. All other
   :doc:`command line </reference/mongod>` and :doc:`configuration
   file </reference/configuration-options>` are available for config
   server instances.

#. Start a :program:`mongos` instance. Use the following command:

   .. code-block:: sh

      mongos --configdb config0.mongodb.example.net,config1.mongodb.example.net,config3.mongodb.example.net --port 27017

#. Access the :program:`mongos` instance using the :program:`mongo`
   shell.

   .. code-block:: sh

      mongo mongos.mongodb.example.net

#. Add shards to the cluster.

   In the steps that follow, you'll use the MongoDB shell to run
   commands against the `mongos` to initialize the cluster.

   First, you need to tell the cluster where to find the individual
   shards. You can do this using the :dbcommand:`addShard` command.

   .. code-block:: javascript

      sh.addShard( "[hostname]:[port]" )

   For example:

   .. code-block:: javascript

      sh.addShard( "mongodb0.example.net:27027" )

   .. note:: In production deployments, all shards should be replica sets.

      .. versionchanged:: 2.0.3

      After version 2.0.3, you may use the above form to add replica
      sets to a cluster and the cluster will automatically discover
      the members of the replica set and adjust its configuration
      accordingly.

      Before version 2.0.3, you must specify the shard in the
      following form: the replica set name, followed by a forward
      slash, followed by a comma-separated list of seeds for the
      replica set. For example, if the name of the replica set is
      "myapp1", then your :func:`sh.addShard` command might resemble:

      .. code-block:: javascript

         sh.addShard( "myapp1/mongodb0.example.net:27027,mongodb1.example.net:27017" )

   Repeat this step for each shards in your cluster.

   .. optional::

      You may specify a "name" as an argument to the
      :dbcommand:`addShard`, as follows:

      .. code-block:: javascript

         db.runCommand( { addShard: mongodb0.example.net, name: "mongodb0" } )
         sh.addShard( mongodb0.example.net, name: "mongodb0" )

      If you do not specify a shard name, then MongoDB will assign a
      name upon creation.

#. Enable sharding for any database that you want to shard.

   MongoDB enables sharding on a per-database basis. To enable
   sharding for a given database, use the :dbcommand:`enableSharding`
   command or the :func:`sh.enableSharding()` shell function.

   .. code-block:: javascript

      db.runCommand( { enableSharding: [database] } )

   Or:

   .. code-block:: javascript

      sh.enableSharding([database])

   Replace ``[database]``  with the name of the database you wish to
   enable sharding on.

   .. note::

      MongoDB creates databases automatically on first use.

      Once you enable sharding for a database, MongoDB assigns a
      "primary" shard for that database, where all data lives
      initially before sharding begins.

#. Enable sharding on a per-collection basis.

   Finally, you must explicitly specify collections to shard.  The
   collections must belong to a database that you has sharding
   enabled. When you shard a collection, you also choose the shard
   key. To get this process started, run the
   :dbcommand:`shardCollection` command or the
   :func:`sh.shardCollection()` shell function.

   .. code-block:: javascript

      db.runCommand( { shardCollection: "[database].[collection]", key: "[shard-key]" } )

   Or:

   .. code-block:: javascript

      sh.shardCollection("[database].[collection]", "key")

   In actual use, these commands would resemble the following:

   .. code-block:: javascript

      db.runCommand( { shardCollection: "myapp.users", key: {username: 1} } )

   Or:

      sh.shardCollection("myapp.users", {username: 1})

    Two points about shard keys. First, the choice of shard key
    is incredibly important, as dramatically affects everything
    from the efficiency of your queries to the distribution of data.
    Second, you cannot change a collection's shard key. Therefore,
    you should spend some time reading and thinking about this topic.
    See the ":ref:`Shard Key Overview <sharding-shard-key>`"
    and :ref:`Shard Internals <sharding-internals-shard-keys>`
    sections for technique for choosing a good shard key.

    If you do not specify a shard key, MongoDB will shard the
    collection using the ``_id`` field, and this is rarely ideal.

Cluster Management
------------------

Once you have a running shard cluster, you'll need to maintain it.
This section describes common maintenance procedures, including how to
add and remove notes, how to manually split chunks, and how to disable
the balancer for backups.

.. _sharding-procedure-add-shard:

Adding a Shard to a Cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~

To add a shard to an *existing* shard cluster, use the following
procedure:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. First, you need to tell the cluster where to find the individual
   shards. You can do this using the :dbcommand:`addShard` command.

   .. code-block:: javascript

      sh.addShard( "[hostname]:[port]" )

   If each shard is just a single server (note: each shard should be a
   replica set in production), you should replace ``[hostname]`` and ``[port]``
   with the hostname and TCP port number of shard where the shard is accessible.

   For example:

   .. code-block:: javascript

      sh.addShard( "mongodb0.example.net:27027" )

   .. note:: In production deployments, all shards should be replica sets.

      .. versionchanged:: 2.0.3

      After version 2.0.3, you may use the above form to add replica
      sets to a cluster and the cluster will automatically discover
      the members of the replica set and adjust its configuration
      accordingly.

      Before version 2.0.3, you must specify the shard in the
      following form: the replica set name, followed by a forward
      slash, followed by a comma-separated list of seeds for the
      replica set. For example, if the name of the replica set is
      "myapp1", then your :func:`sh.addShard` command might resemble:

      .. code-block:: javascript

         sh.addShard( "myapp1/mongodb0.example.net:27027,mongodb1.example.net:27017" )

   Repeat this step for each shards in your cluster.

   .. optional::

      You may specify a "name" as an argument to the
      :dbcommand:`addShard`, as follows:

      .. code-block:: javascript

         db.runCommand( { addShard: mongodb0.example.net, name: "mongodb0" } )
         sh.addShard( mongodb0.example.net, name: "mongodb0" )

      If you do not specify a shard name, then MongoDB will assign a
      name upon creation.

.. note::

   It may take some time for :term:`chunks` to migrate to
   the new shard.

   See the ":ref:`Balancing and Distribution <sharding-balancing>`"
   section for an overview of the balancing operation and the ":ref:`Balancing Internals
   <sharding-balancing-internals>`" section for additional information.

.. _sharding-procedure-remove-shard:

Removing a Shard from a Cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To remove a :term:`shard` from a :term:`shard cluster`, you must:

- Begin moving :term:`chunks` off of the shard.

- Ensure that this shard is not the "primary" shard for any databases
  in the cluster. If it is, move the "primary" status for these databases to
  other shards.

- Finally, remove the shard from the cluster's configuration.

.. note::

   To successfully migrate data from a shard, the :term:`balancer`
   process **must** be active.

The formal procedure for to remove a shard is as follows:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Determine the name of the shard you will be removing.

   You must specify the name of the shard. You may have specified this
   shard name when you first ran the :dbcommand:`addShard` command. If not,
   you can find out the name of the shard by running the :dbcommand:`listshards`
   (or :func:`sh.status()` or :dbcommand:`printShardingStatus`) command.

   In the examples that follow, we'll assume we're removing a shard called ``mongodb0``.

#. Begin removing chunks from the shard.

   Start by running the :dbcommand:`removeShard` command. This will
   start "draining" chunks from the shard you''re removing.

   .. code-block:: javascript

      db.runCommand( { removeshard: "mongodb0" } )

   This operation will return the following response immediately:

   .. code-block:: javascript

      { msg : "draining started successfully" , state: "started" , shard :"mongodb0" , ok : 1 }

   Depending on your network capacity and the amount of data in your
   cluster, this operation can take anywhere from a few minutes to several
   days to complete.

#. Check the progress of the migration.

   You can run the :dbcommand:`removeShard` again at any stage of the
   process to check the progress of the migration, as follows:

   .. code-block:: javascript

      db.runCommand( { removeshard: "mongodb0" } )

   The output should look something like this:

   .. code-block:: javascript

      { msg: "draining ongoing" , Â state: "ongoing" , remaining: { chunks: 42, dbs : 1 }, ok: 1 }

   In the ``remaining`` sub document, a counter displays the
   total number of chunks that MongoDB must migrate to other shards,
   and the number of MongoDB databases that have "primary" status on
   this shard.

   Continue checking the status of the `removeshard` command until
   the number of chunks remaining is 0. Then you can proceed to the next step.

#. Move any databases to other shards in the cluster.

   Databases with non-sharded collections store these collections
   on a so-called "primary" shard. The following step is necessary
   only when the shard you want to remove is also the "primary" shard
   for one or more databases.

   Issue the following command at the :program:`mongo` shell:

   .. code-block:: javascript

      db.runCommand( { movePrimary: "myapp", to: "mongodb1" })

   This command will migrate all remaining non-sharded data in the
   database named ``myapp`` to the shard named ``mongodb1``.

   .. warning::

      Do not run the :dbcommand:`movePrimary` until you have *finished*
      draining the shard.

   This command can be long-running. It will not return until MongoDB
   completes moving all data. The response from this command will
   resemble the following:

   .. code-block:: javascript

      { "primary" : "mongodb1", "ok" : 1 }

#. Run :dbcommand:`removeShard` again to clean up all metadata
   information and finalize the removal, as follows:

   .. code-block:: javascript

      db.runCommand( { removeshard: "mongodb0" } )

   When successful, this command will return a document like this:

   .. code-block:: javascript

      { msg: "remove shard completed succesfully" , stage: "completed", host: "mongodb0", ok : 1 }

Once the value if "state" is "completed", you may safely stop the processes
comprising the ``mongodb0`` shard.

Chunk Management
----------------

This section describes various operations on
:term:`chunks <chunk>` in :term:`shard clusters <shard cluster>`. In
most cases MongoDB automates these processes; however, in some cases,
particularly when you're just establishing a shard cluster, you may
need to create and manipulate chunks directly.

.. _sharding-procedure-create-split:

Splitting Chunks
~~~~~~~~~~~~~~~~

Normally, MongoDB splits :term:`chunk` following inserts or updates
when a chunk exceeds the designated :ref:`chunk size
<sharding-chunk-size>`.

Still, you may want to split chunks manually if:

- you have a large amount of data in your cluster that is *not* split,
  as is the case after creating a shard cluster with existing data

- you expect to add a large amount of data to data that would at least
  initially reside in a single chunk or shard.

  .. example::

     You plan to insert a large amount of data as the result of an
     import process with :term:`shard key` values between ``300``
     and ``400``, *but* all values of your shard key between ``250``
     and ``500`` are within a single chunk.

  Use :func:`sh.status()` to determine the current chunks ranges across
  the cluster.

TODO expand this section

To split chunks manually, use either the :func:`sh.splitAt()` or
:func:`sh.splitFind()` helpers in the :program:`mongo` shell.
These helpers wrap the :dbcommand:`split` command.

TODO: examples of using both helpers.

.. note::

   You cannot merge chunks once you've split them.

.. _sharding-balancing-modify-chunk-size:

Modifying Chunk Size
~~~~~~~~~~~~~~~~~~~~

When you initialize a shard cluster, MongoDB selects a default chunks
size of 64 MB. This default work well for most deployments. However,
if you notice that the automatic migrations are incurring a level of
I/O that your hardware can't handle, you may want to reduce the
default chunks size. For the automatic splits and migrations, a small
chunk size reduces to total number of documents the database has to
scan and decreases the total amount of data to migrate.

To modify the chunk size, use the following procedure:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the config database:

   .. code-block:: javascript

      use config

#. Issue the following :func:`save() <db.collection.save()>`
   operation:

   .. code-block:: javascript

      db.settings.save( { _id:"chunksize", value: <size> } )

   Where the value of ``<size>`` reflects the new chunk size in
   megabytes. Here, you're essentially writing a document whose values
   store the global chunk size configuration value.

TODO how does this new size affect the runtime option?

.. note::

   Modifying the chunk size has serveral limitations:

   - Automatic splitting only occurs when inserting :term:`documents 3
     <document>` or updating existing documents; if you lower the
     chunk size it may take for all chunks to split to the new size.

   - Splits cannot be "undone;" if you increase the chunk size,
     existing chunks must grow through insertion or updates until they
     reach the new size.

.. seealso:: :setting:`chunkSize` or :option:`--chunkSize <mongos --chunkSize>`.

.. _sharding-balancing-manual-migration:

Migrating Chunks
~~~~~~~~~~~~~~~~

In most circumstances, you should let the automatic balancer
migrate :term:`chunks <chunk>` between :term:`shards <shard>`.
However, you may want to migrate chunks manually in a few cases.
If you're pre-splitting a sharded cluster, then you'll definitely
have to migrate the chunks manually so that all the chunks will be
evenly distributed across all shards. If you find that an active shard
cluster is out of balance and that the balancer isn't rectifying the situation,
then in this case you'll also probably want to migrate manually.

To migrate chunks, use the :dbcommand:`moveChunk` command.

The following example assumes that the field ``username`` is the
:term:`shard key` for a collection named ``users`` in the ``myapp``
database, and that the value ``smith`` exists within the :term:`chunk`
you want to migrate.

To move this chunk, you would issue the following command from a :program:`mongo`
shell connected to any :program:`mongos` instance.

.. code-block:: javascript

   db.adminCommand({moveChunk : "myapp.users", find : {username : "smith"}, to : "mongodb-shard3.example.net"})

This command moves the chunk that includes the shard key value "smith" to the
:term:`shard` named ``mongodb-shard3.example.net``. The command will
block until the migration is complete.

.. note::

   You can specify shard names using the ``name`` argument to the
   :dbcommand:`addShard` command. If you do
   not specify a name, MongoDB will assign a name automatically.

   To return a list of shards, use the :dbcommand:`listshards`
   command.

.. index:: balancing; operations
.. _sharding-balancing-operations:

Balancer Operations
-------------------

TODO modify this section to include the helper functions in the shell.

This section provides an overview of common administrative procedures
related to balancing and the balancing process.

.. seealso:: ":ref:`sharding-balancing`" and the
   :dbcommand:`moveChunk` that provides manual :term:`chunk`
   migrations.

.. _sharding-balancing-check-lock:

Check the Balancer Lock
~~~~~~~~~~~~~~~~~~~~~~~

To see if the balancer process is active in your :term:`shard
cluster`, do the following:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the config database:

   .. code-block:: javascript

      use config

#. Use the following query to return the balancer lock:

   .. code-block:: javascript

      db.locks.find( { _id : "balancer" } );

   You can also use the following shell helper to return the same
   information:

   .. code-block:: javascript

      sh.getBalancerState()

When this command returns, you will see output like the following:

.. code-block:: javascript

   { "_id" : "balancer", "process" : "guaruja:1292810611:1804289383", "state" : 2, "ts" : ObjectId("4d0f872630c42d1978be8a2e"), "when" : "Mon Dec 20 2010 11:41:10 GMT-0500 (EST)", "who" : "guaruja:1292810611:1804289383:Balancer:846930886", "why" : "doing balance round" }

Here's what this tells you:

- The balancer originates from the :program:`mongos` running on the
  system with the hostname ``guaruja``.

- The value in the ``state`` field indicates that a :program:`mongos`
  has the lock. For version 2.0 and later, the value of an active lock
  is ``2``; for earlier versions the value is ``1``.

  .. note::

     The following shell helper will quickyl tell you whether the balancer is running:

     .. code-block:: javascript

        sh.isBalancerRunning()

.. _sharding-schedule-balancing-window:

Schedule the Balancing Window
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some situations, particularly when your data set grows slowly and a
migration can impact performance, it's useful to be able to ensure
that the balancer is active only at certain times.  Use the following
procedure to specify a window during which the :term:`balancer` will
be able to migrate chunks:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the config database:

   .. code-block:: javascript

      use config

#. Use an operation modeled on the folloiwng example :func:`update()
   <db.collection.update()>` operation to modify the balancer's
   window:

   .. code-block:: javascript

      db.settings.update({ _id : "balancer" }, { $set : { activeWindow : { start : "<start-time>", stop : "<stop-time>" } } }, true )

   Replace ``<start-time>`` and ``<end-time>`` with time values in
   24-hour ``H:MM`` format that describe the beginning and end
   bounds of the balancing window. For instance, running the following
   will force the balancer to run between 11PM and 6AM only:

   .. code-block:: javascript

      db.settings.update({ _id : "balancer" }, { $set : { activeWindow : { start : "23:00", stop : "6:00" } } }, true )

.. note::

   The balancer window must be sufficient to *complete* the migration
   of all data inserted during the day.

   As data insert rates can change based on activity and usage
   patterns, it's important to ensure that the balancing window you
   select will be sufficient to support the needs of your deployment.

.. _sharding-balancing-disable-temporally:

Disable the Balancer
~~~~~~~~~~~~~~~~~~~~

By default the balancer may run at any time and only moves chunks as
needed. To disable the balancer for a short period of time and prevent
all migrations, use the following procedure:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to disable the balancer:

   .. code-block:: javascript

      sh.setBalancerState(false)

#. Later, issue the following command to enable the balancer:

   .. code-block:: javascript

      sh.setBalancerState(true)

It's important to note that if a balancing round is in progress, you will
have to wait for that to complete before the balancer is officially disabled.
After disabling, you can use the :func:`sh.getBalancerState()` shell function
to determine whether the balancer is in fact disabled.

The above process and the :func:`sh.setBalancerState()` helper provide a
wrapper on the following process, which may be useful if you need to
run this operation from a driver that does not have helper functions:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the config database:

   .. code-block:: javascript

      use config

#. Issue the following update to disable the balancer:

   .. code-block:: javascript

      db.settings.update( { _id: "balancer" }, { $set : { stopped: true } } , true );

#. To enable the balancer again, just alter the value of 'stopped':

   .. code-block:: javascript

      db.settings.update( { _id: "balancer" }, { $set : { stopped: false } } , true );

.. index:: config servers; operations
.. _sharding-procedure-config-server:

Config Server Maintenance
-------------------------

Config servers store all shard cluster metadata, perhaps most notably,
the mapping from :term:`chunks` to :term:`shards`.
This section provides an overview of the basic
procedures to migrate, replace, and maintain these servers.

.. seealso:: :ref:`sharding-config-server`

Upgrading from One Config Server to Three Config Servers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For redundancy, all production :term:`shard clusters <shard cluster>` should
deploy three config servers processes on three different machines.

However, you may deploy a single config server for testing. Do not do
this for production deployments. If you're in this situation, you'll
want to move to three config servers immediately. The following
process shows how to do this.

#. Shut down all existing MongoDB processes. This includes:

   - all :program:`mongod` instances or :term:`replica sets <replica set>`
     that provide your shards.

   - the :program:`mongod` instance that provides your existing config
     database.

   - all :program:`mongos` instances in your cluster.

#. Copy the entire :setting:`dbpath` file system tree for the existing
   config server to the two machines that will provide the second and third
   config servers. These commands, issued on the system with the existing
   config database, may look like the following:

   .. code-block:: sh

      rsync -az /data/configdb mongo-config1.example.net:/data/configdb
      rsync -az /data/configdb mongo-config2.example.net:/data/configdb

#. Start all three config servers, using the same invocation that you
   use for the existing config server.

   .. code-block:: sh

      mongod --configsvr

#. Restart all shard :program:`mongod`s and :program:`mongos` processes.

.. _sharding-process-config-server-migrate-same-hostname:

Migrating Config Servers with the Same Hostname
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~:

Use this process when you need to migrate a config server to a new
system but the new system will be accessible using the same host
name.

#. Shut down the config server that you're moving.

   This will render all config data for your cluster "read only:" your
   application will still be able read and write data to the cluster,
   but the cluster will be unable to split chunks as they grow or migrate chunks
   between shards. For short periods of time this is acceptable.

#. Change the DNS entry that points to the system that provided the old
   config server, so that the *same* hostname points to the new
   system.

   How you do this depends on how you organize your DNS and
   hostname resolution services.

#. Move the entire :setting:`dbpath` file system tree from the system
   that provides the old config server to the system that will provide
   the new config server. This command, issued on the original system,
   may resemble the following:

   .. code-block:: sh

      rsync -az /data/configdb mongo-config0.example.net:/data/configdb

#. Start the config instance on the new system. The default invocation
   is:

   .. code-block:: sh

      mongod --configsrv

When you start the third config server, your cluster will become
writable and it will be able to create new splits and migrate chunks
as needed.

.. _sharding-process-config-server-migrate-different-hostname:

Migrating Config Servers with Different Hostname
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use this process when you need to migrate a config database to a new
server and it *will not* be accessible via the same host name. If
possible, avoid changing the hostname so that you can use the
:ref:`previous procedure <sharding-process-config-server-migrate-same-hostname>`.

#. Shut down the config server that you're moving.

   This will render all config data for your cluster "read only:" your
   application will still be able read and write data to the cluster,
   but the cluster cannot split chunks as they grow or migrate chunks
   between shards. For short periods of time this is acceptable.

#. Move the entire :setting:`dbpath` file system tree from the system
   that serves the old config server to the system that will serve
   the new config server. This command, issued on the original system,
   may resemble the following:

   .. code-block:: sh

      rsync -az /data/configdb mongodb.config2.example.net:/data/configdb

#. Start the config instance on the new system. The default invocation
   is:

   .. code-block:: sh

      mongod --configsrv

#. Shut down all existing MongoDB processes. This includes:

   - all :program:`mongod` instances or :term:`replica sets <replica set>`
     that provide your shards.

   - the :program:`mongod` instances that provide your existing
     config databases.

   - all :program:`mongos` instances in your cluster.

#. Restart all :program:`mongod` processes that provide the shard
   servers.

#. Update the :option:`--configdb <mongos --configdb>` parameter (or
   :setting:`configdb`) for all :program:`mongos` instances and
   restart all :program:`mongos` instances.

Replacing a Config Server
~~~~~~~~~~~~~~~~~~~~~~~~~

Use this procedure only if you need to replace one of your config
servers after it becomes inoperable (e.g. hardware failure.) This
process assumes that the hostname of the instance will not change. If
you must change the hostname of the instance, use the process for
:ref:`migrating a config server to a different hostname
<sharding-process-config-server-migrate-different-hostname>`.

#. Provision a new system, with the same hostname as the previous
   host.

   You will have to ensure that the new system has the same IP address
   and hostname as the system it is replacing *or* you will need to
   modify the DNS records and wait for them to propagate.

#. Shut down *one* (and only one) of the existing config servers. Copy
   all this host's :setting:`dbpath` file system tree from the current system
   to the system that will provide the new config server. This
   command, issued on the system with the data files, may resemble the
   following:

   .. code-block:: sh

      rsync -az /data/configdb mongodb.config2.example.net:/data/configdb

#. Restart the config server process that you used in the previous
   step to copy the data files to the new config server instance.

#. Start the new config server instance. The default invocation is:

   .. code-block:: sh

      mongod --configsrv

Backup Cluster Metadata
~~~~~~~~~~~~~~~~~~~~~~~

Because the shard cluster remains largely operational [#read-only]
without one the config databases :program:`mongod` instances, creating
a backup of the cluster metadata from the config database is very
straightforward:

#. Shut down one of the :term:`config databases`.

#. Create a full copy of the data files (i.e. the path specified by
   the :setting:`dbpath` option for the config instance.

#. Restart the original configuration server.

.. seealso:: :doc:`backups`.

.. [#read-only] While one of the three config servers unavailable, no
   the cluster cannot split any chunks nor can it migrate chunks
   between shards. Your application will be able to write data to the
   cluster. The ":ref:`sharding-config-server`" section of the
   documentation provides more information on this topic.

.. index:: troubleshooting; sharding
.. index:: sharding; troubleshooting
.. _sharding-troubleshooting:

Troubleshooting
---------------

The two most important factors in maintaining a successful shard cluster are:

- :ref:`choosing an appropriate shard key <sharding-internals-shard-keys>` and

- :ref:`sufficient capacity to support current and future operations
  <sharding-capacity-planning>`.

You can prevent most issues encountered with sharding by ensuring that
you choose the best possible :term:`shard key` for your deployment and
ensure that you are always adding additional capacity to your cluster
well before the current resources become saturated. Continue reading
for specific issues you may encounter in a production environment.

.. _sharding-troubleshooting-not-splitting:

All Data Remains on One Shard
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Your cluster must have sufficient data for sharding to make
sense. Sharding works by migrating chunks between the shards until
each shard has roughly the same number of chunks.

The default chunk size is 64 megabytes. MongoDB will not begin
migrations until the shard with the most chunks has 8 more chunks than
the shard with the fewest chunks. While the default chunk size is
configurable with the :setting:`chunkSize` setting, these behaviors
help prevent unnecessary chunk migrations, which can degrade the
performance of your cluster as a whole.

If you have just deployed a shard cluster, make sure that you have
enough data to make sharding effective. If you do not have sufficient
data to create more than eight 64 megabyte chunks, then all data will
remain on one shard. Either lower the :ref:`chunk size
<sharding-chunk-size>` setting, or add more data to the cluster.

As a related problem, the system will split chunks only on
inserts or updates, which means that if you configure sharding and do not
continue to issue insert and update operations, the database will not
create any chunks. You can either wait until your application inserts
data *or* :ref:`split chunks manually <sharding-procedure-create-split>`.

Finally, if your shard key has a low :ref:`cardinality
<sharding-shard-key-cardinality>`, MongoDB may not be able to create
sufficient splits among the data.

One Shard Receives Too Much Traffic
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some situations, a single shard or a subset of the cluster will
receive a disproportionate percentage of the traffic and workload from
the application. In almost all cases this is the result of a shard key
that does not effectively allow :ref:`write scaling
<sharding-shard-key-write-scaling>`.

It's also possible that you have some hot chunks. In this case, you may
be able to solve the problem by splitting and then migrating parts of
these chunks.

In the worst case, you may have to consider re-sharding your data
and :ref:`choosing a different shard key <sharding-internals-choose-shard-key>`
to correct this pattern.

The Cluster Won't Balance
~~~~~~~~~~~~~~~~~~~~~~~~~

If you've just deployed your shard cluster, you may want to
consider the :ref:`troubleshooting suggestions for a new cluster where
data remains on a single shard <sharding-troubleshooting-not-splitting>`.

If you maintain a shard cluster that balanced data initially, but
later develops an uneven distribution of data, see if any of the following
conditions applies:

- You have deleted or removed a significant amount of data from the
  cluster. If you've added additional data, it may have a
  different distribution with regards to its shard key.

- Your :term:`shard key` has low :ref:`cardinality <sharding-shard-key-cardinality>`
  and MongoDB cannot split the chunks any further.

- Your data set is growing faster than the balancer can distribute
  data around the cluster. This is uncommon and
  typically is the result of:

  - a :ref:`balancing window <sharding-schedule-balancing-window>` that
    is too short, given the rate of data growth.

  - an uneven distribution of :ref:`write operations
    <sharding-shard-key-write-scaling>` that requires more data
    migration. You may have to choose a different shard key to resolve
    this issue.

  - poor network connectivity between shards, which may lead to chunk
    migrations that take too long to complete. Investigate your
    network configuration and interconnections between shards.

Migrations Render Cluster Unusable
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If migrations consume too many system resources to allow your
application to use MongoDB effectively, there are two good solutions:

#. If migrations only interrupt your clusters sporadically, you can
   limit the :ref:`balancing window
   <sharding-schedule-balancing-window>` to prevent balancing activity
   during peek hours. Ensure that there is enough time remaining to
   keep the data from getting out of balance.

#. If the balancer is always migrating chunks to the detriment of
   overall cluster performance:

   - You may want to attempt :ref:`decreasing the chunk size <sharding-balancing-modify-chunk-size>`
     to limit the amount of data moved at each migration.

   - Your cluster may be over capacity, and you may want to attempt to
     :ref:`add one or two shards <sharding-procedure-add-shard>` to
     the cluster to distribute load.

Alternatively, you may have chosen a shard key that causes your
application to direct all writes to a single shard, which in turn
requires the balancer to migrate most data shortly after writing
it. Consider redeploying sharding with a shard key that provides
better :ref:`write scaling <sharding-shard-key-write-scaling>`.

Disable Balancing During Backups
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If MongoDB migrates a chunk while you are taking a :doc:`backup
</administration/backup>`, you can end with an inconsistent snapshot
of your shard cluster. You should never run a backup unless you're
certain that you have disabled the balancer. There are two ways to
ensure this:

- Set the :ref:`balancing window <sharding-schedule-balancing-window>`
  so that the balancer is inactive while you're creating the
  backup. Ensure that the backup process can complete while you have
  the balancer disabled.

- :ref:`manually disable the balancer <sharding-balancing-disable-temporally>`
  for the duration of the backup procedure.

Either way, confirm that you have disabled the balancer by running
`sh.getBalancerState()` before you start your backups.
