:orphan:

=====================================================
Development Release Notes for 3.2.0 Release Candidate
=====================================================

.. default-domain:: mongodb

MongoDB 3.2 is currently in development.

.. warning::

   While 3.2 release candidates are available, these versions of
   MONGODB are for **testing purposes only and not for production use**.

WiredTiger as Default
---------------------

Starting in 3.2, MongoDB uses the WiredTiger as the default storage
engine.

To specify the MMAPv1 storage engine, you must specify the storage
engine setting either:

- On the command line with the ``--storageEngine`` option:

  .. code-block:: sh

     mongod --storageEngine mmapv1

- Or in a :doc:`configuration file </reference/configuration-options>`,
  using the :setting:`storage.engine` setting:

  .. code-block:: yaml

     storage:
        engine: mmapv1

.. note::

   For existing deployments, if you do not specify the
   ``--storageEngine`` or the :setting:`storage.engine` setting,
   MongoDB 3.2 can automatically determine the storage engine
   used to create the data files in the ``--dbpath`` or
   :setting:`storage.dbPath`.

   If specifying ``--storageEngine`` or :setting:`storage.engine`,
   :program:`mongod` will not start if ``dbPath`` contains data files
   created by a storage engine other than the one specified.

.. seealso:: :ref:`3.2-storage-engine-compatibility`

.. _3.2-rel-notes-rs-enhancements:

Replication Election Enhancements
---------------------------------

Starting in MongoDB 3.2, MongoDB reduces replica set failover time
and accelerates the detection of multiple simultaneous primaries.

As part of this enhancement, MongoDB introduces a version 1 of the
replication protocol. New replica sets will, by default, use
:rsconf:`protocolVersion: 1 <protocolVersion>`. Previous versions of
MongoDB use version 0 of the protocol.

In addition, MongoDB introduces a new :doc:`replica set configuration
</reference/replica-configuration>` option
:rsconf:`~settings.electionTimeoutMillis`.
:rsconf:`~settings.electionTimeoutMillis` specifies the time limit in
milliseconds for detecting when a replica set's primary is unreachable:

- Higher values result in slower failovers but decreased sensitivity to
  primary node or network slowness or spottiness.

- Lower values result in faster failover, but increased sensitivity to
  primary node or network slowness or spottiness.

:rsconf:`~settings.electionTimeoutMillis` only applies if using the
version 1 of the :rsconf:`replication protocol <protocolVersion>`.

.. _3.2-rel-notes-sharded-cluster:

Sharded Cluster Enhancements
----------------------------

MongoDB 3.2 deprecates the use of three mirrored :program:`mongod`
instances for config servers.

Instead, starting in 3.2, the :doc:`config servers
</core/sharded-cluster-config-servers>` for a sharded cluster will, by
default, be deployed as a replica set. The replica set config servers must
run the WiredTiger storage engine.

This change improves consistency across the config servers, since
MongoDB can take advantage of the standard replica set read and write
protocols for sharding config data. In addition, this allows a sharded
cluster to have more than 3 config servers since a replica set can have
up to 50 members.

Restrictions
~~~~~~~~~~~~

The following restrictions apply to a replica set configuration when
used for config servers:

- Must have zero :doc:`arbiters </core/replica-set-arbiter>`.

- Must have no :doc:`delayed members
  </core/replica-set-delayed-member>`.

- Must build indexes (i.e. no member should have
  :data:`~replSetGetConfig.members[n].buildIndexes` setting set to
  false).

Set up Config Servers for a New Sharded Cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#. Start all the config servers with both the ``--configsvr`` and
   ``--replSet <name>`` options:

   .. code-block:: javascript

      mongod --configsvr --replSet configReplSet --port <port> --dbpath <path>

   Or if using a :doc:`configuration file
   </reference/configuration-options>`, include the
   :setting:`sharding.clusterRole` and
   :setting:`replication.replSetName` setting:

   .. code-block:: yaml

      sharding:
         clusterRole: configsvr
      replication:
         replSetName: configReplSet
      net:
         port: <port>
      storage:
         dbpath: <path>

#. Connect a :program:`mongo` shell to one of the config servers and
   run :method:`rs.initiate()` to initiate the replica set. You must
   specify :rsconf:`configsvr: true <configsvr>` in the initial replica
   set configuration document.

   .. code-block:: javascript

      rs.initiate( {
         _id: "configReplSet",
         configsvr: true,
         members: [ 
            { _id: 0, host: "<host1>:<port1>" },
            { _id: 1, host: "<host2>:<port2>" }
            { _id: 2, host: "<host3>:<port3>" }
         ]
      } )

#. Start one or more :program:`mongos` instances. For the
   ``--configdb`` option, specify the config server replica set name
   followed by the config server hostnames and ports:

   .. code-block:: sh

      mongos --configdb configReplSet/<cfgsvr1:port1>,<cfgsvr2:port2>,<cfgsvr3:port3>

#. To add shards to the sharded cluster, see
   :ref:`sharding-setup-add-shards`.

#. To enable sharding for a database, see
   :ref:`sharding-setup-enable-sharding`.

#. To shard a collection, see :ref:`sharding-setup-shard-collection`.

.. _3.2-rel-notes-readConcern:

``readConcern``
---------------

For the WiredTiger storage engine, MongoDB 3.2 introduces the
``readConcern`` option for replica sets and replica set shards. The
``readConcern`` option allows clients to choose a level of isolation
for their reads. You can specify a ``readConcern`` of ``"majority"`` to
read data that has been written to a majority of nodes and thus cannot
be rolled back. By default, MongoDB uses a ``readConcern`` of
``"local"`` which does not guarantee that the read data would not be
rolled back.

Considerations
~~~~~~~~~~~~~~

``readConcern`` requires MongoDB drivers updated for 3.2.

To use ``readConcern`` level of ``"majority"``, you must start the
:program:`mongod` instances with ``--enableMajorityReadConcern``
command line option or, if using a configuration file, the
``replication.enableMajorityReadConcern`` setting.

``readConcern`` Option
~~~~~~~~~~~~~~~~~~~~~~

The ``readConcern`` option takes a document that can specify the
``level`` of ``readConcern``.

.. code-block:: javascript

   readConcern: { level: <majority|local> }

``level`` Field
````````````````

Specify either the string ``"majority"`` or ``"local"``.

.. list-table:: 
   :header-rows: 1
   :widths: 10 90

   * - ``level``
     - Description

   * - ``"local"``

     - Default. The query will return the node's most recent copy of
       data. Provides no guarantee that the data has been written to a
       majority of the nodes.

   * - ``"majority"``

     - The query will return the node's most recent copy of the data
       confirmed as having been written to a majority of the nodes.

.. COMMENT OUT but add to final place for readConcern
   The most recent data on a node, regardless of the level, may not
   reflect the most recent version of the data in the system.


Operations That Support ``readConcern``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ``readConcern`` is available for the following operations:

.. list-table::
   :header-rows: 1

   * - Operation
     - Example

   * - :dbcommand:`find`

     - The :dbcommand:`getMore` command uses the ``readConcern`` level
       specified in the originating ``find`` command.

       The :program:`mongo` shell :method:`~db.collection.find()`
       method does not support ``readConcern``; however,
       drivers updated for 3.2 may provide support for
       ``readConcern`` in their respective ``find`` method. Refer to
       your specific driver for details.

       .. code-block:: javascript

          db.runCommand(
             {
               find: "restaurants",
               filter: { rating: { $lt: 5 } },
               readConcern: { level: "majority" }
             }
          )

   * - :method:`~db.collection.aggregate()`

     - To use the ``readConcern`` level of ``"majority"`` with
       :method:`db.collection.aggregate()`, you cannot include the
       :pipeline:`$out` stage.

       .. code-block:: javascript

          db.restaurants.aggregate(
             [ { $match: { rating: { $lt: 5 } } } ],
             { readConcern: { level: "majority" } }
          )

   * - :dbcommand:`aggregate`

     - To use the ``readConcern`` level of ``"majority"`` with
       :dbcommand:`aggregate`, you cannot include the :pipeline:`$out`
       stage.

       The :dbcommand:`getMore` command uses the ``readConcern`` level
       specified in the originating :dbcommand:`aggregate` command.

       .. code-block:: javascript

          db.runCommand(
             {
               aggregate: "restaurants",
               pipeline: [ { $match: { rating: { $lt: 5 } } } ],
               readConcern: { level: "majority" }
             }
          )

   * - :dbcommand:`distinct`

     - .. code-block:: javascript

          db.runCommand(
             {
                distinct: "restaurants",
                key: "rating",
                query: { cuisine: "italian" },
                readConcern: { level: "majority" }
             }
          )

   * - :dbcommand:`count`

     - To use the ``readConcern`` level of ``"majority"`` with
       :dbcommand:`count`, you must specify a nonempty ``query``
       condition.

       .. code-block:: javascript

          db.runCommand(
             {
               count: "restaurants",
               query: { rating: { $gte: 4 } },
               readConcern: { level: "majority" }
             }
          )

   * - :dbcommand:`explain`

     - .. code-block:: javascript

          db.runCommand(
             {
               explain: { count: "products", query: { quantity: { $gt: 50 } } },
               verbosity: "queryPlanner",
               readConcern: { level: "majority" } 
             }
          )

   * - :dbcommand:`parallelCollectionScan`

     - .. code-block:: javascript

          db.runCommand(
             {
               parallelCollectionScan: "restaurants",
               numCursors: 5,
               readConcern: { level: "majority" } 
             }
          )

   * - :dbcommand:`geoNear`

     - .. code-block:: javascript

          db.runCommand(
             {
               geoNear: "restaurants",
               near: { type: "Point", coordinates: [ -73.9667, 40.78 ] },
               spherical: true,
               query: {cuisine: "coffee"},
               readConcern: { level: "majority" }
             }
          )

   * - :dbcommand:`geoSearch`

     - .. code-block:: javascript

          db.runCommand(
             {
               geoNear: "restaurants",
               near: { type: "Point", coordinates: [ -73.9667, 40.78 ] },
               spherical: true,
               cuisine: "bakery",
               readConcern: { level: "majority" }
             }
          )

Partial Indexes
---------------

MongoDB 3.2 provides the option to create indexes that only index
the documents in a collection that meet a specified filter expression.
By indexing a subset of the documents in a collection, partial indexes
have lower storage requirements and reduced performance costs for index
creation and maintenance.

Create a Partial Index
~~~~~~~~~~~~~~~~~~~~~~

To create a partial index, use the
:method:`db.collection.createIndex()` method with the new
``partialFilterExpression`` option.

The following example creates a compound index that indexes only the
documents with the ``rating`` field greater than 5.

.. code-block:: javascript

   db.restaurants.createIndex(
      { cuisine: 1, name: 1 },
      { partialFilterExpression: { rating: { $gt: 5 } } }
   )

``partialFilterExpression`` Option
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can specify a ``partialFilterExpression`` option for all MongoDB
:doc:`index types </core/index-types>`.

The ``partialFilterExpression`` accepts a document that specifies the
condition using:

- equality expressions (i.e. ``field: value`` or using the :query:`$eq`
  operator),

- :query:`$exists: true <$exists>` expression,

- :query:`$gt`, :query:`$gte`, :query:`$lt`, :query:`$lte` expressions,

- :query:`$type` expressions,

- :query:`$and` operator at the top-level only

Restrictions
~~~~~~~~~~~~

- MongoDB will not use the partial index if the index results in an
  incomplete result set for the query or sort operation.

  To use the partial index, a query **must** contain the filter
  expression (or a modified filter expression that specifies a subset
  of the filter expression) as part of its query condition.

  For example, given the following index:

  .. code-block:: javascript
  
     db.restaurants.createIndex(
        { cuisine: 1, name: 1 },
        { partialFilterExpression: { rating: { $gt: 5 } } }
     )

  The following queries can use the index since the query predicates
  includes a modified filter expressions ``rating: 6`` and ``rating: {
  $gte: 8 }`` that are subsets of the filter expression ``ratings: {
  $gt: 5 }``:

  .. code-block:: javascript

     db.restaurants.find( { rating: 6 } )
     db.restaurants.find( { cuisine: "Italian", rating: { $gte: 8 } } )

  However, the following queries cannot use the partial index:

  .. code-block:: javascript

     db.restaurants.find( { rating: { $lt: 8 } } )
     db.restaurants.find( { cuisine: "Italian" } )

- In MongoDB, you cannot create multiple versions of an index that
  differ only in the options. As such, you cannot create multiple
  partial indexes that differ only by the filter expression.

- You cannot specify both the ``partialFilterExpression`` option and
  the ``sparse`` option.

- Earlier versions of MongoDB do not support partial indexes. If using
  sharded clusters or replica set, all nodes must be version 3.2.

- ``_id`` indexes cannot be partial indexes.

- Shard key indexes cannot be partial indexes.

Comparison with the ``sparse`` Index
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. tip::
   Partial indexes represent a superset of the functionality offered by
   sparse indexes and should be preferred over sparse indexes.

Partial indexes offer a more expressive mechanism than
:doc:`/core/index-sparse` indexes to determine which documents are
indexed.

Sparse indexes selects documents to index *solely* based on the
existence of the indexed field, or for compound indexes, the existence
the indexed fields.

Partial indexes determine the index entries based on the specified
filter. The filter can include fields other than the index keys and
specify conditions other than just an existence check. For example,
a partial index can implement the same behavior as a sparse index:

.. code-block:: javascript

   db.contacts.createIndex( 
      { name: 1 }, 
      { partialFilterExpression: { name: { $exists: true } } } 
   )

This partial index supports the same queries as a sparse index on the
``name`` field.

However, a partial index can also specify filter expressions on fields
other than the index key. For example, the following operation creates
a partial index, where the index is on the ``name`` field but the
filter expression is on the ``email`` field:

.. code-block:: javascript

   db.contacts.createIndex(
      { name: 1 },
      { partialFilterExpression: { email: { $exists: true } } }
   )

For the query optimizer to choose this partial index, the query
predicate must include a non-null match on the ``email`` field as well
as a condition on the ``name`` field.

For example, the following query can use the index:

.. code-block:: javascript

   db.contacts.find( { name: "xyz", email: { $regex: /\.org$/ } } )

However, the following query cannot use the index:

.. code-block:: javascript

   db.contacts.find( { name: "xyz", email: { $exists: false } }

.. _3.2-rel-notes-document-validation:

Document Validation
-------------------

Starting in 3.2, MongoDB provides the capability to validate
documents during updates and insertions. Validation rules are specified
on a per-collection basis. MongoDB provides the following new options:

- ``validator`` option to specify validation rules.

- :ref:`3.2-rel-notes-validationLevel` option to determine how strictly
  MongoDB applies the validation rules to existing documents during an
  update.

- :ref:`3.2-rel-notes-validationAction` option to determine whether to
  ``error`` and reject documents that violate the validation rules, or
  ``warn`` about the violations in the log but allow invalid documents.

To create a validation specification, include the new ``validator``
option when creating a collection using
:method:`db.createCollection()`, or modifying the behavior of a
collection using :dbcommand:`collMod`. You can also include the
optional ``validationLevel`` and ``validationAction`` fields. By
default, MongoDB uses ``strict`` validation level and ``error``
validation action.

.. note::

   Validation occurs during updates and inserts; i.e. existing documents
   do not undergo validation checks until modification.

``validator``
~~~~~~~~~~~~~

The ``validator`` option takes a document that specifies the validation
rules or expressions. You can specify the expressions using the same
operators as the :ref:`query operators <query-selectors>` with the
exception of the following operators: :query:`$geoNear`,
:query:`$near`, :query:`$nearSphere`, :query:`$text`, :query:`$where`.

Add Validation to a New Collection
``````````````````````````````````

The following example creates a ``contacts`` collection with a validator
that specifies that inserted or updated documents should match at least
one of three following conditions:

- the ``phone`` field is a string
- the ``email`` field matches the regular expression
- the ``status`` field is either ``Unknown`` or ``Incomplete``.

.. code-block:: javascript

   db.createCollection( "contacts", {
      validator: { $or: 
         [
            { phone: { $type: "string" } },
            { email: { $regex: /@mongodb\.com$/ } },
            { status: { $in: [ "Unknown", "Incomplete" ] } }
         ]
      }
   } )

With the validator in place, the following insert operation will fail
validation:

.. code-block:: javascript

   db.contacts.insert( { name: "xyz", status: "A" } )

The method returns the error in the ``WriteResult``:

.. code-block:: yaml

   WriteResult({
      "nInserted" : 0,
      "writeError" : {
         "code" : 121,
         "errmsg" : "Document failed validation"
      }
   })

Add Validation to an Existing Collection
````````````````````````````````````````

To modify the validation rules for an existing collection, use the
:dbcommand:`collMod` command with the ``validator`` option:

.. code-block:: javascript

   db.runCommand( { 
      collMod: "contacts",
      validator: { $or: [ { phone: { $exists: true } }, { email: { $exists: true } } ] }
   } )

.. note::

   Validation occur during updates and inserts; i.e. existing documents
   do not undergo validation checks until modification.

View Validation Level for a Collection
``````````````````````````````````````

To view the validation specifications for a collection, use the
:method:`db.getCollectionInfos( )` method:

.. code-block:: javascript

   db.getCollectionInfos( { name: "contacts" } )

The method returns the following array with a document that contains
information about the ``contacts`` collection:

.. code-block:: javascript

   [
      {
         "name" : "contacts",
         "options" : {
            "validator" : {
               "$or" : [
                  {
                     "phone" : {
                        "$exists" : true
                     }
                  },
                  {
                     "email" : {
                        "$exists" : true
                     }
                  }
               ]
            }
         }
      }
   ]

The method does not return ``validationLevel`` and ``validationAction``
unless they are explicitly set.

Validation Restrictions
```````````````````````

- You cannot specify a validator for collections in the ``admin``,
  ``local``, and ``config`` databases.

- You cannot specify a validator for ``system.*`` collections.

Bypass Validation
`````````````````

By passing a new option ``bypassDocumentValidation``, the following
commands can bypass validation per operation:

- :dbcommand:`applyOps` command

- :dbcommand:`copydb` command

- :dbcommand:`findAndModify` command and
  :method:`db.collection.findAndModify()` method

- :dbcommand:`mapReduce` command and
  :method:`db.collection.mapReduce()` method

- :dbcommand:`insert` command

- :dbcommand:`update` command

- :pipeline:`$out` for the :dbcommand:`aggregate` command and
  :method:`db.collection.aggregate()` method

.. _3.2-rel-notes-validationLevel:

``validationLevel``
~~~~~~~~~~~~~~~~~~~

The ``validationLevel`` determines how strictly MongoDB applies the
validation rules to existing documents during an update.

.. list-table::
   :header-rows: 1
   :widths: 20 80
   
   * - validationLevel
     - Description

   * - ``"off"``

     - No validation for inserts or updates.

   * - ``"strict"``

     - The default level. Apply validation rules to all inserts and all
       updates.

   * - ``"moderate"``

     - Apply validation rules to inserts and to updates on existing
       *valid* documents. Do not apply rules to updates on existing
       *invalid* documents.

.. _3.2-rel-notes-validationAction:

``validationAction``
~~~~~~~~~~~~~~~~~~~~

The ``validationAction`` option determines whether to ``error`` on
invalid documents or just ``warn`` about the violations but allow
invalid documents.

.. important::

   Validation of documents only applies to those documents as
   determined by the ``validationLevel``.

.. list-table::
   :header-rows: 1
   :widths: 20 80
   
   * - validationAction
     - Description

   * - ``"error"``

     - Documents must pass validation before the write occurs.
       Otherwise, the write operation fails.

   * - ``"warn"``

     - Documents do not have to pass validation. If the document fails
       validation, the write operation logs the validation failure.

Aggregation Framework Enhancements
----------------------------------

MongoDB introduces:

- New stages, accumulators, and expressions.

- :ref:`Availability of accumulator expressions
  <3.2-agg-accumulator-availability>` in :pipeline:`$project` stage.

- :ref:`Performance improvements <3.2-agg-shard-optimization>` on
  sharded clusters.

New Aggregation Stages
~~~~~~~~~~~~~~~~~~~~~~

.. list-table::
   :header-rows: 1
   :widths: 10 40 50

   * - Stage
     - Description
     - Syntax

   * - :pipeline:`$sample`
     - Randomly selects N documents from its input.
     - ``{ $sample: { size: <positive integer> } }``

   * - :pipeline:`$indexStats`
     - Returns statistics on index usage.
     - ``{ $indexStats: { } }``

   * - :pipeline:`$lookup`

     - Performs a left outer join with another collection.

     - .. code-block:: none

          {
             $lookup:
               {
                 from: <collection to join>,
                 localField: <fieldA>,
                 foreignField: <fieldB>,
                 as: <output array field>
               }
          }

New Accumulators for ``$group`` Stage
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. list-table::
   :header-rows: 1
   :widths: 10 40 50

   * - Accumulator
     - Description
     - Syntax

   * - :group:`$stdDevSamp`
     - Calculates standard deviation.
     - ``{ stdDevSamp: <array> }``

   * - :group:`$stdDevPop`
     - Calculates population standard deviation.
     - ``{ stdDevPop: <array> }``

New Aggregation Arithmetic Operators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. list-table::
   :header-rows: 1
   :widths: 10 40 50

   * - Operator
     - Description
     - Syntax

   * - :expression:`$sqrt`
     - Calculates the square root.
     - ``{ $sqrt: <number> }``

   * - :expression:`$abs`
     - Returns the absolute value of a number.
     - ``{ $abs: <number> }``

   * - :expression:`$log`
     - Calculates the log of a number in the specified base.
     - ``{ $log: [ <number>, <base> ] }``

   * - :expression:`$log10`
     - Calculates the log base 10 of a number.
     - ``{ $log10: <number> }``

   * - :expression:`$ln`
     - Calculates the natural log of a number.
     - ``{ $ln: <number> }``

   * - :expression:`$pow`
     - Raises a number to the specified exponent.
     - ``{ $pow: [ <number>, <exponent> ] }``

   * - :expression:`$exp`
     - Raises *e* to the specified exponent.
     - ``{ exp: <number> }``

   * - :expression:`$trunc`
     - Truncates a number to its integer.
     - ``{ $trunc: <number> }``

   * - :expression:`$ceil`

     - Returns the smallest integer greater than or equal to the
       specified number.

     - ``{ $ceil: <number> }``

   * - :expression:`$floor`
     - Returns the largest integer less than or equal to the specified
       number.
     - ``{ floor: <number> }``

New Aggregation Array Operators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. list-table::
   :header-rows: 1
   :widths: 10 35 55

   * - Operator
     - Description
     - Syntax

   * - :expression:`$slice`
     - Returns a subset of an array.
     - ``{ $slice: [ <array>, <n> ] }``

       or

       ``{ $slice: [ <array>, <position>, <n> ] }``

   * - :expression:`$arrayElemAt`
     - Returns the element at the specified array index.
     - ``{ $arrayElemAt: [ <array>, <idx> ] }``

   * - :expression:`$concatArrays`
     - Concatenates arrays.
     - .. code-block:: javascript

          {
            $concatArrays: [ <array1>, <array2>, ... ]
          }

   * - :expression:`$isArray`
     - Determines if the operand is an array.
     - ``{ $isArray: [ <expression> ] }``

   * - :expression:`$filter`
     - Selects a subset of the array based on the condition.
     - .. code-block:: javascript

          { 
            $filter:
              {
                input: <array>, 
                as: <string>, 
                cond: <expression>
              }
          }

.. _3.2-agg-accumulator-availability:

Accumulator Expression Availability
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Starting in version 3.2, the following accumulator expressions,
previously only available in the :pipeline:`$group` stage, are now also
available in the :pipeline:`$project` stage:

- :group:`$avg`
- :group:`$min`
- :group:`$max`
- :group:`$sum`
- :group:`$stdDevPop`
- :group:`$stdDevSamp`

When used as part of the :pipeline:`$project` stage, these accumulator
expressions can accept either:

- A single argument: ``<accumulator> : <arg>``

- Multiple arguments: ``<accumulator> : [ <arg1>, <arg2>, ... ]``

Single Argument: ``<arg>``
``````````````````````````

If the argument is an array, the accumulator operates on the elements
of the array to return a single value. If the argument is not an array,
the accumulator operates on the single argument.

For example, given a collection ``examples`` with the following two
documents:

.. code-block:: javascript

   { _id: 1, a: [ 1, 19, 29, 100 ] }
   { _id: 2, a: 5 }

The following aggregation operation uses ``$sum`` in the
:pipeline:`$project` stage to return the sum of the field ``a`` for
each document:

.. code-block:: javascript

   db.examples.aggregate([ 
      { $project: { sumOfa: { $sum: "$a" } } }
   ])

The operation returns the following documents:

.. code-block:: javascript

   { "_id" : 1, "sumOfa" : 149 }
   { "_id" : 2, "sumOfa" : 5 }

Multiple arguments: ``[ <arg1>, <arg2>, ... ]``
```````````````````````````````````````````````

The accumulator operates on the arguments to return a single value.

If any of the arguments is an array, the accumulator does not traverse
into the array elements.

For example, given a collection ``examples`` with the following two
documents:

.. code-block:: javascript

   { _id: 1, a: [ [ 10, 5 ], 10 ], b: [ 5, 10 ] }
   { _id: 2, a: 10, b: [ [ [ 2, 3 ], 10 ], 15, 5 ] }

The following aggregation operation uses ``$sum`` in the
:pipeline:`$project` stage to return the sum of the field ``a`` for
each document:

.. code-block:: javascript

   db.examples.aggregate([ 
      { $project: { sumAandB: { $sum: [ "$a", "$b" ] } } }
   ])

The operation returns the following documents:

.. code-block:: javascript

   { "_id" : 1, "sumAandB" : 0 }
   { "_id" : 2, "sumAandB" : 10 }

Non-numerical Arguments
```````````````````````

When used as part of the :pipeline:`$project` stage, :group:`$sum`,
:group:`$avg`, :group:`$stdDevSamp`, and :group:`$stdDevPop`
accumulators ignore non-numerical arguments. :group:`$min` and
:group:`$max` uses the :ref:`specified BSON comparison order
<bson-types-comparison-order>` for values of different types.

For example, given a collection ``examples`` with the following two
documents:

.. code-block:: javascript

   { _id: 1, a: [ 1, 19, 29, 100 ] }
   { _id: 2, a: [ -999, 3.4, "hello", 872 ] }

The following aggregation operation uses the :group:`$max` and
:group:`$sum` accumulators in the :pipeline:`$project` stage to
calculate the maximum and the sum of the values in the array ``a``.
:group:`$sum` ignores the non-numerical elements in the array:

.. code-block:: javascript

   db.examples.aggregate([ 
      { $project: { max: { $max:"$a" }, sum: { $sum: "$a" } } }
   ])

The operation returns the following documents:

.. code-block:: javascript

   { "_id" : 1, "max" : 100, "sum" : 149 }
   { "_id" : 2, "max" : "hello", "sum" : -123.60000000000002 }

General Enhancements
~~~~~~~~~~~~~~~~~~~~

``$project`` New Arrays
```````````````````````

In MongoDB 3.2, :pipeline:`$project` stage supports using the
square brackets ``[]`` to directly create new array fields.

For example, if a collection includes the following document:

.. code-block:: javascript

   { "_id" : ObjectId("55ad167f320c6be244eb3b95"), "x" : 1, "y" : 1 }

The following operation projects the fields ``x`` and ``y`` as elements
in a new field ``myArray``:

.. code-block:: javascript

   db.collection.aggregate( [ { $project: { myArray: [ "$x", "$y" ] } } ] )

The operation returns the following document:

.. code-block:: javascript

   { "_id" : ObjectId("55ad167f320c6be244eb3b95"), "myArray" : [ 1, 1 ] }

If array specification includes fields that are non-existent in a
document, the operation substitutes ``null`` as the value for that
field.

For example, given the same document as above, the following operation
projects the fields ``x``, ``y``, and a non-existing field
``$someField`` as elements in a new field ``myArray``:

.. code-block:: javascript

   db.collection.aggregate( [ { $project: { myArray: [ "$x", "$y", "$someField" ] } } ] )

The operation returns the following document:

.. code-block:: javascript

   { "_id" : ObjectId("55ad167f320c6be244eb3b95"), "myArray" : [ 1, 1, null ] }

``minDistance`` for ``$geoNear``
`````````````````````````````````

MongoDB 3.2 introduces the ``minDistance`` option for the
:pipeline:`$geoNear` stage. Use the new option to specify the minimum
distance from the center point that the documents can be. MongoDB
limits the results to those documents that fall outside the specified
distance from the center point. 

Specify the distance in meters for GeoJSON data and in radians for
legacy coordinate pairs. For example:

.. code-block:: javascript

   db.places.aggregate([ 
      {
        $geoNear: {
           near: { type: "Point", coordinates: [ -73.99279 , 40.719296 ] },
           distanceField: "dist.calculated",
           minDistance: 2,
           query: { type: "public" },
           includeLocs: "dist.location",
           num: 5,
           spherical: true
        }
      }
   ])

``$unwind`` Enhancements
````````````````````````

:pipeline:`$unwind` stage no longer errors on non-array operand. If the
operand does not resolve to an array but is not missing, null, or an
empty array, :pipeline:`$unwind` treats the operand as a single element
array.

In addition to the existing syntax for :pipeline:`$unwind` stage ``{
$unwind: <field path> }``, :pipeline:`$unwind` can take an alternate
syntax:

.. code-block:: javascript

   { $unwind:
      {
        path: <field path>,
        includeArrayIndex: <boolean>,
        preserveNullAndEmptyArrays: <boolean>
      }
   }

.. list-table::
   :header-rows: 1
   :widths: 30 10 60

   * - Field
     - Type
     - Description

   * - ``path``

     - String

     - Field path to an array field. To specify a field path, prefix
       the field name with a dollar sign ``$`` and enclose in quotes.

   * - ``includeArrayIndex``

     - Boolean

     - Optional. If ``true``, for the document output for the i\
       :sup:`th` element in the array ``path``, the array is replaced
       by the document ``{index: i, value: <ith element> }``. If the
       ``path`` does not resolve to an array, :pipeline:`$unwind` uses
       the default behavior to replace the field by the value.

       If ``false``, :pipeline:`$unwind` replaces the array in the
       ``path`` field with the element in the array.

       The default value is ``false``.

   * - ``preserveNullAndEmptyArrays``

     - Boolean

     - Optional. If ``true``, if the ``path`` is null, missing, or an
       empty array, :pipeline:`$unwind` outputs the document unmodified.
       If ``false``, :pipeline:`$unwind` does not output a document if the
       ``path`` is null, missing, or an empty array.

       The default value is ``false``.

For example, consider a collection ``inventory`` with the following
documents:

.. code-block:: javascript

   { "_id" : 1, "item" : "shirt", "colors" : [ "blue", "red" ] }
   { "_id" : 2, "item" : "cable", "colors" : [ ] }
   { "_id" : 3, "item" : "chocolate", "colors" : "dark" }
   { "_id" : 4, "item" : "candy" }
   { "_id" : 5, "item" : "chocolate", "colors" : null }

The following :pipeline:`$unwind` operations are equivalent and return
a document for each element in the ``colors`` field. If the ``colors``
field does not resolve to an array but is not missing, null, or an
empty array, :pipeline:`$unwind` treats the non-array operand as a
single element array.

.. code-block:: javascript

   db.inventory.aggregate( [ { $unwind: "$colors" } ] )
   db.inventory.aggregate( [ { $unwind: { path: "$colors" } } ] )

The operation returns the following documents:

.. code-block:: javascript

   { "_id" : 1, "item" : "shirt", "colors" : "blue" }
   { "_id" : 1, "item" : "shirt", "colors" : "red" }
   { "_id" : 3, "item" : "chocolate", "colors" : "dark" }

The following :pipeline:`$unwind` operation uses the
``includeArrayIndex`` option to output also the array index of the
array element.

.. code-block:: javascript

   db.inventory.aggregate( [ { $unwind: { path: "$colors", includeArrayIndex: true } } ] )

The operation replaces the ``colors`` field with a document that
contains a ``value`` field for the element and ``index`` field for the
array index of the element. If the ``colors`` field does not resolve to
an array but is not missing, null, or an empty array, the
``includeArrayIndex`` has no effect.

.. code-block:: javascript

   { "_id" : 1, "item" : "shirt", "colors" : { "index" : NumberLong(0), "value" : "blue" } }
   { "_id" : 1, "item" : "shirt", "colors" : { "index" : NumberLong(1), "value" : "red" } }
   { "_id" : 3, "item" : "chocolate", "colors" : "dark" }

The following :pipeline:`$unwind` operation uses the
``preserveNullAndEmptyArrays`` option to include in the output those
documents where ``colors`` field is missing, null or an empty array.

.. code-block:: javascript

   db.inventory.aggregate( [ 
      { $unwind: { path: "$colors", preserveNullAndEmptyArrays: true } }
   ] )

In addition to unwinding the documents where the ``colors`` is an array
of elements or a non-null, non-array field, the operation outputs, without
modification, those documents where the ``colors`` field is missing, null
or an empty array:

.. code-block:: javascript

   { "_id" : 1, "item" : "shirt", "colors" : "blue" }
   { "_id" : 1, "item" : "shirt", "colors" : "red" }
   { "_id" : 2, "item" : "cable", "colors" : [ ] }
   { "_id" : 3, "item" : "chocolate", "colors" : "dark" }
   { "_id" : 4, "item" : "candy" }
   { "_id" : 5, "item" : "chocolate", "colors" : null }

.. _3.2-agg-shard-optimization:

Optimization
~~~~~~~~~~~~

MongoDB improves the overall performance of the pipeline
in large sharded clusters.

If the pipeline starts with an exact :pipeline:`$match` on a shard key,
the entire pipeline runs on the matching shard only. Previously, the
pipeline would have been split, and the work of merging it would have
to be done on the primary shard.

For aggregation operations that run on multiple shards, if the
operations do not require running on the database's primary shard,
these operations can route the results to any shard to merge the
results and avoid overloading the primary shard for that database.
Aggregation operations that require running on the database's primary
shard are the :pipeline:`$out` stage and :pipeline:`$lookup` stage.

Compatibility
~~~~~~~~~~~~~

For compatibility changes, see :ref:`3.2-agg-compatibility`.

MongoDB Tools Enhancements
--------------------------

``mongodump`` and ``mongorestore`` Support Archive Files and ``stdout/in``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With the new ``--archive`` option, :program:`mongodump` and
:program:`mongorestore` add support for archive files and standard
output/input streams. This enhancement allows for the streaming of the
dump data over a network device via a pipe.

``mongodump``
   With the new ``--archive`` option, :program:`mongodump` can write the
   output to a single archive file or to the standard output (``stdout``).

   To output the dump to an archive file, run :program:`mongodump` with
   the new ``--archive`` option and the archive filename. For example, the
   following operation creates a file ``test.20150715.archive`` that
   contains the dump of the ``test`` database.

   .. code-block:: sh

      mongodump --archive=test.20150715.archive --db test

   To output the dump to the standard output stream in order to pipe to
   another process, run :program:`mongodump` with the ``archive``
   option but *omit* the filename:

   .. code-block:: sh

      mongodump --archive --db test --port 27017 | mongorestore --archive --port 27018

   .. note::

      You cannot use the ``--archive`` option with the
      :option:`--out` option.

``mongorestore``
   With the new ``--archive`` option, :program:`mongorestore` can restore
   from an archive file or from the standard input (``stdin``).

   To restore from an archive file, run :program:`restore` with the new
   ``--archive`` option and the archive filename. For example, the
   following operation restores the ``test`` database from the file
   ``test.20150715.archive``.

   .. code-block:: sh

      mongorestore --archive=test.20150715.archive --db test

   To restore full data dump from the standard input, run
   :program:`mongorestore` with the ``archive`` option but *omit* the
   filename:

   .. code-block:: sh

      mongodump --archive --db test --port 27017 | mongorestore --archive --port 27018

   .. note::

      - You cannot use the ``--archive`` option with the ``--dir`` option.

      - :program:`mongorestore` still supports the positional ``-`` parameter
        to restore a *single* collection from the standard input.

``mongodump`` and ``mongorestore`` Support Compressed Data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With the new ``--gzip`` option, :program:`mongodump` and
:program:`mongorestore` add support for compressed data dumps. This
enhancement reduces storage space for the dump files.


``mongodump``
   With the new ``--gzip`` option, :program:`mongodump` can compress
   its output. If :program:`mongodump` outputs to the dump directory,
   the new feature compresses the individual files. The files have the
   suffix ``.gz``.
   
   If :program:`mongodump` outputs to an archive file or the standard
   out stream, the new feature compresses the archive file or the data
   output to the stream.

   To compress the files in the output dump directory, run
   :program:`mongodump` with the new ``--gzip`` option. For example,
   the following operation outputs compressed files into the default
   ``dump`` directory.

   .. code-block:: sh

      mongodump --gzip --db test

   To compress the archive file output by :program:`mongodump`, use the
   ``--gzip`` option in conjunction with the :option:`--archive`
   option, specifying the name of the compressed file.

   .. code-block:: sh

      mongodump --archive=test.20150715.gz --gzip --db test

``mongorestore``
   With the new ``--gzip`` option, :program:`mongorestore` can restore
   from compressed files or data stream created by :program:`mongodump`.

   To restore from a dump directory that contains compressed files, run
   :program:`restore` with the new ``--gzip`` option. For example, the
   following operation restores the ``test`` database from the
   compressed files located in the default ``dump`` directory:

   .. code-block:: sh

      mongorestore --gzip --db test

   To restore from a compressed archive file, run :program:`restore`
   with the ``--gzip`` option in conjunction with the new ``--archive``
   option, specifying the name of the compressed filename. For example,
   the following operation restores the ``test`` database from the file
   ``test.20150715.gz``.

   .. code-block:: sh

      mongorestore --gzip --archive=test.20150715.gz --db test

.. _3.2-rel-notes-encryption:

Encrypted Storage Engine
------------------------

.. include:: /includes/fact-enterprise-only-admonition.rst

.. important:: Available for the WiredTiger Storage Engine only.

Encryption at rest, when used in conjunction with transport encryption
and good security policies that protect relevant accounts, passwords,
and encryption keys, can help ensure compliance with security and
privacy standards, including HIPAA, PCI-DSS, and FERPA.

MongoDB Enterprise 3.2 introduces a native encryption option for the
WiredTiger storage engine. This feature allows MongoDB to encrypt data
files such that only parties with the decryption key can decode and
read the data.

Encryption Process
~~~~~~~~~~~~~~~~~~

If encryption is enabled, the default encryption mode that MongoDB
Enterprise uses is the ``AES256-CBC`` (or 256-bit Advanced Encryption
Standard in Cipher Block Chaining mode) via OpenSSL. AES-256 uses a
symmetric key; i.e. the same key to encrypt and decrypt text. MongoDB
Enterprise also supports authenticated encryption ``AES256-GCM`` (or
256-bit Advanced Encryption Standard in Galois/Counter Mode). FIPS mode
encryption is also available.

The data encryption includes:

- Generating an system key.

- Generating keys for each database.

- Encrypting data with the database keys.

- Encrypting the database keys with the system key.

The encryption occur transparently in the storage layer; i.e. all data
files are fully encrypted from a filesystem perspective, and data only
exists in an unencrypted state in memory and during transmission.

To encrypt all of MongoDB's network traffic, you can use TLS/SSL
(Transport Layer Security/Secure Sockets Layer). See
:doc:`/tutorial/configure-ssl` and
:doc:`/tutorial/configure-ssl-clients`.

Key Management
~~~~~~~~~~~~~~

.. important:: Secure management of the encryption keys is critical.

The database keys are internal to the server and are only paged to disk
in an encrypted format. MongoDB never pages the system key to disk
under any circumstances.

Only the system key is external to the server (i.e. kept separate from
the data and the database keys), and requires external management. To
manage the system key, MongoDB's encrypted storage engine supports two
key management options:

- Integration with a third party key management appliance via the Key
  Management Interoperability Protocol (KMIP). **Recommended**

- Local key management via a keyfile.

To configure MongoDB for encryption and use one of the two key
management options, see
:doc:`/tutorial/manage-encryption-key`

Encryption and Replication
~~~~~~~~~~~~~~~~~~~~~~~~~~

Encryption is not a part of replication:

- System keys and database keys are not replicated, and 

- Data is not natively encrypted over the wire.

Although you could reuse the same key for the nodes, MongoDB recommends
the use of individual keys for each node as well as the use of
transport encryption.

.. seealso:: :ref:`rotate-encryption-keys`

Deprecation of 32-bit Binaries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Starting in 3.2, 32-bit binaries are deprecated and will be
unavailable in future releases.

.. _3.2-geo-enhancements:

General Enhancements
--------------------

.. _3.2-diagnostic-data-capture:

Diagnostic Data Capture
~~~~~~~~~~~~~~~~~~~~~~~

To facilitate analysis of the MongoDB server behavior by MongoDB
engineers, MongoDB 3.2 introduces a diagnostic data collection
mechanism for logging server statistics to diagnostic files at periodic
intervals. By default, the mechanism captures data at 1 second
intervals. To modify the interval, see
:parameter:`diagnosticDataCollectionPeriodMillis`.

MongoDB creates a ``diagnostic.data`` directory under the
:program:`mongod` instance's ``--dbpath`` or :setting:`storage.dbPath`.
The diagnostic data is stored in files under this directory.

The maximum size of the diagnostic files is configurable with the
:parameter:`diagnosticDataCollectionFileSizeMB`, and the maximum size
of the ``diagnostic.data`` directory is configurable with
:parameter:`diagnosticDataCollectionDirectorySizeMB`.

The default values for the capture interval and the maximum sizes are
chosen to provide useful data to MongoDB engineers with minimal impact
on performance and storage size. Typically, these values will only need
modifications as requested by MongoDB engineers for specific diagnostic
purposes.

.. _3.2-relnotes-2dsphere-index:

Geospatial Optimization
~~~~~~~~~~~~~~~~~~~~~~~

MongoDB 3.2 introduces a version 3 of the :doc:`2dsphere
indexes </core/2dsphere>`, which indexes the :doc:`GeoJSON geometries
</reference/geojson>` at a finer gradation. The new version improves
performance of :doc:`2dsphere indexes </core/2dsphere>` queries over
smaller regions. In addition, for both :doc:`2d indexes </core/2d>` and
:doc:`2dsphere indexes </core/2dsphere>`, the performance of
Geo near queries has been improved for dense datasets.

.. seealso:: :ref:`3.2-2dsphere-index-compatibility`

Bit Test Query Operators
~~~~~~~~~~~~~~~~~~~~~~~~

MongoDB 3.2 provides new query operators to test bit values:

- :query:`$bitsAllSet`

- :query:`$bitsAllClear`

- :query:`$bitsAnySet`

- :query:`$bitsAnyClear`

SpiderMonkey JavaScript Engine
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

MongoDB 3.2 uses SpiderMonkey as the JavaScript engine for the
:program:`mongo` shell and :program:`mongod` server. SpiderMonkey
provides support for additional platforms and has an improved memory
management model.

This change affects all JavaScript behavior including the commands
:dbcommand:`mapReduce`, :dbcommand:`group`, and the query operator
:query:`$where`; *however*, this change should be completely
transparent to the user.

.. seealso:: :ref:`3.2-spidermonkey-compatibility`

``mongo`` Shell and CRUD API
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To provide consistency with the MongoDB drivers' CRUD
(Create/Read/Update/Delete) API, the :program:`mongo` shell introduces
additional CRUD methods that are consistent with the drivers' CRUD API:

.. list-table::
   :header-rows: 1

   * - New API
     - Description

   * - :method:`db.collection.bulkWrite()`

     - Equivalent to initializing :method:`Bulk()` operations builder,
       using :ref:`Bulk methods <bulk-methods>` to add operations, and
       running :method:`Bulk.execute()` to execute the operations.

       MongoDB 3.2 deprecates :method:`Bulk()` and its associated
       :doc:`methods </reference/method/js-bulk>`.

   * - :method:`db.collection.deleteMany()`
     - Equivalent to :method:`db.collection.remove()`.

   * - :method:`db.collection.deleteOne()`

     - Equivalent to :method:`db.collection.remove()` with the
       ``justOne`` set to true; i.e.

       :method:`db.collection.remove( \<query\>, true ) <db.collection.remove>` or
       :method:`db.collection.remove( \<query\>, { justOne: true } ) <db.collection.remove>`.

   * - :method:`db.collection.findOneAndDelete()`

     - Equivalent to :method:`db.collection.findAndModify()` method
       with ``remove`` set to true.
   
   * - :method:`db.collection.findOneAndReplace()`

     - Equivalent to :method:`db.collection.findAndModify()` method
       with ``update`` set to a replacement document.

   * - :method:`db.collection.findOneAndUpdate()`

     - Equivalent to :method:`db.collection.findAndModify()` method
       with ``update`` set to a document that specifies modifications
       using :doc:`update operators </reference/operator/update>`.

   * - :method:`db.collection.insertMany()`
     - Equivalent to :method:`db.collection.insert()` method
       with an array of documents as the parameter.

   * - :method:`db.collection.insertOne()`
     - Equivalent to :method:`db.collection.insert()` method
       with a single document as the parameter.

   * - :method:`db.collection.replaceOne()`

     - Equivalent to :method:`db.collection.update( \<query\>,
       \<update\> ) <db.collection.update()>` method with a replacement
       document as the ``<update>`` parameter.

   * - :method:`db.collection.updateMany()`

     - Equivalent to :method:`db.collection.update( \<query\>,
       \<update\>, { multi: true, ... }) <db.collection.update()>`
       method with an ``<update>`` document that specifies
       modifications using :doc:`update operators
       </reference/operator/update>` and the ``multi`` option set to
       true.

   * - :method:`db.collection.updateOne()`
   
     - Equivalent to :method:`db.collection.update( \<query\>,
       \<update\> ) <db.collection.update()>` method with an
       ``<update>`` document that specifies modifications using
       :doc:`update operators </reference/operator/update>`.

WiredTiger and ``fsyncLock``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Starting in MongoDB 3.2, the WiredTiger storage engine supports the
:dbcommand:`fsync` command with the ``lock`` option or the
:program:`mongo` shell method :method:`db.fsyncLock()`. That is, for
the WiredTiger storage engine, these operations can guarantee that the
data files do not change, ensuring consistency for the purposes of
creating backups.

.. comment:  Refer DOCS-4751 as well as an audit to see the pages to update in 3.2 manual. You must do an audit.

Text Search Enhancements
------------------------

.. _3.2-relnotes-text-index-v3:

``text`` Index Version 3
~~~~~~~~~~~~~~~~~~~~~~~~

MongoDB 3.2 introduces a version 3 of the :doc:`text index
</core/index-text>`. Key features of the new version of the index are:

- Improved :ref:`case insensitivity <text-index-case-insensitivity>`.

- :ref:`Diacritic insensitivity <text-index-diacritic-insensitivity>`.

- Additional :ref:`delimiters for tokenization
  <text-index-tokenization-delimiters>`.

Starting in MongoDB 3.2, version 3 is the default version for new
:doc:`text </core/index-text>`. indexes.

.. seealso:: :ref:`3.2-text-index-compatibility`

.. _3.2-relnotes-text-operator:

``$text`` Operator Enhancements
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :query:`$text` operator adds support for:

- :ref:`case sensitive text search <text-operator-case-sensitivity>`
  with the new ``$caseSensitive`` option, and
  
- :ref:`diacritic sensitive text search
  <text-operator-diacritic-sensitivity>` with the new
  ``$diacriticSensitive`` option.

For more information and examples, see the :query:`$text` operator
reference sections :ref:`text-operator-case-sensitivity` and
:ref:`text-operator-diacritic-sensitivity`.

Support for Additional Languages
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/fact-enterprise-only-admonition.rst

Starting in 3.2, MongoDB Enterprise provides support for the
following languages: Arabic, Farsi (specifically Dari and Iranian
Persian dialects), Urdu, Simplified Chinese, and Traditional Chinese.

For details, see :doc:`/tutorial/text-search-with-rlp`.

Changes Affecting Compatibility
-------------------------------

Some MongoDB 3.2 changes can affect compatibility and may require user
actions. For a detailed list of compatibility changes, see
:doc:`/release-notes/3.2-compatibility`.
