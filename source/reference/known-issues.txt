=================================
Known Issues in the |k8s-op-full|
=================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

.. _disable_auth_pods_never_reconcile:

``mongos`` Instances Fail to Reach Ready State After Disabling Authentication
-----------------------------------------------------------------------------

.. note::

   This issue applies only to :term:`sharded clusters <sharded cluster>`
   that meet the following criteria: 

   - Deployed using the |k8s-op-short| 1.13.0
   - Use X.509 authentication 
   - Use :k8sdocs:`kubernetes.io/tls
     </concepts/configuration/secret/#tls-secrets>` secrets for |tls|
     certificates for the MongoDB Agent

If you disable authentication by setting 
:setting:`spec.security.auth.enabled` to  ``false``, the |mongos| Pods 
never reach a ``ready`` state.

As a workaround, delete each |mongos| Pod in your deployment. 

Run the following command to list all of your Pods:

.. code-block:: sh

   kubectl get pods

For each Pod with a name that contains ``mongos``, delete it with the
following command:

.. code-block:: sh

   kubectl delete pod <podname>

When you delete a Pod, Kubernetes recreates it. Each Pod that Kubernetes
recreates receives the updated configuration and can reach a ``READY`` 
state. To confirm that all of your |mongos| Pods are ``READY``, run the
following command:

.. code-block:: sh

   kubectl get pods -n <namespace>

A response like the following indicates that all of your |mongos| Pods
are ``READY``:

.. code-block:: sh
   :copyable: false
   :emphasize-lines: 7-8

   NAME                                           READY   STATUS    RESTARTS   AGE
   mongodb-enterprise-operator-6495bdd947-ttwqf   1/1     Running   0          50m
   my-sharded-cluster-0-0                         1/1     Running   0          12m
   my-sharded-cluster-1-0                         1/1     Running   0          12m
   my-sharded-cluster-config-0                    1/1     Running   0          12m
   my-sharded-cluster-config-1                    1/1     Running   0          12m
   my-sharded-cluster-mongos-0                    1/1     Running   0          11m
   my-sharded-cluster-mongos-1                    1/1     Running   0          11m
   om-0                                           1/1     Running   0          42m
   om-db-0                                        2/2     Running   0          44m
   om-db-1                                        2/2     Running   0          43m
   om-db-2                                        2/2     Running   0          43m

.. _app-db-secret-no-reconcile:

Update |tls| Secret for the Application Database
------------------------------------------------

.. note::

   This issue applies only to |onprem| resources deployed using the
   |k8s-op-short| 1.13.0.

The |k8s-op-short| doesn't reconcile resources when you modify the
secret that contains the Application Database's |tls| certificate.
To force the |k8s-op-short| to reconcile resources, scale the operator 
down to zero replicas, then scale it up to one.

.. note::

   This is a safe operation. Scaling the
   ``mongodb-enterprise-operator`` deployment does not affect the
   availability of your deployed |onprem| and database resources.

Run the following command to scale down:

.. code-block:: sh

   kubectl scale deployment mongodb-enterprise-operator --replicas=0 -n <namespace>
     
Run the following command to scale up:

.. code-block:: sh

   kubectl scale deployment mongodb-enterprise-operator --replicas=1 -n <namespace>


.. _k8s-private-cluster-on-gke:

Update Google Firewall Rules to Fix WebHook Issues
--------------------------------------------------

When you deploy |k8s-op-short| to |gke| private clusters, the
|k8s-mdbrscs| or MongoDBOpsManager resource creation could time out.
The following message might appear in the logs:

  Error setting state to reconciling: Timeout: request did not
  complete within requested timeout 30s".

Google configures its firewalls to restrict access to your |k8s|
|k8s-pods|. To use the webhook service,
:gcp:`add a new firewall rule </kubernetes-engine/docs/how-to/private-clusters#add_firewall_rules>`
to grant |gke| control plane access to your webhook service.

The |k8s-op-short| webhook service runs on port 443.

Configure Persistent Storage Correctly
--------------------------------------

If there are no
:k8sdocs:`persistent volumes </concepts/storage/persistent-volumes/>`
available when you create a resource, the resulting |k8s-pod| stays in
transient state and the Operator fails  (after 20 retries) with the
following error:

.. code-block:: sh

   Failed to update Ops Manager automation config: Some agents failed to register

To prevent this error, either:

- Provide |k8s-pvs| or
- Set ``persistent : false`` for the resource

For testing only, you may also set ``persistent : false``. This
*must not be used in production*, as data is not preserved between
restarts.

Remove Resources before Removing |k8s|
--------------------------------------

Sometimes |onprem| can diverge from |k8s|. This mostly occurs when
|k8s| resources are removed manually. |onprem| can keep displaying an
Automation Agent which has been shut down.

If you want to remove deployments of MongoDB on |k8s|, use the
resource specification to delete resources first so no dead Automation
Agents remain.

Create Separate Namespaces for |k8s-op-short| and MongoDB Resources
-------------------------------------------------------------------

The best strategy is to create |k8s-op-short| and its resources in
different namespaces so that the following operations would work
correctly:

.. code-block:: sh

   kubectl delete pods --all

or

.. code-block:: sh

   kubectl delete namespace mongodb

If the |k8s-op-short| and resources sit in the same ``mongodb``
|k8s-ns|, then operator would also be removed in the same operation.
This would mean that it could not clean the configurations, which
would have to be done in the |application|.

.. _https-enablement-issues:

HTTPS Enabled After Deployment
------------------------------

We recommend that you enable |https| *before* deploying your |onprem| resources.
However, if you enable |https| after deployment,
your managed resources can no longer communicate with |onprem| and 
the |k8s-op-short| reports your resources' status as ``Failed``.

To resolve this issue, you must delete your |k8s-pods| by
running the following command for each Pod:

.. code-block:: sh

   kubectl delete pod <replicaset-pod-name>
      
After deletion, |k8s| automatically restarts the deleted Pods. 
During this period, the resource is unreachable and incurs 
downtime.

.. seealso::
   
   - :ref:`config-https`
   - :ref:`k8s-troubleshooting`

Difficulties with Updates
-------------------------

In some cases, the |k8s-op-short| can stop receiving change events. As
this problem is hard to reproduce, the recommended workaround is to
delete the operator pod. |k8s| starts the new |k8s-op-short|
automatically and starts working correctly:

.. code-block:: sh

   kubectl get pods;
   kubectl delete pod mongodb-enterprise-operator-<podId>`

.. cond:: onprem

   .. seealso::

      :doc:`Kubernetes Operator installation </tutorial/install-k8s-operator>`

.. _unable-to-upgrade-appdb-agent-in-om:

Unable to Update the {+mdbagent+} on Application Database Pods
-----------------------------------------------------------------

You can't use |onprem| to upgrade the {+mdbagent+}\s that run on the
Application Database Pods. The {+mdbagent+} version that runs on these
Pods is embedded in the Application Database Docker image.

You can use the |k8s-op-short| to upgrade the {+mdbagent+} version on
Application Database Pods as MongoDB publishes new images.

.. seealso::

   - :ref:`meko-appdb-agent-version`
   - :ref:`registry-appdb-version`

Machine Memory vs. Container Memory
-----------------------------------

MongoDB versions older than 3.6.13, 4.0.9, and 4.1.9 report host system 
RAM, not container RAM.
