.. _custom-analyzers:

================
Custom Analyzers
================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

Overview
--------

An |fts| analyzer prepares a set of documents to be indexed by 
performing a series of operations to transform, filter, and group 
sequences of characters. You can define a custom analyzer to suit your 
specific indexing needs.

.. _custom-analyzers-syntax:

Syntax 
------

A custom analyzer has the following syntax:

.. code-block:: json

   "analyzers": [
     {
       "name": "<name>",
       "charFilters": [ <list-of-character-filters> ],
       "tokenizer": {
         "type": "<tokenizer-type>"
       },
       "tokenFilters": [ <list-of-token-filters> ]
     }
   ]

.. _custom-analyzers-attrs:

Attributes 
----------

A custom analyzer has the following attributes:

.. list-table::
   :widths: 15 20 45 15
   :header-rows: 1

   * - Attribute
     - Type
     - Description
     - Required?

   * - ``name``
     - string
     - Name of the custom analyzer. Names must be unique within an 
       index, and may not start with any of the following strings:

       - ``lucene.``
       - ``builtin.``
       - ``mongodb.``

     - yes

   * - ``charFilters``
     - list of objects
     - Array containing zero or more character filters. See 
       :ref:`custom-analyzers-usage` for more information.
     - no

   * - ``tokenizer``
     - object
     - Tokenizer to use to create tokens. See 
       :ref:`custom-analyzers-usage` for more information.
     - yes

   * - ``tokenFilters``
     - list of objects
     - Array containing zero or more token filters. See 
       :ref:`custom-analyzers-usage` for more information.
     - no

.. _custom-analyzers-usage:

Usage 
-----

To use a custom analyzer when indexing a collection, include the 
following in your :ref:`index definition <ref-index-definitions>` ``analyzers`` field:

1. *Optional*. Specify one or more :ref:`character filters 
   <char-filters-ref>`. Character filters examine text one character at 
   a time and perform filtering operations.

2. *Required*. Specify the :ref:`tokenizer <tokenizers-ref>`. An 
   analyzer uses a tokenizer to split chunks of text into groups, or 
   tokens, for indexing purposes. For example, the whitespace tokenizer 
   splits text fields into individual words based on where whitespace 
   occurs.

3. *Optional*. Specify one or more :ref:`token filters 
   <token-filters-ref>`. After the tokenization step, the resulting 
   tokens can pass through one or more token filters. A token filter 
   performs operations such as:
   
   - Stemming, which reduces related words, such as "talking", "talked",
     and "talks" to their root word "talk".
   - Redaction, the removal of sensitive information from public 
     documents.

.. note:: 

   The text passes through character filters first, then a tokenizer, 
   and then the token filters.

.. _custom-analyzers-eg-coll:

Example Collection 
------------------

This page contains sample index definitions and query examples for 
character filters, tokenizers, and token filters. These examples use a 
sample ``minutes`` collection with the following documents:

.. code-block:: json

   { 
     "_id": 1, 
     "page_updated_by": {
       "last_name": "AUERBACH",
       "first_name": "Siân",
       "email": "auerbach@example.com",
       "phone": "123-456-7890"
     },
     "text" : "<head> This page deals with department meetings. </head>"
   }
   { 
     "_id": 2, 
     "page_updated_by": {
       "last_name": "OHRBACH",
       "first_name": "Noël",
       "email": "ohrbach@example.com",
       "phone": "123-456-0987"
     },
     "text" : "The head of the sales department spoke first."
   }
   { 
     "_id": 3, 
     "page_updated_by": {
       "last_name": "LEWINSKY",
       "first_name": "Brièle",
       "email": "lewinsky@example.com",
       "phone": "123-456-9870"
     },
     "text" : "<body>We'll head out to the conference room by noon.</body>" 
   }
   { 
     "_id": 4, 
     "page_updated_by": {
       "last_name": "LEVINSKI",
       "first_name": "François",
       "email": "levinski@example.com",
       "phone": "123-456-8907"
     },
     "text" : "<body>The page has been updated with the items on the agenda.</body>" 
   }

.. _character-filters:
.. _char-filters-ref:

Character Filters
-----------------

Character filters always require a type field, and some take additional
options as well.

.. code-block:: json

   "charFilters": [
     {
       "type": "<filter-type>",
       "<additional-option>": <value>
     }
   ]

|fts| supports four types of character filters:

.. list-table::
   :widths: 30 70
   :header-rows: 1

   * - Type
     - Description

   * - :ref:`htmlStrip <htmlStrip-ref>`
     - Strips out HTML constructs.

   * - :ref:`icuNormalize <icuNormalize-ref>`
     - Normalizes text with the `ICU <http://site.icu-project.org/>`__
       Normalizer. Based on Lucene's `ICUNormalizer2CharFilter
       <https://lucene.apache.org/core/8_3_0/analyzers-icu/org/apache/lucene/analysis/icu/ICUNormalizer2CharFilter.html>`__.

   * - :ref:`mapping <mapping-ref>`
     - Applies user-specified normalization mappings to characters. 
       Based on Lucene's `MappingCharFilter
       <https://lucene.apache.org/core/8_0_0/analyzers-common/org/apache/lucene/analysis/charfilter/MappingCharFilter.html>`__.

   * - :ref:`persian <persian-ref>`
     - Replaces instances of `zero-width non-joiner
       <https://en.wikipedia.org/wiki/Zero-width_non-joiner>`__ with 
       ordinary space. Based on Lucene's `PersianCharFilter
       <https://lucene.apache.org/core/8_0_0/analyzers-common/org/apache/lucene/analysis/fa/PersianCharFilter.html>`__.

.. _htmlStrip-ref:

htmlStrip
~~~~~~~~~

The ``htmlStrip`` character filter has the following attributes:

.. list-table::
   :widths: 10 10 45 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this character filter. Must be ``htmlStrip``.
     - yes
     - 

   * - ``ignoredTags``
     - array of strings
     - A list of HTML tags to exclude from filtering.
     - no
     - 

.. example::

   The following example index definition uses a custom analyzer 
   named ``htmlStrippingAnalyzer``. It uses the ``htmlStrip`` 
   character filter to remove all HTML tags from the text except 
   the ``a`` tag in the ``minutes`` collection. It uses the 
   :ref:`standard tokenizer <standard-tokenizer-ref>` and no 
   token filters.

   .. code-block:: json

     {
       "analyzer": "htmlStrippingAnalyzer",
       "mappings": {
         "dynamic": true
       },
       "analyzers": [{
         "name": "htmlStrippingAnalyzer",
         "charFilters": [{
           "type": "htmlStrip",
           "ignoredTags": ["a"]
         }],
         "tokenizer": {
           "type": "standard"
         },
         "tokenFilters": []
       }]
     }

   The following search operation looks for occurrences of the 
   string ``head`` in the ``text`` field of the ``minutes`` 
   collection.

   .. code-block:: json

     db.minutes.aggregate([   
       {     
         $search: {
           text: {
             query: "head",
             path: "text"
           }
         }
       },
       {
         $project: {
           "_id": 1,
           "text": 1
         }
       }
     ])

   The query returns the following results:

   .. code-block:: json
     :copyable: false
    
     { "_id" : 2, "text" : "The head of the sales department spoke first." }
     { "_id" : 3, "text" : "<body>We'll head out to the conference room by noon.</body>" }

   The document with ``_id: 1`` is not returned, because the 
   string ``head`` is part of the HTML tag ``<head>``. The 
   document with ``_id: 3`` contains HTML tags, but the string 
   ``head`` is elsewhere so the document is a match.

.. _icuNormalize-ref:

icuNormalize
~~~~~~~~~~~~

The ``icuNormalize`` character filter has the following attribute:

.. list-table::
   :widths: 10 10 60 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this character filter. Must be ``icuNormalize``.
     - yes
     - 

.. example::

   The following example index definition uses a custom analyzer named
   ``normalizingAnalyzer``. It uses the ``icuNormalize`` character
   filter, the :ref:`whitespace tokenizer <whitespace-tokenizer-ref>`
   and no token filters.

   .. code-block:: json

      {
        "analyzer": "normalizingAnalyzer",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "normalizingAnalyzer",
            "charFilters": [
              {
                "type": "icuNormalize"
              }
            ],
            "tokenizer": {
              "type": "whitespace"
            },
            "tokenFilters": []
          }
        ]
      }

.. _mapping-ref:

mapping
~~~~~~~

The ``mapping`` character filter has the following attributes:

.. list-table::
   :widths: 10 10 60 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this character filter. Must be ``mapping``.
     - yes
     - 
 
   * - ``mappings``
     - object
     - An object containing a comma-separated list of mappings. A 
       mapping indicates that one character or group of characters 
       should be substituted for another, in the format 
       ``<original> : <replacement>``.
     - yes
     -

.. example::

   The following example index definition uses a custom analyzer named
   ``mappingAnalyzer``. It uses the ``mapping`` character filter to
   replace instances of ``\\`` with ``/``. It uses the :ref:`keyword
   tokenizer <keyword-tokenizer-ref>` and no token filters.

   .. code-block:: json

      {
        "analyzer": "mappingAnalyzer",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "mappingAnalyzer",
            "charFilters": [
              {
                "type": "mapping",
                "mappings": {
                  "\\": "/"
                }
              }
            ],
            "tokenizer": {
              "type": "keyword"
            },
            "tokenFilters": []
          }
        ]
      }

   .. seealso:: 
   
      The :ref:`shingle-tf-ref` token filter for a sample index 
      definition and query. 

.. _persian-ref:

persian
~~~~~~~

The ``persian`` character filter has the following attribute:

.. list-table::
   :widths: 10 10 60 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this character filter. Must be ``persian``.
     - yes
     - 

.. example::

   The following example index definition uses a custom analyzer named
   ``persianCharacterIndex``. It uses the ``persian`` character filter,
   the ``whitespace`` tokenizer and no token filters.

   .. code-block:: json

      {
        "analyzer": "persianCharacterIndex",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "persianCharacterIndex",
            "charFilters": [
              {
                "type": "persian"
              }
            ],
            "tokenizer": {
              "type": "whitespace"
            },
            "tokenFilters": []
          }
        ]
      }

.. _tokenizers-ref:

Tokenizers
----------

A custom analyzer's tokenizer determines how |fts| splits up text into
discrete chunks for indexing.

Tokenizers always require a type field, and some take additional options
as well.

.. code-block:: json

   "tokenizer": {
     "type": "<tokenizer-type>",
     "<additional-option>": "<value>"
   }

|fts| supports the following tokenizer options:

.. list-table::
   :widths: 30 70
   :header-rows: 1

   * - Name
     - Description

   * - :ref:`standard <standard-tokenizer-ref>`
     - Tokenize based on word break rules from the `Unicode Text 
       Segmentation algorithm 
       <https://www.unicode.org/L2/L2019/19034-uax29-34-draft.pdf>`__.

   * - :ref:`keyword <keyword-tokenizer-ref>`
     - Tokenize the entire input as a single token.

   * - :ref:`whitespace <whitespace-tokenizer-ref>`
     - Tokenize based on occurrences of whitespace between words.

   * - :ref:`nGram <ngram-tokenizer-ref>`
     - Tokenize into text chunks, or "n-grams", of given sizes. You 
       can't use the :ref:`nGram <ngram-tokenizer-ref>` tokenizer in 
       :ref:`synonym <synonyms-ref>` or :ref:`autocomplete 
       <bson-data-types-autocomplete>` mapping 
       definitions.

   * - :ref:`edgeGram <edgegram-tokenizer-ref>`
     - Tokenize input from the left side, or "edge", of a text input 
       into n-grams of given sizes. You can't use the :ref:`edgeGram 
       <edgegram-tokenizer-ref>` tokenizer in :ref:`synonym 
       <synonyms-ref>` or :ref:`autocomplete 
       <bson-data-types-autocomplete>` mapping definitions.

   * - :ref:`regexCaptureGroup <regexcapturegroup-tokenizer-ref>`
     - Match a regular expression pattern to extract tokens.

   * - :ref:`regexSplit <regexSplit-tokenizer-ref>`
     - Split tokens with a regular-expression based delimiter.

   * - :ref:`uaxUrlEmail <uaxUrlEmail-tokenizer-ref>`
     - Tokenize :abbr:`URLs (Uniform Resource Locator)` and email 
       addresses. Although :ref:`<uaxUrlEmail-tokenizer-ref>` tokenizer 
       tokenizes based on word break rules from the `Unicode Text 
       Segmentation algorithm 
       <http://www.unicode.org/L2/L2019/19034-uax29-34-draft.pdf>`__, 
       we recommend using :ref:`<uaxUrlEmail-tokenizer-ref>` 
       tokenizer only when the indexed field value includes :abbr:`URLs 
       (Uniform Resource Locator)` and email addresses. For fields that 
       do not include :abbr:`URLs (Uniform Resource Locator)` or email 
       addresses, use the :ref:`<standard-tokenizer-ref>` tokenizer to 
       create tokens based on word break rules.

.. _standard-tokenizer-ref:

standard
~~~~~~~~

The ``standard`` tokenizer has the following attributes:

.. list-table::
   :widths: 20 10 50 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this tokenizer. Must be ``standard``.
     - yes
     -

   * - ``maxTokenLength``
     - integer
     - Maximum length for a single token. Tokens greater than this 
       length are split at ``maxTokenLength`` into multiple tokens.
     - no
     - 255

.. example::

   The following example index definition uses a custom analyzer named
   ``standardShingler``. It uses the ``standard`` tokenizer and the
   :ref:`shingle token filter <shingle-tf-ref>`.

   .. code-block:: json

      {
        "analyzer": "standardShingler",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "standardShingler",
            "charFilters": [],
            "tokenizer": {
              "type": "standard",
              "maxTokenLength": 10,
            },
            "tokenFilters": [
              {
                "type": "shingle",
                "minShingleSize": 2,
                "maxShingleSize": 3
              }
            ]
          }
        ]
      }

   .. seealso:: 
   
      The :ref:`regex-tf-ref` token filter for a sample index 
      definition and query. 
   
.. _keyword-tokenizer-ref:

keyword
~~~~~~~

The ``keyword`` tokenizer has the following attribute:

.. list-table::
   :widths: 10 10 60 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this tokenizer. Must be ``keyword``.
     - yes
     -

.. example::

   The following example index definition uses a custom analyzer named
   ``keywordTokenizingIndex``. It uses the ``keyword`` tokenizer and a
   regular expression token filter that redacts email addresses.

   .. code-block:: json

      {
        "analyzer": "keywordTokenizingIndex",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "keywordTokenizingIndex",
            "charFilters": [],
            "tokenizer": {
              "type": "keyword"
            },
            "tokenFilters": [
              {
                "type": "regex",
                "pattern": "^([a-z0-9_\\.-]+)@([\\da-z\\.-]+)\\.([a-z\\.]{2,5})$",
                "replacement": "redacted",
                "matches": "all"
              }
            ]
          }
        ]
      }

.. _whitespace-tokenizer-ref:

whitespace
~~~~~~~~~~

The ``whitespace`` tokenizer has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this tokenizer. Must be ``whitespace``.
     - yes
     -

   * - ``maxTokenLength``
     - integer
     - Maximum length for a single token. Tokens greater than this 
       length are split at ``maxTokenLength`` into multiple tokens.
     - no
     - 255

.. example::

   The following example index definition uses a custom analyzer named
   ``whitespaceLowerer``. It uses the ``whitespace`` tokenizer and a
   token filter that lowercases all tokens.

   .. code-block:: json

      {
        "analyzer": "whitespaceLowerer",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "whitespaceLowerer",
            "charFilters": [],
            "tokenizer": {
              "type": "whitespace"
            },
            "tokenFilters": [
              {
                "type": "lowercase"
              }
            ]
          }
        ]
      }

   .. seealso:: 
   
      The :ref:`shingle-tf-ref` token filter for a sample index 
      definition and query. 

.. _ngram-tokenizer-ref:

nGram
~~~~~

The ``nGram`` tokenizer has the following attributes:

.. list-table::
   :widths: 10 10 60 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this tokenizer. Must be ``nGram``.
     - yes
     -
 
   * - ``minGram``
     - integer
     - Number of characters to include in the shortest token created.
     - yes
     - 
 
   * - ``maxGram``
     - integer
     - Number of characters to include in the longest token created.
     - yes
     - 

.. example::

   The following example index definition uses a custom analyzer named
   ``ngramShingler``. It uses the ``nGram`` tokenizer to create tokens
   between 2 and 5 characters long and the :ref:`shingle token filter
   <shingle-tf-ref>`.

   .. code-block:: json

      {
        "analyzer": "ngramShingler",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "ngramShingler",
            "charFilters": [],
            "tokenizer": {
              "type": "nGram",
              "minGram": 2,
              "maxGram": 5
            },
            "tokenFilters": [
              {
                "type": "shingle",
                "minShingleSize": 2,
                "maxShingleSize": 3
              }
            ]
          }
        ]
      }

.. _edgegram-tokenizer-ref:

edgeGram
~~~~~~~~

The ``edgeGram`` tokenizer has the following attributes:

.. list-table::
   :widths: 10 10 60 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this tokenizer. Must be ``edgeGram``.
     - yes
     -
 
   * - ``minGram``
     - integer
     - Number of characters to include in the shortest token created.
     - yes
     - 
 
   * - ``maxGram``
     - integer
     - Number of characters to include in the longest token created.
     - yes
     - 

.. example::

   The following example index definition uses a custom analyzer named
   ``edgegramShingler``. It uses the ``edgeGram`` tokenizer to create tokens
   between 2 and 5 characters long starting from the first character of
   text input and the :ref:`shingle token filter <shingle-tf-ref>`.

   .. code-block:: json

      {
        "analyzer": "edgegramShingler",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "edgegramShingler",
            "charFilters": [],
            "tokenizer": {
              "type": "edgeGram",
              "minGram": 2,
              "maxGram": 5
            },
            "tokenFilters": [
              {
                "type": "shingle",
                "minShingleSize": 2,
                "maxShingleSize": 3
              }
            ]
          }
        ]
      }

.. _regexcapturegroup-tokenizer-ref:

regexCaptureGroup
~~~~~~~~~~~~~~~~~

The ``regexCaptureGroup`` tokenizer has the following attributes:

.. list-table::
   :widths: 10 10 60 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this tokenizer. Must be ``regexCaptureGroup``.
     - yes
     -
 
   * - ``pattern``
     - string
     - A regular expression to match against.
     - yes
     - 
 
   * - ``group``
     - integer
     - Index of the character group within the matching expression to 
       extract into tokens. Use ``0`` to extract all character groups.
     - yes
     - 

.. example::

   The following example index definition uses a custom analyzer named
   ``phoneNumberExtractor``. It uses the ``regexCaptureGroup`` tokenizer
   to creates a single token from the first US-formatted phone number
   present in the text input.

   .. code-block:: json

      {
        "analyzer": "phoneNumberExtractor",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "phoneNumberExtractor",
            "charFilters": [],
            "tokenizer": {
              "type": "regexCaptureGroup",
              "pattern": "^\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b$",
              "group": 0
            },
            "tokenFilters": []
          }
        ]
      }

.. _regexSplit-tokenizer-ref:

regexSplit
~~~~~~~~~~

The ``regexSplit`` tokenizer has the following attributes:

.. list-table::
   :widths: 10 10 60 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this tokenizer. Must be ``regexSplit``.
     - yes
     -
 
   * - ``pattern``
     - string
     - A regular expression to match against.
     - yes
     - 

.. example::

   The following example index definition uses a custom analyzer named
   ``dashSplitter``. It uses the ``regexSplit`` tokenizer
   to create tokens from hyphen-delimited input text.

   .. code-block:: json

      {
        "analyzer": "dashSplitter",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "dashSplitter",
            "charFilters": [],
            "tokenizer": {
              "type": "regexSplit",
              "pattern": "[-]+"
            },
            "tokenFilters": []
          }
        ]
      }

.. _uaxUrlEmail-tokenizer-ref:

uaxUrlEmail
~~~~~~~~~~~

The ``uaxUrlEmail`` tokenizer has the following attributes:

.. list-table::
   :widths: 10 10 60 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type`` 
     - string 
     - The type of this tokenizer. Must be ``uaxUrlEmail``.
     - yes 
     - 

   * - ``maxTokenLength``
     - int 
     - The maximum number of characters in one token. 
     - no 
     - ``255``

.. example:: 

   The following example index definition uses a custom analyzer named
   ``emailUrlExtractor``. It uses the ``uaxUrlEmail`` tokenizer to 
   create tokens up to ``200`` characters long each for all text, 
   including email addresses and :abbr:`URLs (Uniform Resource 
   Locator)`, in the input. It converts all text to lowercase using the 
   :ref:`lowercase <lowercase-tf-ref>` token filter.

   .. code-block:: json

      {
        "analyzer": "emailUrlExtractor",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "emailUrlExtractor",
            "charFilters": [],
            "tokenizer": {
              "type": "uaxUrlEmail",
              "maxTokenLength": "200"
            },
            "tokenFilters": [
              {
                "type": "lowercase"
              }
            ]
          }
        ]
      }

.. _token-filters-ref:

Token Filters
-------------

Token Filters always require a type field, and some take additional 
options as well.

.. code-block:: json

   "tokenFilters": [
     {
       "type": "<token-filter-type>",
       "<additional-option>": <value>
     }
   ]

|fts| supports the following token filters:

.. list-table::
   :widths: 30 70
   :header-rows: 1

   * - Name
     - Description

   * - :ref:`asciiFolding <asciiFolding-tf-ref>`
     - Converts alphabetic, numeric, and symbolic unicode characters 
       that are not in the `Basic Latin Unicode block 
       <https://en.wikipedia.org/wiki/Basic_Latin_(Unicode_block)>`__ 
       to their ASCII equivalents, if available. 

   * - :ref:`daitchMokotoffSoundex <daitchmokotoffsoundex-tf-ref>`
     - Creates tokens for words that sound the same based on 
       `Daitch-Mokotoff Soundex <https://en.wikipedia.org/wiki/Daitch%E2%80%93Mokotoff_Soundex>`__ 
       phonetic algorithm. This filter can generate multiple encodings 
       for each input, where each encoded token is a 6 digit number. 

       .. note:: 

          Don't use :ref:`<daitchmokotoffsoundex-tf-ref>` token filter 
          in: 
          
          - :ref:`Synonym <synonyms-ref>` or :ref:`autocomplete 
            <bson-data-types-autocomplete>` mapping definitions.
          - Operators where ``fuzzy`` is enabled. |fts| supports the 
            ``fuzzy`` option for the following operators: 

            - :ref:`autocomplete-ref`
            - :ref:`term-ref` 
            - :ref:`text-ref`

   * - :ref:`lowercase <lowercase-tf-ref>`
     - Normalizes token text to lowercase.

   * - :ref:`length <length-tf-ref>`
     - Removes tokens that are too short or too long.

   * - :ref:`icuFolding <icufolding-tf-ref>`
     - Applies character folding from `Unicode Technical Report #30
       <http://www.unicode.org/reports/tr30/tr30-4.html>`__.

   * - :ref:`icuNormalizer <icunormalizer-tf-ref>`
     - Normalizes tokens using a standard `Unicode Normalization Mode
       <https://unicode.org/reports/tr15/>`__.

   * - :ref:`nGram <ngram-tf-ref>`
     - Tokenizes input into n-grams of configured sizes. You can't use 
       the :ref:`nGram <ngram-tf-ref>` token filter in :ref:`synonym 
       <synonyms-ref>` or :ref:`autocomplete 
       <bson-data-types-autocomplete>` mapping definitions.

   * - :ref:`edgeGram <edgegram-tf-ref>`
     - Tokenize input from the left side, or "edge", of a text input 
       into n-grams of configured sizes. You can't use the 
       :ref:`edgeGram <edgegram-tf-ref>` token filter in 
       :ref:`synonym <synonyms-ref>` or :ref:`autocomplete 
       <bson-data-types-autocomplete>` mapping definitions.

   * - :ref:`shingle <shingle-tf-ref>`
     - Constructs shingles (token n-grams) from a series of tokens. You 
       can't use the :ref:`shingle <shingle-tf-ref>` token filter in 
       :ref:`synonym <synonyms-ref>` or :ref:`autocomplete 
       <bson-data-types-autocomplete>` mapping definitions.

   * - :ref:`regex <regex-tf-ref>`
     - Applies a regular expression to each token, replacing matches 
       with a specified string.

   * - :ref:`snowballStemming <snowballstemming-tf-ref>`
     - Stems tokens using a `Snowball-generated stemmer
       <https://snowballstem.org/>`__.

   * - :ref:`stopword <stopword-tf-ref>`
     - Removes tokens that correspond to the specified stop words. 
       This token filter doesn't analyze the specified stop words.

   * - :ref:`trim <trim-tf-ref>`
     - Trims leading and trailing whitespace from tokens.

.. _asciiFolding-tf-ref: 

asciiFolding
~~~~~~~~~~~~

The ``asciiFolding`` token filter has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this token filter. Must be ``asciiFolding``.
     - yes
     -

   * - ``originalTokens`` 
     - string 
     - Specifies whether to include or omit the original tokens in the 
       output of the token filter. Value can be one of the following: 

       - ``include`` - to include the original tokens with the 
         converted tokens in the output of the token filter. We 
         recommend this value if you want to support queries on both 
         the original tokens as well as the converted forms. 
       - ``omit`` - to omit the original tokens and include only the 
         converted tokens in the output of the token filter. Use this 
         value if you want to query only on the converted forms of the 
         original tokens. 

     - no 
     - ``omit``

.. example:: 

   The following example index definition uses a custom analyzer 
   named ``asciiConverter``. It uses the :ref:`standard tokenizer
   <standard-tokenizer-ref>` with the ``asciiFolding`` token filter to 
   index the fields in the :ref:`example <custom-analyzers-eg-coll>` 
   collection and convert the field values to their ASCII equivalent. 

   .. code-block:: json 

      {
        "analyzer": "asciiConverter",
        "searchAnalyzer": "asciiConverter",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "asciiConverter",
            "tokenizer": {
              "type": "standard"
            },
            "tokenFilters": [
              {
                "type": "asciiFolding"
              }
            ]
          }
        ]
      }

   The following query searches the ``first_name`` field for names 
   using their ASCII equivalent.

   .. code-block:: json 

      db.minutes.aggregate([
        {
          $search: {
            "index": "default",
            "text": {
              "query": "Sian",
              "path": "page_updated_by.first_name"
            }
          }
        },
        {
          $project: {
            "_id": 1,
            "page_updated_by.last_name": 1,
            "page_updated_by.first_name": 1
          }
        }
      ])

   |fts| returns the following results: 

   .. code-block:: json
      :copyable: false 

      [
        {
           _id: 1,
           page_updated_by: { last_name: 'AUERBACH', first_name: 'Siân'}
        }
      ]

.. _daitchmokotoffsoundex-tf-ref: 

daitchMokotoffSoundex
~~~~~~~~~~~~~~~~~~~~~

The ``daitchMokotoffSoundex`` token filter has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this token filter. Must be ``daitchMokotoffSoundex``.
     - yes
     -

   * - ``originalTokens`` 
     - string 
     - Specifies whether to include or omit the original tokens in the 
       output of the token filter. Value can be one of the following: 

       - ``include`` - to include the original tokens with the encoded 
         tokens in the output of the token filter. We recommend this 
         value if you want queries on both the original tokens as well 
         as the encoded forms. 
       - ``omit`` - to omit the original tokens and include only the 
         encoded tokens in the output of the token filter. Use this 
         value if you want to only query on the encoded forms of the 
         original tokens. 

     - no 
     - ``include``

.. example::

   The following example index definition uses a custom analyzer 
   named ``dmsAnalyzer``. It uses the :ref:`standard tokenizer
   <standard-tokenizer-ref>` with the ``daitchMokotoffSoundex`` 
   token filter to index and query for words that sound the same 
   as their encoded forms.

   .. code-block:: json 

     {
       "analyzer": "dmsAnalyzer",
       "searchAnalyzer": "dmsAnalyzer",
       "mappings": {
         "dynamic": true
       },
       "analyzers": [
         {
           "name": "dmsAnalyzer",
           "tokenizer": {
             "type": "standard"
           },
           "tokenFilters": [
             {
               "type": "daitchMokotoffSoundex",
               "originalTokens": "include"
             }
           ]
         }
       ]
     }

   The following query searches for terms that sound similar to 
   ``AUERBACH`` in the ``page_updated_by.last_name`` field of 
   the ``minutes`` collection.

   .. code-block:: json 

      db.minutes.aggregate([
        {
          $search: {
            "index": "default",
            "text": {
              "query": "AUERBACH",
              "path": "page_updated_by.last_name"
            }
          }
        },
        {
          $project: {
            "_id": 1,
            "page_updated_by.last_name": 1
          }
        }
      ])

   The query returns the following results: 

   .. code-block:: json 
     :copyable: false 

     { "_id" : 1, "page_updated_by" : { "last_name" : "AUERBACH" } }
     { "_id" : 2, "page_updated_by" : { "last_name" : "OHRBACH" } }

   |fts| returns documents with ``_id: 1`` and ``_id: 2`` 
   because the terms in both documents are phonetically similar, 
   and are coded using the same six digit ``097500``.

.. _lowercase-tf-ref:

lowercase
~~~~~~~~~

The ``lowercase`` token filter has the following attribute:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this token filter. Must be ``lowercase``.
     - yes
     -

.. example::
         
   The following example index definition uses a custom analyzer named 
   ``lowercaser``. It uses the :ref:`standard tokenizer 
   <standard-tokenizer-ref>` with the ``lowercase`` token filter to 
   lowercase all tokens.

   .. code-block:: json

      {
        "analyzer": "lowercaser",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "lowercaser",
            "charFilters": [],
            "tokenizer": {
              "type": "standard"
            },
            "tokenFilters": [
              {
                "type": "lowercase"
              }
            ]
          }
        ]
      }

   .. seealso:: 
   
      The :ref:`regex-tf-ref` token filter for a sample index 
      definition and query. 

.. _length-tf-ref:

length
~~~~~~

The ``length`` token filter has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this token filter. Must be ``length``.
     - yes
     -

   * - ``min``
     - integer
     - The minimum length of a token. Must be less than or equal to 
       ``max``.
     - no
     - 0

   * - ``max``
     - integer
     - The maximum length of a token. Must be greater than or equal to 
       ``min``.
     - no
     - 255

.. example::

   The following example index definition uses a custom analyzer named
   ``longOnly``. It uses the ``length`` token filter to index only 
   tokens that are at least 20 UTF-16 code units long after tokenizing 
   with the :ref:`standard tokenizer <standard-tokenizer-ref>`.

   .. code-block:: json

      {
        "analyzer": "longOnly",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "longOnly",
            "charFilters": [],
            "tokenizer": {
              "type": "standard"
            },
            "tokenFilters": [
              {
                "type": "length",
                "min": 20
              }
            ]
          }
        ]
      }

.. _icufolding-tf-ref:

icuFolding
~~~~~~~~~~

The ``icuFolding`` token filter has the following attribute:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this token filter. Must be ``icuFolding``.
     - yes
     -

.. example::

   The following example index definition uses a custom analyzer named
   ``diacriticFolder``. It uses the :ref:`keyword tokenizer
   <keyword-tokenizer-ref>` with the ``icuFolding`` token filter to 
   apply foldings from `UTR#30 Character Foldings
   <https://unicode.org/reports/tr30/>`__. Foldings include accent
   removal, case folding, canonical duplicates folding, and many others
   detailed in the report.

   .. code-block:: json

      {
        "analyzer": "diacriticFolder",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "diacriticFolder",
            "charFilters": [],
            "tokenizer": {
              "type": "keyword"
            },
            "tokenFilters": [
              {
                "type": "icuFolding"
              }
            ]
          }
        ]
      }

.. _icunormalizer-tf-ref:

icuNormalizer
~~~~~~~~~~~~~

The ``icuNormalizer`` token filter has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this token filter. Must be ``icuNormalizer``.
     - yes
     -

   * - ``normalizationForm``
     - string
     - Normalization form to apply. Accepted values are:

       - ``nfd`` (Canonical Decomposition)
       - ``nfc`` (Canonical Decomposition, followed by Canonical 
         Composition)
       - ``nfkd`` (Compatibility Decomposition)
       - ``nfkc`` (Compatibility Decomposition, followed by Canonical 
         Composition)

       For more information about the supported normalization forms, see
       `Section 1.2: Normalization Forms, UTR#15 <https://unicode.org/reports/tr15/#Norm_Forms>`__.

     - no
     - ``nfc``

.. example::

   The following example index definition uses a custom analyzer named
   ``normalizer``. It uses the :ref:`whitespace tokenizer
   <whitespace-tokenizer-ref>`, then normalizes
   tokens by Canonical Decomposition, followed by Canonical Composition.

   .. code-block:: json

      {
        "analyzer": "normalizer",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "normalizer",
            "charFilters": [],
            "tokenizer": {
              "type": "whitespace"
            },
            "tokenFilters": [
              {
                "type": "icuNormalizer",
                "normalizationForm": "nfc"
              }
            ]
          }
        ]
      }

.. _ngram-tf-ref:

nGram
~~~~~

The ``nGram`` token filter has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this token filter. Must be ``nGram``.
     - yes
     -

   * - ``minGram``
     - integer
     - The minimum length of generated n-grams. Must be less than or 
       equal to ``maxGram``.
     - yes
     - 

   * - ``maxGram``
     - integer
     - The maximum length of generated n-grams. Must be greater than or 
       equal to ``minGram``.
     - yes
     - 

   * - ``termNotInBounds``
     - string
     - Accepted values are:

       - ``include``
       - ``omit``

       If ``include`` is specified, tokens shorter than ``minGram`` or
       longer than ``maxGram`` are indexed as-is. If ``omit`` is 
       specified, those tokens are not indexed.
     - no
     - ``omit``


.. example::

   The following example index definition uses a custom analyzer named
   ``persianAutocomplete``. It functions as an autocomplete analyzer for
   Persian and other languages that use the zero-width non-joiner
   character. It performs the following operations:
   
   - Normalizes zero-width non-joiner characters with the :ref:`persian
     character filter <persian-ref>`. 
   - Tokenizes by whitespace with the :ref:`whitespace tokenizer
     <whitespace-tokenizer-ref>`.
   - Applies a series of token filters: 
   
     - ``icuNormalizer``
     - ``shingle``
     - ``nGram``

   .. code-block:: json

      {
        "analyzer": "persianAutocomplete",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "persianAutocomplete",
            "charFilters": [
              {
                "type": "persian"
              }
            ],
            "tokenizer": {
              "type": "whitespace"
            },
            "tokenFilters": [
              {
                "type": "icuNormalizer",
                "normalizationForm": "nfc"
              },
              {
                "type": "shingle",
                "minShingleSize": 2,
                "maxShingleSize": 3
              },
              {
                "type": "nGram",
                "minGram": 1,
                "maxGram": 10
              }
            ]
          }
        ]
      }

.. _edgegram-tf-ref:

edgeGram
~~~~~~~~

The ``edgeGram`` token filter has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this token filter. Must be ``edgeGram``.
     - yes
     -

   * - ``minGram``
     - integer
     - The minimum length of generated n-grams. Must be less than or 
       equal to ``maxGram``.
     - yes
     - 

   * - ``maxGram``
     - integer
     - The maximum length of generated n-grams. Must be greater than or 
       equal to ``minGram``.
     - yes
     - 

   * - ``termNotInBounds``
     - string
     - Accepted values are:

       - ``include``
       - ``omit``

       If ``include`` is specified, tokens shorter than ``minGram`` or
       longer than ``maxGram`` are indexed as-is. If ``omit`` is 
       specified, those tokens are not indexed.
     - no
     - ``omit``

.. example::

   The following example index definition uses a custom analyzer named
   ``englishAutocomplete``. It performs the following operations:

   - Tokenizes with the :ref:`standard tokenizer 
     <standard-tokenizer-ref>`.
   - Token filtering with the following filters:

     - ``icuFolding``
     - ``shingle``
     - ``edgeGram``

   .. code-block:: json

      {
        "analyzer": "englishAutocomplete",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "englishAutocomplete",
            "charFilters": [],
            "tokenizer": {
              "type": "standard"
            },
            "tokenFilters": [
              {
                "type": "icuFolding"
              },
              {
                "type": "shingle",
                "minShingleSize": 2,
                "maxShingleSize": 3
              },
              {
                "type": "edgeGram",
                "minGram": 1,
                "maxGram": 10
              }
            ]
          }
        ]
      }

   .. seealso:: 
   
      The :ref:`shingle-tf-ref` token filter for a sample index 
      definition and query. 

.. _shingle-tf-ref:

shingle
~~~~~~~

The ``shingle`` token filter has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this token filter. Must be ``shingle``.
     - yes
     -

   * - ``minShingleSize``
     - integer
     - Minimum number of tokens per shingle. Must be less than or equal
       to ``maxShingleSize``.
     - yes
     -

   * - ``maxShingleSize``
     - integer
     - Maximum number of tokens per shingle. Must be greater than or 
       equal to ``minShingleSize``.
     - yes
     -

.. example::

   The following example index definition uses two custom 
   analyzers, ``emailAutocompleteIndex`` and 
   ``emailAutocompleteSearch``, to implement autocomplete-like 
   functionality. |fts| uses the ``emailAutocompleteIndex`` 
   analyzer during index creation to:
    
   - Replace ``@`` characters in a field with ``AT``
   - Create tokens with the :ref:`whitespace 
     <whitespace-tokenizer-ref>` tokenizer
   - Shingle tokens 
   - Create :ref:`edgegram-tf-ref` of those shingled tokens
  
   |fts| uses the ``emailAutocompleteSearch`` analyzer during a 
   search to:

   - Replace ``@`` characters in a field with ``AT``
   - Create tokens with the :ref:`whitespace tokenizer 
     <whitespace-tokenizer-ref>` tokenizer
    
   .. code-block:: json
      :emphasize-lines: 34-38

      {
       "analyzer": "lucene.keyword",
       "mappings": {
         "dynamic": true,
         "fields": {
           "page_updated_by": {
             "type": "document",
             "fields": {
               "email": {
                 "type": "string",
                 "analyzer": "emailAutocompleteIndex",
                 "searchAnalyzer": "emailAutocompleteSearch",
               }
             }
           }
         }
       },
       "analyzers": [
         {
           "name": "emailAutocompleteIndex",
           "charFilters": [
             {
               "mappings": {
                 "@": "AT"
               },
               "type": "mapping"
             }
           ],
           "tokenizer": {
             "maxTokenLength": 15,
             "type": "whitespace"
           },
           "tokenFilters": [
             {
               "maxShingleSize": 3,
               "minShingleSize": 2,
               "type": "shingle"
             },
             {
               "maxGram": 15,
               "minGram": 2,
               "type": "edgeGram"
             }
           ]
         },
         {
           "name": "emailAutocompleteSearch",
           "charFilters": [
             {
               "mappings": {
                 "@": "AT"
               },
               "type": "mapping"
             }
           ],
           "tokenizer": {
             "maxTokenLength": 15,
             "type": "whitespace"
           }
         }
       ]
     }

   The following query searches for an email address in the 
   ``page_updated_by.email`` field of the ``minutes`` collection:

   .. code-block:: json 

      db.minutes.aggregate([
        {
          $search: {
            "index": "default",
            "text": {
              "query": "auerbach@ex",
               "path": "page_updated_by.email"
            }
          }
        }
      ])

   The query returns the following results:

   .. code-block:: json 
      :copyable: false 

      {  
        "_id" : 1, 
        "page_updated_by" : { 
          "last_name" : "AUERBACH", 
          "first_name" : "Siân", 
          "email" : "auerbach@example.com", 
          "phone" : "123-456-7890" 
        }, 
        "text" : "<head> This page deals with department meetings. </head>" 
      }

.. _regex-tf-ref:

regex
~~~~~

The ``regex`` token filter has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this token filter. Must be ``regex``.
     - yes
     -

   * - ``pattern``
     - string
     - Regular expression pattern to apply to each token.
     - yes
     - 

   * - ``replacement``
     - string
     - Replacement string to substitute wherever a matching pattern
       occurs.
     - yes
     - 

   * - ``matches``
     - string
     - Acceptable values are:

       - ``all``
       - ``first``

       If ``matches`` is set to ``all``, replace all matching patterns.
       Otherwise, replace only the first matching pattern.

     - yes
     -

.. example::

   The following index definition uses a custom analyzer named 
   ``emailRedact`` for indexing the ``page_updated_by.email`` 
   field in the ``minutes`` collection. It uses the 
   :ref:`standard tokenizer <standard-tokenizer-ref>`. It first 
   applies the lowercase token filter to turn uppercase 
   characters in the field to lowercase and then finds strings 
   that look like email addresses and replaces them with the word 
   ``redacted``.

   .. code-block:: json
 
     {
       "analyzer": "lucene.standard",
       "mappings": {
         "dynamic": false,
         "fields": {
           "page_updated_by": {
             "type": "document",
             "fields": {
               "email": {
                 "type": "string",
                 "analyzer": "emailRedact"
               }
             }
           }
         }
       },
       "analyzers": [
         {
           "charFilters": [],
           "name": "emailRedact",
           "tokenizer": {
             "type": "standard"
           },
           "tokenFilters": [
             {
               "type": "lowercase"
             },
             {
               "matches": "all",
               "pattern": "^([a-z0-9_\\.-]+)@([\\da-z\\.-]+)\\.([a-z\\.]{2,5})$",
               "replacement": "redacted",
               "type": "regex"
             }
           ]
         }
       ]
     }

   The following query searches for the term ``example`` in the 
   ``page_updated_by.email`` field of the ``minutes`` collection. 

   .. code-block:: 

     db.minutes.aggregate([
       {
         $search: {
           "index": "default",
           "text": {
             "query": "example",
             "path": "page_updated_by.email"
           }
         }
       }
     ])

   |fts| doesn't return any results for the query because the 
   ``page_updated_by.email`` field does not contain any instances 
   of the word ``example`` that are not in an email address. 
   |fts| replaces strings that match the regular expression 
   provided in the custom analyzer with the word ``redacted``.

.. _snowballstemming-tf-ref:

snowballStemming
~~~~~~~~~~~~~~~~

The ``snowballStemming`` token filter has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this token filter. Must be ``snowballStemming``.
     - yes
     -

   * - ``stemmerName``
     - string
     - The following values are valid:

       - ``arabic``
       - ``armenian``
       - ``basque``
       - ``catalan``
       - ``danish``
       - ``dutch``
       - ``english``
       - ``finnish``
       - ``french``
       - ``german``
       - ``german2`` (Alternative German language stemmer. Handles the 
         umlaut by expanding ü to ue in most contexts.)
       - ``hungarian``
       - ``irish``
       - ``italian``
       - ``kp`` (Kraaij-Pohlmann stemmer, an alternative stemmer for 
         Dutch.)
       - ``lithuanian``
       - ``lovins`` (The first-ever published "Lovins JB" stemming 
         algorithm.)
       - ``norwegian``
       - ``porter`` (The original Porter English stemming algorithm.)
       - ``portuguese``
       - ``romanian``
       - ``russian``
       - ``spanish``
       - ``swedish``
       - ``turkish``
     - yes
     -

.. example::

   The following example index definition uses a custom analyzer named
   ``frenchStemmer``. It uses the ``lowercase`` token filter and the
   :ref:`standard tokenizer <standard-tokenizer-ref>`, followed
   by the ``french`` variant of the ``snowballStemming`` token filter.

   .. code-block:: json

      {
        "analyzer": "frenchStemmer", 
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "frenchStemmer",
            "charFilters": [],
            "tokenizer": {
              "type": "standard"
            },
            "tokenFilters": [
              {
                "type": "lowercase"
              },
              {
                "type": "snowballStemming",
                "stemmerName": "french"
              }
            ]
          }
        ]
      }

.. _stopword-tf-ref:

stopword 
~~~~~~~~

The ``stopword`` token filter has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this token filter. Must be ``stopword``.
     - yes
     - 

   * - ``tokens``
     - array of strings
     - The list of stop words that correspond to the tokens to remove. 
       Value must be one or more stop words.
     - yes
     - 

   * - ``ignoreCase``
     - boolean
     - The flag that indicates whether or not to ignore case of stop 
       words when filtering the tokens to remove. The value can be one 
       of the following: 

       - ``true`` - to ignore case and remove all tokens that match the 
         specified stop words
       - ``false`` - to be case-sensitive and remove only tokens that 
         exactly match the specified case

       If omitted, defaults to ``true``.

     - no
     - true

.. example:: 

   The following example index definition uses a custom analyzer named
   It uses the ``stopword`` token filter after the 
   :ref:`whitespace tokenizer <whitespace-tokenizer-ref>` to remove 
   the tokens that match the defined stop words ``is``, ``the``, and 
   ``at``. The token filter is case-insensitive and will remove all 
   tokens that match the specified stop words.

   .. code-block:: json

      {  
        "analyzer": "tokenTrimmer", 
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "stopwordRemover",
            "charFilters": [],
            "tokenizer": {
              "type": "whitespace"
            },
            "tokenFilters": [
              {
                "type": "stopword",
                "tokens": ["is", "the", "at"]
              }
            ]
          }
        ]
      }

.. _trim-tf-ref:

trim
~~~~

The ``trim`` token filter has the following attribute:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - The type of this token filter. Must be ``trim``.
     - yes
     -

.. example::

   The following example index definition uses a custom analyzer named
   ``tokenTrimmer``. It uses the ``trim`` token filter after the 
   :ref:`keyword tokenizer <keyword-tokenizer-ref>` to remove leading 
   and trailing whitespace in the tokens created by the :ref:`keyword 
   tokenizer <keyword-tokenizer-ref>`.

   .. code-block:: json

      
        "analyzer": "tokenTrimmer", 
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "tokenTrimmer",
            "charFilters": [],
            "tokenizer": {
              "type": "keyword"
            },
            "tokenFilters": [
              {
                "type": "trim"
              }
            ]
          }
        ]
      }
