.. _adf-out-stage:

========
``$out``
========

.. meta::
   :description: Use `$out` to write aggregation pipeline results to Atlas clusters, AWS S3, Azure Blob Storage, or Google Cloud Storage.

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

.. |out| replace:: :manual:`$out </reference/operator/aggregation/out>`
.. |convert| replace:: :manual:`$convert </reference/operator/aggregation/convert>`


|out| takes documents returned by the aggregation pipeline and writes
them to a specified collection. The |out| operator must be the last
stage in the :manual:`aggregation pipeline 
</reference/operator/aggregation-pipeline/>`. In {+adf+}, you can use 
|out| to write data from any one of the :ref:`supported 
<config-adf>` {+fdi+} stores or multiple :ref:`supported <config-adf>` 
{+fdi+} stores when using :ref:`federated queries <federated-queries>` 
to any one of the following: 

- |service| cluster :manual:`namespace 
  </reference/limits/#faq-dev-namespace>`
- |aws| |s3| buckets with read and write permissions
- |azure| Blob Storage containers with read and write permissions
  
You must :ref:`connect <fdi-connect>` to your {+fdi+} to use |out|.

.. tabs::

   .. tab:: S3
      :tabid: s3

   .. tab:: Azure
      :tabid: azure

   .. tab:: GCP
      :tabid: gcp

   .. tab:: Atlas Cluster
      :tabid: atlas

.. _adf-out-stage-perms:

Permissions Required
--------------------

.. tabs::

   .. tab:: S3
      :tabid: s3

      You must have:

      - A {+fdi+} configured for an |s3| bucket with read and write
        permissions or :aws:`s3:PutObject 
        </AmazonS3/latest/dev/using-with-s3-actions.html#using-with-s3-actions-related-to-objects>`
        permissions.
      - A MongoDB user with the :atlas:`atlasAdmin 
        </security-add-mongodb-users/#database-user-privileges>` 
        role or a custom role with the :authaction:`outToS3` privilege. 

   .. tab:: Azure
      :tabid: azure

      You must have:

      - A {+fdi+} configured for |azure| Blob Storage with an `Azure Role 
        <https://learn.microsoft.com/en-us/azure/storage/blobs/assign-azure-role-data-access?tabs=portal>`__ 
        that has read and write permissions.
      - A MongoDB user with the :atlas:`atlasAdmin 
        </security-add-mongodb-users/#database-user-privileges>` 
        role or a custom role with the :authaction:`outToAzure`
        privilege. 

   .. tab:: GCP
      :tabid: gcp

      You must have:

      - A {+fdi+} configured for a {+gcs+} bucket with access to a `GCP
        Service Account <https://cloud.google.com/iam/docs/manage-access-service-accounts>`__.
      - A MongoDB user with the :atlas:`atlasAdmin 
        </security-add-mongodb-users/#database-user-privileges>` 
        role or a custom role with the :authaction:`outToGCP`
        privilege. 

   .. tab:: Atlas Cluster
      :tabid: atlas

      .. note:: 

         .. include:: /includes/data-federation/fact-out-stage-limitation.rst

      You must be a database user with one of the following roles:

      - :authrole:`readWriteAnyDatabase`
      - :atlasrole:`atlasAdmin`
      - A custom role with the following privileges:

        - :manual:`insert </reference/privilege-actions/#insert>` and
        - :manual:`remove </reference/privilege-actions/#remove>`

.. _adf-out-stage-syntax:

Syntax
------

.. tabs::

   .. tab:: S3
      :tabid: s3

      .. code-block:: json
         :linenos:

         {
           "$out": {
             "s3": {
               "bucket": "<bucket-name>",
               "region": "<aws-region>",
               "filename": "<file-name>",
               "format": {
                 "name": "<file-format>",
                 "maxFileSize": "<file-size>",
                 "maxRowGroupSize": "<row-group-size>",
                 "columnCompression": "<compression-type>"
               },
               "errorMode": "stop"|"continue"
             }
           }
         }

   .. tab:: Azure
      :tabid: azure

      .. code-block:: json
         :linenos:

         {
           "$out": {
             "azure": {
               "serviceURL": "<storage-account-url>", 
               "containerName": "<container-name>", 
               "region": "<azure-region>",
               "filename": "<file-name>", 
               "format": {
                 "name": "<file-format>",
                 "maxFileSize": "<file-size>",
                 "maxRowGroupSize": "<row-group-size>",
                 "columnCompression": "<compression-type>"
               },
               "errorMode": "stop"|"continue"
             }
           }
         }

   .. tab:: GCP
      :tabid: gcp

      .. code-block:: json
         :linenos:

         {
           "$out": {
             "gcs": {
               "bucket": "<bucket-name>",
               "region": "<aws-region>",
               "filename": "<file-name>",
               "format": {
                 "name": "<file-format>",
                 "maxFileSize": "<file-size>",
                 "maxRowGroupSize": "<row-group-size>",
                 "columnCompression": "<compression-type>"
               },
               "errorMode": "stop"|"continue"	       
             }
           }
         }

   .. tab:: Atlas Cluster
      :tabid: atlas

      .. code-block:: json
         :linenos:

         {
           "$out": {
             "atlas": {
               "projectId": "<atlas-project-ID>",
               "clusterName": "<atlas-cluster-name>",
               "db": "<atlas-database-name>",
               "coll": "<atlas-collection-name>"
             }
           }
         }

.. _adf-out-stage-fields:

Fields
------

.. tabs::

   .. tab:: S3
      :tabid: s3

      .. list-table::
         :header-rows: 1
         :widths: 10 10 70 10

         * - Field
           - Type
           - Description
           - Necessity

         * - ``s3``
           - object
           - Location to write the documents from the aggregation
             pipeline.
           - Required

         * - ``s3.bucket``
           - string
           - Name of the |s3| bucket to write the documents from
             the aggregation pipeline to.

             The generated call to |s3| inserts a ``/`` between
             ``s3.bucket`` and ``s3.filename``. Don't append a ``/``
             to your ``s3.bucket`` string. 

             For example, if you set ``s3.bucket`` to  ``myBucket``
             and ``s3.filename`` to ``myPath/myData``, 
             {+adf+} writes the output location as follows:

             .. code-block:: sh
                :copyable: false

                s3://myBucket/myPath/myData.[n].json
                  

           - Required

         * - ``s3.region``
           - string
           - Name of the |aws| region in which the bucket is hosted. If 
             omitted, uses the {+fdi+} :ref:`configuration 
             <config-adf>` to determine the region where the specified 
             ``s3.bucket`` is hosted.
           - Optional

         * - ``s3.filename``
           - string
           - Name of the file to write the documents from the
             aggregation pipeline to. Filename can be constant or
             :ref:`created dynamically <adf-out-stage-egs>` from the
             fields in the documents that reach the |out| stage. Any
             filename expression you provide must evaluate to a
             ``string`` data type. 
             
             :gold:`IMPORTANT:` If there are any files on |s3| 
             with the same name and path as the newly generated files, 
             |out| overwrites the existing files with the newly 
             generated files.

             The generated call to |s3| inserts a ``/`` between
             ``s3.bucket`` and ``s3.filename``. Don't prepend a ``/`` 
             to your ``s3.filename`` string. 
                
             For example, if you set ``s3.filename`` to  ``myPath/myData``
             and ``s3.bucket`` to ``myBucket``, 
             {+adf+} writes the output location as follows:

             .. code-block:: sh
                :copyable: false

                s3://myBucket/myPath/myData.[n].json

           - Required

         * - ``s3.format``
           - object
           - Details of the file in |s3|.
           - Required

         * - | ``s3``
             | ``.format``
             | ``.name``
           - enum
           - Format of the file in |s3|. Value can be one of the
             following:

             - ``bson``
             - ``bson.gz``
             - ``csv``
             - ``csv.gz``
             - ``json`` :sup:`1`
             - ``json.gz`` :sup:`1`
             - ``parquet``
             - ``tsv``
             - ``tsv.gz``

             :sup:`1` For this format, |out| writes data in 
             :manual:`MongoDB Extended JSON </reference/mongodb-extended-json/>` format.

             To learn more, see :ref:`Limitations <adf-out-stage-limitations>`.

           - Required

         * - | ``s3``
             | ``.format``
             | ``.maxFileSize``
           - bytes
           - Maximum size of the uncompressed |bson| file in |s3|.
             When converting from |bson| to your preferred format, the
             resulting output file might be smaller (for example, when
             converting to Parquet) or larger (for example, when
             converting to CSV). 

             If a document is larger than the ``maxFileSize``, {+adf+}
             writes the document to its own file. The following
             suffixes are supported:

             .. list-table::
                :widths: 30 70

                * - Base 10: scaling in multiples of 1000
                  - - ``B``
                    - ``KB``
                    - ``MB``
                    - ``GB``
                    - ``TB``
                    - ``PB``
                * - Base 2: scaling in multiples of 1024
                  - - ``KiB``
                    - ``MiB``
                    - ``GiB``
                    - ``TiB``
                    - ``PiB``

             If omitted, defaults to ``200MiB``.

             When the file size limit for the current file is reached,
             {+adf+} creates a new file in |s3|. It appends a ``1``
             before the filename extension for the first file. For each
             subsequent file, {+adf+} increments the appended number by
             one. 

             For example, ``<filename>.1.<fileformat>``, and 
             ``<filename>.2.<fileformat>``.

           - Optional

         * - | ``s3``
             | ``.format``
             | ``.maxRowGroupSize``
           - string 
           - Supported for Parquet file format only.

             Maximum row group size to use when writing to Parquet 
             file. If omitted, defaults to ``128 MiB`` or the value of 
             the ``s3.format.maxFileSize``, whichever is smaller. The 
             maximum allowed value is ``1 GB``.
           - Optional

         * - | ``s3``
             | ``.format``
             | ``.columnCompression``
           - string
           - Supported for Parquet file format only.

             Compression type to apply for compressing data inside a 
             Parquet file when formatting the Parquet file. Valid 
             values are: 

             - ``gzip``
             - ``snappy``
             - ``uncompressed``

             If omitted, defaults to ``snappy``. 
             
             To learn more, see :ref:`adf-data-formats`.

           - Optional

         * - ``errorMode``
           - enum
           - Specifies how {+adf+} should proceed if there are errors 
             when processing a document. For example, if {+adf+} 
             encounters an array in a document when {+adf+} is writing 
             to a CSV file, {+adf+} uses this value to determine 
             whether or not to skip the document and process other 
             documents. Valid values are: 

             - ``continue`` to skip the document and continue 
               processing the remaining documents. {+adf+} also writes 
               the document that caused the error to an error file. 
               
               To learn more see, :ref:`Errors <adf-out-stage-errors>`.
                
             - ``stop`` to stop at that point and not process 
               the remaining documents.

             If omitted, defaults to ``continue``.

           - Optional

   .. tab:: Azure
      :tabid: azure

      .. list-table::
         :header-rows: 1
         :widths: 10 10 70 10

         * - Field
           - Type
           - Description
           - Necessity

         * - ``azure``
           - object
           - Location to write the documents from the aggregation
             pipeline.
           - Required

         * - ``azure.serviceURL``
           - string
           - |url| of the |azure| storage account in which to
             write documents from the aggregation pipeline.
           - Required

         * - ``azure.containerName``
           - string
           - Name of the |azure| Blob Storage container in which to
             write documents from the aggregation pipeline.
           - Required

         * - ``azure.region``
           - string
           - Name of the |azure| region which hosts the Blob Storage container.
           - Required

         * - ``azure.filename``
           - string
           - Name of the file in which to
             write documents from the aggregation pipeline.

             Accepts constant value, or values that evaluate to ``string``
             :ref:`created dynamically <adf-out-stage-egs>` from the
             fields in the documents that reach the |out| stage. 
             If there are any files in |azure| Blob Storage
             with the same name and path as the newly generated files, 
             |out| overwrites the existing files with the newly 
             generated files.
           - Required

         * - ``azure.format``
           - object
           - Details of the file in |azure| Blob Storage.
           - Required

         * - | ``azure``
             | ``.format``
             | ``.name``
           - enum
           - Format of the file in |azure| Blob Storage. Value can be one of the
             following:

             - ``bson``
             - ``bson.gz``
             - ``csv``
             - ``csv.gz``
             - ``json`` :sup:`1`
             - ``json.gz`` :sup:`1`
             - ``parquet``
             - ``tsv``
             - ``tsv.gz``

             :sup:`1` For this format, |out| writes data in 
             :manual:`MongoDB Extended JSON </reference/mongodb-extended-json/>` format.

             To learn more, see :ref:`Limitations <adf-out-stage-limitations>`.

           - Required

         * - | ``azure``
             | ``.format``
             | ``.maxFileSize``
           - bytes
           - Maximum size of the uncompressed |bson| document in |azure|
             Blob Storage. When converting from |bson| to your preferred
             format, the resulting output file might be smaller (for
             example, when converting to Parquet) or larger (for
             example, when converting to CSV). 

             If a document is larger than the ``maxFileSize``, {+adf+}
             writes the document to its own file. The following
             suffixes are supported:

             .. list-table::
                :widths: 30 70

                * - Base 10: scaling in multiples of 1000
                  - - ``B``
                    - ``KB``
                    - ``MB``
                    - ``GB``
                    - ``TB``
                    - ``PB``
                * - Base 2: scaling in multiples of 1024
                  - - ``KiB``
                    - ``MiB``
                    - ``GiB``
                    - ``TiB``
                    - ``PiB``

             If omitted, defaults to ``200MiB``.
           
             When the file size limit for the current file is reached,
             {+adf+}  creates a new file in |s3|. It appends a ``1``
             before the filename extension for the first file. For each
             subsequent file, {+adf+}  increments the appended number by
             one. 

             For example, ``<filename>.1.<fileformat>``, and 
             ``<filename>.2.<fileformat>``.

           - Optional

         * - | ``azure``
             | ``.format``
             | ``.maxRowGroupSize``
           - string 
           - Supported for Parquet file format only.

             Maximum row group size to use when writing to Parquet 
             file. If omitted, defaults to ``128 MiB`` or the value of 
             the ``azure.format.maxFileSize``, whichever is smaller. The 
             maximum allowed value is ``1 GB``.
           - Optional

         * - | ``azure``
             | ``.format``
             | ``.columnCompression``
           - string
           - Supported for Parquet file format only.

             Compression type to apply for compressing data inside a 
             Parquet file when formatting the Parquet file. Valid 
             values are: 

             - ``gzip``
             - ``snappy``
             - ``uncompressed``

             If omitted, defaults to ``snappy``. 
             
             To learn more, see :ref:`adf-data-formats`.

           - Optional

         * - ``errorMode``
           - enum
           - Specifies how {+adf+} should proceed when it encounters an error
             while processing a document. Valid values are: 

             - ``continue`` to skip the document and continue 
               processing the remaining documents. {+adf+} records the error
               in an error file.
                
             - ``stop`` to stop without processing
               the remaining documents. {+adf+} records the error in an
               error file.

             If omitted, defaults to ``continue``.

             To learn more, see :ref:`Errors <adf-out-stage-errors>`.

           - Optional

   .. tab:: GCP
      :tabid: gcp

      .. list-table::
         :header-rows: 1
         :widths: 10 10 70 10

         * - Field
           - Type
           - Description
           - Necessity

         * - ``gcs``
           - object
           - Location to write the documents from the aggregation
             pipeline.
           - Required

         * - ``gcs.bucket``
           - string
           - Name of the {+gcs+} bucket to write the documents from
             the aggregation pipeline to.

             The generated call to {+gcp+} inserts 
             a ``/`` between ``gcs.bucket`` and ``gcs.filename``. 
             Don't append a ``/`` to your ``gcs.bucket`` string.

             For example, if you set ``gcs.bucket`` to  ``myBucket`` and ``gcs.filename`` 
             to ``myPath/myData``, {+adf+} writes the output location as follows:

             .. code-block:: sh
                :copyable: false

                gcs://myBucket/myPath/myData.[n].json

           - Required

         * - ``gcs.region``
           - string
           - Name of the |aws| region in which the bucket is hosted. If 
             omitted, uses the {+fdi+} :ref:`configuration 
             <config-adf>` to determine the region where the specified 
             ``gcs.bucket`` is hosted.
           - Optional

         * - ``gcs.filename``
           - string
           - Name of the file to write the documents from the
             aggregation pipeline to. Filename can be constant or
             :ref:`created dynamically <adf-out-stage-egs>` from the
             fields in the documents that reach the |out| stage. Any
             filename expression you provide must evaluate to a
             ``string`` data type. If there are any files on {+gcs+} 
             with the same name and path as the newly generated files, 
             |out| overwrites the existing files with the newly 
             generated files.

             The generated call to {+gcs+} inserts a ``/`` between
             ``gcs.bucket`` and ``gcs.filename``. Don't prepend a ``/`` 
             to your ``gcs.filename`` string. 
              
             For example, if you set ``gcs.filename`` to  ``myPath/myData``
             and ``gcs.bucket`` to ``myBucket``, 
             {+adf+} writes the output location as follows:

             .. code-block:: sh
                :copyable: false

                gcs://myBucket/myPath/myData.[n].json

           - Required

         * - ``gcs.format``
           - object
           - Details of the file in {+gcs+}.
           - Required

         * - | ``gcs``
             | ``.format``
             | ``.name``
           - enum
           - Format of the file in {+gcs+}. Value can be one of the
             following:

             - ``bson``
             - ``bson.gz``
             - ``csv``
             - ``csv.gz``
             - ``json`` :sup:`1`
             - ``json.gz`` :sup:`1`
             - ``parquet``
             - ``tsv``
             - ``tsv.gz``

             :sup:`1` For this format, |out| writes data in 
             :manual:`MongoDB Extended JSON </reference/mongodb-extended-json/>` format.

             To learn more, see :ref:`Limitations <adf-out-stage-limitations>`.

           - Required
	   
   .. tab:: Atlas Cluster
      :tabid: atlas

      .. list-table::
         :header-rows: 1
         :widths: 10 10 70 10

         * - Field
           - Type
           - Description
           - Necessity

         * - ``atlas``
           - object
           - Location to write the documents from the aggregation
             pipeline.
           - Required

         * - ``clusterName``
           - string
           - Name of the |service| cluster.
           - Required

         * - ``coll``
           - string
           - Name of the collection on the |service| cluster.
           - Required

         * - ``db``
           - string
           - Name of the database on the |service| cluster that contains
             the collection.
           - Required

         * - ``projectId``
           - string
           - Unique identifier of the project that contains the
             |service| cluster. The project ID must be the ID of the
             project that contains your {+fdi+}. If omitted, defaults to
             the ID of the project that contains your {+fdi+}.
           - Optional

.. _adf-out-stage-options:

Options 
-------

.. list-table::
   :header-rows: 1
   :widths: 20 10 60 10

   * - Option
     - Type
     - Description 
     - Necessity

   * - ``background``
     - boolean
     - Flag to run aggregation operations in the background. If 
       omitted, defaults to ``false``. When set to ``true``, {+adf+} 
       runs the queries in the background. 

       .. code-block:: json 
          :copyable: false 

          { "background" : true }
       
       Use this option if you want to submit other new queries without 
       waiting for currently running queries to complete or disconnect 
       your {+fdi+} connection while the queries continue to run in the 
       background. 
     
     - Optional

.. _adf-out-stage-egs:

Examples
--------

.. tabs::

   .. tab:: S3
      :tabid: s3

      **Create a Filename**

      The following examples show |out| syntaxes for dynamically
      creating a filename from a constant string or from the fields of
      the same or different data types in the documents that reach the
      |out| stage.

      Simple String Example
      ~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 1 GiB of data as compressed |bson| files to
         an |s3| bucket named ``my-s3-bucket``. 

         Using the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "s3": {
                  "bucket": "my-s3-bucket",
                  "filename": "big_box_store/",
                  "format": {
                    "name": "bson.gz"
                  }
                }
              }
            }

         The ``s3.region`` is omitted and so, {+adf+} determines the 
         region where the bucket named ``my-s3-bucket`` is hosted from 
         the storage configuration. |out| writes five compressed |bson| 
         files:

         1. The first 200 MiB of data to a file that |out| names
            ``big_box_store/1.bson.gz``.

            - The value of ``s3.filename`` serves as a constant in
              each filename. This value doesn't depend upon any
              document field or value.

            - Your ``s3.filename`` ends with a delimiter, so {+adf+}
              appends the counter after the constant.

            - If it didn't end with a delimiter, {+adf+} would have
              added a ``.`` between the constant and the counter,
              like ``big_box_store.1.bson.gz``

            - Because you didn't change the maximum file size using
              ``s3.format.maxFileSize``, {+adf+} uses the default
              value of 200 MiB.

         2. The second 200 MiB of data to a new file that |out| names
            ``big_box_store/2.bson.gz``.

         3. Three more files that |out| names
            ``big_box_store/3.bson.gz`` through
            ``big_box_store/5.bson.gz``.

      Single Field from Documents
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 90 MiB of data to |json| files to an |s3|
         bucket named ``my-s3-bucket``.

         Using the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "s3": {
                  "bucket": "my-s3-bucket",
                  "region": "us-east-1",
                  "filename": {"$toString": "$saleDate"},
                  "format": {
                    "name": "json",
                    "maxFileSize": "100MiB"
                  }
                }
              }
            }

         |out| writes 90 MiB of data to |json| files in the root of the
         bucket. Each |json| file contains all of the documents with the
         same ``saleDate`` value. |out| names each file using the
         documents' ``saleDate`` value converted to a string.

      Multiple Fields from Documents
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 176 MiB of data as |bson| files to an |s3|
         bucket named ``my-s3-bucket``.

         Using the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "s3": {
                  "bucket": "my-s3-bucket",
                  "region": "us-east-1",
                  "filename": {
                    "$concat": [
                      "persons/",
                      "$name", "/",
                      "$uniqueId", "/"
                    ]
                  },
                  "format": {
                    "name": "bson",
                    "maxFileSize": "200MiB"
                  }
                }
              }
            }

         |out| writes 176 MiB of data to |bson| files. To name each
         file, |out| concatenates:

         - A constant string ``persons/`` and, from the documents:

           - The string value of the ``name`` field,
           - A forward slash (``/``), 
           - The string value of the ``uniqueId`` field, and
           - A forward slash (``/``).

         Each |bson| file contains all of the documents with the same
         ``name`` and ``uniqueId`` values. |out| names each file using
         the documents' ``name`` and ``uniqueId`` values.


      Multiple Types of Fields from Documents
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 154 MiB of data as compressed |json| files
         to an |s3| bucket named ``my-s3-bucket``.

         Consider the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "s3": {
                  "bucket": "my-s3-bucket",
                  "region": "us-east-1",
                  "filename": {
                    "$concat": [
                      "big-box-store/",
                      {
                        "$toString": "$storeNumber"
                      }, "/",
                      {
                        "$toString": "$saleDate"
                      }, "/",
                      "$partId", "/"
                    ]
                  },
                  "format": {
                    "name": "json.gz",
                    "maxFileSize": "200MiB"
                  }
                }
              }
            }

         |out| writes 154 MiB of data to compressed |json| files, where
         each file contains all documents with the same 
         ``storeNumber``, ``saleDate``, and ``partId`` values.  To 
         name each file, |out| concatenates:

         - A constant string value of ``big-box-store/``,
         - A string value of a unique store number in the
           ``storeNumber`` field,
         - A forward slash (``/``),
         - A string value of the date from the ``saleDate`` field, 
         - A forward slash (``/``), 
         - A string value of part ID from the ``partId`` field, and 
         - A forward slash (``/``).

   .. tab:: Azure
      :tabid: azure

      **Create a Filename**

      The following examples show |out| syntaxes for dynamically
      creating a filename from a constant string or from the fields of
      the same or different data types in the documents that reach the
      |out| stage.

      Simple String Example
      ~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 1 GiB of data as compressed |bson| files to
         an |azure| storage account ``mystorageaccount`` and 
         container named ``my-container``. 

         Using the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "azure": {
                  "serviceURL": "http://mystorageaccount.blob.core.windows.net/",
                  "container": "my-container",
                  "filename": "big_box_store/",
                  "format": {
                    "name": "bson.gz"
                  }
                }
              }
            }

         The ``azure.region`` is omitted and so {+adf+} determines the 
         region where the container named ``my-container`` is hosted from 
         the storage configuration. |out| writes five compressed |bson| 
         files:

         1. The first 200 MiB of data to a file that |out| names
            ``big_box_store/1.bson.gz``.

            - The value of ``azure.filename`` serves as a constant in
              each filename. This value doesn't depend upon any
              document field or value.

            - Your ``azure.filename`` ends with a delimiter, so {+adf+}
              appends the counter after the constant.

            - If it didn't end with a delimiter, {+adf+} would have
              added a ``.`` between the constant and the counter,
              like ``big_box_store.1.bson.gz``

            - Because you didn't change the maximum file size using
              ``azure.format.maxFileSize``, {+adf+} uses the default
              value of 200 MiB.

         2. The second 200 MiB of data to a new file that |out| names
            ``big_box_store/2.bson.gz``.

         3. Three more files that |out| names
            ``big_box_store/3.bson.gz`` through
            ``big_box_store/5.bson.gz``.

      Single Field from Documents
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 90 MiB of data to |json| files to an |azure| Blob Storage
         container named ``my-container``.

         Using the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "azure": {
                  "serviceURL": "http://mystorageaccount.blob.core.windows.net/",
                  "container": "my-container",
                  "region": "eastus2",
                  "filename": {"$toString": "$saleDate"},
                  "format": {
                    "name": "json",
                    "maxFileSize": "100MiB"
                  }
                }
              }
            }

         |out| writes 90 MiB of data to |json| files in the root of the
         container. Each |json| file contains all of the documents with the
         same ``saleDate`` value. |out| names each file using the
         documents' ``saleDate`` value converted to a string.

      Multiple Fields from Documents
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 176 MiB of data as |bson| files to an |azure| Blob Storage
         container named ``my-container``.

         Using the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "azure": {
                  "serviceURL": "http://mystorageaccount.blob.core.windows.net/",
                  "container": "my-container",
                  "region": "eastus2",
                  "filename": {
                    "$concat": [
                      "persons/",
                      "$name", "/",
                      "$uniqueId", "/"
                    ]
                  },
                  "format": {
                    "name": "bson",
                    "maxFileSize": "200MiB"
                  }
                }
              }
            }

         |out| writes 176 MiB of data to |bson| files. To name each
         file, |out| concatenates:

         - A constant string ``persons/`` and, from the documents:

           - The string value of the ``name`` field,
           - A forward slash (``/``), 
           - The string value of the ``uniqueId`` field, and
           - A forward slash (``/``).

         Each |bson| file contains all of the documents with the same
         ``name`` and ``uniqueId`` values. |out| names each file using
         the documents' ``name`` and ``uniqueId`` values.


      Multiple Types of Fields from Documents
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 154 MiB of data as compressed |json| files
         to an |azure| Blob Storage container named ``my-container``.

         Consider the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "azure": {
                  "serviceURL": "http://mystorageaccount.blob.core.windows.net/",
                  "container": "my-container",
                  "region": "eastus2",
                  "filename": {
                    "$concat": [
                      "big-box-store/",
                      {
                        "$toString": "$storeNumber"
                      }, "/",
                      {
                        "$toString": "$saleDate"
                      }, "/",
                      "$partId", "/"
                    ]
                  },
                  "format": {
                    "name": "json.gz",
                    "maxFileSize": "200MiB"
                  }
                }
              }
            }

         |out| writes 154 MiB of data to compressed |json| files, where
         each file contains all documents with the same 
         ``storeNumber``, ``saleDate``, and ``partId`` values.  To 
         name each file, |out| concatenates:

         - A constant string value of ``big-box-store/``,
         - A string value of a unique store number in the
           ``storeNumber`` field,
         - A forward slash (``/``),
         - A string value of the date from the ``saleDate`` field, 
         - A forward slash (``/``), 
         - A string value of part ID from the ``partId`` field, and 
         - A forward slash (``/``).

   .. tab:: GCP
      :tabid: gcp

      **Create a Filename**

      The following examples show |out| syntaxes for dynamically
      creating a filename from a constant string or from the fields of
      the same or different data types in the documents that reach the
      |out| stage.

      Simple String Example
      ~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 1 GiB of data as compressed |bson| files to
         an {+gcs+} bucket named ``my-gcs-bucket``. 

         Using the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "gcs": {
                  "bucket": "my-gcs-bucket",
                  "filename": "big_box_store/",
                  "format": {
                    "name": "bson.gz"
                  }
                }
              }
            }

         The ``gcs.region`` is omitted and so, {+adf+} determines the 
         region where the bucket named ``my-gcs-bucket`` is hosted from 
         the storage configuration. |out| writes five compressed |bson| 
         files:

         1. The first 200 MiB of data to a file that |out| names
            ``big_box_store/1.bson.gz``.

            - The value of ``gcs.filename`` serves as a constant in
              each filename. This value doesn't depend upon any
              document field or value.

            - Your ``gcs.filename`` ends with a delimiter, so {+adf+}
              appends the counter after the constant.

            - If it didn't end with a delimiter, {+adf+} would have
              added a ``.`` between the constant and the counter,
              like ``big_box_store.1.bson.gz``

            - Because you didn't change the maximum file size using
              ``gcs.format.maxFileSize``, {+adf+} uses the default
              value of 200 MiB.

         2. The second 200 MiB of data to a new file that |out| names
            ``big_box_store/2.bson.gz``.

         3. Three more files that |out| names
            ``big_box_store/3.bson.gz`` through
            ``big_box_store/5.bson.gz``.

      Single Field from Documents
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 90 MiB of data to |json| files to a {+gcs+}
         bucket named ``my-gcs-bucket``.

         Using the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "gcs": {
                  "bucket": "my-gcs-bucket",
                  "region": "us-central1",
                  "filename": {"$toString": "$saleDate"},
                  "format": {
                    "name": "json",
                    "maxFileSize": "100MiB"
                  }
                }
              }
            }

         |out| writes 90 MiB of data to |json| files in the root of the
         bucket. Each |json| file contains all of the documents with the
         same ``saleDate`` value. |out| names each file using the
         documents' ``saleDate`` value converted to a string.

      Multiple Fields from Documents
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 176 MiB of data as |bson| files to a {+gcs+}
         bucket named ``my-gcs-bucket``.

         Using the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "gcs": {
                  "bucket": "my-gcs-bucket",
                  "region": "us-central1",
                  "filename": {
                    "$concat": [
                      "persons/",
                      "$name", "/",
                      "$uniqueId", "/"
                    ]
                  },
                  "format": {
                    "name": "bson",
                    "maxFileSize": "200MiB"
                  }
                }
              }
            }

         |out| writes 176 MiB of data to |bson| files. To name each
         file, |out| concatenates:

         - A constant string ``persons/`` and, from the documents:

           - The string value of the ``name`` field,
           - A forward slash (``/``), 
           - The string value of the ``uniqueId`` field, and
           - A forward slash (``/``).

         Each |bson| file contains all of the documents with the same
         ``name`` and ``uniqueId`` values. |out| names each file using
         the documents' ``name`` and ``uniqueId`` values.


      Multiple Types of Fields from Documents
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 154 MiB of data as compressed |json| files
         to a {+gcs+} bucket named ``my-gcs-bucket``.

         Consider the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "gcs": {
                  "bucket": "my-gcs-bucket",
                  "region": "us-central1",
                  "filename": {
                    "$concat": [
                      "big-box-store/",
                      {
                        "$toString": "$storeNumber"
                      }, "/",
                      {
                        "$toString": "$saleDate"
                      }, "/",
                      "$partId", "/"
                    ]
                  },
                  "format": {
                    "name": "json.gz",
                    "maxFileSize": "200MiB"
                  }
                }
              }
            }

         |out| writes 154 MiB of data to compressed |json| files, where
         each file contains all documents with the same 
         ``storeNumber``, ``saleDate``, and ``partId`` values.  To 
         name each file, |out| concatenates:

         - A constant string value of ``big-box-store/``,
         - A string value of a unique store number in the
           ``storeNumber`` field,
         - A forward slash (``/``),
         - A string value of the date from the ``saleDate`` field, 
         - A forward slash (``/``), 
         - A string value of part ID from the ``partId`` field, and 
         - A forward slash (``/``).

   .. tab:: Atlas Cluster
      :tabid: atlas

      Write to Collection on |service| {+Cluster+}
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

      This |out| syntax sends the aggregated data to a 
      ``sampleDB.mySampleData`` collection in the |service| cluster 
      named ``myTestCluster``. The syntax doesn't specify a project ID; 
      |out| uses the ID of the project that contains your {+fdi+}.

      .. example::

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "atlas": {
                  "clusterName": "myTestCluster",
                  "db": "sampleDB",
                  "coll": "mySampleData"
                }
              }
            }

Run a Query in the background
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following example shows |out| syntax for running an aggregation 
pipeline that ends with the |out| stage in the background.

.. tabs::
   :hidden:

   .. tab:: S3
      :tabid: s3

      .. example::

         .. code-block:: json 

            db.runCommand({
              "aggregate": "my-collection",
              "pipeline": [
                { 
                  "$out": { 
                    "s3": {
                      "bucket": "my-s3-bucket",
                      "filename": { "$toString": "$saleDate" }
                      "format": { 
                        "name": "json" 
                      }
                    }
                  } 
                }
              ],
              { "background" : true }
            })

         |out| writes to |json| files in the root of the bucket in the 
         background. Each |json| file contains all of the documents 
         with the same ``saleDate`` value. |out| names each file using 
         the documents' ``saleDate`` value converted to a string. 

   .. tab:: Azure
      :tabid: azure

      .. example::

         .. code-block:: json 

            db.runCommand({
              "aggregate": "my-collection",
              "pipeline": [
                { 
                  "$out": { 
                    "azure": {
                      "serviceURL": "http://mystorageaccount.blob.core.windows.net/",
                      "container": "my-container",
                      "filename": {"$toString": "$saleDate"},
                      "format": {
                        "name": "json"
                      }
                    } 
                  }
                }
              ], 
              { "background" : true }
            })

         |out| writes to |json| files in the root of the |azure| Blob Storage container
         in the background. Each |json| file contains all of the documents 
         with the same ``saleDate`` value. |out| names each file using 
         the documents' ``saleDate`` value converted to a string. 

   .. tab:: GCP
      :tabid: gcp

      .. example::

         .. code-block:: json 

            db.runCommand({
              "aggregate": "my-collection",
              "pipeline": [
                { 
                  "$out": { 
                    "gcs": {
                      "bucket": "my-gcs-bucket",
                      "filename": { "$toString": "$saleDate" }
                      "format": { 
                        "name": "json" 
                      }
                    }
                  } 
                }
              ],
              { "background" : true }
            })

         |out| writes to |json| files in the root of the bucket in the 
         background. Each |json| file contains all of the documents 
         with the same ``saleDate`` value. |out| names each file using 
         the documents' ``saleDate`` value converted to a string.

   .. tab:: Atlas Cluster
      :tabid: atlas

      .. example:: 

         .. code-block:: json 
            
            db.runCommand({
              "aggregate": "my-collection",
              "pipeline": [
                {
                  "$out": {
                    "atlas": {
                      "clusterName": "myTestCluster",
                      "db": "sampleDB",
                      "coll": "mySampleData"
                    }
                  }
                }
              ], 
              { background: true }
            })

         |out| writes to ``sampleDB.mySampleData`` collection in the 
         |service| cluster named ``myTestCluster`` in the background.

.. _adf-out-stage-limitations:
.. _csv_and_tsv_format:

Limitations
-----------

.. tabs::

   .. tab:: S3
      :tabid: s3

      String Data Type
      ~~~~~~~~~~~~~~~~

      {+adf+} interprets empty strings (``""``) as ``null`` values when
      parsing filenames. If you want {+adf+} to generate parseable
      filenames, wrap the field references that could have ``null``
      values using |convert| with an empty string ``onNull`` value.

      .. example::

         This example shows how to handle null values in the ``year``
         field when creating a filename from the field value.

         .. code-block:: json
            :copyable: false
            :linenos:

            {
              "$out": {
                "s3": {
                  "bucket": "my-s3-bucket",
                  "region": "us-east-1",
                  "filename": {
                    "$concat": [
                      "big-box-store/",
                      {
                        "$convert": {
                          "input": "$year",
                          "to": "string",
                          "onNull": ""
                        }
                      }, "/"
                    ]
                  },
                  "format": {
                    "name": "json.gz",
                    "maxFileSize": "200MiB"
                  }
                }
              }
            }

      Number of Unique Fields
      ~~~~~~~~~~~~~~~~~~~~~~~

      When writing to :ref:`CSV <adf-csv-tsv-data>`,
      :ref:`TSV <adf-csv-tsv-data>`, or :ref:`Parquet <adf-parquet-data>`
      file format, {+adf+} doesn't support more than 32000 unique fields.
      
      CSV and TSV File Format
      ~~~~~~~~~~~~~~~~~~~~~~~

      When writing to CSV or TSV format, {+adf+} does not support the 
      following data types in the documents: 

      - Arrays 
      - DB pointer 
      - JavaScript 
      - JavaScript code with scope
      - Minimum or maximum key data type

      In a CSV file, {+adf+} represents nested documents using the dot 
      (``.``) notation. For example, {+adf+} writes 
      ``{ x: { a: 1, b: 2 } }`` as the following in the CSV file: 

      .. code-block:: shell 
         :copyable: false 

         x.a,x.b 
         1,2
      
      {+adf+} represents all other data types as strings. Therefore, 
      the data types in MongoDB read back from the CSV file may not be 
      the same as the data types in the original |bson| documents from 
      which the data types were written.

      Parquet File Format
      ~~~~~~~~~~~~~~~~~~~

      For Parquet, {+adf+} reads back fields with null or undefined 
      values as missing because Parquet doesn't distinguish between 
      null or undefined values and missing values. Although {+adf+} 
      supports all data types, for |bson| data types that do not have 
      a direct equivalent in Parquet, such as JavaScript, regular 
      expression, etc., it:
             
      - Chooses a representation that allows 
        the resulting Parquet file to be read back using a non-MongoDB 
        tool.
      - Stores a MongoDB schema in the Parquet file's key/value 
        metadata so that {+adf+} can reconstruct the original |bson| 
        document with the correct data types if the Parquet file is 
        read back by {+adf+}.

      .. example:: 

         Consider the following |bson| documents:

         .. code-block:: json
            :copyable: false 

            {
              "clientId": 102, 
              "phoneNumbers": ["123-4567", "234-5678"], 
              "clientInfo": {
                "name": "Taylor",
                "occupation": "teacher"
              }
            }
            {
              "clientId": "237", 
              "phoneNumbers" ["345-6789"]
              "clientInfo": {
                "name": "Jordan"
              }
            }

         If you write the preceding |bson| documents to Parquet format using
         :ref:`$out to S3 <adf-out-stage>`, the Parquet file schema for your
         |bson| documents would look similar to the following: 

         .. code-block:: shell 
            :copyable: false 

            message root {
              optional group clientId {
                optional int32 int;
                optional binary string; (STRING)
              }
              optional group phoneNumbers (LIST) {
                repeated group list {
                  optional binary element (STRING);
                }
              }
              optional group clientInfo {
                optional binary name (STRING);
                optional binary occupation (STRING);
              }
            }

         Your Parquet data on |s3| would look similar to the following: 

         .. code-block:: shell
            :copyable: false 
            :linenos:

            clientId:
            .int = 102
            phoneNumbers:
            .list:
            ..element = "123-4567"
            .list:
            ..element = "234-5678"
            clientInfo:
            .name = "Taylor"
            .occupation = "teacher"

            clientId:
            .string = "237"
            phoneNumbers:
            .list:
            ..element = "345-6789"
            clientInfo:
            .name = "Jordan"

         The preceding example demonstrates how {+adf+} handles complex data
         types: 

         - {+adf+} maps documents at all levels to a Parquet group.
         - {+adf+} encodes arrays using the ``LIST`` logical type and the
           mandatory three-level list or element structure. To learn more,
           see `Lists
           <https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#nested-types>`__. 
         - {+adf+} maps polymorphic |bson| fields to a group of multiple single-type
           columns because Parquet doesn't support polymorphic columns. 
           {+adf+} names the group after the |bson| field. In the preceding
           example, {+adf+} creates a Parquet group named ``clientId`` for
           the polymorphic field named ``clientId`` with two children named
           after its |bson| types, ``int`` and ``string``.

   .. tab:: Azure
      :tabid: azure

      String Data Type
      ~~~~~~~~~~~~~~~~

      {+adf+} interprets empty strings (``""``) as ``null`` values when
      parsing filenames. If you want {+adf+} to generate parseable
      filenames, wrap the field references that could have ``null``
      values using |convert| with an empty string ``onNull`` value.

      .. example::

         This example shows how to handle null values in the ``year``
         field when creating a filename from the field value.

         .. code-block:: json
            :copyable: false
            :linenos:

            {
              "$out": {
                "azure": {
                  "serviceURL": "http://mystorageaccount.blob.core.windows.net/",
                  "container": "my-container",
                  "region": "eastus2",
                  "filename": {
                    "$concat": [
                      "big-box-store/",
                      {
                        "$convert": {
                          "input": "$year",
                          "to": "string",
                          "onNull": ""
                        }
                      }, "/"
                    ]
                  },
                  "format": {
                    "name": "json.gz",
                    "maxFileSize": "200MiB"
                  }
                }
              }
            }

      Number of Unique Fields
      ~~~~~~~~~~~~~~~~~~~~~~~

      When writing to :ref:`CSV <adf-csv-tsv-data>`,
      :ref:`TSV <adf-csv-tsv-data>`, or :ref:`Parquet <adf-parquet-data>`
      file format, {+adf+} doesn't support more than 32000 unique fields.
      
      CSV and TSV File Format
      ~~~~~~~~~~~~~~~~~~~~~~~

      When writing to CSV or TSV format, {+adf+} does not support the 
      following data types in the documents: 

      - Arrays 
      - DB pointer 
      - JavaScript 
      - JavaScript code with scope
      - Minimum or maximum key data type

      In a CSV file, {+adf+} represents nested documents using the dot 
      (``.``) notation. For example, {+adf+} writes 
      ``{ x: { a: 1, b: 2 } }`` as the following in the CSV file: 

      .. code-block:: shell 
         :copyable: false 

         x.a,x.b 
         1,2
      
      {+adf+} represents all other data types as strings. Therefore, 
      the data types in MongoDB read back from the CSV file may not be 
      the same as the data types in the original |bson| documents from 
      which the data types were written.

      Parquet File Format
      ~~~~~~~~~~~~~~~~~~~

      For Parquet, {+adf+} reads back fields with null or undefined 
      values as missing because Parquet doesn't distinguish between 
      null or undefined values and missing values. Although {+adf+} 
      supports all data types, for |bson| data types that do not have 
      a direct equivalent in Parquet, such as JavaScript, regular 
      expression, etc., it:
             
      - Chooses a representation that allows 
        the resulting Parquet file to be read back using a non-MongoDB 
        tool.
      - Stores a MongoDB schema in the Parquet file's key/value 
        metadata so that {+adf+} can reconstruct the original |bson| 
        document with the correct data types if the Parquet file is 
        read back by {+adf+}.

      .. example:: 

         Consider the following |bson| documents:

         .. code-block:: json
            :copyable: false 

            {
              "clientId": 102, 
              "phoneNumbers": ["123-4567", "234-5678"], 
              "clientInfo": {
                "name": "Taylor",
                "occupation": "teacher"
              }
            }
            {
              "clientId": "237", 
              "phoneNumbers" ["345-6789"]
              "clientInfo": {
                "name": "Jordan"
              }
            }

         If you write the preceding |bson| documents to Parquet format using
         :ref:`$out to Azure <adf-out-stage>`, the Parquet file schema for your
         |bson| documents would look similar to the following: 

         .. code-block:: shell 
            :copyable: false 

            message root {
              optional group clientId {
                optional int32 int;
                optional binary string (STRING);
              }
              optional group phoneNumbers (LIST) {
                repeated group list {
                  optional binary element (STRING);
                }
              }
              optional group clientInfo {
                optional binary name (STRING);
                optional binary occupation (STRING);
              }
            }

         Your Parquet data in |azure| Blob Storage would look similar to the following: 

         .. code-block:: shell
            :copyable: false 
            :linenos:

            clientId:
            .int = 102
            phoneNumbers:
            .list:
            ..element = "123-4567"
            .list:
            ..element = "234-5678"
            clientInfo:
            .name = "Taylor"
            .occupation = "teacher"

            clientId:
            .string = "237"
            phoneNumbers:
            .list:
            ..element = "345-6789"
            clientInfo:
            .name = "Jordan"

         The preceding example demonstrates how {+adf+} handles complex data
         types: 

         - {+adf+} maps documents at all levels to a Parquet group.
         - {+adf+} encodes arrays using the ``LIST`` logical type and the
           mandatory three-level list or element structure. To learn more,
           see `Lists
           <https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#nested-types>`__. 
         - {+adf+} maps polymorphic |bson| fields to a group of multiple single-type
           columns because Parquet doesn't support polymorphic columns. 
           {+adf+} names the group after the |bson| field. In the preceding
           example, {+adf+} creates a Parquet group named ``clientId`` for
           the polymorphic field named ``clientId`` with two children named
           after its |bson| types, ``int`` and ``string``.

   .. tab:: GCP
      :tabid: gcp

      String Data Type
      ~~~~~~~~~~~~~~~~

      {+adf+} interprets empty strings (``""``) as ``null`` values when
      parsing filenames. If you want {+adf+} to generate parseable
      filenames, wrap the field references that could have ``null``
      values using |convert| with an empty string ``onNull`` value.

      .. example::

         This example shows how to handle null values in the ``year``
         field when creating a filename from the field value.

         .. code-block:: json
            :copyable: false
            :linenos:

            {
              "$out": {
                "gcs": {
                  "bucket": "my-gcs-bucket",
                  "region": "us-central1",
                  "filename": {
                    "$concat": [
                      "big-box-store/",
                      {
                        "$convert": {
                          "input": "$year",
                          "to": "string",
                          "onNull": ""
                        }
                      }, "/"
                    ]
                  },
                  "format": {
                    "name": "json.gz",
                    "maxFileSize": "200MiB"
                  }
                }
              }
            }

      Number of Unique Fields
      ~~~~~~~~~~~~~~~~~~~~~~~

      When writing to :ref:`CSV <adf-csv-tsv-data>`,
      :ref:`TSV <adf-csv-tsv-data>`, or :ref:`Parquet <adf-parquet-data>`
      file format, {+adf+} doesn't support more than 32000 unique fields.
      
      CSV and TSV File Format
      ~~~~~~~~~~~~~~~~~~~~~~~

      When writing to CSV or TSV format, {+adf+} does not support the 
      following data types in the documents: 

      - Arrays 
      - DB pointer 
      - JavaScript 
      - JavaScript code with scope
      - Minimum or maximum key data type

      In a CSV file, {+adf+} represents nested documents using the dot 
      (``.``) notation. For example, {+adf+} writes 
      ``{ x: { a: 1, b: 2 } }`` as the following in the CSV file: 

      .. code-block:: shell 
         :copyable: false 

         x.a,x.b 
         1,2
      
      {+adf+} represents all other data types as strings. Therefore, 
      the data types in MongoDB read back from the CSV file may not be 
      the same as the data types in the original |bson| documents from 
      which the data types were written.

      Parquet File Format
      ~~~~~~~~~~~~~~~~~~~

      For Parquet, {+adf+} reads back fields with null or undefined 
      values as missing because Parquet doesn't distinguish between 
      null or undefined values and missing values. Although {+adf+} 
      supports all data types, for |bson| data types that do not have 
      a direct equivalent in Parquet, such as JavaScript, regular 
      expression, etc., it:
             
      - Chooses a representation that allows 
        the resulting Parquet file to be read back using a non-MongoDB 
        tool.
      - Stores a MongoDB schema in the Parquet file's key/value 
        metadata so that {+adf+} can reconstruct the original |bson| 
        document with the correct data types if the Parquet file is 
        read back by {+adf+}.

      .. example:: 

         Consider the following |bson| documents:

         .. code-block:: json
            :copyable: false 

            {
              "clientId": 102, 
              "phoneNumbers": ["123-4567", "234-5678"], 
              "clientInfo": {
                "name": "Taylor",
                "occupation": "teacher"
              }
            }
            {
              "clientId": "237", 
              "phoneNumbers" ["345-6789"]
              "clientInfo": {
                "name": "Jordan"
              }
            }

         If you write the preceding |bson| documents to Parquet format using
         :ref:`$out to GCP <adf-out-stage>`, the Parquet file schema for your
         |bson| documents would look similar to the following: 

         .. code-block:: shell 
            :copyable: false 

            message root {
              optional group clientId {
                optional int32 int;
                optional binary string; (STRING)
              }
              optional group phoneNumbers (LIST) {
                repeated group list {
                  optional binary element (STRING);
                }
              }
              optional group clientInfo {
                optional binary name (STRING);
                optional binary occupation (STRING);
              }
            }

         Your Parquet data on {+gcs+} would look similar to the following: 

         .. code-block:: shell
            :copyable: false 
            :linenos:

            clientId:
            .int = 102
            phoneNumbers:
            .list:
            ..element = "123-4567"
            .list:
            ..element = "234-5678"
            clientInfo:
            .name = "Taylor"
            .occupation = "teacher"

            clientId:
            .string = "237"
            phoneNumbers:
            .list:
            ..element = "345-6789"
            clientInfo:
            .name = "Jordan"

         The preceding example demonstrates how {+adf+} handles complex data
         types: 

         - {+adf+} maps documents at all levels to a Parquet group.
         - {+adf+} encodes arrays using the ``LIST`` logical type and the
           mandatory three-level list or element structure. To learn more,
           see `Lists
           <https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#nested-types>`__. 
         - {+adf+} maps polymorphic |bson| fields to a group of multiple single-type
           columns because Parquet doesn't support polymorphic columns. 
           {+adf+} names the group after the |bson| field. In the preceding
           example, {+adf+} creates a Parquet group named ``clientId`` for
           the polymorphic field named ``clientId`` with two children named
           after its |bson| types, ``int`` and ``string``.

   .. tab:: Atlas Cluster
      :tabid: atlas

      This section applies only to cloud service provider storage offerings.

.. _adf-out-stage-errors:

Error Output
------------

.. tabs::

   .. tab:: S3
      :tabid: s3

      {+adf+} uses the error handling mechanism described below for documents
      that enter the :pipeline:`$out` stage and cannot be written for one of the following reasons:
      
      - The ``s3.filename`` does not evaluate to a string value. 
      - The ``s3.filename`` evaluates to a file that cannot be written to.
      - The ``s3.format.name`` is set to ``csv``, ``tsv``, ``csv.gz``, or ``tsv.gz`` and the document passed to :pipeline:`$out`
        contains data types that are not supported by the specified file format. For a full list of unsupported data types, see :ref:`CSV and TSV File Format <csv_and_tsv_format>`. 

      
      If :pipeline:`$out` encounters one of the above errors while processing a document, {+adf+} writes to the following three special error files in the path
      ``s3://<bucket-name>/atlas-data-lake-<correlation-id>/``: 

      .. list-table:: Error Files
         :widths: 10 30
         :header-rows: 1

         * - Error File Name
           - Description
         * - out-error-docs/<i>.json
           - {+adf+} writes the document that encountered an error to this file. 
             ``i`` begins with ``1`` and increments whenever the file being written to reaches the ``maxFileSize``. 
             Then, any further documents are written to the new file ``out-error-docs/<i+1>.json``. 
         * - out-error-index/<i>.json
           - {+adf+} writes an error message to this file.
             Each error message contains a description of the error and an index value ``n`` that begins with ``0``
             and increments with each additional error message written to the file.        
             ``i`` begins with ``1`` and increments whenever the file being written to reaches the ``maxFileSize``.  
             Then, any further error messages are written to the new file ``out-error-docs/<i+1>.json``. 
         * - out-error-summary.json
           - {+adf+} writes a single summary document for each type of error encountered during an aggregation operation to this file.
             Each summary document contains a description of the type of error and a count of the number of documents that encountered that type of error.

      .. example::

         This example shows how to generate error files using :pipeline:`$out` in a {+fdi+}. 

         The following aggregation pipeline sorts documents in the ``analytics.customers`` sample dataset collection 
         by descending customer birthdate and attempts to write the ``_id``, ``name`` and ``accounts`` fields of the 
         youngest three customers to the file named ``youngest-customers.csv`` in the |s3| bucket named ``customer-data``.   
        
         .. code-block:: json
            :copyable: true

            db.customers.aggregate([ 
               { $sort: { "birthdate"  :  -1 } }, 
               { $unset: [ "username", "address", "email", "tier_and_details", "birthdate" ] }, 
               { $limit: 3 }, 
               { $out: {
                    "s3": {
                       "bucket": "customer-data", 
                       "filename": "youngest-customers",
                       "region":"us-east-2", 
                       "format": {
                         "name": "csv"
                       }
                    }
                } 
             ])

         Because ``accounts`` is an array field, :pipeline:`$out` encounters an error when it tries to write
         a document to ``s3.format.name`` ``csv``. To handle these errors, {+adf+} writes to the following three error files: 
        
         - The following output shows the first of three documents written to the ``out-error-docs/1.json`` file: 

           .. code-block:: json
              :copyable: false
              :caption: s3://customer-data/atlas-data-lake-1773b3d5e2a7f3858530daf5/out-error-docs/1.json

              {
                "_id" : {"$oid":"5ca4bbcea2dd94ee58162ba7"},
                "name": "Marc Cain",
                "accounts": [{"$numberInt":"980440"}, {"$numberInt":"626807"}, {"$numberInt":"313907"}, {"$numberInt":"218101"}, {"$numberInt":"157495"}, {"$numberInt":"736396"}],
              }
        
         - The following output shows the first of three error messages written to the ``out-error-index/1.json`` file.  
           The ``n`` field starts at 0 and increments for each error written to the file.  

           .. code-block:: json
              :copyable: false
              :caption: s3://customer-data/atlas-data-lake-1773b3d5e2a7f3858530daf5/out-error-index/1.json

              { 
                "n" : {"$numberInt": "0"},
                "error" : "field accounts is of unsupported type array"
              }  

         - The following output shows the error summary document written to the ``out-error-summary`` file. 
           The ``count`` field represents the number of documents passed to :pipeline:`$out` that encountered 
           an error due to the ``accounts`` array field. 

           .. code-block:: json
              :copyable: false
              :caption: s3://customer-data/atlas-data-lake-1773b3d5e2a7f3858530daf5/out-error-summary.json

                {
                 "errorType": "field accounts is of unsupported type array",
                 "count": {"$numberInt":"3"}
                } 

   .. tab:: Azure
      :tabid: azure

      {+adf+} uses the error handling mechanism described below for documents
      that enter the :pipeline:`$out` stage and cannot be written for one of the following reasons:
      
      - The ``azure.filename`` does not evaluate to a string value. 
      - The ``azure.filename`` evaluates to a file that cannot be written to.
      - The ``azure.format.name`` is set to ``csv``, ``tsv``, ``csv.gz``, or ``tsv.gz`` and the document passed to :pipeline:`$out`
        contains data types that are not supported by the specified file format. For a full list of unsupported data types, 
        see :ref:`CSV and TSV File Format <csv_and_tsv_format>`. 
      
      If :pipeline:`$out` encounters one of the above errors while processing a document, 
      {+adf+} writes to the following three special error files in the path
      ``http://<storage-account>.blob.core.windows.net/<container-name>/atlas-data-lake-<correlation-id>/``:

      .. list-table:: Error Files
        :widths: 10 30
        :header-rows: 1

        * - Error File Name
          - Description
        * - out-error-docs/<i>.json
          - {+adf+} writes the document that encountered an error to this file. 

            ``i`` begins with ``1`` and increments whenever the file being written to reaches the ``maxFileSize``. 
            Then, any further documents are written to the new file ``out-error-docs/<i+1>.json``. 
        * - out-error-index/<i>.json
          - {+adf+} writes an error message to this file.
            Each error message contains a description of the error and an index value ``n`` that begins with ``0``
            and increments with each additional error message written to the file.
            
            ``i`` begins with ``1`` and increments whenever the file being written to reaches the ``maxFileSize``.  
            Then, any further error messages are written to the new file ``out-error-docs/<i+1>.json``. 
        * - out-error-summary.json
          - {+adf+} writes a single summary document for each type of error encountered during an aggregation operation to this file.
            Each summary document contains a description of the type of error and a count of the number of documents that encountered that type of error.


      .. example::

         This example shows how to generate error files using :pipeline:`$out` in a {+fdi+}. 

         The following aggregation pipeline sorts documents in the ``analytics.customers`` sample dataset collection 
         by descending customer birthdate and attempts to write the ``_id``, ``name`` and ``accounts`` fields of the 
         youngest three customers to the file named ``youngest-customers.csv`` in the |azure| Blob Storage container 
         named ``customer-data``.
        
         .. code-block:: json
            :copyable: true

            db.customers.aggregate([ 
               { $sort: { "birthdate"  :  -1 } }, 
               { $unset: [ "username", "address", "email", "tier_and_details", "birthdate" ] }, 
               { $limit: 3 }, 
               { $out: {
                    "azure": {
                       "serviceURL": "https://myserviceaccount.blob.core.windows.net"
                       "container": "customer-data", 
                       "filename": "youngest-customers",
                       "region":"eastus2", 
                       "format": {
                         "name": "csv"
                       }
                    }
                } 
             ])

         Because ``accounts`` is an array field, :pipeline:`$out` encounters an error when it tries to write
         a document to ``azure.format.name`` ``csv``. To handle these errors, {+adf+} writes to the following three error files: 
        
         - The following output shows the first of three documents written to the ``out-error-docs/1.json`` file: 

           .. code-block:: json
              :copyable: false
              :caption: http://mystorageaccount.blob.core.windows.net/customer-data/atlas-data-lake-1773b3d5e2a7f3858530daf5/out-error-docs/1.json

              {
                "_id" : {"$oid":"5ca4bbcea2dd94ee58162ba7"},
                "name": "Marc Cain",
                "accounts": [{"$numberInt":"980440"}, {"$numberInt":"626807"}, {"$numberInt":"313907"}, {"$numberInt":"218101"}, {"$numberInt":"157495"}, {"$numberInt":"736396"}],
              }
        
         - The following output shows the first of three error messages written to the ``out-error-index/1.json`` file.  
           The ``n`` field starts at 0 and increments for each error written to the file.  

           .. code-block:: json
              :copyable: false
              :caption: http://mystorageaccount.blob.core.windows.net/customer-data/atlas-data-lake-1773b3d5e2a7f3858530daf5/out-error-index/1.json

              { 
                "n" : {"$numberInt": "0"},
                "error" : "field accounts is of unsupported type array"
              }  

         - The following output shows the error summary document written to the ``out-error-summary`` file. 
           The ``count`` field represents the number of documents passed to :pipeline:`$out` that encountered 
           an error due to the ``accounts`` array field. 

           .. code-block:: json
              :copyable: false
              :caption: http://mystorageaccount.blob.core.windows.net/customer-data/atlas-data-lake-1773b3d5e2a7f3858530daf5/out-error-summary.json

                {
                 "errorType": "field accounts is of unsupported type array",
                 "count": {"$numberInt":"3"}
                } 

   .. tab:: GCP
      :tabid: gcp

      {+adf+} uses the error handling mechanism described below for documents
      that enter the :pipeline:`$out` stage and cannot be written for one of the following reasons:
      
      - The ``gcs.filename`` does not evaluate to a string value. 
      - The ``gcs.filename`` evaluates to a file that cannot be written to.
      - The ``gcs.format.name`` is set to ``csv``, ``tsv``, ``csv.gz``, or ``tsv.gz`` and the document passed to :pipeline:`$out`
        contains data types that are not supported by the specified file format. For a full list of unsupported data types, see :ref:`CSV and TSV File Format <csv_and_tsv_format>`. 

      
      If :pipeline:`$out` encounters one of the above errors while processing a document, {+adf+} writes to the following three special error files in the path
      ``gcs://<bucket-name>/atlas-data-lake-<correlation-id>/``: 

      .. list-table:: Error Files
         :widths: 10 30
         :header-rows: 1

         * - Error File Name
           - Description
         * - out-error-docs/<i>.json
           - {+adf+} writes the document that encountered an error to this file. 
             ``i`` begins with ``1`` and increments whenever the file being written to reaches the ``maxFileSize``. 
             Then, any further documents are written to the new file ``out-error-docs/<i+1>.json``. 
         * - out-error-index/<i>.json
           - {+adf+} writes an error message to this file.
             Each error message contains a description of the error and an index value ``n`` that begins with ``0``
             and increments with each additional error message written to the file.        
             ``i`` begins with ``1`` and increments whenever the file being written to reaches the ``maxFileSize``.  
             Then, any further error messages are written to the new file ``out-error-docs/<i+1>.json``. 
         * - out-error-summary.json
           - {+adf+} writes a single summary document for each type of error encountered during an aggregation operation to this file.
             Each summary document contains a description of the type of error and a count of the number of documents that encountered that type of error.

      .. example::

         This example shows how to generate error files using :pipeline:`$out` in a {+fdi+}. 

         The following aggregation pipeline sorts documents in the ``analytics.customers`` sample dataset collection 
         by descending customer birthdate and attempts to write the ``_id``, ``name`` and ``accounts`` fields of the 
         youngest three customers to the file named ``youngest-customers.csv`` in the {+gcs+} bucket named ``customer-data``.   
        
         .. code-block:: json
            :copyable: true

            db.customers.aggregate([ 
               { $sort: { "birthdate"  :  -1 } }, 
               { $unset: [ "username", "address", "email", "tier_and_details", "birthdate" ] }, 
               { $limit: 3 }, 
               { $out: {
                    "gcs": {
                       "bucket": "customer-data", 
                       "filename": "youngest-customers",
                       "region":"us-central1", 
                       "format": {
                         "name": "csv"
                       }
                    }
                } 
             ])

         Because ``accounts`` is an array field, :pipeline:`$out` encounters an error when it tries to write
         a document to ``gcs.format.name`` ``csv``. To handle these errors, {+adf+} writes to the following three error files: 
        
         - The following output shows the first of three documents written to the ``out-error-docs/1.json`` file: 

           .. code-block:: json
              :copyable: false
              :caption: gcs://customer-data/atlas-data-lake-1773b3d5e2a7f3858530daf5/out-error-docs/1.json

              {
                "_id" : {"$oid":"5ca4bbcea2dd94ee58162ba7"},
                "name": "Marc Cain",
                "accounts": [{"$numberInt":"980440"}, {"$numberInt":"626807"}, {"$numberInt":"313907"}, {"$numberInt":"218101"}, {"$numberInt":"157495"}, {"$numberInt":"736396"}],
              }
        
         - The following output shows the first of three error messages written to the ``out-error-index/1.json`` file.  
           The ``n`` field starts at 0 and increments for each error written to the file.  

           .. code-block:: json
              :copyable: false
              :caption: gcs://customer-data/atlas-data-lake-1773b3d5e2a7f3858530daf5/out-error-index/1.json

              { 
                "n" : {"$numberInt": "0"},
                "error" : "field accounts is of unsupported type array"
              }  

         - The following output shows the error summary document written to the ``out-error-summary`` file. 
           The ``count`` field represents the number of documents passed to :pipeline:`$out` that encountered 
           an error due to the ``accounts`` array field. 

           .. code-block:: json
              :copyable: false
              :caption: gcs://customer-data/atlas-data-lake-1773b3d5e2a7f3858530daf5/out-error-summary.json

                {
                 "errorType": "field accounts is of unsupported type array",
                 "count": {"$numberInt":"3"}
                }

   .. tab:: Atlas Cluster
      :tabid: atlas

      This section applies only to cloud service provider storage.

.. seealso::

   - :manual:`$out Aggregation Stage Reference 
     </reference/operator/aggregation/out>` 
   - `Tutorial: Federated Queries and $out to S3 
     <https://www.mongodb.com/developer/products/atlas/atlas-data-lake-federated-queries-out-aws-s3/>`__
