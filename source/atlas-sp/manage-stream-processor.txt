.. _streams-manage-processor:

=====================================
Manage Stream Processors
=====================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

.. include:: includes/atlas-sp/private-preview.rst

An {+atlas-sp+} stream processor applies the logic of a uniquely named 
:ref:`stream aggregation pipeline <stream-aggregation>` to your 
streaming data. {+atlas-sp+} saves each stream processor definition to
persistent storage so that it can be reused. You can only use a given 
stream processor in the 
:ref:`{+spi+} <manage-spi>` its definition is 
stored in.

Prerequisites
-------------

To create and manage a stream processor, you must have:

- A :ref:`{+spi+} <manage-spi>`
- ``mongosh``
- |service| :authrole:`Project Owner` privileges

.. This list will change as we add additional ways of performing these
   procedures. For the August 4th deadline, we limit this to mongosh 
   methods only.

.. _streams-manage-create:

Create a Stream Processor
-------------------------

To create a new stream processor with ``mongosh``, use the 
``sp.createStreamProcessor()`` method. It has the following syntax:

.. code-block:: sh

   sp.createStreamProcessor(<name>, <pipeline>, <options>)

.. list-table::
   :widths: 20 10 50 20
   :header-rows: 1

   * - Argument
     - Type
     - Necessity
     - Description

   * - ``name``
     - string
     - Required
     - Logical name for the stream processor. This must be unique
       within the {+spi+}.

   * - ``pipeline``
     - array
     - Required
     - :ref:`Stream aggregation pipeline <stream-aggregation>` you
       want to apply to your streaming data.

   * - ``options``
     - object
     - Optional
     - Object defining various optional settings for your stream
       processor.

   * - ``options.dlq``
     - object
     - Conditional
     - Object assigning a 
       :term:`dead letter queue` for your {+spi+}. This field is 
       necessary if you define the ``options`` field.

   * - ``options.dlq.connectionName``
     - string
     - Conditional
     - Human-readable label that identifies a connection in your 
       connection registry. This connection must reference an 
       |service| {+cluster+}. This field is necessary if you define the
       ``options.dlq`` field.

   * - ``options.dlq.db``
     - string
     - Conditional
     - Name of an |service| database on the {+cluster+} specified 
       in ``options.dlq.connectionName``. This field is necessary if 
       you define the ``options.dlq`` field.

   * - ``options.dlq.coll``
     - string
     - Conditional
     - Name of a collection in the database specified in
       ``options.dlq.db``. This field is necessary if you 
       define the ``options.dlq`` field.

.. procedure::
   :style: normal

   .. step:: Connect to your {+spi+}.
      
      Use the connection string associated with your {+spi+}
      to connect using :ref:`mongosh <connect-mongo-shell>`.

      .. example::

         The following command connects to a {+spi+} as a user named
         ``streamOwner`` using SCRAM-SHA-256 authentication:

         .. code-block:: sh

            mongosh "mongodb://atlas-stream-78xq9gi1v4b4288x06a73e9f-zi30g.virginia-usa.a.query.mongodb-qa.net/?authSource=%24external&authMechanism=MONGODB-X509" \\ 
            --tls --authenticationDatabase admin --username streamOwner

         Provide your user password when prompted.

   .. step:: Define a pipeline.

      In the ``mongosh`` prompt, assign an array containing the
      aggregation stages you want to apply to a variable named 
      ``pipeline``. 
      
      The following example uses the ``stuff`` topic in
      the  ``myKafka`` connection in the connection registry as the 
      :pipeline:`$source`, matches records where the ``temperature`` 
      field has a value of ``46`` and emits the processed messages to 
      the ``output`` topic of the ``mySink`` connection in 
      the connection registry:

      .. code-block:: sh

         pipeline = [
          {$source: {"connectionName": "myKafka", "topic": "stuff", partitionCount: 3}},
          {$match: { temperature: 46 }},
          {
            "$emit": {
              "connectionName": "mySink",
              "topic" : "output",
            }  
          }
         ]

   .. step:: (Optional) Define a :term:`DLQ <dead letter queue>`.

      In the ``mongosh`` prompt, assign an object containing the
      following properties of your DLQ:

      - Connection name
      - Database name
      - Collection name

      The following example defines a DLQ over the ``cluster01``
      connection, in the ``metadata.dlq`` database collection.

      .. code-block:: sh

         deadLetter = {
           connection = "cluster01",
           database = "metadata",
           collection = "dlq"
         }

   .. step:: Create a stream processor.

      The following command creates a stream processor named 
      ``proc01`` that applies the logic defined in ``pipeline``.
      Documents that throw errors in processing are written to the
      DLQ defined in ``deadLetter``.

      .. code-block:: sh
      
         sp.createStreamProcessor("proc01", pipeline, deadLetter)

.. _streams-list-procs:

List Available Stream Processors
--------------------------------

To list all available stream processors on the current {+spi+} with 
``mongosh``, use the ``sp.listStreamProcessors()`` method. It returns 
a list of documents containing the name, start time, current state, and 
pipeline associated with each stream processor. It has the following 
syntax:

.. code-block:: sh

   sp.listStreamProcessors(<filter>)

``<filter>`` is a document specifying which field(s) to filter the list 
by.

.. example::

   The following example shows a return value for an unfiltered 
   request:

   .. io-code-block::
      :copyable: true

      .. input:: 
         :language: sh

         sp.listStreamProcessors()

      .. output:: 
         :language: json
         :linenos:

         {
           "id": '0135',
           "name": "proc01",
           "last_modified": ISODate("2023-03-20T20:15:54.601Z"),
           "state": "RUNNING",
           "error_msg": '',
           "pipeline": [{
             $source: {
               "connectionName": "myKafka", 
               "topic": "stuff", 
               "partitionCount": 1
             }
           },
           {
             $match: { 
               temperature: 46 
             }
           },
           {
             "$emit": {
               "connectionName": "mySink",
               "topic" : "output",
             }  
           }]
         },
         {   
           "id": '0218',
           "name": "proc02",
           "last_modified": ISODate("2023-03-21T20:17:33.601Z"),
           "state": "STOPPED",
           "error_msg": '',
           "pipeline": [{
             $source: {
               "connectionName": "myKafka", 
               "topic": "things", 
               "partitionCount": 1
             }
           },
           {
             $match: { 
               temperature: 41 
             }
           },
           {
             "$emit": {
               "connectionName": "mySink",
               "topic" : "results",
             }  
           }]
         }

   If you run the command again on the same {+spi+}, filtering for a 
   ``"state"`` of ``"running"``, you see the following output:

   .. io-code-block::
      :copyable: true

      .. input:: 
         :language: sh

         sp.listStreamProcessors({"state": "running"})

      .. output:: 
         :language: json
         :linenos:

         {
           "id": '0135',
           "name": "proc01",
           "last_modified": ISODate("2023-03-20T20:15:54.601Z"),
           "state": "RUNNING",
           "error_msg": '',
           "pipeline": [{
             $source: {
               "connectionName": "myKafka", 
               "topic": "stuff", 
               "partitionCount": 1
             }
           },
           {$match: {
             temperature: 46
             }
           },
           {
             "$emit": {
               "connectionName": "mySink",
               "topic" : "output",
             }  
           }]
         }

.. _streams-manage-start:

Start a Stream Processor
------------------------

To start an existing stream processor with ``mongosh``, use the 
``sp.<streamprocessor>.start()`` method. ``<streamprocessor>`` must be 
the name of a stream processor defined for the current {+spi+}.

For example, to start a stream processor named ``proc01``, run the 
following command:

.. code-block:: sh

   sp.proc01.start()

This method returns: 

- ``true`` if the stream processor exists and is not currently running. 

- ``false`` if you try to start a stream processor that doesn't exist, 
  or exists and is not currently running.

Stop a Stream Processor
-------------------------

To stop an existing stream processor with ``mongosh``, use the 
``sp.<streamprocessor>.stop()`` method. ``<streamprocessor>`` must be 
the name of a currently running stream processor defined for the 
current {+spi+}.

For example, to stop a stream processor named ``proc01``, run the 
following command:

.. code-block:: sh

   sp.proc01.stop()

This method returns: 

- ``true`` if the stream processor exists and is currently running. 

- ``false`` if the stream processor doesn't exist, or if the stream 
  processor is not currently running.

Drop a Stream Processor
-------------------------

To delete an existing stream processor with ``mongosh``, use the 
``sp.<streamprocessor>.drop()`` method. ``<streamprocessor>`` must be 
the name of a stream processor defined for the current {+spi+}. You 
must stop a stream processor before you can drop it.

For example, to drop a stream processor named ``proc01``, run the 
following command:

.. code-block:: sh

   sp.proc01.drop()

This method returns: 

- ``true`` if the stream processor exists and is currently stopped.

- ``false`` if the stream processor doesn't exist, or if the stream 
  processor is currently running. 

When you drop a stream processor, all resources that {+atlas-sp+} 
provisioned for it are destroyed, along with all saved state.

Sample from a Stream Processor
------------------------------

To return an array of sampled results from an existing stream processor 
to ``STDOUT`` with ``mongosh``, use the 
``sp.<streamprocessor>.sample()`` method. ``<streamprocessor>`` must be 
the name of a currently running stream processor defined for the 
current {+spi+}. For example, the following command samples from a 
stream processor named ``proc01``.

.. code-block:: sh

   sp.proc01.sample()

This command runs continuously until you cancel it using ``CTRL-C``, or
until the returned samples cumulatively reach 40 MB in size.

View Statistics of a Stream Processor
-------------------------------------

To return a document summarizing the current status of an existing 
stream processor with ``mongosh``, use the 
``sp.<streamprocessor>.stats()`` method. ``streamprocessor`` must be 
the name of a currently running stream processor defined for the 
current {+spi+}. It has the following syntax:

.. code-block:: sh

   sp.<streamprocessor>.stats(<scale>)

``scale`` is an integer that sets the unit of size to use for the size
of items in the output. By default, item size is displayed in bytes. To
display in KB, specify a ``scale`` of ``1024``. 

The output document has the following fields:

.. list-table::
   :widths: 20 20 60
   :header-rows: 1

   * - Field
     - Type
     - Description

   * - ``ns``
     - string
     - The namespace the stream processor is defined in.

   * - ``name``
     - string
     - The name of the stream processor.

   * - ``status``
     - integer
     - The error status of the stream processor. ``0`` indicates a
       correct run state, ``1`` indicates an error state.

   * - ``scaleFactor``
     - integer
     - The scale in which the size field displays. If set to ``1``,
       sizes display in bytes. If set to ``1024``, sizes display in
       kilobytes.

   * - ``inputDocs``
     - integer
     - The number of documents published to the stream. A document
       is considered 'published' to the stream once it passes
       through the :pipeline:`$source` stage, not when it passes 
       through the entire pipeline.

   * - ``inputBytes``
     - integer
     - Total number of bytes or kilobytes published to the stream. 
       Bytes are considered 'published' to the stream once they pass
       through the :pipeline:`$source` stage, not when it passes
       throug the entire pipeline.

   * - ``outputDocs``
     - integer
     - The number of documents processed by the stream. A document is
       considered 'processed' by the stream once it passes through the
       entire pipeline.

   * - ``outputBytes``
     - integer
     - The number of bytes or kilobytes published to the stream. Bytes
       are considered 'processed' by the stream once they pass through
       the entire pipeline.

For example, the following shows the status of a stream processor named 
``proc01`` on a {+spi+} named ``inst01`` with item sizes displayed in 
KB:

.. code-block:: sh

   sp.proc01.stats(1024)

   {
     ns: 'inst01',
     name: 'proc01',
     status: 0,
     scaleFactor: 1024, 
     inputDocs: 12,
     inputBytes: 1333,
     outputDocs: 5,
     outputBytes: 607,
   }
