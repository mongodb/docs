Use MongoDB's :manual:`aggregation pipeline
</core/aggregation-pipeline/>` to apply filtering rules and perform
aggregation operations when reading data from MongoDB into Spark.

.. include:: /includes/example-load-dataframe.rst

Add the ``option()`` method to ``spark.read()`` from
within the ``pyspark`` shell to specify an aggregation pipeline
to use when creating a DataFrame.

.. code-block:: none

   pipeline = "{'$match': {'type': 'apple'}}"
   df = spark.read.format("mongodb").option("pipeline", pipeline).load()
   df.show()

In the ``pyspark`` shell, the operation prints the following output:

.. code-block:: none

   +---+---+-----+
   |_id|qty| type|
   +---+---+-----+
   |1.0|5.0|apple|
   +---+---+-----+
