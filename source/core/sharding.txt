.. index:: fundamentals; sharding
.. _sharding-fundamentals:

=====================
Sharding Fundamentals
=====================

.. default-domain:: mongodb

MongoDB's sharding system allows users to :term:`partition` the data of a
:term:`collection` within a database to distribute documents
across a number of :program:`mongod` instances or :term:`shards <shard>`.

Sharded clusters allow increases in write capacity, provide the ability to
support larger working sets, and raise the limits of total data size beyond
the physical resources of a single node.

This document provides an overview of the fundamental concepts and
operation of sharding with MongoDB.

.. seealso:: The ":doc:`/sharding`" index for a list of all documents
   in this manual that contain information related to the operation
   and use of shard clusters in MongoDB. This includes:

   - :doc:`/core/sharding-internals`
   - :doc:`/administration/sharding`
   - :doc:`/administration/sharding-architectures`

   If you are not yet familiar with sharding, see the :doc:`Sharding
   FAQ </faq/sharding>` document.

Overview
--------

Features
~~~~~~~~

With sharding MongoDB automatically distributes data among a
collection of :program:`mongod` instances. Sharding, as implemented in
MongoDB has the following features:

.. glossary::

   Range-based Data Partitioning
      MongoDB distributes documents among :term:`shards <shard>` based
      on the value of the :ref:`shard key <sharding-shard-key>`. Each
      :term:`chunk` represents a block of :term:`documents <document>`
      with values that fall within a specific range. When chunks grow
      beyond the :ref:`chunk size <sharding-chunk-size>`, MongoDB
      divides the chunks into smaller chunks (i.e. :term:`splitting
      <split>`) based on the shard key.

   Automatic Data Volume Distribution
      The sharding system automatically balances data across the
      cluster without intervention from the application
      layer. Effective automatic sharding depends on a well chosen
      :ref:`shard key <sharding-shard-key>`, but requires no
      additional complexity, modifications, or intervention from
      developers.

   Transparent Query Routing
      Sharding is completely transparent to the application layer,
      because all connections to a shard cluster go through
      :program:`mongos`. Sharding in MongoDB requires some
      :ref:`basic initial configuration <sharding-procedure-setup>`,
      but ongoing function is entirely transparent to the application.

   Horizontal Capacity
      Sharding increases capacity in two ways:

      #. Effective partitioning of data can provide additional write
         capacity by distributing the write load over a number of
         :program:`mongod` instances.

      #. Given a shard key with sufficient :ref:`cardinality
         <sharding-shard-key-cardinality>`, partitioning data allows
         users to increase the potential amount of data to mange
         with MongoDB and expand the :term:`working set`.

A typical :term:`shard cluster` consists of config servers which
store metadata that maps :term:`chunks <chunk>` to shards, the
:program:`mongod` instances which hold data (i.e the :term:`shards
<shard>`,) and lightweight routing processes, :doc:`mongos
</reference/mongos>`, which route operations to the correct shard
based the cluster configuration.

Indications
~~~~~~~~~~~

While sharding is a powerful and compelling feature, it comes with
significant :ref:`infrastructure requirements <sharding-requirements>`
and some limited complexity costs. As a result, it's important to use
sharding only as necessary, and when indicated by actual operational
requirements. Consider the following overview of indications it may be
time to consider sharding.

You should consider deploying a :term:`shard cluster`, if:

- your data set approaches or exceeds the storage capacity of a single
  node in your system.

- the size of your system's active :term:`working set` *will soon*
  exceed the capacity of the *maximum* amount of RAM for your system.

- your system has a large amount of write activity, a single
  MongoDB instance cannot write data fast enough to meet demand, and
  all other approaches have not reduced contention.

If these attributes are not present in your system, sharding will only
add additional complexity to your system without providing much benefit.
If you plan to eventually partition your data, you should 
consider which collections you will want to shard along with
the corresponding shard keys.

.. _sharding-capacity-planning:

.. warning::

   It takes time and resources to deploy sharding, and if your system
   has *already* reached or exceeded its capacity, you will have a
   difficult time deploying sharding without impacting your
   application.

   As a result, if you think you're going to need sharding eventually,
   it's critical that you **do not** wait until your system is
   overcapacity to enable sharding.

.. index:: sharding; requirements
.. _sharding-requirements:

Requirements
------------

.. _sharding-requirements-infrastructure:

Infrastructure
~~~~~~~~~~~~~~

A :term:`shard cluster` has the following components:

- Three :term:`config servers <config database>`.

  These special :program:`mongod` instances store the metadata for the
  cluster. The :program:`mongos` instances cache this data and use it
  to determine which :term:`shard` is responsible for which
  :term:`chunk`.  

  For testing purposes you may deploy a shard cluster with a single
  configuration server, but this is not recommended for production.

  .. warning::

     If you choose to run a single config server and it becomes
     inoperable for any reason, the cluster will be unusable.

- Two or more :program:`mongod` instances, to hold data.

  These are "normal," :program:`mongod` instances that hold all of the
  actual data for the cluster.

  Typically, one or more :term:`replica sets <replica set>`, consisting of
  multiple :program:`mongod` instances, compose a shard cluster. The members
  of the replica set provide redundancy for the data and increase the
  overall reliability and robustness of the cluster.

  .. warning::

     MongoDB enables data :term:`partitioning <partition>`
     (i.e. sharding) on a *per collection* basis. You *must* access
     all data in a shard cluster via the :program:`mongos`
     instances.

- One or more :program:`mongos` instances.

  These nodes cache cluster metadata from the config servers and
  direct queries from the application layer to the shards that hold
  the data.

  .. note::

     In most situations :program:`mongos` instances use minimal
     resources, and you can run them on your application servers
     without impacting application performance. However, if you use
     the :term:`aggregation framework` some processing may occur on
     the :program:`mongos` instances which can cause them to require more
     system resources.

Data
~~~~

Your cluster must manage a significant quantity of data for sharding
to have an effect on your collection. The default :term:`chunk` size
is 64 megabytes, [#chunk-size]_ and the :ref:`balancer
<sharding-balancing>` will not begin moving data until the imbalance
of chunks in the cluster exceeds the :ref:`migration threshold
<sharding-migration-thresholds>`.

Practically, this means that unless your cluster has enough data,
chunks will remain on the same shard. You can set a smaller chunk
size, or :ref:`manually create splits in your collection
<sharding-procedure-create-split>` using the :method:`sh.splitFind()`
and :method:`sh.splitAt()` operations in the :program:`mongo` shell.
Remember that the default chunk size and migration threshold are
explicitly configured to prevent unnecessary splitting or migrations.

While there are some exceptional situations where you may need to
shard a small collection of data, most of the time the additional
complexity added by sharding is not worth the operational costs unless
you need the additional concurrency/capacity for some reason. If you
have a small data set, the chances are that a properly configured
single MongoDB instance or replica set will be more than sufficient
for your persistence layer needs.

.. [#chunk-size] While the default chunk size is 64 megabytes, the
   size is :option:`user configurable <mongos --chunkSize>`. When
   deciding :term:`chunk` size, MongoDB (for defaults) and users (for
   custom values) must consider that: smaller chunks offer the
   possibility of more even data distribution, but increase the
   likelihood of chunk migrations. Larger chunks decrease the need for
   migrations, but increase the amount of time required for a chunk
   migration. See the ":ref:`sharding-chunk-size`" section in the
   :doc:`sharding-internals` document for more information on this
   topic.

.. index:: sharding; localhost
.. _sharding-localhost:

Sharding and "localhost" Addresses
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Because all components of a :term:`shard cluster` must communicate
with each other over the network, there are special restrictions
regarding the use of localhost addresses:

If you use either "localhost" or "``127.0.0.1``" as the host
identifier, then you must use "localhost" or "``127.0.0.1``" for *all*
host settings for any MongoDB instances in the cluster.  This applies
to both the ``host`` argument to :dbcommand:`addShard` and the value
to the :option:`mongos --configdb` run time option. If you mix
localhost addresses with remote host address, MongoDB will produce
errors.

.. index:: shard key
   single: sharding; shard key

.. _sharding-shard-key:
.. _shard-key:

Shard Keys
----------

.. TODO link this section to <glossary:shard key>

"Shard keys" refer to the :term:`field` that exists in every
:term:`document` in a collection that that MongoDB uses to distribute
documents among the :term:`shards <shard>`. Shard keys, like
:term:`indexes <index>`, can be either a single field, or may be a
compound key, consisting of multiple fields.

Remember, MongoDB's sharding is range-based: each :term:`chunk` holds
documents having specific range of values for the "shard key". Thus,
choosing the correct shard key can have a great impact on the
performance, capability, and functioning of your database and cluster.

Appropriate shard key choice depends on the schema of your data and
the way that your application queries and writes data to the database.

The ideal shard key:

- is easily divisible which makes it easy for MongoDB to distribute
  content among the shards. Shard keys that have a limited number of
  possible values are not ideal as they can result in some chunks that
  are "unsplitable." See the ":ref:`sharding-shard-key-cardinality`"
  section for more information.

- will distribute write operations among the cluster, to prevent any
  single shard from becoming a bottleneck. Shard keys that have a high
  correlation with insert time are poor choices for this reason;
  however, shard keys that have higher "randomness" satisfy this
  requirement better. See the   ":ref:`sharding-shard-key-write-scaling`"
  section for additional background.

- will make it possible for the :program:`mongos` to return most query
  operations directly from a single *specific* :program:`mongod`
  instance. Your shard key should be the primary field used by your
  queries, and fields with a high degree of "randomness" are poor
  choices for this reason. See the ":ref:`sharding-shard-key-query-isolation`"
  section for specific examples.

The challenge when selecting a shard key is that there is not always
an obvious choice. Often, an existing field in your collection may not be
the optimal key. In those situations, computing a special purpose
shard key into an additional field or using a compound shard key may
help produce one that is more ideal.

.. index:: sharding; config servers
.. index:: config servers
.. _sharding-config-server:

Config Servers
--------------

Config servers maintain the shard metadata in a config
database.  The :term:`config database <config database>` stores
the relationship between :term:`chunks <chunk>` and where they reside
within a :term:`shard cluster`. Without a config database, the
:program:`mongos` instances would be unable to route queries or write
operations within the cluster.

Config servers *do not* run as replica sets. Instead, a :term:`shard
cluster` operates with a group of *three* config servers which use a
two-phase commit process that ensures immediate consistency and
reliability.

For testing purposes you may deploy a shard cluster with a single
config server, but this is not recommended for production.

.. warning::

     If you choose to run a single config server and it becomes
     inoperable for any reason, the cluster will be unusable.

The actual load on configuration servers is small because each 
:program:`mongos` instances maintains a cached copy of the configuration 
database. 
MongoDB will write data to the config server only when:

- Creating splits in existing chunks, which happens as data in
  existing chunks exceeds the maximum chunk size.

- Migrating a chunk between shards.

If one or two configuration instances become unavailable, the
cluster's metadata becomes *read only*. It is still possible to read
and write data from the shards, but no chunk migrations or splits will
occur until all three servers are accessible. At the same time, config
server data is only read in the following situations:

- A new :program:`mongos` starts for the first time, or an existing
  :program:`mongos` restarts.

- After a chunk migration, the :program:`mongos` instances update
  themselves with the new cluster metadata.

If all three config servers are inaccessible, you can continue to use
the cluster as long as you don't restart the :program:`mongos`
instances until the after config servers are accessible again. If you
restart the :program:`mongos` instances and there are no accessible
config servers, the :program:`mongos` would be unable to direct
queries or write operations to the cluster.

Because the configuration data is small relative to the amount of data
stored in a cluster, the amount of activity is relatively low, and 100%
up time is not required for a functioning shard cluster. As a result,
backing up the config servers is not difficult. Backups of config
servers are critical as shard clusters become totally inoperable when
you lose all configuration instances and data. Precautions to ensure
that the config servers remain available and intact are critical.

  .. note::
  
     Configuration servers maintain metadata for only one shard cluster.
     You must have a separate configuration server or servers for each
     shard cluster you configure.


.. index:: mongos
.. _sharding-mongos:

:program:`mongos` and Querying
------------------------------

.. seealso:: ":doc:`/reference/mongos`" and the :program:`mongos`\-only
   settings: ":setting:`test`" and :setting:`chunkSize`.

Operations
~~~~~~~~~~

The :program:`mongos` provides a single unified interface to a sharded
cluster for applications using MongoDB. Except for the selection of a
:term:`shard key`, application developers and administrators need not
consider any of the :doc:`internal details of sharding <sharding-internals>`.

:program:`mongos` caches data from the :ref:`config server
<sharding-config-server>`, and uses this to route operations from
applications and clients to the :program:`mongod` instances.
:program:`mongos` have no *persistent* state and consume
minimal system resources.

The most common practice is to run :program:`mongos` instances on the
same systems as your application servers, but you can maintain
:program:`mongos` instances on the shards or on other dedicated
resources.

.. note::

   .. versionchanged:: 2.1

   Some aggregation operations using the :dbcommand:`aggregate`
   command (i.e. :method:`db.collection.aggregate()`,) will cause
   :program:`mongos` instances to require more CPU resources than in
   previous versions. This modified performance profile may dictate
   alternate architecture decisions if you use the :term:`aggregation
   framework` extensively in a sharded environment.

.. _sharding-query-routing:

Routing
~~~~~~~

:program:`mongos` uses information from :ref:`config servers
<sharding-config-server>` to route operations to the cluster as
efficiently as possible. In general, operations in a sharded
environment are either:

1. Targeted at a single shard or a limited group of shards based on
   the shard key.

2. Broadcast to all shards in the cluster that hold documents in a
   collection.

When possible you should design your operations to be as targeted as
possible. Operations have the following targeting characteristics:

- Query operations broadcast to all shards [#namespace-exception]_
  **unless** the :program:`mongos` can determine which shard or shard
  stores this data.

  For queries that include the shard key, :program:`mongos` can target
  the query at a specific shard. For example:

  For queries that contain a component of the shard key, the
  :program:`mongos` may be able to target the query at a limited
  subset of shards.

- All :method:`insert() <db.collection.insert()>` operations target to
  one shard.

- All single :method:`update() <db.collection.update()>` operations
  target to one shard. This includes :term:`upsert` operations.

- The :program:`mongos` broadcasts multi-update operations to every
  shard.

- The :program:`mongos` broadcasts :method:`remove()
  <db.collection.remove()>` operations to every shard unless the
  operation specifies the shard key in full.

While some operations must broadcast to all shards, you can improve
performance by using as many targeted operations as possible by
ensuring that your operations include the shard key.

.. [#namespace-exception] If a shard does not store chunks from a
   given collection, queries for documents in that collection are not
   broadcast to that shard.

.. index:: balancing
.. _sharding-balancing:

Balancing and Distribution
--------------------------

Balancing refers to the process that MongoDB uses to redistribute data
within a :term:`shard cluster` when some :term:`shards <shard>` have a
greater number of :term:`chunks <chunk>` than other shards. The
balancing process attempts to minimize the impact that balancing can
have on the cluster, by:

- Only moving one chunk at a time.

- Only initiating a balancing round when the difference in number of
  chunks between the shard with the greatest and the shard with the
  least number of chunks exceeds the :ref:`migration threshold
  <sharding-migration-thresholds>`.

Additionally, it's possible to disable the balancer on a temporary
basis for maintenance and to limit the window during which it runs to
prevent the balancing process from impacting production traffic.

.. seealso:: The ":ref:`"Balancing Internals
   <sharding-balancing-internals>`" and :ref:`Balancer Operations
   <sharding-balancing-operations>` for more information on balancing.

.. note::

   The balancing procedure for :term:`shard clusters <shard cluster>`
   is entirely transparent to the user and application layer. This
   documentation is only included for your edification and possible
   troubleshooting purposes.

.. index:: sharding; security
.. _sharding-security:

Security
--------

.. TODO migrate this content to /administration/security.txt when that
   document exists. See DOCS-79 for tracking this document.

.. note::

   You should always run shard clusters in trusted networking
   environments that control access to the cluster using network rules
   and restrictions to ensure that only known traffic reaches your
   :program:`mongod` and :program:`mongos` instances.

.. warning::  Limitations

   .. versionchanged:: 2.2
      Read only authentication is fully supported in shard
      clusters. Previously, in version 2.0, shard clusters would not
      enforce read-only limitations.

   .. versionchanged:: 2.0
      Shard clusters support authentication. Previously, in version
      1.8, shard clusters will not support authentication and access
      control. You must run your sharded systems in trusted
      environments.

To control access to a shard cluster, you must set the
:setting:`keyFile` option on all components of the shard cluster. Use
the :option:`--keyFile <mongos --keyFile>` run-time option or :the
setting:`keyFile` configuration option for all :program:`mongos`,
configuration instances, and shard :program:`mongod` instances.

There are two classes of security credentials in a shard cluster:
credentials for "admin" users (i.e. for the "admin" database) and
credentials for all other databases. These credentials reside in
different locations within the shard cluster and have different roles:

#. Admin database credentials reside on the config servers, to receive
   admin access to the cluster you *must* authenticate a session while
   connected to a :program:`mongos` instance using the "admin"
   database.

#. Other database credentials reside on the *primary* shard for the
   database.

This means that you *can* authenticate to these users and databases
while connected directly to the primary shard for a database. However,
for clarity and consistency all interactions between the client and
the database should use a :program:`mongos` instance.

.. note::

   Individual shards can store administrative credentials to their
   instance, which only permit access to a single shard. MongoDB
   stores these credentials in the shards "admin" databases and these
   credentials are *completely* distinct from the cluster-wide
   administrative credentials.
