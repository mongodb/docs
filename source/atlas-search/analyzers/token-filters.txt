.. _token-filters-ref:

=============
Token Filters
=============

.. default-domain:: mongodb

.. meta::
   :keywords: atlas search, token filter, stemming, reduce related words, redaction, remove sensitive information, create tokens, tokenize, asciiFolding, convert characters, daitchMokotoffSoundex, words that sound the same, edgeGram, take tokens from edge for n gram, icuFolding, character folding, icuNormalizer, normalize token, length, remove long tokens, remove short tokens, lowercase, nGram, configure token size, regex, regular expression, reverse, reverse string, shingle, token n grams, snowballStemming, stem tokens, stopword, remove words, trim, remove whitespace, custom analyzer, create an Atlas Search index
   :description: Use token filters in an Atlas Search custom analyzer to modify tokens, such as by stemming, lowercasing, or redacting sensitive information.

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol


Token Filters always require a type field, and some take additional 
options as well.

.. code-block:: json

   "tokenFilters": [
     {
       "type": "<token-filter-type>",
       "<additional-option>": <value>
     }
   ]

|fts| supports the following token filters.

.. _asciiFolding-tf-ref: 

asciiFolding
------------

The ``asciiFolding`` token filter converts alphabetic, numeric, and 
symbolic unicode characters that are not in the `Basic Latin Unicode 
block <https://en.wikipedia.org/wiki/Basic_Latin_(Unicode_block)>`__ 
to their ASCII equivalents, if available. It has the following 
attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type. 
       Value must be ``asciiFolding``.
     - yes
     -

   * - ``originalTokens`` 
     - string 
     - String that specifies whether to include or omit the original
       tokens in the output of the token filter. Value can be one of
       the following: 

       - ``include`` - include the original tokens with the 
         converted tokens in the output of the token filter. We 
         recommend this value if you want to support queries on both 
         the original tokens as well as the converted forms. 
       - ``omit`` - omit the original tokens and include only the 
         converted tokens in the output of the token filter. Use this 
         value if you want to query only on the converted forms of the 
         original tokens. 

     - no 
     - ``omit``

.. example:: 

   The following index definition example uses a custom analyzer 
   named ``asciiConverter``. It uses the :ref:`standard tokenizer
   <standard-tokenizer-ref>` with the ``asciiFolding`` token filter to 
   index the fields in the :ref:`example <custom-analyzers-eg-coll>` 
   collection and convert the field values to their ASCII equivalent. 

   .. code-block:: json 

      {
        "analyzer": "asciiConverter",
        "searchAnalyzer": "asciiConverter",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "asciiConverter",
            "tokenizer": {
              "type": "standard"
            },
            "tokenFilters": [
              {
                "type": "asciiFolding"
              }
            ]
          }
        ]
      }

   The following query searches the ``first_name`` field for names 
   using their ASCII equivalent.

   .. code-block:: json 

      db.minutes.aggregate([
        {
          $search: {
            "index": "default",
            "text": {
              "query": "Sian",
              "path": "page_updated_by.first_name"
            }
          }
        },
        {
          $project: {
            "_id": 1,
            "page_updated_by.last_name": 1,
            "page_updated_by.first_name": 1
          }
        }
      ])

   |fts| returns the following results: 

   .. code-block:: json
      :copyable: false 

      [
        {
           _id: 1,
           page_updated_by: { last_name: 'AUERBACH', first_name: 'Si√¢n'}
        }
      ]

.. _daitchmokotoffsoundex-tf-ref: 

daitchMokotoffSoundex
---------------------

The ``daitchMokotoffSoundex`` token filter creates tokens for words 
that sound the same based on the `Daitch-Mokotoff Soundex 
<https://en.wikipedia.org/wiki/Daitch%E2%80%93Mokotoff_Soundex>`__ 
phonetic algorithm. This filter can generate multiple encodings for 
each input, where each encoded token is a 6 digit number. 

.. note:: 

   Don't use the ``daitchMokotoffSoundex`` token filter in: 
          
   - :ref:`Synonym <synonyms-ref>` or :ref:`autocomplete 
     <bson-data-types-autocomplete>` mapping definitions.
   - Operators where ``fuzzy`` is enabled. |fts| supports the ``fuzzy`` 
     option for the following operators: 

     - :ref:`autocomplete-ref`
     - :ref:`text-ref` 

It has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``daitchMokotoffSoundex``.
     - yes
     -

   * - ``originalTokens`` 
     - string 
     - String that specifies whether to include or omit the original 
       tokens in the output of the token filter. Value can be one of 
       the following: 

       - ``include`` - include the original tokens with the encoded 
         tokens in the output of the token filter. We recommend this 
         value if you want queries on both the original tokens as well 
         as the encoded forms. 
       - ``omit`` - omit the original tokens and include only the 
         encoded tokens in the output of the token filter. Use this 
         value if you want to only query on the encoded forms of the 
         original tokens. 

     - no 
     - ``include``

.. example::

   The following index definition example uses a custom analyzer 
   named ``dmsAnalyzer``. It uses the :ref:`standard tokenizer
   <standard-tokenizer-ref>` with the ``daitchMokotoffSoundex`` 
   token filter to index and query for words that sound the same 
   as their encoded forms.

   .. code-block:: json 

     {
       "analyzer": "dmsAnalyzer",
       "searchAnalyzer": "dmsAnalyzer",
       "mappings": {
         "dynamic": true
       },
       "analyzers": [
         {
           "name": "dmsAnalyzer",
           "tokenizer": {
             "type": "standard"
           },
           "tokenFilters": [
             {
               "type": "daitchMokotoffSoundex",
               "originalTokens": "include"
             }
           ]
         }
       ]
     }

   The following query searches for terms that sound similar to 
   ``AUERBACH`` in the ``page_updated_by.last_name`` field of 
   the ``minutes`` collection.

   .. code-block:: json 

      db.minutes.aggregate([
        {
          $search: {
            "index": "default",
            "text": {
              "query": "AUERBACH",
              "path": "page_updated_by.last_name"
            }
          }
        },
        {
          $project: {
            "_id": 1,
            "page_updated_by.last_name": 1
          }
        }
      ])

   The query returns the following results: 

   .. code-block:: json 
     :copyable: false 

     { "_id" : 1, "page_updated_by" : { "last_name" : "AUERBACH" } }
     { "_id" : 2, "page_updated_by" : { "last_name" : "OHRBACH" } }

   |fts| returns documents with ``_id: 1`` and ``_id: 2`` 
   because the terms in both documents are phonetically similar, 
   and are coded using the same six digit ``097500``.

.. _edgegram-tf-ref:

edgeGram
--------

The ``edgeGram`` token filter tokenizes input from the left side, or 
"edge", of a text input into n-grams of configured sizes. You can't use 
the :ref:`edgeGram <edgegram-tf-ref>` token filter in :ref:`synonym 
<synonyms-ref>` or :ref:`autocomplete <bson-data-types-autocomplete>` 
mapping definitions. It has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type. 
       Value must be ``edgeGram``.
     - yes
     -

   * - ``minGram``
     - integer
     - Number that specifies the minimum length of generated n-grams.
       Value must be less than or 
       equal to ``maxGram``.
     - yes
     - 

   * - ``maxGram``
     - integer
     - Number that specifies the maximum length of generated n-grams.
       Value must be greater than or 
       equal to ``minGram``.
     - yes
     - 

   * - ``termNotInBounds``
     - string
     - String that specifies whether to index tokens shorter than
       ``minGram`` or longer than ``maxGram``. Accepted
       values are:

       - ``include``
       - ``omit``

       If ``include`` is specified, tokens shorter than ``minGram`` or
       longer than ``maxGram`` are indexed as-is. If ``omit`` is 
       specified, those tokens are not indexed.
     - no
     - ``omit``

.. example::

   The following index definition example uses a custom analyzer named
   ``englishAutocomplete``. It performs the following operations:

   - Tokenizes with the :ref:`standard tokenizer 
     <standard-tokenizer-ref>`.
   - Token filtering with the following filters:

     - ``icuFolding``
     - ``shingle``
     - ``edgeGram``

   .. code-block:: json

      {
        "analyzer": "englishAutocomplete",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "englishAutocomplete",
            "charFilters": [],
            "tokenizer": {
              "type": "standard"
            },
            "tokenFilters": [
              {
                "type": "icuFolding"
              },
              {
                "type": "shingle",
                "minShingleSize": 2,
                "maxShingleSize": 3
              },
              {
                "type": "edgeGram",
                "minGram": 1,
                "maxGram": 10
              }
            ]
          }
        ]
      }

   .. seealso:: 
   
      The :ref:`shingle-tf-ref` token filter for a sample index 
      definition and query. 

.. _englishPossessive-tf-ref:

englishPossessive
---------------------

The ``englishPossessive`` token filter removes possessives 
(trailing ``'s``) from words. It has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``englishPossessive``.
     - yes
     - 

.. example::

   The following index definition example uses a custom analyzer named 
   ``englishPossessiveStemmer`` on the :ref:`minutes 
   <custom-analyzers-eg-coll>` collection. After it applies the 
   :ref:`standard <standard-tokenizer-ref>` tokenizer to create tokens 
   based on word break rules, it applies the :ref:`englishPossessive 
   <englishPossessive-tf-ref>` token filter to remove possessives 
   (trailing ``'s``) from the tokens.

   .. code-block:: json

      {  
        "analyzer": "englishPossessiveStemmer", 
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "englishPossessiveStemmer",
            "charFilters": [],
            "tokenizer": {
              "type": "standard"
            },
            "tokenFilters": [
              {
                "type": "englishPossessive"
              }
            ]
          }
        ]
      }

   The following query searches the ``title`` field in the 
   :ref:`minutes <custom-analyzers-eg-coll>` collection for the term 
   ``team``.

   .. io-code-block::
      :copyable: true
   
      .. input::
         :language: json

         db.minutes.aggregate([
           {
             $search: {
               "index": "default",
               "text": {
                 "query": "team",
                 "path": "title"
               }
             } 
           },
           {
             $project: {
               "_id": 1,
               "title": 1
             }
           }
         ])

      .. output::
         :language: json

         [
           {
             _id: 1,
             title: "The team's weekly meeting"
           },
           { 
             _id: 2,
             title: 'The check-in with sales team'
           }
         ]

   |fts| returns results that contain the term ``team`` in the 
   ``title`` field. |fts| returns the document with ``_id: 1`` because 
   |fts| transforms ``team's`` in the ``title`` field to the token 
   ``team`` during analysis.

.. _flattenGraph-tf-ref:

flattenGraph
------------

The ``flattenGraph`` token filter transforms a token filter graph into 
a flat form suitable for indexing. If you use the 
:ref:`wordDelimiterGraph <wordDelimiterGraph-tf-ref>` token filter, use 
this filter after the :ref:`wordDelimiterGraph 
<wordDelimiterGraph-tf-ref>` token filter. It has the following 
attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``flattenGraph``.
     - yes
     - 

.. example::

   The following index definition uses a custom analyzer called 
   ``wordDelimiterGraphFlatten``. After it applies the :ref:`whitespace 
   <whitespace-tokenizer-ref>` tokenizer to create tokens based on 
   occurrences of whitespace between words, it applies the 
   :ref:`wordDelimiterGraph <wordDelimiterGraph-tf-ref>` token 
   filter to split tokens based on sub-words, generate tokens for the 
   original words, and also protect the word ``SIGN_IN`` from 
   delimination. It applies the :ref:`flattenGraph 
   <flattenGraph-tf-ref>` token filter after the  
   :ref:`wordDelimiterGraph <wordDelimiterGraph-tf-ref>` token 
   filter.

   .. code-block:: json

      {  
        "mappings": {
          "dynamic": true,
          "fields": {
            "message": {
              "type": "string",
              "analyzer": "wordDelimiterGraphFlatten"
            }
          }
        },
        "analyzers": [
          {
            "name": "wordDelimiterGraphFlatten",
            "charFilters": [],
            "tokenizer": {
              "type": "whitespace"
            },
            "tokenFilters": [
              {
                "type": "wordDelimiterGraph",
                "delimiterOptions" : {
                  "generateWordParts" : true,
                  "preserveOriginal" : true
                },
                "protectedWords": {
                  "words": [
                    "SIGN_IN"
                  ],
                  "ignoreCase": false
                }
              },
              {
                "type": "flattenGraph"
              }
            ]
          }
        ]
      }

   The following query searches the ``message`` field in the 
   :ref:`minutes <custom-analyzers-eg-coll>` collection for the term 
   ``sign``.

   .. io-code-block::
      :copyable: true
   
      .. input::
         :language: json

         db.minutes.aggregate([
           {
             $search: {
               "index": "default",
               "text": {
                 "query": "sign",
                 "path": "message"
               }
             } 
           },
           {
             $project: {
               "_id": 1,
               "message": 1
             }
           }
         ])

      .. output::
         :language: json

         [
           {
             _id: 3,
             message: 'try to sign-in'
           }  
         ]

   |fts| returns the document with ``_id: 3`` in the results for the 
   query term ``sign`` even though the document contains the 
   hyphenated term ``sign-in`` in the ``title``. The 
   :ref:`wordDelimiterGraph <wordDelimiterGraph-tf-ref>` token 
   filter creates a token filter graph and the :ref:`flattenGraph 
   <flattenGraph-tf-ref>` token filter transforms the token filter 
   graph into a flat form suitable for indexing.

.. _icufolding-tf-ref:

icuFolding
----------

The ``icuFolding`` token filter applies character folding from `Unicode 
Technical Report #30 <http://www.unicode.org/reports/tr30/tr30-4.html>`__. 
It has the following attribute:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``icuFolding``.
     - yes
     -

.. example::

   The following index definition example uses a custom analyzer named
   ``diacriticFolder``. It uses the :ref:`keyword tokenizer
   <keyword-tokenizer-ref>` with the ``icuFolding`` token filter to 
   apply foldings from `UTR#30 Character Foldings
   <https://unicode.org/reports/tr30/>`__. Foldings include accent
   removal, case folding, canonical duplicates folding, and many others
   detailed in the report.

   .. code-block:: json

      {
        "analyzer": "diacriticFolder",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "diacriticFolder",
            "charFilters": [],
            "tokenizer": {
              "type": "keyword"
            },
            "tokenFilters": [
              {
                "type": "icuFolding"
              }
            ]
          }
        ]
      }

.. _icunormalizer-tf-ref:

icuNormalizer
-------------

The ``icuNormalizer`` token filter normalizes tokens using a standard 
`Unicode Normalization Mode <https://unicode.org/reports/tr15/>`__. It 
has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``icuNormalizer``.
     - yes
     -

   * - ``normalizationForm``
     - string
     - Normalization form to apply. Accepted values are:

       - ``nfd`` (Canonical Decomposition)
       - ``nfc`` (Canonical Decomposition, followed by Canonical 
         Composition)
       - ``nfkd`` (Compatibility Decomposition)
       - ``nfkc`` (Compatibility Decomposition, followed by Canonical 
         Composition)

       To learn more about the supported normalization forms, see
       `Section 1.2: Normalization Forms, UTR#15 
       <https://unicode.org/reports/tr15/#Norm_Forms>`__.

     - no
     - ``nfc``

.. example::

   The following index definition example uses a custom analyzer named
   ``normalizer``. It uses the :ref:`whitespace tokenizer
   <whitespace-tokenizer-ref>`, then normalizes
   tokens by Canonical Decomposition, followed by Canonical Composition.

   .. code-block:: json

      {
        "analyzer": "normalizer",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "normalizer",
            "charFilters": [],
            "tokenizer": {
              "type": "whitespace"
            },
            "tokenFilters": [
              {
                "type": "icuNormalizer",
                "normalizationForm": "nfc"
              }
            ]
          }
        ]
      }

.. _kStemming-tf-ref:

kStemming
---------

The ``kStemming`` token filter combines algorithmic stemming with a 
built-in dictionary for the english language to stem words. It expects 
lowercase text and doesn't modify uppercase text. It has the following 
attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``kStemming``.
     - yes
     - 

.. example::

   The following index definition example uses a custom analyzer named 
   ``kStemmer``. After it applies the :ref:`standard 
   <standard-tokenizer-ref>` tokenizer and the :ref:`lowercase 
   <lowercase-tf-ref>` token filter, it applies the :ref:`kStemming 
   <kStemming-tf-ref>` token filter.

   .. code-block:: json
   
      {  
        "analyzer": "kStemmer", 
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "kStemmer",
            "tokenizer": {
              "type": "standard"
            },
            "tokenFilters": [
              {
                "type": "lowercase"
              },
              {
                "type": "kStemming"
              }
            ]
          }
        ]
      }

   The following query searches the ``text.en_US`` field in the 
   :ref:`minutes <custom-analyzers-eg-coll>` collection for the term 
   ``Meeting``.

   .. io-code-block::
      :copyable: true
   
      .. input::
         :language: json

         db.minutes.aggregate([
           {
             $search: {
               "index": "default",
               "text": {
                 "query": "Meeting",
                 "path": "text.en_US"
               }
             } 
           },
           {
             $project: {
               "_id": 1,
               "text.en_US": 1
             }
           }
         ])

      .. output::
         :language: json

         [
           {
             _id: 1,
             text: {
               en_US: '<head> This page deals with department meetings. </head>'
             }
           }
         ]

   |fts| returns the document with ``_id: 1`` because the 
   :ref:`lowercase <lowercase-tf-ref>` token filter normalizes token 
   text to lowercase. The :ref:`kStemming <kStemming-tf-ref>` token 
   filter lets |fts| match the plural ``meetings`` in the 
   ``text.en_US`` field of the document to the singular query term.

.. _length-tf-ref:

length
------

The ``length`` token filter removes tokens that are too short or too 
long. It has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``length``.
     - yes
     -

   * - ``min``
     - integer
     - Number that specifies the minimum length of a token.
       Value must be less than or equal to 
       ``max``.
     - no
     - 0

   * - ``max``
     - integer
     - Number that specifies the maximum length of a token.
       Value must be greater than or equal to 
       ``min``.
     - no
     - 255

.. example::

   The following index definition example uses a custom analyzer named
   ``longOnly``. It uses the ``length`` token filter to index only 
   tokens that are at least 20 UTF-16 code units long after tokenizing 
   with the :ref:`standard tokenizer <standard-tokenizer-ref>`.

   .. code-block:: json

      {
        "analyzer": "longOnly",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "longOnly",
            "charFilters": [],
            "tokenizer": {
              "type": "standard"
            },
            "tokenFilters": [
              {
                "type": "length",
                "min": 20
              }
            ]
          }
        ]
      }

.. _lowercase-tf-ref:

lowercase
---------

The ``lowercase`` token filter normalizes token text to lowercase. It 
has the following attribute:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``lowercase``.
     - yes
     -

.. example::
         
   The following index definition example uses a custom analyzer named 
   ``lowercaser``. It uses the :ref:`standard tokenizer 
   <standard-tokenizer-ref>` with the ``lowercase`` token filter to 
   lowercase all tokens.

   .. code-block:: json

      {
        "analyzer": "lowercaser",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "lowercaser",
            "charFilters": [],
            "tokenizer": {
              "type": "standard"
            },
            "tokenFilters": [
              {
                "type": "lowercase"
              }
            ]
          }
        ]
      }

   .. seealso:: 
   
      The :ref:`regex-tf-ref` token filter for a sample index 
      definition and query. 

.. _ngram-tf-ref:

nGram
-----

The ``nGram`` token filter tokenizes input into n-grams of configured 
sizes. You can't use the :ref:`nGram <ngram-tf-ref>` token filter in 
:ref:`synonym <synonyms-ref>` or :ref:`autocomplete 
<bson-data-types-autocomplete>` mapping definitions. It has the 
following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type. Value must be ``nGram``.
     - yes
     -

   * - ``minGram``
     - integer
     - Number that specifies the minimum length of generated n-grams.
       Value must be less than or 
       equal to ``maxGram``.
     - yes
     - 

   * - ``maxGram``
     - integer
     - Number that specifies the maximum length of generated n-grams.
       Value must be greater than or 
       equal to ``minGram``.
     - yes
     - 

   * - ``termNotInBounds``
     - string
     - String that specifies whether to index tokens shorter than
       ``minGram`` or longer than ``maxGram``. Accepted values are:

       - ``include``
       - ``omit``

       If ``include`` is specified, tokens shorter than ``minGram`` or
       longer than ``maxGram`` are indexed as-is. If ``omit`` is 
       specified, those tokens are not indexed.
     - no
     - ``omit``


.. example::

   The following index definition example uses a custom analyzer named
   ``persianAutocomplete``. It functions as an autocomplete analyzer for
   Persian and other languages that use the zero-width non-joiner
   character. It performs the following operations:
   
   - Normalizes zero-width non-joiner characters with the :ref:`persian
     character filter <persian-ref>`. 
   - Tokenizes by whitespace with the :ref:`whitespace tokenizer
     <whitespace-tokenizer-ref>`.
   - Applies a series of token filters: 
   
     - ``icuNormalizer``
     - ``shingle``
     - ``nGram``

   .. code-block:: json

      {
        "analyzer": "persianAutocomplete",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "persianAutocomplete",
            "charFilters": [
              {
                "type": "persian"
              }
            ],
            "tokenizer": {
              "type": "whitespace"
            },
            "tokenFilters": [
              {
                "type": "icuNormalizer",
                "normalizationForm": "nfc"
              },
              {
                "type": "shingle",
                "minShingleSize": 2,
                "maxShingleSize": 3
              },
              {
                "type": "nGram",
                "minGram": 1,
                "maxGram": 10
              }
            ]
          }
        ]
      }

.. _porterStemming-tf-ref:

porterStemming
--------------

The ``porterStemming`` token filter uses the porter stemming algorithm 
to remove the common morphological and inflectional suffixes from 
words in English. It expects lowercase text and doesn't work as 
expected for uppercase text. It has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``porterStemming``.
     - yes
     - 

.. example::

   The following index definition example uses a custom analyzer named 
   ``porterStemmer``. After it applies the :ref:`standard 
   <standard-tokenizer-ref>` tokenizer and the :ref:`lowercase 
   <lowercase-tf-ref>` token filter, it applies the 
   :ref:`porterStemming <porterStemming-tf-ref>` token filter.

   .. code-block:: json

      {  
        "analyzer": "porterStemmer", 
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
         {
           "name": "porterStemmer",
           "tokenizer": {
             "type": "standard"
           },
           "tokenFilters": [
             {
               "type": "lowercase"
             },
             {
               "type": "porterStemming"
             }
           ]
         }
       ]
     }

   The following query searches the ``title`` field in the 
   :ref:`minutes <custom-analyzers-eg-coll>` collection for the term 
   ``Meet``.

   .. io-code-block::
      :copyable: true
   
      .. input::
         :language: json

         db.minutes.aggregate([
           {
             $search: {
               "index": "default",
               "text": {
                 "query": "Meet",
                 "path": "title"
               }
             } 
           },
           {
             $project: {
               "_id": 1,
               "title": 1
             }
           }
         ])

      .. output::
         :language: json

         [
           {
             _id: 1,
             title: "The team's weekly meeting"
           },
           {
             _id: 3,
             title: 'The regular board meeting'
           }
         ]

   |fts| returns the documents with ``_id: 1`` and ``_id: 2`` because 
   the :ref:`lowercase <lowercase-tf-ref>` token filter normalizes 
   token text to lowercase. The :ref:`porterStemming 
   <porterStemming-tf-ref>` token filter lets |fts| match ``meeting`` 
   in the ``title`` field of the documents to the ``meet`` token.

.. _regex-tf-ref:

regex
-----

The ``regex`` token filter applies a regular expression to each token, 
replacing matches with a specified string. It has the following 
attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter.
       Value must be ``regex``.
     - yes
     -

   * - ``pattern``
     - string
     - Regular expression pattern to apply to each token.
     - yes
     - 

   * - ``replacement``
     - string
     - Replacement string to substitute wherever a matching pattern
       occurs.
     - yes
     - 

   * - ``matches``
     - string
     - Acceptable values are:

       - ``all``
       - ``first``

       If ``matches`` is set to ``all``, replace all matching patterns.
       Otherwise, replace only the first matching pattern.

     - yes
     -

.. example::

   The following index definition uses a custom analyzer named 
   ``emailRedact`` for indexing the ``page_updated_by.email`` 
   field in the ``minutes`` collection. It uses the 
   :ref:`standard tokenizer <standard-tokenizer-ref>`. It first 
   applies the lowercase token filter to turn uppercase 
   characters in the field to lowercase and then finds strings 
   that look like email addresses and replaces them with the word 
   ``redacted``.

   .. code-block:: json
 
     {
       "analyzer": "lucene.standard",
       "mappings": {
         "dynamic": false,
         "fields": {
           "page_updated_by": {
             "type": "document",
             "fields": {
               "email": {
                 "type": "string",
                 "analyzer": "emailRedact"
               }
             }
           }
         }
       },
       "analyzers": [
         {
           "charFilters": [],
           "name": "emailRedact",
           "tokenizer": {
             "type": "standard"
           },
           "tokenFilters": [
             {
               "type": "lowercase"
             },
             {
               "matches": "all",
               "pattern": "^([a-z0-9_\\.-]+)@([\\da-z\\.-]+)\\.([a-z\\.]{2,5})$",
               "replacement": "redacted",
               "type": "regex"
             }
           ]
         }
       ]
     }

   The following query searches for the term ``example`` in the 
   ``page_updated_by.email`` field of the ``minutes`` collection. 

   .. code-block:: 

     db.minutes.aggregate([
       {
         $search: {
           "index": "default",
           "text": {
             "query": "example",
             "path": "page_updated_by.email"
           }
         }
       }
     ])

   |fts| doesn't return any results for the query because the 
   ``page_updated_by.email`` field doesn't contain any instances 
   of the word ``example`` that aren't in an email address. 
   |fts| replaces strings that match the regular expression 
   provided in the custom analyzer with the word ``redacted``.

.. _reverse-tf-ref:

reverse 
-------

The ``reverse`` token filter reverses each string token. It has the 
following attribute: 

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter.
       Value must be ``reverse``.
     - yes
     -

.. example:: 

   The following index definition example for the :ref:`minutes 
   <custom-analyzers-eg-coll>` collection uses a custom analyzer named 
   ``keywordReverse``. It performs the following operations:

   - Uses :ref:`dynamic mapping <static-dynamic-mappings>` 
   - Tokenizes with the :ref:`keyword tokenizer 
     <keyword-tokenizer-ref>`
   - Applies the ``reverse`` token filter to tokens

   .. code-block:: json
      :emphasize-lines: 14-16

      {
        "analyzer": "lucene.keyword",
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "keywordReverse",
            "charFilters": [],
            "tokenizer": {
              "type": "keyword"
            },
            "tokenFilters": [
              {
                "type": "reverse"
              }
            ]
          }
        ]
      }

   The following query searches the ``page_updated_by.email`` field in 
   the ``minutes`` collection using the :ref:`wildcard-ref` operator to 
   match any characters preceding the characters ``@example.com`` in 
   reverse order. The ``reverse`` token filter can speed up leading 
   wildcard queries.

   .. code-block:: json 

      db.minutes.aggregate([
        {
          "$search": {
            "index": "default",
            "wildcard": {
              "query": "*@example.com",
              "path": "page_updated_by.email",
              "allowAnalyzedField": true
            }
          }
        },
        {
          "$project": {
            "_id": 1,
            "page_updated_by.email": 1,
          }
        }
      ])

   For the preceding query, |fts| applies the custom analyzer to the 
   wildcard query to transform the query as follows: 

   .. code-block:: shell 
      :copyable: false 

      moc.elpmaxe@*

   |fts| then runs the query against the indexed tokens, which are also 
   reversed. The query returns the following documents:

   .. code-block:: json
      :copyable: false 

      [
        { _id: 1, page_updated_by: { email: 'auerbach@example.com' } },
        { _id: 2, page_updated_by: { email: 'ohrback@example.com' } },
        { _id: 3, page_updated_by: { email: 'lewinsky@example.com' } },
        { _id: 4, page_updated_by: { email: 'levinski@example.com' } }
      ]

.. _shingle-tf-ref:

shingle
-------

The ``shingle`` token filter constructs shingles (token n-grams) from a 
series of tokens. You can't use the ``shingle`` token filter in 
:ref:`synonym <synonyms-ref>` or :ref:`autocomplete 
<bson-data-types-autocomplete>` mapping definitions. It has the 
following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``shingle``.
     - yes
     -

   * - ``minShingleSize``
     - integer
     - Minimum number of tokens per shingle. Must be less than or equal
       to ``maxShingleSize``.
     - yes
     -

   * - ``maxShingleSize``
     - integer
     - Maximum number of tokens per shingle. Must be greater than or 
       equal to ``minShingleSize``.
     - yes
     -

.. example::

   The following index definition example uses two custom 
   analyzers, ``emailAutocompleteIndex`` and 
   ``emailAutocompleteSearch``, to implement autocomplete-like 
   functionality. |fts| uses the ``emailAutocompleteIndex`` 
   analyzer during index creation to:
    
   - Replace ``@`` characters in a field with ``AT``
   - Create tokens with the :ref:`whitespace 
     <whitespace-tokenizer-ref>` tokenizer
   - Shingle tokens 
   - Create :ref:`edgegram-tf-ref` of those shingled tokens
  
   |fts| uses the ``emailAutocompleteSearch`` analyzer during a 
   search to:

   - Replace ``@`` characters in a field with ``AT``
   - Create tokens with the :ref:`whitespace tokenizer 
     <whitespace-tokenizer-ref>` tokenizer
    
   .. code-block:: json
      :emphasize-lines: 34-38

      {
       "analyzer": "lucene.keyword",
       "mappings": {
         "dynamic": true,
         "fields": {
           "page_updated_by": {
             "type": "document",
             "fields": {
               "email": {
                 "type": "string",
                 "analyzer": "emailAutocompleteIndex",
                 "searchAnalyzer": "emailAutocompleteSearch",
               }
             }
           }
         }
       },
       "analyzers": [
         {
           "name": "emailAutocompleteIndex",
           "charFilters": [
             {
               "mappings": {
                 "@": "AT"
               },
               "type": "mapping"
             }
           ],
           "tokenizer": {
             "maxTokenLength": 15,
             "type": "whitespace"
           },
           "tokenFilters": [
             {
               "maxShingleSize": 3,
               "minShingleSize": 2,
               "type": "shingle"
             },
             {
               "maxGram": 15,
               "minGram": 2,
               "type": "edgeGram"
             }
           ]
         },
         {
           "name": "emailAutocompleteSearch",
           "charFilters": [
             {
               "mappings": {
                 "@": "AT"
               },
               "type": "mapping"
             }
           ],
           "tokenizer": {
             "maxTokenLength": 15,
             "type": "whitespace"
           }
         }
       ]
     }

   The following query searches for an email address in the 
   ``page_updated_by.email`` field of the ``minutes`` collection:

   .. code-block:: json 

      db.minutes.aggregate([
        {
          $search: {
            "index": "default",
            "text": {
              "query": "auerbach@ex",
               "path": "page_updated_by.email"
            }
          }
        }
      ])

   The query returns the following results:

   .. code-block:: json 
      :copyable: false 

      {  
        "_id" : 1, 
        "page_updated_by" : { 
          "last_name" : "AUERBACH", 
          "first_name" : "Si√¢n", 
          "email" : "auerbach@example.com", 
          "phone" : "123-456-7890" 
        }, 
        "title": "The team's weekly meeting",
        "message": "try to siGn-In",
        "text": {
          "en_US": "<head> This page deals with department meetings. </head>"
        }
      }

.. _snowballstemming-tf-ref:

snowballStemming
----------------

The ``snowballStemming`` token filters Stems tokens using a 
`Snowball-generated stemmer <https://snowballstem.org/>`__. It has the 
following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``snowballStemming``.
     - yes
     -

   * - ``stemmerName``
     - string
     - The following values are valid:

       - ``arabic``
       - ``armenian``
       - ``basque``
       - ``catalan``
       - ``danish``
       - ``dutch``
       - ``english``
       - ``estonian``
       - ``finnish``
       - ``french``
       - ``german``
       - ``german2`` (Alternative German language stemmer. Handles the 
         umlaut by expanding √º to ue in most contexts.)
       - ``hungarian``
       - ``irish``
       - ``italian``
       - ``kp`` (Kraaij-Pohlmann stemmer, an alternative stemmer for 
         Dutch.)
       - ``lithuanian``
       - ``lovins`` (The first-ever published "Lovins JB" stemming 
         algorithm.)
       - ``norwegian``
       - ``porter`` (The original Porter English stemming algorithm.)
       - ``portuguese``
       - ``romanian``
       - ``russian``
       - ``spanish``
       - ``swedish``
       - ``turkish``
     - yes
     -

.. example::

   The following example index definition uses a custom analyzer named
   ``frenchStemmer``. It uses the ``lowercase`` token filter and the
   :ref:`standard tokenizer <standard-tokenizer-ref>`, followed
   by the ``french`` variant of the ``snowballStemming`` token filter.

   .. code-block:: json

      {
        "analyzer": "frenchStemmer", 
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "frenchStemmer",
            "charFilters": [],
            "tokenizer": {
              "type": "standard"
            },
            "tokenFilters": [
              {
                "type": "lowercase"
              },
              {
                "type": "snowballStemming",
                "stemmerName": "french"
              }
            ]
          }
        ]
      }

.. _spanishPluralStemming-tf-ref:

spanishPluralStemming
---------------------

The ``spanishPluralStemming`` token filter stems spanish plural words. 
It expects lowercase text. It has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``spanishPluralStemming``.
     - yes
     - 

.. example::

   The following index definition specifies that |fts| dynamically 
   index all fields in the ``minutes`` collection with the ``standard`` 
   analyzer. The index definition example specifies a custom analyzer 
   named ``spanishPluralStemmer`` for the ``text.es_MX`` field. The custom 
   analyzer specifies that |fts| convert spanish terms to lowercase
   and tokenize plural spanish words into their singular form.

   .. code-block:: json

      {  
        "analyzer": "spanishPluralStemmer", 
        "mappings": {
          "dynamic": true,
          "fields": {
            "text.es_MX": {
              "analyzer": "spanishPluralStemmer",
              "searchAnalyzer": "spanishPluralStemmer",
              "type": "string"
            }
          }
        },
        "analyzers": [
          {
            "name": "spanishPluralStemmer",
            "charFilters": [],
            "tokenizer": {
              "type": "standard"
            },
            "tokenFilters": [
              {
                "type": "lowercase"
              },
              {
                "type": "spanishPluralStemming"
              }
            ]
          }
        ]
      }

   The following query searches the ``text.es_MX`` field in the 
   :ref:`minutes <custom-analyzers-eg-coll>` collection for the spanish 
   term ``punto``.

   .. io-code-block::
      :copyable: true
   
      .. input::
         :language: json

         db.minutes.aggregate([
           {
             $search: {
               "index": "default",
               "text": {
                 "query": "punto",
                 "path": "text.es_MX"
               }
             } 
           },
           {
             $project: {
               "_id": 1,
               "text.es_MX": 1
             }
           }
         ])
   
      .. output::
         :language: json

         [
           {
             _id: 4,
             text : {
               es_MX: 'La p√°gina ha sido actualizada con los puntos de la agenda.',
             }
           }
         ]

   |fts| returns the document with ``_id: 4`` because the 
   ``text.es_MX`` field in the document contains the plural term 
   ``puntos``. |fts| matches this document for the query term ``punto`` 
   because |fts| analyzes ``puntos`` as ``punto`` by stemming the 
   plural (``s``) from the term.

.. _stempel-tf-ref:

stempel
-------

The ``stempel`` token filter uses Lucene's 
`default Polish stemmer table 
<https://lucene.apache.org/core/9_2_0/analysis/stempel/org/apache/lucene/analysis/pl/PolishAnalyzer.html#DEFAULT_STEMMER_FILE>`__ to stem 
words in the Polish language. It expects lowercase text. It has the 
following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``stempel``.
     - yes
     - 

.. example::

   The following index definition example uses a custom analyzer named 
   ``stempelStemmer``. After it applies the :ref:`standard 
   <standard-tokenizer-ref>` tokenizer and the :ref:`lowercase 
   <lowercase-tf-ref>` token filter, it applies the :ref:`stempel 
   <stempel-tf-ref>` token filter to the ``text.pl_PL`` field. 

   .. code-block:: json

      {  
        "analyzer": "stempelStemmer", 
        "mappings": {
          "dynamic": true,
          "fields": {
            "text.pl_PL": {
              "analyzer": "stempelStemmer",
              "searchAnalyzer": "stempelStemmer",
              "type": "string"
            }
          }
        },
        "analyzers": [
          {
            "name": "stempelStemmer",
            "charFilters": [],
            "tokenizer": {
              "type": "standard"
            },
            "tokenFilters": [
              {
                "type": "lowercase"
              },
              {
                "type": "stempel"
              }
            ]
          }
        ]
      }

   The following query searches the ``text.pl_PL`` field in the 
   :ref:`minutes <custom-analyzers-eg-coll>` collection for the Polish 
   term ``punkt``.

   .. io-code-block::
      :copyable: true
   
      .. input::
         :language: json

         db.minutes.aggregate([
           {
             $search: {
               "index": "default",
               "text": {
                 "query": "punkt",
                 "path": "text.pl_PL"
               }
             } 
           },
           {
             $project: {
               "_id": 1,
               "text.pl_PL": 1
             }
           }
         ])
   
      .. output::
         :language: json

         [
           {
             _id: 4,
             text: {
               pl_PL: 'Strona zosta≈Ça zaktualizowana o punkty porzƒÖdku obrad.'
             }
           }
         ]

   |fts| returns the document with ``_id: 4`` because the 
   ``text.pl_PL`` field in the document contains the plural term 
   ``punkty``. |fts| matches this document for the query term ``punkt`` 
   because |fts| analyzes ``punkty`` as ``punkt`` by stemming the 
   plural (``y``) from the term.

.. _stopword-tf-ref:

stopword 
--------

The ``stopword`` token filter removes tokens that correspond to the 
specified stop words. This token filter doesn't analyze the specified 
stop words. It has the following attributes:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``stopword``.
     - yes
     - 

   * - ``tokens``
     - array of strings
     - List that contains the stop words that correspond to the tokens
       to remove. 
       Value must be one or more stop words.
     - yes
     - 

   * - ``ignoreCase``
     - boolean
     - Flag that indicates whether to ignore the case of stop 
       words when filtering the tokens to remove. The value can be one 
       of the following: 

       - ``true`` - ignore case and remove all tokens that match the 
         specified stop words
       - ``false`` - be case-sensitive and remove only tokens that 
         exactly match the specified case

       If omitted, defaults to ``true``.

     - no
     - true

.. example:: 

   The following index definition example uses a custom analyzer named
   It uses the ``stopword`` token filter after the 
   :ref:`whitespace tokenizer <whitespace-tokenizer-ref>` to remove 
   the tokens that match the defined stop words ``is``, ``the``, and 
   ``at``. The token filter is case-insensitive and will remove all 
   tokens that match the specified stop words.

   .. code-block:: json

      {  
        "analyzer": "tokenTrimmer", 
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "stopwordRemover",
            "charFilters": [],
            "tokenizer": {
              "type": "whitespace"
            },
            "tokenFilters": [
              {
                "type": "stopword",
                "tokens": ["is", "the", "at"]
              }
            ]
          }
        ]
      }

.. _trim-tf-ref:

trim
----

The ``trim`` token filter trims leading and trailing whitespace from 
tokens. It has the following attribute:

.. list-table::
   :widths: 20 10 40 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``trim``.
     - yes
     -

.. example::

   The following index definition example uses a custom analyzer named
   ``tokenTrimmer``. It uses the ``trim`` token filter after the 
   :ref:`keyword tokenizer <keyword-tokenizer-ref>` to remove leading 
   and trailing whitespace in the tokens created by the :ref:`keyword 
   tokenizer <keyword-tokenizer-ref>`.

   .. code-block:: json

      {
        "analyzer": "tokenTrimmer", 
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "tokenTrimmer",
            "charFilters": [],
            "tokenizer": {
              "type": "keyword"
            },
            "tokenFilters": [
              {
                "type": "trim"
              }
            ]
          }
        ]
      }

.. _wordDelimiterGraph-tf-ref:

wordDelimiterGraph
------------------

The ``wordDelimiterGraph`` token filter splits tokens into sub-tokens 
based on configured rules. We recommend that you don't use this token 
filter with the :ref:`standard <standard-tokenizer-ref>` tokenizer 
because this tokenizer removes many of the intra-word delimiters that 
this token filter uses to determine boundaries. It has the following 
attributes:

.. list-table::
   :widths: 20 15 35 10 10
   :header-rows: 1

   * - Name
     - Type
     - Description
     - Required?
     - Default

   * - ``type``
     - string
     - Human-readable label that identifies this token filter type.
       Value must be ``wordDelimiterGraph``.
     - yes
     - 
   * - ``delimiterOptions``
     - object
     - Object that contains the rules that determine how to split words 
       into sub-words.  
     - no
     - {}

   * - | ``delimterOptions`` 
       | ``.generateWordParts``
     - boolean
     - Flag that indicates whether to split tokens based on sub-words. 
       For examples,  if ``true``, this option splits ``PowerShot`` 
       into ``Power`` and ``Shot``.
     - no
     - true
   
   * - | ``delimterOptions``
       | ``.generateNumberParts``
     - boolean
     - Flag that indicates whether to split tokens based on 
       sub-numbers. For examples,  if ``true``, this option splits 
       ``100-2`` into ``100`` and ``2``.
     - no
     - true
  
   * - | ``delimterOptions``
       | ``.concatenateWords``
     - boolean
     - Flag that indicates whether to concatenate runs of sub-words. 
       For examples,  if ``true``, this option concatenates 
       ``wi-fi`` into ``wifi``. [1]_
     - no
     - false
  
   * - | ``delimterOptions`` 
       | ``.concatenateNumbers``
     - boolean
     - Flag that indicates whether to concatenate runs of sub-numbers. 
       For examples,  if ``true``, this option concatenates 
       ``100-2`` into ``1002``. [1]_
     - no
     - false
  
   * - | ``delimterOptions`` 
       | ``.concatenateAll``
     - boolean
     - Flag that indicates whether to concatenate all runs. 
       For examples,  if ``true``, this option concatenates 
       ``wi-fi-100-2`` into ``wifi1002``. [1]_
     - no
     - false
  
   * - | ``delimterOptions`` 
       | ``.preserveOriginal``
     - boolean
     - Flag that indicates whether to generate tokens of the original 
       words. [1]_
     - no
     - true
  
   * - | ``delimterOptions`` 
       | ``.splitOnCaseChange``
     - boolean
     - Flag that indicates whether to split tokens based on letter-case 
       transitions. For examples,  if ``true``, this option splits 
       ``camelCase`` into ``camel`` and ``Case``.
     - no
     - true
  
   * - | ``delimterOptions`` 
       | ``.splitOnNumerics``
     - boolean
     - Flag that indicates whether to split tokens based on 
       letter-number transitions. For examples,  if ``true``, this 
       option splits ``g2g`` into ``g``, ``2``, and ``g``.
     - no
     - true
  
   * - | ``delimterOptions`` 
       | ``.stemEnglishPossessive``
     - boolean
     - Flag that indicates whether to remove trailing possessives from 
       each sub-word. For examples,  if ``true``, this option changes 
       ``who's`` into ``who```.
     - no
     - true

   * - | ``delimterOptions`` 
       | ``.ignoreKeywords``
     - boolean
     - Flag that indicates whether to skip tokens with the ``keyword`` 
       attribute set to ``true``.
     - no
     - false

   * - ``protectedWords``
     - object
     - Object that contains options for protected words.
     - no
     - {}

   * - | ``protectedWords``
       | ``.words``
     - array
     - List that contains the tokens to protect from delimination. If 
       you specify ``protectedWords``, you must specify this option.
     - conditional
     -  

   * - | ``protectedWords``
       | ``.ignoreCase``
     - boolean
     - Flag that indicates whether to ignore case sensisitivity for 
       protected words.
     - no
     - ``true``

.. [1] If ``true``, apply the :ref:`flattenGraph <flattenGraph-tf-ref>` token filter after this option to make the token stream suitable for indexing.

.. example::

   The following index definition example:
   
   - Uses a custom analyzer named ``wordDelimiterGraphAnalyzer``.
   - Doesn't try and split ``is``, ``the``, and ``at``. The exclusion 
     is case sensitive. For example ``Is`` and ``tHe`` are not excluded.
   - Splits tokens on case changes and removes tokens that contain only 
     alphabetical letters from the English alphabet.
   
   After it applies the :ref:`whitespace 
   <whitespace-tokenizer-ref>` tokenizer, it applies the 
   :ref:`wordDelimiterGraph <wordDelimiterGraph-tf-ref>` token filter. 

   .. code-block:: json

      {  
        "analyzer": "wordDelimiterGraphAnalyzer", 
        "mappings": {
          "dynamic": true
        },
        "analyzers": [
          {
            "name": "wordDelimiterGraphAnalyzer",
            "charFilters": [],
            "tokenizer": {
              "type": "whitespace"
            },
            "tokenFilters": [
              {
                "type": "wordDelimiterGraph",
                "protectedWords": {
                  "words": ["is", "the", "at"],
                  "ignoreCase": false
                },
                "delimiterOptions" : {
                  "generateWordParts" : false,
                  "splitOnCaseChange" : true
                }
              }
            ]
          }
        ]
      }

   The following query searches the ``title`` field in the 
   :ref:`minutes <custom-analyzers-eg-coll>` collection for the term 
   ``App2``.

   .. io-code-block::
      :copyable: true

      .. input::
         :language: json

         db.minutes.aggregate([
           {
             $search: {
               "index": "default",
               "text": {
                 "query": "App2",
                 "path": "title"
               }
             } 
           },
           {
             $project: {
               "_id": 1,
               "title": 1
             }
           }
         ])
   
      .. output::
         :language: json

         [
           {
             _id: 4,
             title: 'The daily huddle on StandUpApp2'
           }
         ]

   |fts| returns the document with ``_id: 4`` because the ``title`` 
   field in the document contains ``App2``. |fts| splits tokens on case 
   changes and removes tokens that contain only alphabetical letters. A 
   search for letters from the english alphabet won't return any 
   results.
