.. _atlas-data-lake:

=====================
|service| |data-lake|
=====================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

.. _atlas-data-lake-about:

About {+adl+}
---------------------

MongoDB {+adl+} is now an analytic-optimized object storage service for 
extracted data. {+adl+} provides an analytic storage service optimized 
for flat or nested data with low latency query performance.

Prerequisites
~~~~~~~~~~~~~

{+adl+} requires an ``M10`` or higher backup-enabled |service| cluster 
with cloud backup jobs running on a specified cadence. To learn more 
about cloud backups, see :atlas:`Back Up Your Database Deployment 
</backup/cloud-backup/overview/>`.

Supported Types of Data Source 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

{+adl+} supports collection snapshots from |service| clusters as a data 
source for extracted data. {+adl+} automatically ingests data from the 
snapshots, and partitions and stores data in an analytics-optimized 
format. It doesn't support creating pipelines for :manual:`Views
</core/views/>`. 

Data Storage Format and Query Support 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

{+adl+} stores data in an analytic oriented format that is based on open 
source standards with support for polymorphic data. Data is fully
managed, partition level indexed, and balanced as data grows. {+adl+}
optimizes data extraction for analytic type queries. When {+adl+}
extracts new data, it re-balances existing files to ensure consistent
performance and minimize data scan.  

{+adl+} stores data in a format that best fits its structure to allow
for fast point-queries and aggregate queries. For point-queries,
{+adl+}\'s storage format improves performance by finding partitions
faster. Aggregate type queries only scan the column required to provide
results. Additionally, {+adl+} partition indexes improve performance for  
aggregate queries by returning results directly from the partition 
index without needing to scan underlying files.  

Sample Uses 
-----------

You can use {+adl+} to: 

- Isolate analytical workloads from your operational cluster.
- Provide a consistent view of cluster data from a snapshot for long 
  running aggregations using ``$out``.
- Query and compare across versions of your cluster data at different 
  points in time.

.. _atlas-data-lake-regions:

{+adl+} Regions 
-----------------------

{+adl+} provides optimized storage in the following |aws| regions:

.. include:: /includes/list-table-supported-aws-regions.rst

{+adl+} automatically selects the region closest to your |service| 
cluster for storing ingested data. 

Billing 
-------

You incur {+adl+} charges per GB per month based on the |aws| region 
where the ingested data is stored. You incur {+adl+} costs for the 
following items:

- Ingestion of data from your data source
- Storage on the cloud object storage

Extraction Costs 
~~~~~~~~~~~~~~~~

{+adl+} charges you for the resources utilized to extract, upload, and 
transfer data. {+adl+} charges for the :atlas:`snapshot export 
</backup/cloud-backup/export/>` operations is based on the following: 

- Cost per GB for snapshot extraction 
- Cost per hour on the |aws| server for snapshot export download 
- Cost per GB per hour for snapshot export restore storage
- Cost per |iops| per hour for snapshot export storage |iops|

Storage Costs 
~~~~~~~~~~~~~

{+adl+} charges for storing and accessing stored data is based on the 
following:

- Cost per GB per day 
- Cost for every one thousand storage access requests when querying
  {+dl+} datasets using {+adf+}. Each access request corresponds to a
  partition of data from a {+dl+} dataset that {+adf+} fetches to
  process for a query. 

  .. note:: 

     You can now set limits on the amount of data that {+adf+} processes
     for your queries to control costs. To learn more, see
     :atlas:`Manage Atlas Data Federation Query Limits
     </data-federation/query/manage-query-limits/>`. 

To learn more, see the |service-pricing|. 

.. toctree::
   :titlesonly:
   :hidden:

   /get-started
   Manage Data Lake Pipeline </manage-adl-dataset-pipeline>
   /limitations
   /release-notes/data-lake
