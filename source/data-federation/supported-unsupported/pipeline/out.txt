.. _adf-out-stage:

========
``$out``
========

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

.. |out| replace:: :manual:`$out </reference/operator/aggregation/out>`
.. |convert| replace:: :manual:`$convert </reference/operator/aggregation/convert>`


|out| takes documents returned by the aggregation pipeline and writes
them to a specified collection. The |out| operator must be the last
stage in the :manual:`aggregation pipeline 
</reference/operator/aggregation-pipeline/>`. In {+adf+}, you can use 
|out| to write data from any one of the :ref:`supported 
<config-adf>` {+fdi+} stores or multiple :ref:`supported <config-adf>` 
{+fdi+} stores when using :ref:`federated queries <federated-queries>` 
to any one of the following: 

- |s3| buckets with read and write permissions
- |service| cluster :manual:`namespace 
  </reference/limits/#faq-dev-namespace>`
  
You must :ref:`connect <fdi-connect>` to your {+fdi+} to use |out|.

.. tabs::

   .. tab:: S3
      :tabid: s3

   .. tab:: Atlas Cluster
      :tabid: atlas


.. _adf-out-stage-perms:

Permissions Required
--------------------

.. tabs::
   :hidden:

   .. tab:: S3
      :tabid: s3

      You must have:

      - A {+fdi+} configured for |s3| bucket with read and write
        permissions or :aws:`s3:PutObject 
        </AmazonS3/latest/dev/using-with-s3-actions.html#using-with-s3-actions-related-to-objects>`
        permissions.
      - A MongoDB user with :atlas:`atlasAdmin 
        </security-add-mongodb-users/#database-user-privileges>` 
        role.

   .. tab:: Atlas Cluster
      :tabid: atlas

      .. note:: 

         .. include:: /includes/data-federation/fact-out-stage-limitation.rst

      You must be a database user with one of the following roles:

      - :authrole:`readWriteAnyDatabase`
      - :atlasrole:`atlasAdmin`
      - A custom role with the following privileges:

        - :manual:`insert </reference/privilege-actions/#insert>` and
        - :manual:`remove </reference/privilege-actions/#remove>`

.. _adf-out-stage-syntax:

Syntax
------

.. tabs::
   :hidden:

   .. tab:: S3
      :tabid: s3

      .. code-block:: json
         :linenos:

         {
           "$out": {
             "s3": {
               "bucket": "<bucket-name>",
               "region": "<aws-region>",
               "filename": "<file-name>",
               "format": {
                 "name": "<file-format>",
                 "maxFileSize": "<file-size>",
                 "maxRowGroupSize": "<row-group-size>",
                 "columnCompression": "<compression-type>"
               },
               "errorMode": "stop"|"continue"
             }
           }
         }

   .. tab:: Atlas Cluster
      :tabid: atlas

      .. code-block:: json
         :linenos:

         {
           "$out": {
             "atlas": {
               "projectId": "<atlas-project-ID>",
               "clusterName": "<atlas-cluster-name>",
               "db": "<atlas-database-name>",
               "coll": "<atlas-collection-name>"
             }
           }
         }

.. _adf-out-stage-fields:

Fields
------

.. tabs::
   :hidden:

   .. tab:: S3
      :tabid: s3

      .. list-table::
         :header-rows: 1
         :widths: 10 10 70 10

         * - Field
           - Type
           - Description
           - Necessity

         * - ``s3``
           - object
           - Location to write the documents from the aggregation
             pipeline.
           - Required

         * - ``s3.bucket``
           - string
           - Name of the |s3| bucket to write the documents from
             the aggregation pipeline to.

             .. important::

                The generated call to |s3| inserts a ``/`` between
                ``s3.bucket`` and ``s3.filename``. Don't append a ``/``
                to your ``s3.bucket`` string. 

                .. example::
                  
                   If you set ``s3.bucket`` to  ``myBucket``
                   and ``s3.filename`` to ``myPath/myData``, 
                   {+adf+} writes the output location as follows:

                   .. code-block:: sh
                      :copyable: false

                      s3://myBucket/myPath/myData.[n].json
                  

           - Required

         * - ``s3.region``
           - string
           - Name of the |aws| region in which the bucket is hosted. If 
             omitted, uses the {+fdi+} :ref:`configuration 
             <config-adf>` to determine the region where the specified 
             ``s3.bucket`` is hosted.
           - Optional

         * - ``s3.filename``
           - string
           - Name of the file to write the documents from the
             aggregation pipeline to. Filename can be constant or
             :ref:`created dynamically <adf-out-stage-egs>` from the
             fields in the documents that reach the |out| stage. Any
             filename expression you provide must evaluate to a
             ``string`` data type. If there are any files on |s3| 
             with the same name and path as the newly generated files, 
             |out| overwrites the existing files with the newly 
             generated files.

             .. important::

                The generated call to |s3| inserts a ``/`` between
                ``s3.bucket`` and ``s3.filename``. Don't prepend a ``/`` 
                to your ``s3.filename`` string. 
                
                .. example:: 

                   If you set ``s3.filename`` to  ``myPath/myData``
                   and ``s3.bucket`` to ``myBucket``, 
                   {+adf+} writes the output location as follows:

                   .. code-block:: sh
                      :copyable: false

                      s3://myBucket/myPath/myData.[n].json

           - Required

         * - ``s3.format``
           - object
           - Details of the file in |s3|.
           - Required

         * - | ``s3``
             | ``.format``
             | ``.name``
           - enum
           - Format of the file in |s3|. Value can be one of the
             following:

             - ``bson``
             - ``bson.gz``
             - ``csv``
             - ``csv.gz``
             - ``json`` :sup:`1`
             - ``json.gz`` :sup:`1`
             - ``parquet``
             - ``tsv``
             - ``tsv.gz``

             :sup:`1` For this format, |out| writes data in 
             :manual:`MongoDB Extended JSON </reference/mongodb-extended-json/>` format.

             .. seealso:: 
             
                :ref:`Limitations <adf-out-stage-limitations>` 

           - Required

         * - | ``s3``
             | ``.format``
             | ``.maxFileSize``
           - bytes
           - Maximum size of the file in |s3|. When the file size limit
             for the current file is reached, a new file is created in
             |s3|. The first file appends a ``1`` before the filename
             extension. For each subsequent file, {+adf+} increments
             the appended number by one.

             .. example::

                ``<filename>.1.<fileformat>``

                ``<filename>.2.<fileformat>``

             If a document is larger than the ``maxFileSize``, {+adf+}
             writes the document to its own file. The following
             suffixes are supported:

             .. list-table::
                :widths: 30 70

                * - Base 10: scaling in multiples of 1000
                  - - ``B``
                    - ``KB``
                    - ``MB``
                    - ``GB``
                    - ``TB``
                    - ``PB``
                * - Base 2: scaling in multiples of 1024
                  - - ``KiB``
                    - ``MiB``
                    - ``GiB``
                    - ``TiB``
                    - ``PiB``

             If omitted, defaults to ``200MiB``.

           - Optional

         * - | ``s3``
             | ``.format``
             | ``.maxRowGroupSize``
           - string 
           - Supported for Parquet file format only.

             Maximum row group size to use when writing to Parquet 
             file. If omitted, defaults to ``128 MiB`` or the value of 
             the ``s3.format.maxFileSize``, whichever is smaller. The 
             maximum allowed value is ``1 GB``.
           - Optional

         * - | ``s3``
             | ``.format``
             | ``.columnCompression``
           - string
           - Supported for Parquet file format only.

             Compression type to apply for compressing data inside a 
             Parquet file when formatting the Parquet file. Valid 
             values are: 

             - ``gzip``
             - ``snappy``
             - ``uncompressed``

             If omitted, defaults to ``snappy``. 
             
             .. seealso:: 
             
                :ref:`adf-data-formats`

           - Optional

         * - ``errorMode``
           - enum
           - Specifies how {+adf+} should proceed if there are errors 
             when processing a document. For example, if {+adf+} 
             encounters an array in a document when {+adf+} is writing 
             to a CSV file, {+adf+} uses this value to determine 
             whether or not to skip the document and process other 
             documents. Valid values are: 

             - ``continue`` to skip the document and continue 
               processing the remaining documents. {+adf+} also writes 
               the document that caused the error to an error file. 
               
               .. seealso:: 
             
                  :ref:`Errors <adf-out-stage-errors>`
                
             - ``stop`` to stop at that point and not process 
               the remaining documents.

             If omitted, defaults to ``continue``.

           - Optional

   .. tab:: Atlas Cluster
      :tabid: atlas

      .. list-table::
         :header-rows: 1
         :widths: 10 10 70 10

         * - Field
           - Type
           - Description
           - Necessity

         * - ``atlas``
           - object
           - Location to write the documents from the aggregation
             pipeline.
           - Required

         * - ``clusterName``
           - string
           - Name of the |service| cluster.
           - Required

         * - ``coll``
           - string
           - Name of the collection on the |service| cluster.
           - Required

         * - ``db``
           - string
           - Name of the database on the |service| cluster that contains
             the collection.
           - Required

         * - ``projectId``
           - string
           - Unique identifier of the project that contains the
             |service| cluster. The project ID must be the ID of the
             project that contains your {+fdi+}. If omitted, defaults to
             the ID of the project that contains your {+fdi+}.
           - Optional

.. _adf-out-stage-options:

Options 
-------

.. list-table::
   :header-rows: 1
   :widths: 20 10 60 10

   * - Option
     - Type
     - Description 
     - Necessity

   * - ``background``
     - boolean
     - Flag to run aggregation operations in the background. If 
       omitted, defaults to ``false``. When set to ``true``, {+adf+} 
       runs the queries in the background. 

       .. code-block:: json 
          :copyable: false 

          { "background" : true }
       
       Use this option if you want to submit other new queries without 
       waiting for currently running queries to complete or disconnect 
       your {+fdi+} connection while the queries continue to run in the 
       background. 
     
     - Optional

.. _adf-out-stage-egs:

Examples
--------

.. tabs::
   :hidden:

   .. tab:: S3
      :tabid: s3

      **Create a Filename**

      The following examples show |out| syntaxes for dynamically
      creating a filename from a constant string or from the fields of
      the same or different data types in the documents that reach the
      |out| stage.

      Simple String Example
      ~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 1 GiB of data as compressed |bson| files to
         an |s3| bucket named ``my-s3-bucket``. 

         Using the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "s3": {
                  "bucket": "my-s3-bucket",
                  "filename": "big_box_store/",
                  "format": {
                    "name": "bson.gz"
                  }
                }
              }
            }

         The ``s3.region`` is omitted and so, {+adf+} determines the 
         region where the bucket named ``my-s3-bucket`` is hosted from 
         the storage configuration. |out| writes five compressed |bson| 
         files:

         1. The first 200 MiB of data to a file that |out| names
            ``big_box_store/1.bson.gz``.

            .. note::

               - The value of ``s3.filename`` serves as a constant in
                 each filename. This value doesn't depend upon any
                 document field or value.

               - Your ``s3.filename`` ends with a delimiter, so {+adf+}
                 appends the counter after the constant.

               - If it didn't end with a delimiter, {+adf+} would have
                 added a ``.`` between the constant and the counter,
                 like ``big_box_store.1.bson.gz``

               - As you didn't change the maximum file size using
                 ``s3.format.maxFileSize``, {+adf+} uses the default
                 value of 200 MiB.

         2. The second 200 MiB of data to a new file that |out| names
            ``big_box_store/2.bson.gz``.

         3. Three more files that |out| names
            ``big_box_store/3.bson.gz`` through
            ``big_box_store/5.bson.gz``.

      Single Field from Documents
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 90 MiB of data to |json| files to an |s3|
         bucket named ``my-s3-bucket``.

         Using the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "s3": {
                  "bucket": "my-s3-bucket",
                  "region": "us-east-1",
                  "filename": {"$toString": "$sale-date"},
                  "format": {
                    "name": "json",
                    "maxFileSize": "100MiB"
                  }
                }
              }
            }

         |out| writes 90 MiB of data to |json| files in the root of the
         bucket. Each |json| file contains all of the documents with the
         same ``sale-date`` value. |out| names each file using the
         documents' ``sale-date`` value converted to a string.

      Multiple Fields from Documents
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 176 MiB of data as |bson| files to an |s3|
         bucket named ``my-s3-bucket``.

         Using the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "s3": {
                  "bucket": "my-s3-bucket",
                  "region": "us-east-1",
                  "filename": {
                    "$concat": [
                      "persons/",
                      "$name", "/",
                      "$unique-id", "/"
                    ]
                  },
                  "format": {
                    "name": "bson",
                    "maxFileSize": "200MiB"
                  }
                }
              }
            }

         |out| writes 176 MiB of data to |bson| files. To name each
         file, |out| concatenates:

         - A constant string ``persons/`` and, from the documents:

           - The string value of the ``name`` field,
           - A forward slash (``/``), 
           - The string value of the ``unique-id`` field, and
           - A forward slash (``/``).

         Each |bson| file contains all of the documents with the same
         ``name`` and ``unique-id`` values. |out| names each file using
         the documents' ``name`` and ``unique-id`` values.


      Multiple Types of Fields from Documents
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

      .. example::

         You want to write 154 MiB of data as compressed |json| files
         to an |s3| bucket named ``my-s3-bucket``.

         Consider the following |out| syntax:

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "s3": {
                  "bucket": "my-s3-bucket",
                  "region": "us-east-1",
                  "filename": {
                    "$concat": [
                      "big-box-store/",
                      {
                        "$toString": "$store-number"
                      }, "/",
                      {
                        "$toString": "$sale-date"
                      }, "/",
                      "$part-id", "/"
                    ]
                  },
                  "format": {
                    "name": "json.gz",
                    "maxFileSize": "200MiB"
                  }
                }
              }
            }

         |out| writes 154 MiB of data to compressed |json| files, where
         each file contains all documents with the same 
         ``store-number``, ``sale-date``, and ``part-id`` values.  To 
         name each file, |out| concatenates:

         - A constant string value of ``big-box-store/``,
         - A string value of a unique store number in the
           ``store-number`` field,
         - A forward slash (``/``),
         - A string value of the date from the ``sale-date`` field, 
         - A forward slash (``/``), 
         - A string value of part ID from the ``part-id`` field, and 
         - A forward slash (``/``).

   .. tab:: Atlas Cluster
      :tabid: atlas

      Write to Collection on Atlas Cluster
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

      This |out| syntax sends the aggregated data to a 
      ``sampleDB.mySampleData`` collection in the |service| cluster 
      named ``myTestCluster``. The syntax doesn't specify a project ID; 
      |out| uses the ID of the project that contains your {+fdi+}.

      .. example::

         .. code-block:: json
            :linenos:

            {
              "$out": {
                "atlas": {
                  "clusterName": "myTestCluster",
                  "db": "sampleDB",
                  "coll": "mySampleData"
                }
              }
            }

Run Query in the background
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following example shows |out| syntax for running an aggregation 
pipeline that ends with the |out| stage in the background.

.. tabs::
   :hidden:

   .. tab:: S3
      :tabid: s3

      .. example::

         .. code-block:: json 
            :emphasize-lines: 13

            db.mySampleData.aggregate(
              [
                { 
                  "$out": { 
                    "atlas": { 
                      "clusterName":"myTestCluster", 
                      "db":"sampleDB", 
                      "coll":"mySampleData" 
                    } 
                  } 
                }
              ], 
              { "background" : true }
            )

         |out| writes to |json| files in the root of the bucket in the 
         background. Each |json| file contains all of the documents 
         with the same ``sale-date`` value. |out| names each file using 
         the documents' ``sale-date`` value converted to a string. 

   .. tab:: Atlas Cluster
      :tabid: atlas

      .. example:: 

         .. code-block:: json 
            :emphasize-lines: 13
            
            db.mySampleData.aggregate(
              [
                {
                  "$out": {
                    "atlas": {
                      "clusterName": "myTestCluster",
                      "db": "sampleDB",
                      "coll": "mySampleData"
                    }
                  }
                }
              ], 
              { background: true }
            )

         |out| writes to ``sampleDB.mySampleData`` collection in the 
         |service| cluster named ``myTestCluster`` in the background.

.. _adf-out-stage-limitations:

Limitations
-----------

.. tabs::
   :hidden:

   .. tab:: S3
      :tabid: s3

      String Data Type
      ~~~~~~~~~~~~~~~~

      {+adf+} interprets empty strings (``""``) as ``null`` values when
      parsing filenames. If you want {+adf+} to generate parseable
      filenames, wrap the field references that could have ``null``
      values using |convert| with an empty string ``onNull`` value.

      .. example::

         This example shows how to handle null values in the ``year``
         field when creating a filename from the field value.

         .. code-block:: json
            :copyable: false
            :linenos:

            {
              "$out": {
                "s3": {
                  "bucket": "my-s3-bucket",
                  "region": "us-east-1",
                  "filename": {
                    "$concat": [
                      "big-box-store/",
                      {
                        "$convert": {
                          "input": "$year",
                          "to": "string",
                          "onNull": ""
                        }
                      }, "/"
                    ]
                  },
                  "format": {
                    "name": "json.gz",
                    "maxFileSize": "200MiB"
                  }
                }
              }
            }

      Number of Unique Fields
      ~~~~~~~~~~~~~~~~~~~~~~~

      When writing to :ref:`CSV <adf-csv-tsv-data>`,
      :ref:`TSV <adf-csv-tsv-data>`, or :ref:`Parquet <adf-parquet-data>`
      file format, {+adf+} doesn't support more than 32000 unique fields.

      .. _csv_and_tsv_format:
      
      CSV and TSV File Format
      ~~~~~~~~~~~~~~~~~~~~~~~

      When writing to CSV or TSV format, {+adf+} does not support the 
      following data types in the documents: 

      - Arrays 
      - DB pointer 
      - JavaScript 
      - JavaScript code with scope
      - Minimum or maximum key data type

      In a CSV file, {+adf+} represents nested documents using the dot 
      (``.``) notation. For example, {+adf+} writes 
      ``{ x: { a: 1, b: 2 } }`` as the following in the CSV file: 

      .. code-block:: shell 
         :copyable: false 

         x.a,x.b 
         1,2
      
      {+adf+} represents all other data types as strings. Therefore, 
      the data types in MongoDB read back from the CSV file may not be 
      the same as the data types in the original |bson| documents from 
      which the data types were written.

      Parquet File Format
      ~~~~~~~~~~~~~~~~~~~

      For Parquet, {+adf+} reads back fields with null or undefined 
      values as missing because Parquet doesn't distinguish between 
      null or undefined values and missing values. Although {+adf+} 
      supports all data types, for |bson| data types that do not have 
      a direct equivalent in Parquet, such as JavaScript, regular 
      expression, etc., it:
             
      - Chooses a representation that allows 
        the resulting Parquet file to be read back using a non-MongoDB 
        tool.
      - Stores a MongoDB schema in the Parquet file's key/value 
        metadata so that {+adf+} can reconstruct the original |bson| 
        document with the correct data types if the Parquet file is 
        read back by {+adf+}.

      .. example:: 

         Consider the following |bson| documents:

         .. code-block:: json
            :copyable: false 

            {
              "clientId": 102, 
              "phoneNumbers": ["123-4567", "234-5678"], 
              "clientInfo": {
                "name": "Taylor",
                "occupation": "teacher"
              }
            }
            {
              "clientId": "237", 
              "phoneNumbers" ["345-6789"]
              "clientInfo": {
                "name": "Jordan"
              }
            }

         If you write the preceding |bson| documents to Parquet format using
         :ref:`$out to S3 <adf-out-stage>`, the Parquet file schema for your
         |bson| documents would look similar to the following: 

         .. code-block:: shell 
            :copyable: false 

            message root {
              optional group clientId {
                optional int32 int;
                optional binary string; (STRING)
              }
              optional group phoneNumbers (LIST) {
                repeated group list {
                  optional binary element (STRING);
                }
              }
              optional group clientInfo {
                optional binary name (STRING);
                optional binary occupation (STRING);
              }
            }

         Your Parquet data on |s3| would look similar to the following: 

         .. code-block:: shell
            :copyable: false 
            :linenos:

            clientId:
            .int = 102
            phoneNumbers:
            .list:
            ..element = "123-4567"
            .list:
            ..element = "234-5678"
            clientInfo:
            .name = "Taylor"
            .occupation = "teacher"

            clientId:
            .string = "237"
            phoneNumbers:
            .list:
            ..element = "345-6789"
            clientInfo:
            .name = "Jordan"

         The preceding example demonstrates how {+adf+} handles complex data
         types: 

         - {+adf+} maps documents at all levels to a Parquet group.
         - {+adf+} encodes arrays using the ``LIST`` logical type and the
           mandatory three-level list or element structure. To learn more,
           see `Lists
           <https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#nested-types>`__. 
         - {+adf+} maps polymorphic |bson| fields to a group of multiple single-type
           columns because Parquet doesn't support polymorphic columns. 
           {+adf+} names the group after the |bson| field. In the preceding
           example, {+adf+} creates a Parquet group named ``clientId`` for
           the poymorphic field named ``clientId`` with two children named
           after its |bson| types, ``int`` and ``string``.

   .. tab:: Atlas Cluster
      :tabid: atlas

      This section applies only to |s3| buckets with read and write 
      permissions.

.. _adf-out-stage-errors:

Error Output
------------

.. tabs::
   :hidden:

   .. tab:: S3
      :tabid: s3

      {+adf+} uses the error handling mechanism described below for documents
      that enter the :pipeline:`$out` stage and cannot be written for one of the following reasons:
      
      - The ``s3.filename`` does not evaluate to a string value. 
      - The ``s3.filename`` evaluates to a file that cannot be written to.
      - The ``s3.format.name`` is set to ``csv``, ``tsv``, ``csv.gz``, or ``tsv.gz`` and the document passed to :pipeline:`$out`
        contains data types that are not supported by the specified file format. For a full list of unsupported data types, see :ref:`CSV and TSV File Format <csv_and_tsv_format>`. 

      
      If :pipeline:`$out` encounters one of the above errors while processing a document, {+adf+} writes to the following three special error files in the path
      ``s3://<bucket-name>/atlas-data-lake-<correlation-id>/``: 

      .. list-table:: Error Files
        :widths: 10 30
        :header-rows: 1

        * - Error File Name
          - Description
        * - out-error-docs/<i>.json
          - {+adf+} writes the document that encountered an error to this file. 

            ``i`` begins with ``1`` and increments whenever the file being written to reaches the ``maxFileSize``. 
            Then, any further documents are written to the new file ``out-error-docs/<i+1>.json``. 
        * - out-error-index/<i>.json
          - {+adf+} writes an error message to this file.
            Each error message contains a description of the error and an index value ``n`` that begins with ``0``
            and increments with each additional error message written to the file.
            
            ``i`` begins with ``1`` and increments whenever the file being written to reaches the ``maxFileSize``.  
            Then, any further error messages are written to the new file ``out-error-docs/<i+1>.json``. 
        * - out-error-summary.json
          - {+adf+} writes a single summary document for each type of error encountered during an aggregation operation to this file.
            Each summary document contains a description of the type of error and a count of the number of documents that encountered that type of error.


      .. example::

         This example shows how to generate error files using :pipeline:`$out` in a {+fdi+}. 

         The following aggregation pipeline sorts documents in the ``analytics.customers`` sample dataset collection 
         by descending customer birthdate and attempts to write the ``_id``, ``name`` and ``accounts`` fields of the 
         youngest three customers to the file named ``youngest-customers.csv`` in the S3 bucket named ``customer-data``.   
        
         .. code-block:: json
            :copyable: true

            db.customers.aggregate([ 
               { $sort: { "birthdate"  :  -1 } }, 
               { $unset: [ "username", "address", "email", "tier_and_details", "birthdate" ] }, 
               { $limit: 3 }, 
               { $out: {
                    "s3": {
                       "bucket": "customer-data", 
                       "filename": "youngest-customers",
                       "region":"us-east-2", 
                       "format": {
                       "name": "csv"
                       }
                    }
                 }
               } 
             ])

         Because ``accounts`` is an array field, :pipeline:`$out` encounters an error when it tries to write
         a document to ``s3.format.name`` ``csv``. To handle these errors, {+adf+} writes to the following three error files: 
        
         - The following output shows the first of three documents written to the ``out-error-docs/1.json`` file: 

           .. code-block:: json
              :copyable: false
              :caption: s3://customer-data/atlas-data-lake-1773b3d5e2a7f3858530daf5/out-error-docs/1.json

              {
                "_id" : {"$oid":"5ca4bbcea2dd94ee58162ba7"},
                "name": "Marc Cain",
                "accounts": [{"$numberInt":"980440"}, {"$numberInt":"626807"}, {"$numberInt":"313907"}, {"$numberInt":"218101"}, {"$numberInt":"157495"}, {"$numberInt":"736396"}],
              }
        
         - The following output shows the first of three error messages written to the ``out-error-index/1.json`` file.  
           The ``n`` field starts at 0 and increments for each error written to the file.  

           .. code-block:: json
              :copyable: false
              :caption: s3://customer-data/atlas-data-lake-1773b3d5e2a7f3858530daf5/out-error-index/1.json

              { 
                "n" : {"$numberInt": "0"},
                "error" : "field accounts is of unsupported type array"
              }  

         - The following output shows the error summary document written to the ``out-error-summary`` file. 
           The ``count`` field represents the number of documents passed to :pipeline:`$out` that encountered 
           an error due to the ``accounts`` array field. 

           .. code-block:: json
              :copyable: false
              :caption: s3://customer-data/atlas-data-lake-1773b3d5e2a7f3858530daf5/out-error-summary.json

                {
                 "errorType": "field accounts is of unsupported type array",
                 "count": {"$numberInt":"3"}
                } 


   .. tab:: Atlas Cluster
      :tabid: atlas

      This section applies only to |s3| buckets with read and write 
      permissions.

.. seealso::

   - :manual:`$out Aggregation Stage Reference 
     </reference/operator/aggregation/out>` 
   - `Tutorial: Federated Queries and $out to S3 
     <https://www.mongodb.com/developer/products/atlas/atlas-data-lake-federated-queries-out-aws-s3/>`__
