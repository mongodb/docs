.. _streams-manage-processor:
.. _atlas-sp-manage-processor:

=========================
Develop Stream Processors
=========================

.. meta::
   :description: Create, start, stop, modify, and view stats for streaming data pipelines

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

.. facet::
   :name: genre
   :values: reference

An {+atlas-sp+} stream processor applies the logic of a uniquely named 
:ref:`stream aggregation pipeline <atlas-sp-aggregation>` to your 
streaming data. {+atlas-sp+} saves each stream processor definition to
persistent storage so that it can be reused. You can only use a given
stream processor in the :ref:`{+spw+} <manage-spi>` in which its definition is
stored. 

.. _atlas-sp-manage-processor-prereqs:

Prerequisites
-------------

To create and manage a stream processor, you must have:

- A :ref:`{+spw+} <manage-spi>`
- A database user with the :atlasrole:`atlasAdmin` role to create
  and run stream processors
- An |service| {+cluster+}

.. _atlas-sp-manage-processor-considerations:

Considerations
--------------

Many stream processor commands require you to specify the name of the
relevant stream processor in the method invocation. The syntax
described in the following sections assumes strictly alphanumeric
names. If your stream processor's name includes non-alphanumeric
characters such as hyphens (``-``) or full stops (``.``), you must
enclose the name in square brackets (``[]``) and double quotes
(``""``) in the method invocation, as in
``sp.["special-name-stream"].stats()``.

.. _streams-manage-process:
.. _atlas-sp-manage-processor-interactive:

Create a Stream Processor Interactively
---------------------------------------

You can create a stream processor interactively with the
:method:`sp.process()` method in {+mongosh+}. Stream processors that you
create interactively exhibit the following behavior:

- Write output and dead letter queue documents to the shell
- Begin running immediately upon creation
- Run for either 10 minutes or until the user stops them
- Don't persist after stopping

Stream processors that you create interactively are intended for
prototyping. To create a persistent stream processor, see
:ref:`streams-manage-create`.

``sp.process()`` has the following syntax:

.. code-block:: sh

   sp.process(<pipeline>)

.. list-table::
   :header-rows: 1
   :widths: 20 20 20 40

   * - Field
     - Type
     - Necessity	  
     - Description

   * - ``pipeline``
     - array
     - Required
     - :ref:`Stream aggregation pipeline <atlas-sp-aggregation>` you
       want to apply to your streaming data.

To create a stream processor interactively: 

.. procedure::
   :style: normal

   .. step:: Connect to your {+spw+}.
      
      Use the connection string associated with your {+spw+}
      to connect using {+mongosh+}.

      .. example::

         The following command connects to a {+spw+} as a user named
         ``streamOwner`` using `x.059 <https://www.mongodb.com/docs/manual/core/security-x.509/>`_ 
         authentication:

         .. code-block:: sh

            mongosh "mongodb://atlas-stream-78xq9gi1v4b4288x06a73e9f-zi30g.virginia-usa.a.query.mongodb-qa.net/?authSource=%24external&authMechanism=MONGODB-X509" \\ 
            --tls --authenticationDatabase admin --username streamOwner

         Provide your user password when prompted.

   .. step:: Define a pipeline.

      In the {+mongosh+} prompt, assign an array containing the
      aggregation stages you want to apply to a variable named 
      ``pipeline``. 
      
      The following example uses the ``stuff`` topic in
      the  ``myKafka`` connection in the connection registry as the 
      :pipeline:`$source`, matches records where the ``temperature`` 
      field has a value of ``46`` and emits the processed messages to 
      the ``output`` topic of the ``mySink`` connection in 
      the connection registry:

      .. code-block:: sh

         pipeline = [
          {$source: {"connectionName": "myKafka", "topic": "stuff"}},
          {$match: { temperature: 46 }},
          {
            "$emit": {
              "connectionName": "mySink",
              "topic" : "output",
            }  
          }
         ]


   .. step:: Create a stream processor.

      The following command creates a stream processor that
      applies the logic defined in ``pipeline``.

      .. code-block:: sh
      
         sp.process(pipeline)

.. _streams-manage-create:
.. _atlas-sp-manage-processor-create:

Create a Stream Processor
-------------------------

To create a stream processor that persists until you drop it: 

.. tabs::

   .. tab:: {+atlas-admin-api+}
      :tabid: admin-api

      The {+atlas-admin-api+} provides an endpoint for 
      creating a stream processor.

      :oas-bump-atlas-op:`Create One Stream Processor <creategroupstreamprocessor>`

   .. tab:: {+atlas-ui+} 
      :tabid: atlas-ui

      .. include:: /atlas-stream-processing/steps-create-stream-processor-ui.rst

   .. tab:: ``mongosh``
      :tabid: mongosh

      To create a new stream processor with {+mongosh+}, use the 
      :method:`sp.createStreamProcessor()` method. It has the following syntax:

      .. code-block:: sh

         sp.createStreamProcessor(<name>, <pipeline>, <options>)

      .. list-table::
        :widths: 20 10 50 20
        :header-rows: 1

        * - Argument
          - Type
          - Necessity
          - Description

        * - ``name``
          - string
          - Required
          - Logical name for the stream processor. This must be unique
            within the {+spw+}. This name should contain only alphanumeric
            characters.

        * - ``pipeline``
          - array
          - Required
          - :ref:`Stream aggregation pipeline <atlas-sp-aggregation>` you
            want to apply to your streaming data.

        * - ``options``
          - object
          - Optional
          - Object defining various optional settings for your stream
            processor.

        * - ``options.dlq``
          - object
          - Conditional
          - Object assigning a 
            :term:`dead letter queue` for your {+spw+}. This field is 
            necessary if you define the ``options`` field.

        * - ``options.dlq.connectionName``
          - string
          - Conditional
          - Human-readable label that identifies a connection in your 
            connection registry. This connection must reference an 
            |service| {+cluster+}. This field is necessary if you define the
            ``options.dlq`` field.

        * - ``options.dlq.db``
          - string
          - Conditional
          - Name of an |service| database on the {+cluster+} specified 
            in ``options.dlq.connectionName``. This field is necessary if 
            you define the ``options.dlq`` field.

        * - ``options.dlq.coll``
          - string
          - Conditional
          - Name of a collection in the database specified in
            ``options.dlq.db``. This field is necessary if you 
            define the ``options.dlq`` field.
	    
        * - ``options.tier``
          - string
          - Optional
          - The tier of the pod to which {+atlas-sp+} assigns the
	    processor. If you do not declare this option,
	    {+atlas-sp+} assigns the processor to a pod of the
	    {+spw+}'s default tier. To learn more, see :ref:`Tiers
	    <atlas-sp-architecture-tiers>`.	   

      .. procedure::
        :style: normal

        .. step:: Connect to your {+spw+}.
            
            Use the connection string associated with your {+spw+}
            to connect using {+mongosh+}.

            a. In the pane for your {+spw+}, click :guilabel:`Connect`.

            #. In the :guilabel:`Connect to your workspace` dialog, 
               select the :guilabel:`Shell` tab.

            #. Copy the connection string displayed in the dialog. It has
               the following format, where
               ``<atlas-stream-processing-url>`` is the URL of your {+spw+}
               and ``<username>`` is the username of a database user with
               the :atlasrole:`atlasAdmin` role:

               .. code-block:: sh

                  mongosh "mongodb://<atlas-stream-processing-url>/" 
                  --tls --authenticationDatabase admin --username <username>  
                  --password <password>   

            #. Paste the connection string into your terminal and replace
               the ``<password>`` placeholder with the credentials for
               the user. Press Enter to run it to connect to your
               {+spw+}.

            .. example::

              The following command connects to a {+spw+} as a user named
              ``streamOwner`` using `x.059 <https://www.mongodb.com/docs/manual/core/security-x.509/>`_
              authentication.
              
              .. code-block:: sh

                  mongosh "mongodb://atlas-stream-78xq9gi1v4b4288x06a73e9f-zi30g.virginia-usa.a.query.mongodb-qa.net/?authSource=%24external&authMechanism=MONGODB-X509" \\ 
                  --tls --authenticationDatabase admin --username streamOwner

              Provide your user password when prompted.

        .. step:: Define a pipeline.

            In the {+mongosh+} prompt, assign an array containing the
            aggregation stages you want to apply to a variable named 
            ``pipeline``. 
            
            The following example pipeline uses the ``stuff`` topic in
            the  ``myKafka`` connection in the connection registry as the 
            :pipeline:`$source`, matches records where the ``temperature`` 
            field has a value of ``46`` and emits the processed messages to 
            the ``output`` topic of the ``mySink`` connection in 
            the connection registry:

            .. code-block:: sh

              pipeline = [
                {$source: {"connectionName": "myKafka", "topic": "stuff"}},
                {$match: { temperature: 46 }},
                {
                  "$emit": {
                    "connectionName": "mySink",
                    "topic" : "output",
                  }  
                }
              ]

        .. step:: (Optional) Define a :term:`DLQ <dead letter queue>`.

            In the {+mongosh+} prompt, assign an object containing the
            following properties of your DLQ:

            - Connection name
            - Database name
            - Collection name

            The following example defines a DLQ over the ``cluster01``
            connection, in the ``metadata.dlq`` database collection.

            .. code-block:: sh

              deadLetter = {
                dlq: {
                  connectionName: "cluster01", 
                  db: "metadata", 
                  coll: "dlq"
                }
              }

        .. step:: Create a stream processor.

            The following command creates a stream processor named 
            ``proc01`` that applies the logic defined in ``pipeline``.
            Documents that throw errors in processing are written to the
            DLQ defined in ``deadLetter``.

            .. code-block:: sh
            
              sp.createStreamProcessor("proc01", pipeline, deadLetter)

.. _streams-manage-start:
.. _atlas-sp-manage-processor-start:

Start a Stream Processor
------------------------

.. include:: /includes/atlas-stream-processing/stopped-processor-state.rst

To start a stream processor: 

.. tabs::

   .. tab:: {+atlas-admin-api+}
      :tabid: admin-api

      The {+atlas-admin-api+} provides the
      :oas-bump-atlas-op:`Start One Stream Processor <startgroupstreamprocessor>` and
      :oas-bump-atlas-op:`Start One Stream Processor with Options
      <startgroupstreamprocessorwith>` endpoints for
      starting a stream processor.

      - To start a stream processor without options, use :oas-bump-atlas-op:`Start
        One Stream Processor <startgroupstreamprocessor>`

      - To start a stream processor and modify properties of the
        :pipeline:`$source` stage, use 
        :oas-bump-atlas-op:`Start One Stream Processor with Options
        <startgroupstreamprocessorwith>` This endpoint
        supports modifying either the ``startAfter`` or
        ``startAtOperationTime`` properties.

   .. tab:: {+atlas-ui+} 
      :tabid: atlas-ui

      To start a stream processor in the {+atlas-ui+}, go to the
      :guilabel:`Stream Processing` page for your {+service+} project
      and click :guilabel:`Manage` in the pane for your {+spw+} to
      view the list of stream processors defined for it. 
      
      Then, click the :guilabel:`Start` icon for your stream processor.

   .. tab:: ``mongosh``
      :tabid: mongosh

      To start a stream processor with {+mongosh+}, use the 
      :method:`sp.processor.start()` method. It has the following syntax:

      .. code-block:: sh

         sp.processor.start(<options>)

      Where ``<options>`` may be one of the following:

      .. list-table::
         :widths: 30 70
         :header-rows: 1

         * - Option
	   - Description


         * - ``startAtOperationTime``
	   - See :ref:`<atlas-sp-agg-source-syntax-coll>`

         * - ``tier``
	   - The tier of the pod to which {+atlas-sp+} assigns the
	     processor. If you do not declare this option,
	     {+atlas-sp+} assigns the processor to a pod of the
	     {+spw+}'s default tier. To learn more, see :ref:`Tiers
	     <atlas-sp-architecture-tiers>`.
	   
      For example, to start a stream processor named ``proc01``, run the 
      following command:

      .. io-code-block::
         :copyable: true

         .. input:: 
            :language: shell 

            sp.proc01.start()

         .. output::
            :language: shell

            { "ok" : 1 }

      This method returns ``{ "ok": 1 }`` if the stream processor exists
      and isn't currently running. If you invoke
      ``sp.processor.start()`` for a stream processor that is not
      ``STOPPED``, {+mongosh+} will return an error.


.. _streams-manage-stop:
.. _atlas-sp-manage-processor-stop:

Stop a Stream Processor
-------------------------

.. include:: /includes/atlas-stream-processing/stopped-processor-state.rst

To stop a stream processor:

.. tabs::

   .. tab:: {+atlas-admin-api+}
      :tabid: admin-api

      The {+atlas-admin-api+} provides an endpoint for 
      stopping a stream processor.

      :oas-bump-atlas-op:`Stop One Stream Processor <stopgroupstreamprocessor>`

   .. tab:: {+atlas-ui+} 
      :tabid: atlas-ui

      To pause a stream processor in the {+atlas-ui+}, go to the
      :guilabel:`Stream Processing` page for your {+service+} project
      and click :guilabel:`Manage` in the pane for your {+spw+} to
      view the list of stream processors defined for it. 

      Then, click the :guilabel:`Pause` icon for your stream processor.

   .. tab:: ``mongosh``
      :tabid: mongosh

      To stop an existing stream processor with {+mongosh+}, use the 
      :method:`sp.processor.stop()` method.

      For example, to stop a stream processor named ``proc01``, run the 
      following command:

      .. io-code-block::
         :copyable: true

         .. input:: 
            :language: shell 

            sp.proc01.stop()

         .. output::
            :language: shell

            { "ok" : 1 }

      This method returns ``{ "ok": 1 }`` if the stream processor exists
      and is currently running. If you invoke ``sp.processor.stop()``
      for a stream processor that is not ``running``, {+mongosh+} will
      return an error.

.. _streams-manage-modify: 

Modify a Stream Processor
-------------------------

You can modify the following elements of an existing stream processor:

- Name
- :ref:`Pipeline <atlas-sp-aggregation>`
- :ref:`Dead Letter Queue <atlas-sp-dlq>`

To modify a stream processor, do the following:

1. :ref:`Stop the stream processor <streams-manage-stop>`. 
2. :ref:`Modify the stream processor <streams-manage-modify-steps>`. 
3. :ref:`Restart the stream processor <streams-manage-start>`.

By default, modified processors restore from the last checkpoint. Alternatively, 
you can set ``resumeFromCheckpoint=false``, in which case the processor only 
retains summary stats. When you modify a processor with open windows, the windows 
are entirely recomputed on the updated pipeline. 

.. include:: /includes/fact-asp-alert-condition.rst

Limitations
~~~~~~~~~~~

When the default setting ``resumeFromCheckpoint=true`` is enabled, the following 
limitations apply:

- You can't modify the ``$source`` stage.
- You can't modify the interval of your window.
- You can't remove a window.
- You can only modify a pipeline with a window if that window has either a 
  ``$group`` or ``$sort`` stage in its inner pipeline.

- You can't change an existing window type. For example, you can't change from a 
  ``$tumblingWindow`` to a ``$hoppingWindow`` or vice versa.
- Processors with windows may reprocess some data as a product of recalculating 
  the windows. 

.. _streams-manage-modify-steps: 

To modify a stream processor:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. tabs::

   .. tab:: {+atlas-admin-api+}
      :tabid: admin-api

      The {+atlas-admin-api+} provides an endpoint for 
      modifying a stream processor.

      :oas-bump-atlas-op:`Modify One Stream Processor <updategroupstreamprocessor>`

   .. tab:: ``mongosh``
      :tabid: mongosh

      Requires {+mongosh+} v2.3.4+. 
      
      Use the ``sp.<streamprocessor>.modify()`` command to modify an existing 
      stream processor. ``<streamprocessor>`` must be the name of a stopped stream 
      processor defined for the current {+spw+}.

      For example, to modify a stream processor named ``proc01``, run the
      following command:

      .. code-block:: javascript

         sp.proc1.modify(<pipeline>, {
            resumeFromCheckpoint: bool, // optional 
            name: string, // optional
            dlq: string, // optional
        }})


      Add a Stage to an Existing Pipeline
      ````````````````````````````````````

      .. code-block:: javascript

         sp.createStreamProcessor("foo", [
           {$source: {
              connectionName: "StreamsAtlasConnection",
              db: "test",
              coll: "test"
          }},
          {$merge: {
              into: {
                  connectionName: "StreamsAtlasConnection",
                  db: "testout",
                  coll: "testout"
              }
          }}
        ])
        sp.foo.start();

      .. code-block:: javascript

         sp.foo.stop();
         sp.foo.modify([
           {$source: {
             connectionName: "StreamsAtlasConnection",
             db: "test",
             coll: "test"
           }},
           {$match: {
             operationType: "insert"
           }},
           {$merge: {
              into: {
              connectionName: "StreamsAtlasConnection",
              db: "testout",
              coll: "testout2"
              }
           }}
         ]);
         sp.foo.start();

      Modify the Input Source of a Stream Processor
      ``````````````````````````````````````````````

      .. code-block:: javascript

         sp.foo.modify([
           {$source: {
             connectionName: "StreamsAtlasConnection",
             db: "test",
             coll: "test",
             config: {
               startAtOperationTime: new Date(now.getTime() - 5 * 60 * 1000)
             }
           }},
           {$match: {
             operationType: "insert"
           }},
           {$merge: {
             into: {
               connectionName: "StreamsAtlasConnection",
               db: "testout",
               coll: "testout2"
             }
           }}
         ], {resumeFromCheckpoint: false});

      Remove a Dead Letter Queue from a Stream Processor
      ```````````````````````````````````````````````````
      .. code-block:: javascript

         sp.foo.stop();
         sp.foo.modify({dlq: {}})
         sp.foo.start();    

      Modify a Stream Processor with a Window
      ````````````````````````````````````````

      .. code-block:: javascript

         sp.foo.stop();
         sp.foo.modify([
           {$source: {
             connectionName: "StreamsAtlasConnection",
             db: "test",
             coll: "test"
           }},
           {$replaceRoot: {newRoot: "$fullDocument"}},
           {$match: {cost: {$gt: 500}}},
           {$tumblingWindow: {
             interval: {unit: "day", size: 1},
             pipeline: [
              {$group: {_id: "$customerId", sum: {$sum: "$cost"}, avg: {$avg: "$cost"}}}
             ]
           }},
           {$merge: {
             into: {
             connectionName: "StreamsAtlasConnection",
             db: "testout",
             coll: "testout"
            }
           }}
         ], {resumeFromCheckpoint: false});
         sp.foo.start();

.. _streams-manage-drop:
.. _atlas-sp-manage-processor-drop:

Drop a Stream Processor
-------------------------

To drop a stream processor: 

.. tabs::

   .. tab:: {+atlas-admin-api+}
      :tabid: admin-api

      The {+atlas-admin-api+} provides an endpoint for 
      deleting a stream processor.

      :oas-bump-atlas-op:`Delete One Stream Processor <deletegroupstreamprocessor>`

   .. tab:: {+atlas-ui+} 
      :tabid: atlas-ui

      To delete a stream processor in the {+atlas-ui+}, go to the
      :guilabel:`Stream Processing` page for your {+service+} project
      and click :guilabel:`Manage` in the pane for your {+spw+} to
      view the list of stream processors defined for it.

      Then, click the :guilabel:`Delete` (:icon:`trash-alt`) icon for
      your stream processor. In the confirmation dialog that appears,
      type the name of the stream processor (``solarDemo``) to confirm
      that you want to delete it, and then click :guilabel:`Delete`.

   .. tab:: ``mongosh``
      :tabid: mongosh

      To delete an existing stream processor with {+mongosh+}, use the 
      :method:`sp.processor.drop()` method.

      For example, to drop a stream processor named ``proc01``, run the 
      following command:

      .. code-block:: sh

         sp.proc01.drop()

      This method returns: 

      - ``true`` if the stream processor exists.

      - ``false`` if the stream processor doesn't exist.

      When you drop a stream processor, all resources that {+atlas-sp+} 
      provisioned for it are destroyed, along with all saved state.

.. _streams-list-procs:
.. _atlas-sp-manage-processor-list:

List Available Stream Processors
--------------------------------

To list all available stream processors:

.. tabs::

   .. tab:: {+atlas-admin-api+}
      :tabid: admin-api

      The {+atlas-admin-api+} provides an endpoint for 
      listing all available stream processors.

      :oas-bump-atlas-op:`List Stream Processors <getgroupstreamprocessors>`

   .. tab:: {+atlas-ui+} 
      :tabid: atlas-ui

      To view the list of stream processors defined for your {+spw+} in
      the {+atlas-ui+}, go to the :guilabel:`Stream Processing` page
      for your {+service+} project and click :guilabel:`Manage` in
      the pane for your {+spw+}.

      The list of stream processors and their statuses displays.

   .. tab:: ``mongosh``
      :tabid: mongosh

      To list all available stream processors on the current {+spw+}
      with {+mongosh+}, use the :method:`sp.listStreamProcessors()`
      method. This returns a list of documents containing the name,
      start time, current state, and pipeline associated with each
      stream processor. It has the following syntax:

      .. code-block:: sh

         sp.listStreamProcessors(<filter>)

      ``<filter>`` is a document specifying which field(s) to filter the list 
      by.

      .. example::

        The following example shows a return value for an unfiltered 
        request:

        .. io-code-block::
            :copyable: true

            .. input:: 
              :language: sh

              sp.listStreamProcessors()

            .. output:: 
              :language: json
              :visible: false
              :linenos:

              {
                id: '0135',
                name: "proc01",
                last_modified: ISODate("2023-03-20T20:15:54.601Z"),
                state: "RUNNING",
                error_msg: '',
                pipeline: [
                  {
                    $source: {
                      connectionName: "myKafka", 
                      topic: "stuff"
                    }
                  },
                  {
                    $match: { 
                      temperature: 46 
                    }
                  },
                  {
                    $emit: {
                      connectionName: "mySink",
                      topic: "output",
                    }  
                  }
                ],
                lastStateChange: ISODate("2023-03-20T20:15:59.442Z")
              },
              {   
                id: '0218',
                name: "proc02",
                last_modified: ISODate("2023-03-21T20:17:33.601Z"),
                state: "STOPPED",
                error_msg: '',
                pipeline: [
                  {
                    $source: {
                      connectionName: "myKafka", 
                      topic: "things"
                    }
                  },
                  {
                    $match: { 
                      temperature: 41 
                    }
                  },
                  {
                    $emit: {
                      connectionName: "mySink",
                      topic: "results",
                    }  
                  }
                ],
                lastStateChange: ISODate("2023-03-21T20:18:26.139Z")
              }

        If you run the command again on the same {+spw+}, filtering for a 
        ``"state"`` of ``"running"``, you see the following output:

        .. io-code-block::
            :copyable: true

            .. input:: 
              :language: sh

              sp.listStreamProcessors({"state": "running"})

            .. output:: 
              :language: json
              :visible: false
              :linenos:

              {
                id: '0135',
                name: "proc01",
                last_modified: ISODate("2023-03-20T20:15:54.601Z"),
                state: "RUNNING",
                error_msg: '',
                pipeline: [
                  {
                    $source: {
                      connectionName: "myKafka", 
                      topic: "stuff"
                    }
                  },
                  {
                    $match: { 
                      temperature: 46 
                    }
                  },
                  {
                    $emit: {
                      connectionName: "mySink",
                      topic: "output",
                    }  
                  }
                ],
                lastStateChange: ISODate("2023-03-20T20:15:59.442Z")
              }

.. _streams-manage-sample:
.. _atlas-sp-manage-processor-sample:

Sample from a Stream Processor
------------------------------

To return an array of sampled results from an existing stream processor
to ``STDOUT`` with {+mongosh+}, use the :method:`sp.processor.sample()`
method. For example, the following command samples from a stream
processor named ``proc01``.

.. code-block:: sh

   sp.proc01.sample()

This command runs continuously until you cancel it by using
``CTRL-C``, or until the returned samples cumulatively reach 40 MB in
size. The stream processor reports invalid documents in the sample in
a ``_dlqMessage`` document of the following form:

.. code-block:: json
   :copyable: false

   {
     _dlqMessage: {
       errInfo: {
	 reason: "<reasonForError>"
       },
       doc: {
	 _id: ObjectId('<group-id>'),
	 ...
       },
       processorName: '<procName>',
       workspaceName: '<workspaceName>',
       dlqTime: ISODate('2024-09-19T20:04:34.263+00:00')
     }
  }

You can use these messages to diagnose data hygiene issues without
defining a :ref:`dead letter queue <atlas-sp-dlq>` collection.

.. _atlas-sp-manage-processor-stats:

View Statistics of a Stream Processor
-------------------------------------

.. include:: /includes/atlas-stream-processing/stopped-processor-state.rst

To view statistics of a stream processor: 

.. tabs::

   .. tab:: {+atlas-admin-api+}
      :tabid: admin-api

      The {+atlas-admin-api+} provides an endpoint for 
      viewing the statistics of a stream processor.

      :oas-bump-atlas-op:`Get One Stream Processor <getgroupstreamprocessor>`
      
   .. tab:: {+atlas-ui+} 
      :tabid: atlas-ui

      To view monitoring for your stream processor, go to the
      :guilabel:`Stream Processing` page for your {+service+} project
      and open the :guilabel:`Monitoring` tab. Then, select your stream
      processor from the :guilabel:`Stream processor` drop-down list at
      the top left of the page.

   .. tab:: ``mongosh``
      :tabid: mongosh

      To return a document summarizing the current status of an existing 
      stream processor with {+mongosh+}, use the 
      :method:`sp.processor.stats()` method. It has the following syntax:

      .. code-block:: sh

         sp.<streamprocessor>.stats({options: {<options>}})

      Where ``options`` is an optional document with the following fields:

      .. list-table::
        :widths: 20 20 60
        :header-rows: 1

        * - Field
          - Type
          - Description

        * - ``scale``
          - integer
          - Unit to use for the size of items in the output. By default, 
            {+atlas-sp+} displays item size in bytes. To display in KB, 
            specify a ``scale`` of ``1024``.

        * - ``verbose``
          - boolean
          - Flag that specifies the verbosity level of the output document. 
            If set to ``true``, the output document contains a subdocument 
            that reports the statistics of each individual operator in your 
            pipeline. Defaults to ``false``.

      The output document has the following fields:

      .. list-table::
        :widths: 30 10 60
        :header-rows: 1

        * - Field
          - Type
          - Description

        * - ``ns``
          - string
          - The namespace the stream processor is defined in.

        * - ``stats``
          - object
          - A document describing the operational state of the stream 
            processor.

        * - ``stats.name``
          - string
          - The name of the stream processor.

        * - ``stats.status``
          - string
          - The status of the stream processor. This field can have the
            following values:

            - ``starting``
            - ``running``
            - ``error``
            - ``stopping``

        * - ``stats.scaleFactor``
          - integer
          - The scale in which the size field displays. If set to ``1``,
            sizes display in bytes. If set to ``1024``, sizes display in
            kilobytes.

        * - ``stats.inputMessageCount``
          - integer
          - The number of documents published to the stream. A document
            is considered 'published' to the stream once it passes
            through the :pipeline:`$source` stage, not when it passes 
            through the entire pipeline.

        * - ``stats.inputMessageSize``
          - integer
          - The number of bytes or kilobytes published to the stream. 
            Bytes are considered 'published' to the stream once they pass
            through the :pipeline:`$source` stage, not when it passes
            through the entire pipeline.

        * - ``stats.outputMessageCount``
          - integer
          - The number of documents processed by the stream. A document is
            considered 'processed' by the stream once it passes through the
            entire pipeline.

        * - ``stats.outputMessageSize``
          - integer
          - The number of bytes or kilobytes processed by the stream. Bytes
            are considered 'processed' by the stream once they pass through
            the entire pipeline.

        * - ``stats.dlqMessageCount``
          - integer
          - The number of documents sent to the :ref:`atlas-sp-dlq`.

        * - ``stats.dlqMessageSize``
          - integer
          - The number of bytes or kilobytes sent to the 
            :ref:`atlas-sp-dlq`.

        * - ``stats.changeStreamTimeDifferenceSecs``
          - integer
          - The difference, in seconds, between the event time represented by
            the most recent change stream :manual:`resume token
            </changeStreams/#resume-tokens-from-change-events>` and the latest
            event in the :term:`oplog`.

        * - ``stats.changeStreamState``
          - token
          - The most recent change stream :manual:`resume token
            </changeStreams/#resume-tokens-from-change-events>`. Only applies
            to stream processors with a change stream source.

        * - ``stats.latency``
          - document
          - Latency statistics for the stream processor as a whole.
            {+atlas-sp+} returns this field only if you pass in the 
            ``verbose`` option.

        * - ``stats.latency.p50``
          - integer
          - The estimated 50th percentile latency of all documents
            processed in the past 30 seconds. If your pipeline includes
	    a window stage, latency measurements include the interval
	    of the window.

            For example, if your ``$tumblingWindow`` stage has an interval
	    of 5 minutes, latency measurements will include those 5 minutes.

        * - ``stats.latency.p99``
          - integer
          - The estimated 99th percentile latency of all documents 
            processed in the past 30 seconds. If your pipeline includes
	    a window stage, latency measurements include the interval
	    of the window.

            For example, if your ``$tumblingWindow`` stage has an interval
	    of 5 minutes, latency measurements will include those 5 minutes.	  

        * - ``stats.latency.start``
          - datetime
          - Wall time at which the most recent 30 second measurement
            window began.

        * - ``stats.latency.end``
          - datetime
          - Wall time at which the most recent 30 second measurement
            window ended.
	  
        * - ``stats.latency.unit``
          - string
          - Unit of time in which latency is counted. This value is
            always ``microseconds``.

        * - ``stats.latency.count``
          - integer
          - Number of documents the stream processor has processed in
            the most recent 30 second measurement window.

        * - ``stats.latency.sum``
          - integer
          - Sum of all inividual latency measurements, in
            microseconds, taken in the most recent 30 second
	    measurement window.
	  
        * - ``stats.stateSize``
          - integer
          - The number of bytes used by windows to store processor state.

        * - ``stats.watermark``
          - integer
          - The timestamp of the current watermark.

        * - ``stats.operatorStats``
          - array
          - The statistics for each operator in the processor pipeline. 
            {+atlas-sp+} returns this field only if you pass in the 
            ``verbose`` option.
            
            ``stats.operatorStats`` provides per-operator versions of many
            core ``stats`` fields:

            - ``stats.operatorStats.name``
            - ``stats.operatorStats.inputMessageCount``
            - ``stats.operatorStats.inputMessageSize``
            - ``stats.operatorStats.outputMessageCount``
            - ``stats.operatorStats.outputMessageSize``
            - ``stats.operatorStats.dlqMessageCount``
            - ``stats.operatorStats.dlqMessageSize``
	    - ``stats.operatorStats.latency``
            - ``stats.operatorStats.stateSize``
              
            ``stats.operatorStats`` includes the following
            unique fields:

            - ``stats.operatorStats.maxMemoryUsage``
            - ``stats.operatorStats.executionTimeMillis``
 
            ``stats.operatorStats`` also includes the following fields given 
            that you have passed in the ``verbose`` option and your 
            processor includes a window stage:

            - ``stats.minOpenWindowStartTime``
            - ``stats.maxOpenWindowStartTime``

        * - ``stats.operatorStats.maxMemoryUsage``
          - integer
          - The maximum memory usage of the operator in bytes or kilobytes.

        * - ``stats.operatorStats.executionTimeSecs``
          - integer
          - The total execution time of the operator in seconds.

        * - ``stats.minOpenWindowStartTime``
          - date
          - The start time of the minimum open window. This value is optional.

        * - ``stats.maxOpenWindowStartTime``
          - date
          - The start time of the maximum open window. This value is optional.

        * - ``stats.kafkaPartitions``
          - array
          - Offset information for an {+kafka+} broker's partitions. 
            ``kafkaPartitions`` applies only to connections using an 
            {+kafka+} source.
       
        * - ``stats.kafkaPartitions.partition``
          - integer
          - The {+kafka+} topic partition number.

        * - ``stats.kafkaPartitions.currentOffset``
          - integer
          - The offset that the stream processor is on for the
            specified partition. This value equals the previous offset
            that the stream processor processed plus ``1``.

        * - ``stats.kafkaPartitions.checkpointOffset``
          - integer
          - The offset that the stream processor last committed to the
            {+kafka+} broker and the checkpoint for the specified
            partition. All messages through this offset are 
            recorded in the last checkpoint.

        * - ``stats.kafkaPartitions.isIdle``
          - boolean
          - The flag that indicates whether the partition is idle. This 
            value defaults to ``false``. 

      For example, the following shows the status of a stream processor named 
      ``proc01`` on a {+spw+} named ``inst01`` with item sizes displayed in 
      KB:

      .. code-block:: sh

        sp.proc01.stats(1024)

        {
          ok: 1,
          ns: 'inst01',
          stats: {
            name: 'proc01',
            status: 'running',
            scaleFactor: Long("1"), 
            inputMessageCount: Long("706028"),
            inputMessageSize: 958685236,
            outputMessageCount: Long("46322"),
            outputMessageSize: 85666332,
            dlqMessageCount: Long("0"),
            dlqMessageSize: Long("0"),
            stateSize: Long("2747968"),
            watermark: ISODate("2023-12-14T14:35:32.417Z"),
            ok: 1
          },
        }
