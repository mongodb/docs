.. _spark-read-conf:

==========================
Read Configuration Options
==========================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

.. _spark-input-conf:

Read Configuration
------------------

You can configure the following properties to read from MongoDB:

.. note::

   If you use ``SparkConf`` to set the connector's read configurations, 
   prefix ``spark.mongodb.read.`` to each property.

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - Property name
     - Description

   * - ``connection.uri``
     - **Required.**
       The connection string in the form
       ``mongodb://host:port/``. The ``host`` can be a hostname, IP
       address, or UNIX domain socket. If the connection string doesn't
       specify a ``port``, it uses the default MongoDB port, ``27017``.

       You can append the other remaining read options to the ``connection.uri``
       setting. See :ref:`configure-input-uri`.

   * - ``database``
     - **Required.**
       The database name to read data from.

   * - ``collection``
     - **Required.**
       The collection name to read data from.

   * - ``localThreshold``
     - The time in milliseconds to choose among multiple MongoDB servers to send a request.

       **Default:** ``15``

   * - ``readPreference.name``
     - The name of the :ref:`Read Preference <replica-set-read-preference-modes>` mode to use.
       
       **Default:** Primary

   * - ``readPreference.tagSets``
     - The ``ReadPreference`` TagSets to use.

   * - ``readConcern.level``
     - The :manual:`Read Concern </reference/read-concern>` level to use.

   * - ``sampleSize``
     - The sample size to use when analyzing the schema.

       **Default:** ``1000``

   * - ``samplePoolSize``
     - The size of the pool to sample from when analyzing the schema.

       **Default:** ``10000``

   * - ``partitioner``
     - The name of the partitioner to use to split collection data into 
       partitions. Partitions are based on a range of values of a field 
       (e.g. ``_id``\s 1 to 100).

       The connector provides the following partitioners:

       - ``SamplePartitioner``
             A general purpose partitioner for
             all deployments. Uses the average document size and random
             sampling of the collection to determine suitable
             partitions for the collection. To learn more about the 
             ``SamplePartitioner``, see the 
             :ref:`configuration settings description <conf-samplepartitioner>`.

       - ``ShardedPartitioner``
             A partitioner for sharded clusters. Partitions the
             collection based on the data chunks. Requires read access
             to the ``config`` database. To learn more about the 
             ``ShardedPartitioner``, see the 
             :ref:`configuration settings description <conf-shardedpartitioner>`.

       - ``PaginateBySizePartitioner``
             A slow, general purpose partitioner for all deployments.
             Creates partitions based on data size. Requires a query
             for every partition. To learn more about the 
             ``PaginateBySizePartitioner``, see the 
             :ref:`configuration settings description <conf-paginatebysizepartitioner>`.

       - ``PaginateIntoPartitionsPartitioner``
             A partitioner that creates a specified number of 
             partitions for a collection. Uses the total number of 
             documents and the average document size in a collection to 
             calculate the number of documents to include in each 
             partition. To learn more about the 
             ``PaginateIntoPartitionsPartitioner``, see the 
             :ref:`configuration settings description <conf-paginateintopartitionspartitioner>`.

       You can also specify a custom partitioner implementation. For
       custom implementations of the ``Partitioner`` interface, provide
       the full class name.

       To configure options for the various partitioners, see
       :ref:`partitioner-conf`.
      
       **Default:** ``SamplePartitioner``

   * - ``partitionerOptions``
     - The custom options to configure the partitioner.

   * - ``registerSQLHelperFunctions``
     - Registers SQL helper functions to allow easy querying of BSON
       types inside SQL queries.

       **Default:** ``false``

   * - ``sql.inferschema.mapTypes.enabled``
     - Enables you to analyze ``MapType`` and ``StructType`` in the data's schema.

       **Default:** ``true``

   * - ``sql.inferschema.mapTypes.minimumKeys``
     - The minimum number of keys a ``StructType`` needs for the
       connector to detect it as a ``MapType``.

       **Default:** ``250``

   * - ``sql.pipeline.includeNullFilters``
     - Includes ``null`` filters in the aggregation pipeline.
      
   * - ``sql.pipeline.includeFiltersAndProjections``
     - Includes any filters and projections in the aggregation pipeline.

   * - ``pipeline``
     - Enables you to apply custom aggregation pipelines to the collection
       before sending it to Spark.

   * - ``hint``
     - The JSON representation of the :manual:`hint </reference/operator/meta/hint/>` to use in the aggregation.

   * - ``collation``
     - The JSON representation of a collation to use in the aggregation.
       The connector creates this with ``Collation.asDocument.toJson``.

   * - ``allowDiskUse``
     -  Enables writing to temporary files during aggregation.

   * - ``batchSize``
     -  The size of the internal batches within the cursor.
     
.. _partitioner-conf:

Partitioner Configuration
~~~~~~~~~~~~~~~~~~~~~~~~~

.. _conf-mongosamplepartitioner:
.. _conf-samplepartitioner:

``SamplePartitioner`` Configuration
```````````````````````````````````

.. include:: /includes/sparkconf-partitioner-options-note.rst

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - Property name
     - Description

   * - ``partitioner.options.partitionKey``
     - A comma-delimited list of fields by which to split the 
       collection data. The fields should be indexed and contain unique 
       values.

       **Default:** ``_id``

   * - ``partitioner.options.partitionSizeMB``
     - The size (in MB) for each partition. Smaller partition sizes 
       create more partitions containing fewer documents.

       **Default:** ``64``

   * - ``partitioner.options.samplesPerPartition``
     - The number of sample documents to take for each partition in 
       order to establish a ``partitionKey`` range for each partition. 
       
       A greater number of ``samplesPerPartition`` helps to find 
       ``partitionKey`` ranges that more closely match the 
       ``partitionSizeMB`` you specify.
       
       .. note::
       
          For sampling to improve performance, ``samplesPerPartition`` 
          must be fewer than the number of documents within each of 
          your partitions.

          You can estimate the number of documents within each of your 
          partitions by dividing your ``partitionSizeMB`` by the 
          average document size (in MB) in your collection.

       **Default:** ``10``

.. example::

   For a collection with 640 documents with an average document 
   size of 0.5 MB, the default ``SamplePartitioner`` configuration 
   values creates 5 partitions with 128 documents per partition.

   The MongoDB Spark Connector samples 50 documents (the default 10 
   per intended partition) and defines 5 partitions by selecting 
   ``partitionKey`` ranges from the sampled documents.

.. _conf-mongoshardedpartitioner:
.. _conf-shardedpartitioner:

``ShardedPartitioner`` Configuration
````````````````````````````````````

The ``ShardedPartitioner`` automatically determines 
partitions to use based on your shard configuration. 

This partitioner is not compatible with hashed shard keys.

.. _conf-mongopaginatebysizepartitioner:
.. _conf-paginatebysizepartitioner:

``PaginateBySizePartitioner`` Configuration
```````````````````````````````````````````

.. include:: /includes/sparkconf-partitioner-options-note.rst

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - Property name
     - Description

   * - ``partitioner.options.partitionKey``
     - A comma-delimited list of fields by which to split the 
       collection data. The fields should be indexed and contain unique 
       values.

       **Default:** ``_id``

   * - ``partitioner.options.partitionSizeMB``
     - The size (in MB) for each partition. Smaller partition sizes 
       create more partitions containing fewer documents.

       **Default:** ``64``

.. _conf-paginateintopartitionspartitioner:

``PaginateIntoPartitionsPartitioner`` Configuration
```````````````````````````````````````````````````

.. include:: /includes/sparkconf-partitioner-options-note.rst

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - Property name
     - Description

   * - ``partitioner.options.partitionKey``
     - A comma-delimited list of fields by which to split the 
       collection data. The fields should be indexed and contain unique 
       values.

       **Default:** ``_id``

   * - ``partitioner.options.maxNumberOfPartitions``
     - The number of partitions to create.

       **Default:** ``64``

.. _spark-change-stream-conf:

Change Streams
--------------

.. note::

   If you use ``SparkConf`` to set the connector's change stream 
   configurations, prefix ``spark.mongodb.change.stream.`` to each 
   property.

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - Property name
     - Description

   * - ``lookup.full.document``

     - Determines what values your change stream returns on update 
       operations.

       The default setting returns the differences between the original 
       document and the updated document.

       The ``updateLookup`` setting returns the differences between the 
       original document and updated document as well as a copy of the 
       entire updated document.

       .. tip::

          For more information on how this change stream option works, 
          see the MongoDB server manual guide 
          :manual:`Lookup Full Document for Update Operation </changeStreams/#lookup-full-document-for-update-operations>`.

       **Default:** "default"

   * - ``publish.full.document.only``

     - If ``true``, this property returns only the changed document 
       instead of the full change stream document. The connector 
       automatically sets the ``lookup.full.document`` property to 
       ``updateLookup`` to receive the updated documents.

       **Default:** ``false``

.. _configure-input-uri:

``connection.uri`` Configuration Setting
----------------------------------------

You can set all :ref:`spark-input-conf` via the read ``connection.uri`` setting.

For example, consider the following example which sets the read
``connection.uri`` setting via ``SparkConf``:

.. note::

   If you use ``SparkConf`` to set the connector's read configurations, 
   prefix ``spark.mongodb.read.`` to the setting.

.. code:: cfg

  spark.mongodb.read.connection.uri=mongodb://127.0.0.1/databaseName.collectionName?readPreference=primaryPreferred

The configuration corresponds to the following separate configuration
settings:

.. code:: cfg

  spark.mongodb.read.connection.uri=mongodb://127.0.0.1/
   spark.mongodb.read.database=databaseName
   spark.mongodb.read.collection=collectionName
   spark.mongodb.read.readPreference.name=primaryPreferred

If you specify a setting both in the ``connection.uri`` and in a separate
configuration, the ``connection.uri`` setting overrides the separate
setting. For example, given the following configuration, the
database for the connection is ``foobar``:

.. code:: cfg

  spark.mongodb.read.connection.uri=mongodb://127.0.0.1/foobar
   spark.mongodb.read.database=bar
