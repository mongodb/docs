=============================================
Real Time Analytics: Hierarchical Aggregation
=============================================

Overview
--------

Problem
~~~~~~~

You have a large amount of event data that you want to analyze at
multiple levels of aggregation.

Solution
~~~~~~~~

This solution assumes that the incoming event data is already
stored in an incoming ``events`` collection. For details on how you might
get the event data into the events collection, please see :doc:`Real Time
Analytics: Storing Log Data <real-time-analytics-storing-log-data>`.

Once the event data is in the events collection, you need to aggregate
event data to the finest time granularity you're interested in. Once that
data is aggregated, you'll use it to aggregate up to the next level of
the hierarchy, and so on. To perform the aggregations, you'll use
MongoDB's ``mapreduce`` command. The schema will use several collections:
the raw data (event) logs and collections for statistics aggregated
hourly, daily, weekly, monthly, and yearly. This solution uses a hierarchical
approach to running your map-reduce jobs. The input and output of each
job is illustrated below:

.. figure:: img/rta-hierarchy1.png
   :align: center
   :alt: Hierarchy

   Hierarchy of statistics collected

Note that the events rolling into the hourly collection is qualitatively
different than the hourly statistics rolling into the daily collection.

.. note::

    **Map/reduce** is a popular aggregation algorithm that is optimized for
    embarrassingly parallel problems. The psuedocode (in Python) of the
    map/reduce algorithm appears below. Note that this psuedocode is for a
    particular type of map/reduce where the results of the
    map/reduce operation are *reduced* into the result collection, allowing
    you to perform incremental aggregation which you'll need in this case.

    .. code-block:: python

        def map_reduce(icollection, query,
            mapf, reducef, finalizef, ocollection):
            '''Psuedocode for map/reduce with output type="reduce" in MongoDB'''
            map_results = defaultdict(list)
            def emit(key, value):
                '''helper function used inside mapf'''
                map_results[key].append(value)


            # The map phase
            for doc in icollection.find(query):
                mapf(doc)


            # Pull in documents from the output collection for
            # output type='reduce'
            for doc in ocollection.find({'_id': {'$in': map_results.keys() } }):
                map_results[doc['_id']].append(doc['value'])


            # The reduce phase
            for key, values in map_results.items():
                reduce_results[key] = reducef(key, values)


            # Finalize and save the results back
            for key, value in reduce_results.items():
                final_value = finalizef(key, value)
                ocollection.save({'_id': key, 'value': final_value})

    The embarrassingly parallel part of the map/reduce algorithm lies in the
    fact that each invocation of mapf, reducef, and finalizef are
    independent of each other and can, in fact, be distributed to different
    servers. In the case of MongoDB, this parallelism can be achieved by
    using sharding on the collection on you're are performing map/reduce.

Schema
------

When designing the schema for event storage, it's important to track whichevents
which have been included in your aggregations and events which have not yet been
included. A simple approach in a relational database would be to use an auto-increment
integer primary key, but this introduces a big performance penalty to
your event logging process as it has to fetch event keys one-by one.

If you're able to batch up your inserts into the event table, you can
still use an auto-increment primary key by using the ``find_and_modify``
command to generate your ``_id`` values:

.. code-block:: python

    >>> obj = db.my_sequence.find_and_modify(
    ...     query={'_id':0},
    ...     update={'$inc': {'inc': 50}}
    ...     upsert=True,
    ...     new=True)
    >>> batch_of_ids = range(obj['inc']-50, obj['inc'])

In most cases, however, it's sufficient to include a timestamp with
each event that you can use as a marker of which events have been
processed and which ones remain to be processed.

This use case assumes that you
are calculating average session length for
logged-in users on a website. Your event format will thus be the
following:

.. code-block:: javascript

    {
        "userid": "rick",
        "ts": ISODate('2010-10-10T14:17:22Z'),
        "length":95
    }

You want to calculate total and average session times for each user at
the hour, day, week, month, and year. In each case, you will also store
the number of sessions to enable MongoDB to incrementally recompute the
average session times. Each of your aggregate documents, then, looks like
the following:

.. code-block:: javascript

    {
       _id: { u: "rick", d: ISODate("2010-10-10T14:00:00Z") },
       value: {
           ts: ISODate('2010-10-10T15:01:00Z'),
           total: 254,
           count: 10,
           mean: 25.4 }
    }

Note in particular the timestamp field in the aggregate document. This allows you
to incrementally update the various levels of the hierarchy.

Operations
----------

In the discussion below, it is assumed that all the events have been
inserted and appropriately timestamped, so your main operations are
aggregating from events into the smallest aggregate (the hourly totals)
and aggregating from smaller granularity to larger granularity. In each
case, the last time the particular aggregation is run is stored in a ``last_run``
variable. (This variable might be loaded from MongoDB or another persistence
mechanism.)

Creating Hourly Views from Event Collections
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Aggregation
```````````

Here, you want to load all the events since your last run until one minute
ago (to allow for some lag in logging events). The first thing you
need to do is create your map function. Even though this solution uses Python
and ``pymongo`` to interface with the MongoDB server, note that the various
functions (``mapf``, ``reducef``, and ``finalizef``) that is passed to the
``mapreduce`` command must be Javascript functions. The map function appears below:

.. code-block:: python

    mapf_hour = bson.Code('''function() {
        var key = {
            u: this.userid,
            d: new Date(
                this.ts.getFullYear(),
                this.ts.getMonth(),
                this.ts.getDate(),
                this.ts.getHours(),
                0, 0, 0);
        emit(
            key,
            {
                total: this.length,
                count: 1,
                mean: 0,
                ts: new Date(); });
    }''')

In this case, it emits key, value pairs which contain the
statistics you want to aggregate as you'd expect, but it also emits a `ts`
value. This will be used in the cascaded aggregations
(hour to day, etc.) to determine when a particular hourly aggregation
was performed.

The reduce function is also fairly straightforward:

.. code-block:: python

    reducef = bson.Code('''function(key, values) {
        var r = { total: 0, count: 0, mean: 0, ts: null };
        values.forEach(function(v) {
            r.total += v.total;
            r.count += v.count;
        });
        return r;
    }''')

A few things are notable here. First of all, note that the returned
document from the reduce function has the same format as the result of
map. This is a characteristic of map/reduce that it's nice
to maintain, as differences in structure between map, reduce, and
finalize results can lead to difficult-to-debug errors. Also note that
the ``mean`` and ``ts`` values are ignored in the ``reduce`` function. These will be
computed in the 'finalize' step:

.. code-block:: python

    finalizef = bson.Code('''function(key, value) {
        if(value.count > 0) {
            value.mean = value.total / value.count;
        }
        value.ts = new Date();
        return value;
    }''')

The finalize function computes the mean value as well as the timestamp you'll
use to write back to the output collection. Now, to bind it all together, here
is the Python code to invoke the ``mapreduce`` command:

.. code-block:: python

    cutoff = datetime.utcnow() - timedelta(seconds=60)
    query = { 'ts': { '$gt': last_run, '$lt': cutoff } }


    db.events.map_reduce(
        map=mapf_hour,
        reduce=reducef,
        finalize=finalizef,
        query=query,
        out={ 'reduce': 'stats.hourly' })


    last_run = cutoff

Through the use you the 'reduce' option on your output, you can safely run this
aggregation as often as you like so long as you update the ``last_run`` variable
each time.

Indexing
````````

Since you'll be running the initial query on the input events
frequently, you'd benefit significantly from an index on the
timestamp of incoming events:

.. code-block:: python

    >>> db.stats.hourly.ensure_index('ts')

Since you're always reading and writing the most recent events, this
index has the advantage of being right-aligned, which basically means MongoDB
only needs a thin slice of the index (the most recent values) in RAM to
achieve good performance.

Deriving Day-Level Data
~~~~~~~~~~~~~~~~~~~~~~~

Aggregation
```````````

In calculating the daily statistics, you'll use the hourly statistics
as input. The daily  map function looks quite similar to the hourly map
function:

.. code-block:: python

    mapf_day = bson.Code('''function() {
        var key = {
            u: this._id.u,
            d: new Date(
                this._id.d.getFullYear(),
                this._id.d.getMonth(),
                this._id.d.getDate(),
                0, 0, 0, 0) };
        emit(
            key,
            {
                total: this.value.total,
                count: this.value.count,
                mean: 0,
                ts: null });
    }''')

There are a few differences to note here. First of all, the aggregation key is
the (userid, date) rather than (userid, hour) to allow
for daily aggregation. Secondly, note that the keys and values ``emit``\ ted
are actually the total and count values from the hourly aggregates
rather than properties from event documents. This will be the case in
all the higher-level hierarchical aggregations.

Since you're using the same format for map output as was used in the
hourly aggregations, you can, in fact, use the same reduce and finalize
functions. The actual Python code driving this level of aggregation is
as follows:

.. code-block:: python

    cutoff = datetime.utcnow() - timedelta(seconds=60)
    query = { 'value.ts': { '$gt': last_run, '$lt': cutoff } }


    db.stats.hourly.map_reduce(
        map=mapf_day,
        reduce=reducef,
        finalize=finalizef,
        query=query,
        out={ 'reduce': 'stats.daily' })


    last_run = cutoff

There are a couple of things to note here. First of all, the query is
not on ``ts`` now, but ``value.ts``, the timestamp written during the
finalization of the hourly aggregates. Also note that you are, in fact,
aggregating from the ``stats.hourly`` collection into the ``stats.daily``
collection.

Indexing
````````

Since you're going to be running the initial query on the hourly
statistics collection frequently, an index on 'value.ts' would be nice
to have:

.. code-block:: python

    >>> db.stats.hourly.ensure_index('value.ts')

Once again, this is a right-aligned index that will use very little RAM
for efficient operation.

Other Operations
~~~~~~~~~~~~~~~~

Aggregation
```````````

Once you have your daily statistics, you can use them to calculate your
weekly and monthly statistics. The weekly map function is as follows:

.. code-block:: python

    mapf_week = bson.Code('''function() {
        var key = {
            u: this._id.u,
            d: new Date(
                this._id.d.valueOf()
                - dt.getDay()*24*60*60*1000) };
        emit(
            key,
            {
                total: this.value.total,
                count: this.value.count,
                mean: 0,
                ts: null });
    }''')

Here, in order to get the group key, you simply takes the date and subtracts days
until you get to the beginning of the week. In the
weekly map function, you'll use the first day of the month as the
group key:

.. code-block:: python

   mapf_month = bson.Code('''function() {
           d: new Date(
               this._id.d.getFullYear(),
               this._id.d.getMonth(),
               1, 0, 0, 0, 0) };
       emit(
           key,
           {
               total: this.value.total,
               count: this.value.count,
               mean: 0,
               ts: null });
   }''')

One thing in particular to notice about these map functions is that they
are identical to one another except for the date calculation. You can use
Python's string interpolation to refactor the map function definitions
as follows:

.. code-block:: python

   mapf_hierarchical = '''function() {
       var key = {
           u: this._id.u,
           d: %s };
       emit(
           key,
           {
               total: this.value.total,
               count: this.value.count,
               mean: 0,
               ts: null });
   }'''


   mapf_day = bson.Code(
       mapf_hierarchical % '''new Date(
               this._id.d.getFullYear(),
               this._id.d.getMonth(),
               this._id.d.getDate(),
               0, 0, 0, 0)''')


   mapf_week = bson.Code(
       mapf_hierarchical % '''new Date(
               this._id.d.valueOf()
               - dt.getDay()*24*60*60*1000)''')


   mapf_month = bson.Code(
       mapf_hierarchical % '''new Date(
               this._id.d.getFullYear(),
               this._id.d.getMonth(),
               1, 0, 0, 0, 0)''')


   mapf_year = bson.Code(
       mapf_hierarchical % '''new Date(
               this._id.d.getFullYear(),
               1, 1, 0, 0, 0, 0)''')

The Python driver can also be refactored so there is much less code
duplication:

.. code-block:: python

   def aggregate(icollection, ocollection, mapf, cutoff, last_run):
       query = { 'value.ts': { '$gt': last_run, '$lt': cutoff } }
       icollection.map_reduce(
           map=mapf,
           reduce=reducef,
           finalize=finalizef,
           query=query,
           out={ 'reduce': ocollection.name })

Once this is defined, you can perform all the aggregations as follows:

.. code-block:: python

   cutoff = datetime.utcnow() - timedelta(seconds=60)
   aggregate(db.events, db.stats.hourly, mapf_hour, cutoff, last_run)
   aggregate(db.stats.hourly, db.stats.daily, mapf_day, cutoff, last_run)
   aggregate(db.stats.daily, db.stats.weekly, mapf_week, cutoff, last_run)
   aggregate(db.stats.daily, db.stats.monthly, mapf_month, cutoff,
       last_run)
   aggregate(db.stats.monthly, db.stats.yearly, mapf_year, cutoff,
       last_run)
   last_run = cutoff

So long as you save/restore the ``last_run`` variable between
aggregations, you can run these aggregations as often as you like since
each aggregation individually is incremental.

Indexing
````````

Your indexes will continue to be on the value's timestamp to ensure
efficient operation of the next level of the aggregation (and they
continue to be right-aligned):

.. code-block:: python

   >>> db.stats.daily.ensure_index('value.ts')
   >>> db.stats.monthly.ensure_index('value.ts')

Sharding
--------

To take advantage of distinct shards when performing map/reduce, your
input collections should be sharded. In order to achieve good balancing
between nodes, you should make sure that the shard key is not
simply the incoming timestamp, but rather something that varies
significantly in the most recent documents. In this case, the username
makes sense as the most significant part of the shard key.

In order to prevent a single, active user from creating a large,
unsplittable chunk, it's best to use a compound shard key with (username,
timestamp) on the events collection.

.. code-block:: python

.. code-block:: pycon

   >>> db.command('shardcollection','events', {
   ... key : { 'userid': 1, 'ts' : 1} } )

.. code-block:: javascript

   { "collectionsharded": "events", "ok" : 1 }

In order to take advantage of sharding on
the aggregate collections, you *must* shard on the ``_id`` field (if you decide
to shard these collections:)

.. code-block:: pycon

   >>> db.command('shardcollection', 'stats.daily')

.. code-block:: javascript

   { "collectionsharded" : "stats.daily", "ok": 1 }

.. code-block:: pycon

   >>> db.command('shardcollection', 'stats.weekly')

.. code-block:: javascript

   { "collectionsharded" : "stats.weekly", "ok" : 1 }

.. code-block:: pycon

   >>> db.command('shardcollection', 'stats.monthly')

.. code-block:: javascript

   { "collectionsharded" : "stats.monthly", "ok" : 1 }

.. code-block:: pycon

   >>> db.command('shardcollection', 'stats.yearly')

.. code-block:: javascript

   { "collectionsharded" : "stats.yearly", "ok" : 1 }

You should also update your map/reduce driver so that it notes the output
should be sharded. This is accomplished by adding 'sharded':True to the
output argument:

.. code-block:: pycon

   ... out={ 'reduce': ocollection.name, 'sharded': True })...
