===================
Backup Preparations
===================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

.. include:: /includes/styles/corrections.rst

Before backing up your cluster or replica set, decide how to back up
the data and what data to back up. This page describes items you must
consider before starting a backup.

.. seealso::

   To learn how Backup works, see
   :ref:`mms-backup-functional-overview`.

.. warning::

   .. include:: /includes/facts/fcv-backup-considerations.rst

.. cond:: onprem

   .. _backup-configuration-options:

   Backup Configuration Options
   ----------------------------

   The backup and recovery requirements of a given system vary to meet
   the cost, performance and data protection standards the system's
   owner sets.

   |mms| Enterprise Backup and Recovery supports five backup
   architectures, each with its own strengths and trade-offs. Consider
   the architecture that meets the data protection requirements for
   your deployment before configuring and deploying your backup
   architecture.

   .. example::

      Consider a system whose requirements include low operational
      costs. The system's owners may have strict limits on what they
      can spend on storage for their backup and recovery configuration.
      They may accept a longer recovery time as a result.

      Conversely, consider a system whose requirements include a low
      :term:`Recovery Time Objective`. The system's owners tolerate
      greater storage costs if it results in a backup and recovery
      configuration that fulfills the recovery requirements.

   |mms| Enterprise Backup and recovery supports the following backup
   architectures:

   - A file system on a |san| with advanced features for filesystem
     backups, such as high availability, compression, or
     :term:`deduplication`
   - A file system on one or more |nas| devices
   - An `AWS S3 <https://aws.amazon.com/s3>`_ :term:`blockstore`
   - MongoDB blockstore in a highly available configuration
   - MongoDB blockstore in a standalone configuration

   .. important::

      We provide the backup architecture recommendations as
      guidance for developing your own data protection approaches.
      Our recommendations don't cover or represent each scenario
      or deployment.

   Backup Method Features
   ~~~~~~~~~~~~~~~~~~~~~~

   .. list-table::
      :header-rows: 1
      :widths: 25 15 15 15 15 15
      :stub-columns: 1

      * - Backup System Feature
        - File System on |san|
        - File System on |nas|
        - AWS S3 Blockstore
        - MongoDB :abbr:`HA (High Availability)` Blockstore
        - MongoDB Blockstore

      * - Snapshot Types
        - Complete [1]_
        - Complete [1]_
        - Many partial
        - Many partial
        - Many partial

      * - Backup Data Deduplication
        - If SAN supports
        - No
        - Yes
        - Yes
        - Yes

      * - Backup Data Compression
        - Yes
        - Depends [2]_
        - Yes
        - Yes
        - Yes

      * - Backup Data Replication
        - If SAN supports
        - No
        - No
        - Yes
        - No

      * - Backup Storage Cost
        - Higher
        - Medium
        - Lower
        - Higher
        - Lower

      * - Staff Time to Manage Backups
        - Medium
        - Medium
        - Lower
        - Higher
        - Medium

      * - Backup |rto|
        - Lower
        - Medium
        - Lower
        - Lower
        - Medium

   .. [1]
          To perform an incremental backup to a
          :term:`File System Store`, the {+mdbagent+}
          slices each :manual:`storage engine </core/storage-engines/>`
          file in the path specified for backup into block(s) of
          data and transfers only changed block(s) to |onprem|.
          |onprem| creates a new :term:`snapshot` from received
          block(s) and copies the remaining unchanged block(s)
          from the previous full backup snapshot. Each incremental
          backup snapshot stored to a file system saves
          network I/O. Each such backup snapshot contains a full
          copy of all required files from a backed up MongoDB
          deployment and does not :term:`de-duplicate <deduplication>`
          records.

   .. [2]
          |onprem| 4.4.5 and later ignores GZIP compression for File
          System Store. Trying to create snapshots to File System
          Stores using GZIP compression fail in previous |onprem|
          versions.

   .. note:: **When Do You Use a Particular Backup Method?**

      - To run backups frequently on large amounts of data and
        restore from backups, consider backing up to a file
        system on a |san|, an AWS |s3| snapshot store, or a MongoDB
        blockstore configured as a replica set or a
        :term:`sharded cluster`.

      - To restore data without relying on MongoDB
        database, consider backing up to an AWS |s3| snapshot store,
        one or more |nas| devices, or a file system on a |san| with
        advanced features for file system backups, such as high
        availability, compression, or :term:`deduplication`.

      - To minimize internal storage and maintenance
        costs, consider backing up to an AWS |s3| snapshot store or
        a MongoDB standalone blockstore.

        A MongoDB standalone blockstore offers limited resilience. If
        the disk fills, this blockstore may go offline. You can
        recover backup snapshots only after adding additional storage.


.. _limits-of-backup:

Backup Sizing Recommendation
----------------------------

When sizing the backup of your data, keep the :term:`replica set` 
size to 2 TB or less of :manual:`uncompressed data 
</reference/command/dbStats/#mongodb-data-dbStats.dataSize>`. If 
your database increases beyond 2 TB of uncompressed data:

- Shard the database
- Keep each shard to 2 TB or less of uncompressed data
  
.. include:: /includes/fact-backup-sizing-recommendation.rst

.. _snapshot-frequency-and-retention:

Snapshot Frequency and Retention Policy
---------------------------------------

.. cond:: onprem

   By default, |mms| takes a base snapshot of your data every 24 hours.

.. cond:: cloud

   By default, |mms| takes a base snapshot of your data every 6 hours.

If desired, administrators can change the frequency of base snapshots
to 6, 8, 12, or 24 hours. |mms| creates snapshots automatically on a
schedule; you cannot take snapshots on demand.

|mms| retains snapshots for the time periods listed in the following
table. 

If you :ref:`terminate a deployment's backup <terminate-backup>`,
|mms| immediately deletes the snapshots that are within the dates
of the current retention policy.

If you :ref:`stop a deployment's backup <stop-backup>`, |mms| stops
taking new snapshots but retains existing snapshots until their listed
expiration date.

.. cond:: onprem

   .. list-table::
      :widths: 30 30 40
      :header-rows: 1

      * - Snapshot
        - Default Retention Policy
        - Maximum Retention Policy
      * - Base snapshot
        - 2 days
        - 5 days (30 days if frequency is 24 hours)
      * - Daily snapshot
        - 0 days
        - 1 year
      * - Weekly snapshot
        - 2 weeks
        - 1 year
      * - Monthly snapshot
        - 1 month
        - 7 years

.. cond:: cloud

   .. list-table::
      :widths: 30 30 40
      :header-rows: 1

      * - Snapshot
        - Default Retention Policy
        - Maximum Retention Policy
      * - Base snapshot
        - 2 days
        - 5 days (30 days if frequency is 24 hours)
      * - Daily snapshot
        - 7 days
        - 1 year
      * - Weekly snapshot
        - 4 weeks
        - 1 year
      * - Monthly snapshot
        - 13 months
        - 7 years

.. include:: /includes/extracts/backup-preparations-pricing.rst

You can change a backed-up deployment's schedule through its
:guilabel:`Edit Snapshot Schedule` menu option, available through the
:guilabel:`Backup` page. Administrators can change snapshot frequency
and retention through the
:doc:`snapshotSchedule resource in the API </reference/api/snapshot-schedule>`.

.. _changing-reference-time:

.. cond:: onprem

   Changing the reference time changes the time of the next scheduled snapshot.
   You can't make the next scheduled snapshot happen sooner than the current
   next snapshot time. The current next snapshot time is the current reference
   time plus the interval between snapshots.

   To determine the time of the next snapshot, compare the current next snapshot
   time to the new reference time:

   - If the new reference time is before the current next snapshot time, the next
     snapshot still occurs after the current next snapshot time. The snapshot
     occurs at the new reference time plus the number of intervals needed to
     surpass the current next snapshot time. If this time has already passed
     when you make the change, the next snapshot occurs at the next occurrance of
     the new reference time. See the first two rows of the following
     table for examples.

   - If the new reference time is after the current next snapshot time, the next snapshot
     occurs at the next occurrance of the new reference time. See the third and
     fourth row of the following table for examples.

   .. list-table::
      :widths: 20 20 20 20 20 20
      :header-rows: 1

      * - Time of Change

        - Current Reference Time

        - Interval Between Snapshots

        - Current Next Snapshot Time

        - New Reference Time

        - Time of Next Snapshot

      * - 08:00 UTC

        - 12:00 UTC

        - 6 hours

        - 12:00 UTC

        - 10:00 UTC

        - 16:00 UTC today

      * - 23:00 UTC

        - 12:00 UTC

        - 6 hours

        - 00:00 UTC

        - 10:00 UTC

        - 04:00 UTC tomorrow

      * - 08:00 UTC

        - 12:00 UTC

        - 6 hours

        - 12:00 UTC

        - 19:00 UTC

        - 19:00 UTC today

      * - 20:00 UTC

        - 12:00 UTC

        - 6 hours

        - 00:00 UTC

        - 19:00 UTC

        - 01:00 UTC tomorrow

   If you change the schedule to save fewer snapshots, |mms| does
   **not** delete existing snapshots to conform to the new schedule. To
   delete unneeded snapshots, see
   :doc:`/tutorial/delete-backup-snapshots`.

.. cond:: cloud

   You can't change the reference time for a snapshot in |mms|.


Limits
~~~~~~

- |mms| does not backup deployments where the total number of
  collections on the deployment meets or exceeds ``100,000``.

- |mms| does not replicate index collection options.

Encryption
~~~~~~~~~~

.. cond:: onprem

   |onprem| can encrypt any backup job stored in a
   :term:`head database` running
   :product:`MongoDB Enterprise <mongodb-enterprise-advanced>` between
   |fcv-link| 3.4 and 4.0 with the
   :ref:`WiredTiger <encrypted-storage-engine>` storage engine.

   .. seealso::

      :doc:`/tutorial/encrypt-snapshots`.

.. cond:: cloud

   You can't store backups using WiredTiger encryption.
   You can store backups using the WiredTiger storage
   engine if you don't enable encryption. If you restore
   from a backup, you restore unencrypted files.

.. _4.2-backup-considerations:

Backup Considerations
---------------------

Databases Running FCV 4.2 and 4.4
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /release-notes/release-advisories/advisories-v4.2-backup.rst

Shared File System
++++++++++++++++++

.. include:: /includes/fact-shared-file-system-reqs.rst

Databases Running FCV 4.0 and Earlier
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. important::

   .. include:: /includes/fact-backup-standalone-restriction.rst

The following considerations apply when your databases run any version
of MongoDB 4.0 or earlier or when they run MongoDB 4.2 with
``"featureCompatibilityVersion" : 4.0``

Garbage Collection of Expired Snapshots
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

|mms| manages expired snapshots using groom jobs. These groom jobs act
differently depending upon which snapshot store contains the snapshots:

.. list-table::
   :widths: 30 70
   :header-rows: 1
   :stub-columns: 1

   * - Snapshot Store
     - How Groom Jobs Work

   * - MongoDB Blockstore
     - Uses additional disk space up to the amount of living blocks for
       each job.

   * - Filesystem Snapshot stores
     - Deletes expired snapshots

   * - S3 snapshot stores
     - May use additional disk space if |mms| creates a snapshot while
       the groom job runs. |mms| can run concurrent groom jobs on S3
       snapshot stores.

.. _namespaces-filter:

Namespaces Filter
~~~~~~~~~~~~~~~~~

The namespaces filter lets you specify which databases and collections
to back up. You create either a :guilabel:`Blacklist` of those to
exclude or a :guilabel:`Whitelist` of those to include. You make your
selections when starting a backup and can later edit them as needed. If
you change the filter in a way that adds data to your backup, a resync
is required.

Use the blacklist to prevent backup of collections that contain logging
data, caches, or other ephemeral data. Excluding these kinds of
databases and collections will allow you to reduce backup time and
costs. Using a blacklist is often preferable to using a whitelist as a
whitelist requires you to intentionally opt in to every namespace you
want backed up.

.. note::

   MongoDB deployments with ``"featureCompatibilityVersion" : 4.2``
   do not support namespaces filters.

.. _considerations-backup-storage-engine:

Storage Engine
~~~~~~~~~~~~~~

To back up MongoDB clusters, use the
:ref:`WiredTiger storage engine <storage-wiredtiger>` storage engine.

If your current backing databases use MMAPv1, upgrade to WiredTiger:

- :manual:`Change Sharded Cluster to WiredTiger </tutorial/change-sharded-cluster-wiredtiger>`

- :manual:`Change Replica Set to WiredTiger </tutorial/change-replica-set-wiredtiger>`

With WiredTiger, |mms| limits backups to deployments with fewer than
100,000 files. Files includes collections and indexes.

MongoDB 4.2 removes MMAPv1 storage. To learn more about storage
engines, see :manual:`Storage </core/storage>` in the MongoDB manual.

Resyncing Production Deployments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For production deployments, it is recommended that as a best practice
you periodically (annually) :doc:`resync </tutorial/resync-backup>` all
backed-up replica sets. When you resync, data is read from a secondary
in each replica set. During resync, no new snapshots are generated.

You may also want to resync your backup after:

- A reduction in data size, such that the size on disk of |mms|'s copy
  of the data is also reduced. This scenario also includes if you:

  - Have a :manual:`TTL index </core/index-ttl>` in place, which
    periodically deletes documents.

  - :manual:`Drop a collection </reference/method/db.collection.drop>`
    (MMAPv1 only).

  - Run a :term:`sharded cluster <sharded cluster>`, and there have
    been a lot of chunks moved off a particular shard.

- A switch in storage engines, if you want |mms| to provide snapshots
  in the new storage engine format.

- A manual build of an index on a replica set in a rolling fashion
  (as per :manual:`Build Indexes on Replica Sets
  </tutorial/build-indexes-on-replica-sets>` in the MongoDB manual).

.. _checkpoint:

Checkpoints
~~~~~~~~~~~

.. include:: /includes/admonitions/important/checkpoints-fcv-4-0-only.rst

For :term:`sharded clusters <sharded cluster>`, checkpoints provide
additional restore points between snapshots. With checkpoints enabled,
|mms| creates restoration points at configurable intervals of every 15,
30 or 60 minutes between snapshots. To enable checkpoints, see
:ref:`enable checkpoints <enable-cluster-checkpoints>`.

To create a checkpoint, |mms| stops the :term:`balancer` and inserts a
token into the :term:`oplog` of each :term:`shard` and :term:`config
server` in the cluster. These checkpoint tokens are lightweight and do
not have a consequential impact on performance or disk use.

Backup does not require checkpoints, and they are disabled by default.

Restoring from a checkpoint requires |mms| to apply the :term:`oplog`
of each shard and config server to the last snapshot captured before
the checkpoint. Restoration from a checkpoint takes longer than
restoration from a snapshot.

.. _snapshot-while-balancer-enabled:

Snapshots when Agent Cannot Stop Balancer
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For :term:`sharded clusters <sharded cluster>` running with FCV 4.0 or
earlier, |mms| disables the :term:`balancer` before taking a cluster
snapshot. In certain situations, such as a long migration or no running
:program:`mongos`, |mms| tries to disable the balancer but cannot. In
such cases, |mms| continues to take cluster snapshots but flags the
snapshots that may have incomplete and/or inconsistent data. Cluster
snapshots taken during an active balancing operation run the risk of
data loss or orphaned data.

Snapshots when Agent Cannot Contact a ``mongod``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For :term:`sharded clusters <sharded cluster>`, if the {+bagent+}
cannot reach a :program:`mongod` process, whether a shard or config
server, then the agent cannot insert a synchronization :term:`oplog`
token. If this happens, |mms| does not create the snapshot and
displays a warning message.
