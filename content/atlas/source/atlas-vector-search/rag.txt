:tabs-selector-position: main

.. facet::
   :name: programming_language
   :values: csharp, go, java, javascript/typescript, python

.. _avs-rag:
.. _ai-key-concepts:

=================================================
Retrieval-Augmented Generation (RAG) with MongoDB
=================================================

.. default-domain:: mongodb

.. facet::
   :name: genre
   :values: tutorial

.. meta::
   :description: Use MongoDB Vector Search to implement retrieval-augmented-generation (RAG) in your generative AI applications.
   :keywords: RAG, retrieval-augmented generation, AI, LLM, vector database, vector search, semantic search, generative search, code example, java, python, go, node.js, Hugging Face, C#, .NET, OpenAI

.. dismissible-skills-card::
   :skill: RAG with MongoDB
   :url: https://learn.mongodb.com/skills?openTab=gen+ai

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

Retrieval-augmented generation (RAG) is an architecture used
to augment large language models (LLMs) with additional data
so that they can generate more accurate responses.
You can implement |rag| in your generative AI applications
by combining an LLM with a retrieval system powered by {+avs+}.

Get Started
-----------

To quickly try RAG with {+avs+}, use the
`Chatbot Demo Builder <https://search-playground.mongodb.com/tools/chatbot-demo-builder/snapshots/new>`__  
in the {+playground+}. To learn more, see :ref:`avs-playground`.

To implement your own RAG system with {+avs+}, see the
:ref:`tutorial <basic-rag-example>` on this page.

Why use RAG?
------------

When working with LLMs, you might encounter the
following limitations:

- Stale data: LLMs are trained on a static dataset
  up to a certain point in time. This means that they
  have a limited knowledge base and might use outdated
  data.

- No access to local data: LLMs don't have access to
  local or personalized data. Therefore, they can
  lack knowledge about specific domains.

- Hallucinations: When training data is incomplete or
  outdated, LLMs can generate inaccurate information.

You can address these limitations by taking
the following steps to implement |rag|:

1. **Ingestion:** Store your custom data as :term:`vector embeddings`
   in a vector database, such as MongoDB. This allows you
   to create a knowledge base of up-to-date and personalized data.

#. **Retrieval:** Retrieve semantically similar
   documents from the database based on the user's question by using
   a search solution, such as {+avs+}. These
   documents augment the LLM with additional, relevant data.

#. **Generation:** Prompt the LLM. The LLM uses
   the retrieved documents as context to generate a more accurate
   and relevant response, reducing hallucinations.

Because |rag| enables tasks such as question answering and
text generation, it's an effective architecture for building
AI chatbots that provide personalized, domain-specific responses.
To create production-ready chatbots, you must configure a server
to route requests and build a user interface on top of your |rag|
implementation.

RAG with {+avs+}
------------------------------

To implement |rag| with {+avs+}, you ingest data into MongoDB,
retrieve documents with {+avs+}, and generate responses
using an LLM. This section describes the components
of a basic, or naive, |rag| implementation
with {+avs+}. For step-by-step instructions, see :ref:`basic-rag-example`.

.. image:: /images/avs/rag-flowchart.png
   :alt: RAG flowchart with MongoDB Vector Search

.. collapsible:: 
   :heading: Learn by Watching 
   :sub_heading: Watch a video that demonstrates how to implement RAG with {+avs+}.

   *Duration: 5 Minutes*

   .. video:: https://www.youtube.com/watch?v=aTRlf9duBcs
    
.. _rag-ingestion:

Ingestion
~~~~~~~~~

Data ingestion for |rag| involves processing your custom
data and storing it in a vector database to prepare it
for retrieval. To create a basic ingestion pipeline with
MongoDB as the vector database, do the following:

.. include:: /includes/avs/rag/avs-rag-ingest-data.rst

.. _rag-retrieval:

Retrieval
~~~~~~~~~

Building a retrieval system involves searching for and returning the
most relevant documents from your vector database to augment the LLM
with. To retrieve relevant documents with {+avs+}, you convert the user's
question into vector embeddings and run a :ref:`vector search query
<avs-queries>` against the data in your MongoDB collection to find 
documents with the most similar embeddings.

To perform basic retrieval with {+avs+}, do the following:

1. Define an :ref:`{+avs+} index <avs-indexes>`
   on the collection that contains your vector embeddings.

#. Choose one of the following methods to retrieve documents
   based on the user's question:

   - Use an :ref:`{+avs+} integration <ai-integrations>`
     with a popular framework or service.
     These integrations include built-in libraries and tools
     that enable you to easily build retrieval systems
     with {+avs+}.

   - Build your own retrieval system. You can define
     your own functions and pipelines to run
     :ref:`{+avs+} queries <avs-queries>`
     specific to your use case.

     To learn how to build a basic retrieval system with {+avs+},
     see :ref:`basic-rag-example`.

.. _rag-generation:

Generation
~~~~~~~~~~

To generate responses, combine your retrieval system with an LLM.
After you perform a vector search to retrieve relevant documents,
you provide the user's question along with the relevant documents as
context to the LLM so that it can generate a more accurate response.

Choose one of the following methods to connect to an LLM:

- Use an :ref:`{+avs+} integration <ai-integrations>`
  with a popular framework or service. These
  integrations include built-in libraries and tools to
  help you connect to LLMs with minimal set-up.

- Call the LLM's API.
  Most AI providers offer APIs to their generative models
  that you can use to generate responses.

- Load an open-source LLM.
  If you don't have API keys or credits,
  you can use an open-source LLM by loading it locally
  from your application. For an example implementation, see the
  :ref:`local-rag` tutorial.

.. _basic-rag-example:

Tutorial
--------

The following example demonstrates how to implement |rag| with a
retrieval system powered by {+avs+}.

----------

.. |arrow| unicode:: U+27A4

|arrow| Use the **Select your language** drop-down menu to set the
language of the examples on this page.

.. tabs-selector:: drivers
   :default-tabid: python

.. tabs-drivers::

   .. tab::
      :tabid: csharp

   .. tab::
      :tabid: go

   .. tab::
      :tabid: java-sync

   .. tab::
      :tabid: nodejs

   .. tab::
      :tabid: python

      .. cta-banner::
         :url: https://github.com/mongodb/docs-notebooks/blob/main/use-cases/rag.ipynb
         :icon: Code

         Work with a runnable version of this tutorial as a :github:`Python notebook <mongodb/docs-notebooks/blob/main/use-cases/rag.ipynb>`.

Prerequisites
~~~~~~~~~~~~~

To complete this example, you must have the following:

.. tabs-drivers::

   .. tab::
      :tabid: csharp

      .. include:: /includes/avs/rag/avs-rag-prerequisites-csharp.rst

   .. tab::
      :tabid: go

      .. include:: /includes/avs/rag/avs-rag-prerequisites-go.rst

   .. tab::
      :tabid: java-sync

      .. include:: /includes/avs/rag/avs-rag-prerequisites-java.rst

   .. tab::
      :tabid: nodejs

      .. include:: /includes/avs/rag/avs-rag-prerequisites-javascript.rst

   .. tab::
      :tabid: python

      .. include:: /includes/avs/rag/avs-rag-prerequisites-python.rst

Procedure
~~~~~~~~~

.. tabs-drivers::

   .. tab::
      :tabid: csharp

      .. include:: /includes/avs/rag/steps-avs-rag-csharp.rst

   .. tab::
      :tabid: go

      .. include:: /includes/avs/rag/steps-avs-rag-go.rst

   .. tab::
      :tabid: java-sync

      .. include:: /includes/avs/rag/steps-avs-rag-java.rst

   .. tab::
      :tabid: nodejs

      .. include:: /includes/avs/rag/steps-avs-rag-javascript.rst

   .. tab::
      :tabid: python

      .. include:: /includes/avs/rag/steps-avs-rag-python.rst

.. _rag-examples:

Next Steps
----------

For additional RAG tutorials, see the following resources:

- To learn how to implement |rag| with popular LLM frameworks
  and AI services, see :ref:`ai-integrations`.

- To learn how to implement |rag| using a local |service| {+deployment+}
  and local models, see :ref:`local-rag`.

- For use-case based tutorials and interactive Python notebooks,
  see :github:`Docs Notebooks Repository </mongodb/docs-notebooks>` and
  :github:`Generative AI Use Cases Repository
  </mongodb-developer/GenAI-Showcase/tree/main>`.

To build AI agents and implement agentic RAG, see :ref:`ai-agents`. 

.. _rag-fine-tuning:

Improve Your Results
~~~~~~~~~~~~~~~~~~~~

To optimize your |rag| applications, 
ensure that you're using a powerful embedding model like
`Voyage AI <https://docs.voyageai.com/docs/introduction>`__
to generate high-quality vector embeddings.

Additionally, {+avs+} supports advanced retrieval systems.
You can seamlessly index vector data along with your other
data in your cluster. This allows you to improve your results
by :ref:`pre-filtering <vectorSearch-agg-pipeline-filter>`
on other fields in your collection or performing
:ref:`hybrid search <as_hybrid-search>` that combine semantic search
with full-text search results.

You can also use the following resources:

- :ref:`avs-improve-results`
- :ref:`avs-performance-tuning`.

To learn more about choosing an embedding model, 
chunking strategies, and evaluations, 
see the following resources:

- :website:`How to Choose the Right Embedding Model for Your LLM Application
  </developer/products/atlas/choose-embedding-model-rag/>`
- :website:`How to Choose the Right Chunking Strategy for Your LLM Application
  </developer/products/atlas/choosing-chunking-strategy-rag/>`
- :website:`How to Evaluate Your LLM Application
  </developer/products/atlas/evaluate-llm-applications-rag>`

.. toctree::
   :titlesonly:

   Playground Chatbot Demo Builder </atlas-vector-search/vector-search-playground>
   