.. _arch-center-is-framework-rapid-agent-deployment:

=======================================
Framework for Rapid AI Agent Deployment
=======================================

.. facet::
   :name: genre
   :values: tutorial

.. meta:: 
   :keywords: agentic AI, gen AI, vector search
   :description: A modular solution for building diagnostic and recommendation agents with MongoDB Atlas

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

MongoDB Atlas enables rapid development of AI agents across industries.

**Use cases:** `App Driven-Analytics
<https://www.mongodb.com/resources/solutions/use-cases/application-driven-intelligence>`__,
`Gen AI <https://www.mongodb.com/use-cases/artificial-intelligence>`__

**Industries:** `Manufacturing and Mobility
<https://www.mongodb.com/solutions/industries/manufacturing>`__,
`Financial Services
<https://www.mongodb.com/solutions/industries/financial-services>`__,
`Insurance <https://www.mongodb.com/solutions/industries/insurance>`__,
`Retail <https://www.mongodb.com/solutions/industries/retail>`__,
`Healthcare
<https://www.mongodb.com/solutions/industries/healthcare>`__,
`Telecommunications
<https://www.mongodb.com/solutions/industries/telecommunications>`__

**Products:** `MongoDB Atlas <http://mongodb.com/atlas>`__, `Atlas
Vector Search
<https://www.mongodb.com/products/platform/atlas-vector-search>`__,
`Atlas Database
<https://www.mongodb.com/products/platform/atlas-database>`__

**Partners:** `LangChain
<https://cloud.mongodb.com/ecosystem/langchain>`__, `Cohere
<https://cloud.mongodb.com/ecosystem/cohere>`__, `AWS Bedrock
<https://cloud.mongodb.com/ecosystem/amazon-bedrock>`__

Solution Overview
-----------------

In 2025, agentic AI applications are no longer just a hobby project.
Companies around the world are investing in software to incorporate
AI agents into their business workflows. The most common use of an AI agent
is to assist with research analysis or writing code.
LangChain’s `recent survey <https://www.langchain.com/stateofaiagents>`__
of over 1,000 professionals across multiple industries showed
that over 51% have already deployed agents in production—with 60%
using the agents for research and summarization tasks.

However, using an AI agent for more complex tasks than research and summarization,
specially when implemented on a large scale, presents certain challenges.

- **Complex data integration and handling:** AI agents often rely on
  disparate data sources and formats (structured logs, unstructured text,
  and sensor streams) making it difficult to unify data for real‐time
  decision‐making. Having all relevant data stored in one database for the
  agent to consume helps speed up the development process.

- **High concurrency and low latency:** Agents in production must handle
  a large volume of requests and respond quickly, which can overwhelm
  databases that aren’t built for high throughput and performance. While
  LLM inference can add latency, database performance is still important,
  especially as agents scale. Production agents will run in parallel, make
  multiple tool calls, and rely on up-to-date information to make
  decisions. A slow database becomes a bottleneck that increases total
  response time, reducing the agent’s ability to act in real time.

- **Data governance and security:** Sensitive data like user logs,
  sensor readings, and PII must be stored and accessed securely all while
  maintaining compliance (e.g. GDPR, SOC 2). Secure databases like MongoDB
  Atlas provide built-in, advanced security controls such as client side
  field-level encryption, queryable encryption, and extensive auditing
  capabilities. These features help ensure that agents only access the
  data they are authorized to see and traceability is maintained.

Figure 1 shows the major components of an AI agent. The agent will first
receive a task from a human or via an automated trigger and will then
use an LLM to generate a chain of thought or follow a predetermined
workflow. The agent will use various tools and models during its run and
store and retrieve data from a memory provider like MongoDB Atlas.

The primary goal of this solution—the agentic framework—is to accelerate
the development of AI-driven workflows by providing a flexible and
extensible foundation. Rather than offering a fixed set of tools or
functionalities, the framework is designed to serve as a starting point
for building intelligent agents tailored to specific use cases across
various industries.

You can follow along this solution and easily set up a toolkit powered
by MongoDB Atlas and LangGraph, and then use it to create many agents
with linear workflows. The simplicity of the solution allows users to
quickly understand how to build a basic agent just by manipulating the
config files. For more complex use cases or advanced agents, you may
extend the framework by adding new tools, nodes, conditions, or
workflows as needed.

By prioritizing adaptability and modularity, the agentic framework
empowers developers to create AI agents that are not only efficient but
also highly customizable to meet the unique demands of their industries
or projects.

.. figure:: /includes/images/industry-solutions/framework-rapid-ai-agent.svg
   :figwidth: 1200px
   :alt: Real-time wind turbine diagnosis

   Figure 1. Basic components of an AI agent

Reference Architectures
-----------------------

With MongoDB
~~~~~~~~~~~~

The agentic framework is a versatile AI-driven recommendation assistant
designed to understand your data, execute a multi-step diagnostic
workflow using LangGraph, and generate actionable insights. It
seamlessly integrates multiple technologies, reading time-series data
from a CSV file or MongoDB (simulating diverse data sources), generating
text embeddings, and performing vector searches to identify similar past
queries. The framework also persists session and run data, ultimately
producing a diagnostic recommendation.

MongoDB serves as the backbone for storing agent profiles, historical
recommendations, time-series data, and session logs, ensuring full
traceability of the diagnostic process while enabling efficient querying
and reusability of past insights.

.. figure:: /includes/images/industry-solutions/framework-rapid-reference-architecture.svg
   :figwidth: 1200px
   :alt: Real-time wind turbine diagnosis

   Figure 2. Agentic AI reference architecture

Why MongoDB
-----------

MongoDB plays a pivotal role in enabling agentic AI. MongoDB's unique
combination of flexibility, performance, and scalability makes it an
ideal choice for enabling artificial intelligence. The MongoDB document
model enables native storage and retrieval of structured,
semi-structured, and unstructured data, and it seamlessly handles
diverse and complex datasets. MongoDB empowers AI agents to react
dynamically to new information, maintain internal states in real-time,
and continuously learn and evolve.

**Flexible data model:** MongoDB's architecture provides a powerful
foundation for agentic AI systems by offering unparalleled flexibility
in data storage. The document model allows you to store varied data such
as time series logs, agent profiles, and recommendation outputs in a
single, unified format. This flexibility means you don’t have to
redesign your database schema every time your data requirements evolve.
MongoDB's flexible schema enables AI agents to store complex,
hierarchical agent states with nested documents that can dynamically
adapt to changing agent characteristics and interaction contexts.
MongoDB is an ideal choice for maintaining complex agent states across
multiple interactions. It supports versioning and tracking agent
evolution, with the ability to pause and resume agent contexts
seamlessly.

**Vector search:** MongoDB Atlas supports native vector search, enabling
fast and efficient similarity searches on vector embeddings. This is
critical for matching current queries with historical data, thereby
enhancing diagnostic accuracy and providing more relevant
recommendations. Vector search can support pattern recognition and
contextual retrieval that significantly enhance quality and decision
making of AI agents. Vector search enables sophisticated information
retrieval that uses a similarity algorithm to provide results that are
more precise, reducing LLM hallucination. MongoDB can efficiently handle
semantic matching, contextual searches, and multi-dimensional data
analysis, all crucial for AI agents' decision making workflows.

**Scalability, performance and high availability:** MongoDB is designed
to scale horizontally, making it capable of handling large volumes of
real-time data. MongoDB’s seamless horizontal scalability supports
large-scale, distributed AI applications with growing data requirements
for AI workloads through distributed and sharding architecture. With
MongoDB, AI agents can scale horizontally, distribute storage and
computational load, and maintain high availability, ensuring consistent
performance even when volume of data grows exponentially. MongoDB
replica sets provide robust high availability out of the box that
underpins the reliability and resilience of AI agents.

`Coinbase
<https://www.mongodb.com/solutions/customer-case-studies/coinbase>`__,
one of the largest cryptocurrency exchanges in the U.S., has effectively
leveraged MongoDB to significantly enhance its platform scalability
across 700 MongoDB clusters and overcome high volumes of unpredictable
market traffic spikes.

**Time series collections:** MongoDB time series collections are
designed to efficiently ingest large volumes of data with high
performance and scalability. These specialized collections enable AI
agents to track and analyze sequential interactions, learning patterns,
and state changes over time, which is essential for maintaining context,
understanding behavioral patterns, and implementing adaptive
decision-making processes. By offering native time series optimizations
such as automatic data compression, improved storage efficiency, and
fast time-based query responses, MongoDB allows AI agents to maintain
extensive current and historical records of their interactions, learning
iterations, and performance metrics without compromising computational
performance or data integrity.

**Seamless integration:** The seamless integration of MongoDB with
agentic frameworks like LangGraph expedites the development of complex,
stateful AI agentic systems. Through native support for JSON-like
documents, dynamic schema, and powerful indexing capabilities, MongoDB
enables LangGraph-powered AI Agents to maintain sophisticated data and
memory structures, track multi-step reasoning processes, and implement
advanced state management that can persist across sessions and provide
comprehensive auditability of AI decision-making processes.

Building the Solution
---------------------

.. procedure::
   :style: normal

   .. step:: **Include meaningful CSV data for your use case**

      The data should be relevant to your use case and respect the
      following guidelines:

      - The CSV file should contain a header row with column names.
      - The first column must be named “timestamp” and contain
        timestamps in the format YYYY-MM-DDTHH:MM:SSZ, e.g.
        2025-02-19T13:00:00Z.
      - The remaining columns should contain relevant data for your use
        case. 
      - There is no column nor row limit, but for framework
        testing purposes the suggestion is to keep the data size as
        small as possible.

      Sample data:

      .. code-block:: none

         timestamp,gdp,interest_rate,unemployment_rate,vix
         2025-02-19T13:00:00Z,2.5,1.75,3.8,15
         2025-02-19T13:05:00Z,2.7,1.80,3.7,18
         2025-02-19T13:10:00Z,2.6,1.85,3.9,22
         2025-02-19T13:15:00Z,2.4,1.70,4.0,10
         2025-02-19T13:20:00Z,2.3,1.65,4.1,20

      Within the same folder you need to add a queries file. This file
      will contain the queries that will be used to showcase vector
      search capabilities as part of the agentic workflow. The file
      should respect the following guidelines:

      - The CSV file should contain a header row with column names. 
      - The first column must be named “query” and contain the queries. 
      - The second column must be named “recommendation” and contain the
        expected recommendations. 
      - There is no column or row limit, but for framework testing
        purposes the suggestion is to keep the data size as small as
        possible.

      Sample data:

      .. code-block:: none

         query,recommendation
         GDP growth slowing,Consider increasing bond assets to mitigate risks from potential economic slowdown.
         GDP showing strong growth,Increase equity assets to capitalize on favorable investment conditions.
         Interest rates rising,Shift focus to bond assets as higher rates may impact borrowing-sensitive sectors.
         Interest rates falling,Increase real estate assets to take advantage of lower borrowing costs.
         Unemployment rate increasing,Reduce equity assets to account for potential economic weakness and reduced consumer spending.
         Unemployment rate decreasing,Increase equity assets to benefit from improved economic conditions and corporate profits.
         VIX above 20,Reduce equity assets to manage risks associated with high market volatility.
         VIX below 12,Increase equity assets to capitalize on low market volatility and investor confidence.
         VIX within normal range (12-20),Maintain current asset allocation as market conditions are stable.
         Combination of rising interest rates and high VIX,Focus on bond assets to hedge against market volatility and borrowing cost impacts.

   .. step:: **Set up MongoDB database and collections**

      Log in to MongoDB Atlas and create a database named
      “agentic_<YOUR_USE_CASE>,” e.g. “agentic_macro_indicators”. Ensure
      the name is reflected in the environment variables. 
      
      Create the following collections:

      - Agent_profiles (for storing agent profiles): You can import some
        sample data to this collection using this file.
      - Queries (for storing queries): You must import the queries from
        the queries.csv file created in Step 1.

   .. step:: **Configure your agentic worklfow through a JSON config file**

      Go to the config folder and create or update the JSON config.json
      file. The file should contain the following structure:

      .. code-block:: json

         {  
            "CSV_DATA": "data/csv/<YOUR_FILE_NAME>.csv",  
            "MDB_DATABASE_NAME": "<YOUR_MONGODB_DATABASE_NAME>",  
            "MDB_TIMESERIES_COLLECTION": "<YOUR_MONGODB_TIMESERIES_COLLECTION_NAME>",  
            "DEFAULT_TIMESERIES_DATA": [  
               {  
                  "timestamp": "<DEFAULT_TIMESTAMP_IN_YYYY-MM-DDTHH:MM:SSZ>"  
                  // Your default data here, check config_example.json for better understanding  
               }  
            ],  
            "CRITICAL_CONDITIONS": {  
               // Below is an example of a critical condition for GDP growth  
               "gdp": {"threshold": 2.5, "condition": "<", "message": "GDP growth slowing: {value}%"}  
               // Other critical conditions for your use case here, check config_example.json for better understanding  
            },  
            "MDB_TIMESERIES_TIMEFIELD": "<YOUR_TIMESTAMP_FIELD_NAME>",  
            "MDB_TIMESERIES_GRANULARITY": "<YOUR_TIMESERIES_GRANULARITY>",  
            "MDB_EMBEDDINGS_COLLECTION": "queries", // Using "queries" collection name for storing queries  
            "MDB_EMBEDDINGS_COLLECTION_VS_FIELD": "query_embedding", // Using "query_embedding" field for storing embeddings  
            "MDB_VS_INDEX": "<YOUR_MONGODB_DATABASE_NAME>_queries_vs_idx", // Replace <YOUR_MONGODB_DATABASE_NAME> with your MongoDB database name  
            "MDB_HISTORICAL_RECOMMENDATIONS_COLLECTION": "historical_recommendations", // Using "historical_recommendations" collection name for storing recommendations  
            "SIMILAR_QUERIES": [  
               // Below is an example of default similar queries for GDP growth  
               {  
                  "query": "GDP growth slowing",  
                  "recommendation": "Consider increasing bond assets to mitigate risks from potential economic slowdown."  
               }  
               // Other similar queries for your use case here, check config_example.json for better understanding  
               // This ones are going to be used for the vector search tool in case something is not found in the queries collection  
            ],  
            "MDB_CHAT_HISTORY_COLLECTION": "chat_history", // Using "chat_history" collection name for storing chat history  
            "MDB_CHECKPOINTER_COLLECTION": "checkpoints", // Using "checkpoints" collection name for storing checkpoints  
            "MDB_LOGS_COLLECTION": "logs", // Using "logs" collection name for storing logs  
            "MDB_AGENT_PROFILES_COLLECTION": "agent_profiles", // Using "agent_profiles" collection name for storing agent profiles  
            "MDB_AGENT_SESSIONS_COLLECTION": "agent_sessions", // Using "agent_sessions" collection name for storing agent sessions  
            "AGENT_PROFILE_CHOSEN_ID": "<YOUR_AGENT_PROFILE_ID>", // Replace <YOUR_AGENT_PROFILE_ID> with the agent profile ID you want to use, check config_example.json for better understanding  
            // Below is an example default agent profile for Portfolio Advisor  
            "DEFAULT_AGENT_PROFILE": {  
               "agent_id": "DEFAULT",  
               "profile": "Default Agent Profile",  
               "role": "Expert Advisor",  
               "kind_of_data": "Specific Data",  
               "motive": "diagnose the query and provide recommendations",  
               "instructions": "Follow procedures meticulously.",  
               "rules": "Document all steps.",  
               "goals": "Provide actionable recommendations."  
            },  
            "EMBEDDINGS_MODEL_NAME": "Cohere Embed English V3 Model (within AWS Bedrock)", // Describing the embeddings model used for creating the chain of thought  
            "EMBEDDINGS_MODEL_ID": "cohere.embed-english-v3", // Model ID for the embeddings model  
            "CHATCOMPLETIONS_MODEL_NAME": "Anthropic Claude 3 Haiku (within AWS Bedrock)", // Describing the chat completions model used for generating responses  
            "CHATCOMPLETIONS_MODEL_ID": "anthropic.claude-3-haiku-20240307-v1:0", // Model ID for the chat completions model  
            // Below is a sample agent workflow graph that uses the tools defined in the agent_tools.py file  
            // PLEASE BE CAREFUL WHEN MODIFYING THIS GRAPH, CONSIDER THAT THE TOOLS DEFINED IN THE AGENT TOOLS FILE ARE USED HERE AS WELL AS THE IMPORTS  
            "AGENT_WORKFLOW_GRAPH": {  
               "nodes": [  
                  {"id": "reasoning_node", "tool": "agent_tools.generate_chain_of_thought_tool"},  
                  {"id": "data_from_csv", "tool": "agent_tools.get_data_from_csv_tool"},  
                  {"id": "process_data", "tool": "agent_tools.process_data_tool"},  
                  {"id": "embedding_node", "tool": "agent_tools.get_query_embedding_tool"},  
                  {"id": "vector_search", "tool": "agent_tools.vector_search_tool"},  
                  {"id": "process_vector_search", "tool": "agent_tools.process_vector_search_tool"},  
                  {"id": "persistence_node", "tool": "agent_tools.persist_data_tool"},  
                  {"id": "recommendation_node", "tool": "agent_tools.get_llm_recommendation_tool"}  
               ],  
               "edges": [  
                  {"from": "reasoning_node", "to": "data_from_csv"},  
                  {"from": "data_from_csv", "to": "process_data"},  
                  {"from": "process_data", "to": "embedding_node"},  
                  {"from": "embedding_node", "to": "vector_search"},  
                  {"from": "vector_search", "to": "process_vector_search"},  
                  {"from": "process_vector_search", "to": "persistence_node"},  
                  {"from": "persistence_node", "to": "recommendation_node"},  
                  {"from": "recommendation_node", "to": "END"}  
               ],  
               "entry_point": "reasoning_node"  
            }  
         }  

      Once the config file is updated, environment variables need to be
      configured and vector embeddings created. Finally the vector
      search index must be created.

      Check out the `readme
      <https://github.com/mongodb-industry-solutions/agentic-framework/tree/a14ed55e9b4a5628da1798b5e4bbc59d695d43a7?tab=readme-ov-file#step-4-configure-your-agentic-workflow-through-a-json-config-file>`__
      for detailed explanations of all the steps.

      .. figure:: /includes/images/industry-solutions/framework-rapid-logical-architecture.svg
         :figwidth: 1200px
         :alt: Real-time wind turbine diagnosis

         Figure 3. Logical architecture

Key Learnings
-------------

Agentic AI ushers in an entirely new type of software—one with the
ability to dramatically alter the way we process data to drive richer
and more efficient business data processing outcomes. In order to
leverage agents within business applications, software delivery teams
will need to learn:

- How agents get instructions for reacting within predetermined workflows.
- How agents access and interact with tooling like APIs and databases.
- How agents persist and manage state.

Many of these items can be considered baseline activities and setup for
software delivery teams, regardless of the actual use case, workflow, or
industry context for which an agent, or agentic workflow, will be
deployed.

For high performing software delivery teams, the use of a framework to
abstract common tasks and components has often been employed to speed up
the development and deployment of business software solutions, as well
as decrease the subsequent complexity of maintaining them. This agentic
framework makes it easier for software delivery teams to get started
with building agentic solutions, as well as maintain them, via common,
configurable components and assets.

Working in this framework can not only teach you and your software teams
how to get started with building agentic solutions, it can also help
them build repeatable patterns and components that make it easier to
scale and maintain your business solutions for the long term.

Authors
-------

- Julian Boronat, MongoDB
- Peyman Parsi, MongoDB
- Jeff Needham, MongoDB
- Luca Napoli, MongoDB
- Humza Akthar, MongoDB