.. index:: administration; sharding
.. _sharding-administration:

==============================
Sharded Cluster Administration
==============================

.. default-domain:: mongodb



.. index:: balancing; operations
.. _sharding-balancing-operations:

Balancer Operations
-------------------

This section describes provides common administrative procedures related
to balancing. For an introduction to balancing, see
:ref:`sharding-balancing`. For lower level information on balancing, see
:ref:`sharding-balancing-internals`.

.. _sharding-balancing-check-lock:

Check the Balancer Lock
~~~~~~~~~~~~~~~~~~~~~~~

To see if the balancer process is active in your :term:`cluster
<sharded cluster>`, do the following:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the :ref:`config-database`:

   .. code-block:: javascript

      use config

#. Use the following query to return the balancer lock:

   .. code-block:: javascript

      db.locks.find( { _id : "balancer" } ).pretty()

When this command returns, you will see output like the following:

.. code-block:: javascript

   {   "_id" : "balancer",
   "process" : "mongos0.example.net:1292810611:1804289383",
     "state" : 2,
        "ts" : ObjectId("4d0f872630c42d1978be8a2e"),
      "when" : "Mon Dec 20 2010 11:41:10 GMT-0500 (EST)",
       "who" : "mongos0.example.net:1292810611:1804289383:Balancer:846930886",
       "why" : "doing balance round" }


This output confirms that:

- The balancer originates from the :program:`mongos` running on the
  system with the hostname ``mongos0.example.net``.

- The value in the ``state`` field indicates that a :program:`mongos`
  has the lock. For version 2.0 and later, the value of an active lock
  is ``2``; for earlier versions the value is ``1``.

.. optional::

   You can also use the following shell helper, which returns a
   boolean to report if the balancer is active:

   .. code-block:: javascript

      sh.getBalancerState()

.. _sharding-schedule-balancing-window:

Schedule the Balancing Window
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some situations, particularly when your data set grows slowly and a
migration can impact performance, it's useful to be able to ensure
that the balancer is active only at certain times.  Use the following
procedure to specify a window during which the :term:`balancer` will
be able to migrate chunks:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the :ref:`config-database`:

   .. code-block:: javascript

      use config

#. Use an operation modeled on the following example :method:`update()
   <db.collection.update()>` operation to modify the balancer's
   window:

   .. code-block:: javascript

      db.settings.update({ _id : "balancer" }, { $set : { activeWindow : { start : "<start-time>", stop : "<stop-time>" } } }, true )

   Replace ``<start-time>`` and ``<end-time>`` with time values using
   two digit hour and minute values (e.g ``HH:MM``) that describe the
   beginning and end boundaries of the balancing window.
   These times will be evaluated relative to the time zone of each individual
   :program:`mongos` instance in the sharded cluster.
   For instance, running the following
   will force the balancer to run between 11PM and 6AM local time only:

   .. code-block:: javascript

      db.settings.update({ _id : "balancer" }, { $set : { activeWindow : { start : "23:00", stop : "6:00" } } }, true )

.. note::

   The balancer window must be sufficient to *complete* the migration
   of all data inserted during the day.

   As data insert rates can change based on activity and usage
   patterns, it is important to ensure that the balancing window you
   select will be sufficient to support the needs of your deployment.

Remove a Balancing Window Schedule
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you have :ref:`set the balancing window
<sharding-schedule-balancing-window>` and wish to remove the schedule
so that the balancer is always running, issue the following sequence
of operations:

.. code-block:: javascript

   use config
   db.settings.update({ _id : "balancer" }, { $unset : { activeWindow : true })

.. _sharding-balancing-disable-temporally:

Disable the Balancer
~~~~~~~~~~~~~~~~~~~~

By default the balancer may run at any time and only moves chunks as
needed. To disable the balancer for a short period of time and prevent
all migration, use the following procedure:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue *one* of the following operations to disable the balancer:

   .. code-block:: javascript

      sh.stopBalancer()

#. Later, issue *one* the following operations to enable the balancer:

   .. code-block:: javascript

      sh.startBalancer()

.. note::

   If a migration is in progress, the system will complete
   the in-progress migration. After disabling, you can use the
   following operation in the :program:`mongo` shell to determine if
   there are no migrations in progress:

   .. code-block:: javascript

      use config
      while( db.locks.findOne({_id: "balancer"}).state ) {
             print("waiting..."); sleep(1000);
      }


The above process and the :method:`sh.setBalancerState()`,
:method:`sh.startBalancer()`, and :method:`sh.stopBalancer()` helpers provide
wrappers on the following process, which may be useful if you need to
run this operation from a driver that does not have helper functions:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the :ref:`config-database`:

   .. code-block:: javascript

      use config

#. Issue the following update to disable the balancer:

   .. code-block:: javascript

      db.settings.update( { _id: "balancer" }, { $set : { stopped: true } } , true );

#. To enable the balancer again, alter the value of "stopped" as follows:

   .. code-block:: javascript

      db.settings.update( { _id: "balancer" }, { $set : { stopped: false } } , true );

.. index:: config servers; operations
.. _sharding-procedure-config-server:

Config Server Maintenance
-------------------------

Config servers store all cluster metadata, most importantly,
the mapping from :term:`chunks <chunk>` to :term:`shards <shard>`.
This section provides an overview of the basic
procedures to migrate, replace, and maintain these servers.

.. seealso:: :ref:`sharding-config-server`

Deploy Three Config Servers for Production Deployments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For redundancy, all production :term:`sharded clusters <sharded cluster>`
should deploy three config servers processes on three different
machines.

Do not use only a single config server for production deployments.
Only use a single config server deployments for testing.  You should
upgrade to three config servers immediately if you are shifting to
production.  The following process shows how to convert a test
deployment with only one config server to production deployment with
three config servers.

#. Shut down all existing MongoDB processes. This includes:

   - all :program:`mongod` instances or :term:`replica sets <replica set>`
     that provide your shards.

   - the :program:`mongod` instance that provides your existing config
     database.

   - all :program:`mongos` instances in your cluster.

#. Copy the entire :setting:`dbpath` file system tree from the
   existing config server to the two machines that will provide the
   additional config servers. These commands, issued on the system
   with the existing :ref:`config-database`, ``mongo-config0.example.net`` may
   resemble the following:

   .. code-block:: sh

      rsync -az /data/configdb mongo-config1.example.net:/data/configdb
      rsync -az /data/configdb mongo-config2.example.net:/data/configdb

#. Start all three config servers, using the same invocation that you
   used for the single config server.

   .. code-block:: sh

      mongod --configsvr

#. Restart all shard :program:`mongod` and :program:`mongos` processes.

.. _sharding-process-config-server-migrate-same-hostname:

Migrate Config Servers with the Same Hostname
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use this process when you need to migrate a config server to a new
system but the new system will be accessible using the same host
name.

#. Shut down the config server that you're moving.

   This will render all config data for your cluster :ref:`read only
   <sharding-config-server>`.

#. Change the DNS entry that points to the system that provided the old
   config server, so that the *same* hostname points to the new
   system.

   How you do this depends on how you organize your DNS and
   hostname resolution services.

#. Move the entire :setting:`dbpath` file system tree from the old
   config server to the new config server. This command, issued on the
   old config server system, may resemble the following:

   .. code-block:: sh

      rsync -az /data/configdb mongo-config0.example.net:/data/configdb

#. Start the config instance on the new system. The default invocation
   is:

   .. code-block:: sh

      mongod --configsvr

When you start the third config server, your cluster will become
writable and it will be able to create new splits and migrate chunks
as needed.

.. _sharding-process-config-server-migrate-different-hostname:

Migrate Config Servers with Different Hostnames
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use this process when you need to migrate a :ref:`config-database` to a new
server and it *will not* be accessible via the same hostname. If
possible, avoid changing the hostname so that you can use the
:ref:`previous procedure <sharding-process-config-server-migrate-same-hostname>`.

#. Shut down the :ref:`config server <sharding-config-server>` you're moving.

   This will render all config data for your cluster "read only:"

   .. code-block:: sh

      rsync -az /data/configdb mongodb.config2.example.net:/data/configdb

#. Start the config instance on the new system. The default invocation
   is:

   .. code-block:: sh

      mongod --configsvr

#. Shut down all existing MongoDB processes. This includes:

   - all :program:`mongod` instances or :term:`replica sets <replica set>`
     that provide your shards.

   - the :program:`mongod` instances that provide your existing
     :ref:`config databases <config-database>`.

   - all :program:`mongos` instances in your cluster.

#. Restart all :program:`mongod` processes that provide the shard
   servers.

#. Update the :option:`--configdb <mongos --configdb>` parameter (or
   :setting:`configdb`) for all :program:`mongos` instances and
   restart all :program:`mongos` instances.

Replace a Config Server
~~~~~~~~~~~~~~~~~~~~~~~

Use this procedure only if you need to replace one of your config
servers after it becomes inoperable (e.g. hardware failure.) This
process assumes that the hostname of the instance will not change. If
you must change the hostname of the instance, use the process for
:ref:`migrating a config server to a different hostname
<sharding-process-config-server-migrate-different-hostname>`.

#. Provision a new system, with the same hostname as the previous
   host.

   You will have to ensure that the new system has the same IP address
   and hostname as the system it's replacing *or* you will need to
   modify the DNS records and wait for them to propagate.

#. Shut down *one* (and only one) of the existing config servers. Copy
   all this host's :setting:`dbpath` file system tree from the current system
   to the system that will provide the new config server. This
   command, issued on the system with the data files, may resemble the
   following:

   .. code-block:: sh

      rsync -az /data/configdb mongodb.config2.example.net:/data/configdb

#. Restart the config server process that you used in the previous
   step to copy the data files to the new config server instance.

#. Start the new config server instance. The default invocation is:

   .. code-block:: sh

      mongod --configsvr

.. note::

   In the course of this procedure *never* remove a config server from
   the :setting:`configdb` parameter on any of the :program:`mongos`
   instances. If you need to change the name of a config server,
   always make sure that all :program:`mongos` instances have three
   config servers specified in the :setting:`configdb` setting at all
   times.

Backup Cluster Metadata
~~~~~~~~~~~~~~~~~~~~~~~

The cluster will remain operational [#read-only]_ without one
of the config database's :program:`mongod` instances, creating a backup
of the cluster metadata from the config database is straight forward:

#. Shut down one of the :term:`config databases <config database>`.

#. Create a full copy of the data files (i.e. the path specified by
   the :setting:`dbpath` option for the config instance.)

#. Restart the original configuration server.

.. seealso:: :doc:`backups`.

.. [#read-only] While one of the three config servers is unavailable,
   the cluster cannot split any chunks nor can it migrate chunks
   between shards. Your application will be able to write data to the
   cluster. The :ref:`sharding-config-server` section of the
   documentation provides more information on this topic.

.. index:: troubleshooting; sharding
.. index:: sharding; troubleshooting
.. _sharding-troubleshooting:

Troubleshooting
---------------

The two most important factors in maintaining a successful sharded cluster are:

- :ref:`choosing an appropriate shard key <sharding-internals-shard-keys>` and

- :ref:`sufficient capacity to support current and future operations
  <sharding-capacity-planning>`.

You can prevent most issues encountered with sharding by ensuring that
you choose the best possible :term:`shard key` for your deployment and
ensure that you are always adding additional capacity to your cluster
well before the current resources become saturated. Continue reading
for specific issues you may encounter in a production environment.

.. _sharding-troubleshooting-not-splitting:

All Data Remains on One Shard
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Your cluster must have sufficient data for sharding to make
sense. Sharding works by migrating chunks between the shards until
each shard has roughly the same number of chunks.

The default chunk size is 64 megabytes. MongoDB will not begin
migrations until the imbalance of chunks in the cluster exceeds the
:ref:`migration threshold <sharding-migration-thresholds>`. While the
default chunk size is configurable with the :setting:`chunkSize`
setting, these behaviors help prevent unnecessary chunk migrations,
which can degrade the performance of your cluster as a whole.

If you have just deployed a sharded cluster, make sure that you have
enough data to make sharding effective. If you do not have sufficient
data to create more than eight 64 megabyte chunks, then all data will
remain on one shard. Either lower the :ref:`chunk size
<sharding-chunk-size>` setting, or add more data to the cluster.

As a related problem, the system will split chunks only on
inserts or updates, which means that if you configure sharding and do not
continue to issue insert and update operations, the database will not
create any chunks. You can either wait until your application inserts
data *or* :ref:`split chunks manually <sharding-procedure-create-split>`.

Finally, if your shard key has a low :ref:`cardinality
<sharding-shard-key-cardinality>`, MongoDB may not be able to create
sufficient splits among the data.

One Shard Receives Too Much Traffic
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some situations, a single shard or a subset of the cluster will
receive a disproportionate portion of the traffic and workload. In
almost all cases this is the result of a shard key that does not
effectively allow :ref:`write scaling <sharding-shard-key-write-scaling>`.

It's also possible that you have "hot chunks." In this case, you may
be able to solve the problem by splitting and then migrating parts of
these chunks.

In the worst case, you may have to consider re-sharding your data
and :ref:`choosing a different shard key <sharding-internals-choose-shard-key>`
to correct this pattern.

The Cluster Does Not Balance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you have just deployed your sharded cluster, you may want to
consider the :ref:`troubleshooting suggestions for a new cluster where
data remains on a single shard <sharding-troubleshooting-not-splitting>`.

If the cluster was initially balanced, but later developed an uneven
distribution of data, consider the following possible causes:

- You have deleted or removed a significant amount of data from the
  cluster. If you have added additional data, it may have a
  different distribution with regards to its shard key.

- Your :term:`shard key` has low :ref:`cardinality <sharding-shard-key-cardinality>`
  and MongoDB cannot split the chunks any further.

- Your data set is growing faster than the balancer can distribute
  data around the cluster. This is uncommon and
  typically is the result of:

  - a :ref:`balancing window <sharding-schedule-balancing-window>` that
    is too short, given the rate of data growth.

  - an uneven distribution of :ref:`write operations
    <sharding-shard-key-write-scaling>` that requires more data
    migration. You may have to choose a different shard key to resolve
    this issue.

  - poor network connectivity between shards, which may lead to chunk
    migrations that take too long to complete. Investigate your
    network configuration and interconnections between shards.

Migrations Render Cluster Unusable
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If migrations impact your cluster or application's performance,
consider the following options, depending on the nature of the impact:

#. If migrations only interrupt your clusters sporadically, you can
   limit the :ref:`balancing window
   <sharding-schedule-balancing-window>` to prevent balancing activity
   during peak hours. Ensure that there is enough time remaining to
   keep the data from becoming out of balance again.

#. If the balancer is always migrating chunks to the detriment of
   overall cluster performance:

   - You may want to attempt :ref:`decreasing the chunk size <sharding-balancing-modify-chunk-size>`
     to limit the size of the migration.

   - Your cluster may be over capacity, and you may want to attempt to
     :ref:`add one or two shards <sharding-procedure-add-shard>` to
     the cluster to distribute load.

It's also possible that your shard key causes your
application to direct all writes to a single shard. This kind of
activity pattern can require the balancer to migrate most data soon after writing
it. Consider redeploying your cluster  with a shard key that provides
better :ref:`write scaling <sharding-shard-key-write-scaling>`.

Disable Balancing During Backups
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If MongoDB migrates a :term:`chunk` during a :doc:`backup
</administration/backups>`, you can end with an inconsistent snapshot
of your :term:`sharded cluster`. Never run a backup while the balancer is
active. To ensure that the balancer is inactive during your backup
operation:

- Set the :ref:`balancing window <sharding-schedule-balancing-window>`
  so that the balancer is inactive during the backup. Ensure that the
  backup can complete while you have the balancer disabled.

- :ref:`manually disable the balancer <sharding-balancing-disable-temporally>`
  for the duration of the backup procedure.

Confirm that the balancer is not active using the
:method:`sh.getBalancerState()` method before starting a backup
operation. When the backup procedure is complete you can reactivate
the balancer process.
