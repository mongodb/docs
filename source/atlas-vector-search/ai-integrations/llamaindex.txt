.. _llamaindex:

===========================================
Get Started with the LlamaIndex Integration
===========================================

.. default-domain:: mongodb

.. facet::
   :name: genre
   :values: tutorial

.. facet::
   :name: programming_language
   :values: python

.. meta::
   :description: Integrate Atlas Vector Search with LlamaIndex to build RAG applications.

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

You can integrate {+avs+} with `LlamaIndex <https://llamaindex.ai/>`__
to implement retrieval-augmented generation (RAG) in your |llm| 
application. This tutorial demonstrates how to start using {+avs+} 
with LlamaIndex to perform semantic search on your data and build a
|rag| implementation. Specifically, you perform the following actions:

#. Set up the environment.
#. Store custom data on |service|.
#. Create an {+avs+} index on your data.
#. Run the following vector search queries:

   - Semantic search.
   - Semantic search with metadata pre-filtering.

#. Implement |rag| by using {+avs+} to answer questions on your data.

.. cta-banner::
   :url: https://github.com/mongodb/docs-notebooks/blob/main/ai-integrations/llamaindex.ipynb?tck=docs
   :icon: Code

   Work with a runnable version of this tutorial as a :github:`Python notebook <mongodb/docs-notebooks/blob/main/ai-integrations/llamaindex.ipynb?tck=docs>`.

Background
----------

LlamaIndex is an open-source framework designed to 
simplify how you connect custom data sources to |llm|\s. 
It provides several tools such as data connectors, indexes,
and query engines to help you load and 
prepare vector embeddings for |rag| applications.

By integrating {+avs+} with LlamaIndex, you can use 
|service| as a vector database and use {+avs+} to
implement |rag| by retrieving semantically similar documents 
from your data. To learn more about |rag|,
see :ref:`ai-key-concepts`.

Prerequisites
-------------

To complete this tutorial, you must have the following:

- .. include:: /includes/avs-examples/shared/avs-requirements-cluster.rst

- .. include:: /includes/avs-examples/shared/avs-requirements-openai-api-key.rst

- An environment to run interactive Python notebooks 
  such as `Colab <https://colab.research.google.com>`__.

.. _llamaindex-environment:

Set Up the Environment
----------------------

.. include:: /includes/set-up-python-notebook-environment.rst

.. include:: /includes/ai-integrations/llamaindex/llamaindex-set-up-environment.rst

Use |service| as a Vector Store
-------------------------------

Then, load custom data into |service| and instantiate |service| as 
a vector database, also called a `vector store 
<https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/>`__.
Copy and paste the following code snippets into your notebook.

.. include:: /includes/ai-integrations/llamaindex/llamaindex-create-vector-store.rst

.. _llamaindex-create-index:

Create the {+avs+} Index
------------------------------------

.. note:: 
   
   To create an {+avs+} index, you must have :authrole:`Project Data Access Admin` 
   or higher access to the |service| project.

.. include:: /includes/ai-integrations/llamaindex/llamaindex-create-index.rst

Run Vector Search Queries
-------------------------

Once |service| builds your index, return 
to your notebook and run vector search queries on your data. 
The following examples demonstrate different queries that you can 
run on your vectorized data.

.. include:: /includes/ai-integrations/llamaindex/llamaindex-query-examples.rst

Answer Questions on Your Data
-----------------------------

This section demonstrates how to implement |rag| in your 
application with {+avs+} and LlamaIndex. Now that you've learned
how to run vector search queries to retrieve semantically 
similar documents, run the following code to use 
{+avs+} to retrieve documents and a LlamaIndex `query engine 
<https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/>`__
to then answer questions based on those documents.

.. include:: /includes/ai-integrations/llamaindex/llamaindex-perform-qa.rst

Next Steps
----------

To explore LlamaIndex's full library of tools for |rag| applications, which includes 
data connectors, indexes, and query engines, see `LlamaHub <https://llamahub.ai>`__.

To extend the application in this tutorial to have back-and-forth conversations, see
`Chat Engine <https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/>`__.

MongoDB also provides the following developer resources:

- `How to Build a RAG System With LlamaIndex, OpenAI, and MongoDB Vector Database
  <https://www.mongodb.com/developer/products/atlas/rag-with-polm-stack-llamaindex-openai-mongodb/>`__
- :github:`MongoDB Developer GitHub Repository </mongodb-developer>`

.. seealso:: 
   
   - `LlamaIndex Documentation <https://docs.llamaindex.ai/en/stable/examples/vector_stores/MongoDBAtlasVectorSearchRAGOpenAI/>`__
   - `LlamaIndex API Reference <https://docs.llamaindex.ai/en/stable/api_reference/storage/vector_store/mongodb/>`__
