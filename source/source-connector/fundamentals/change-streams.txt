.. _kafka-source-change-streams:

==============
Change Streams
==============

.. meta::
   :description: Learn about using change streams in a MongoDB Kafka source connector to receive real-time updates on data changes and configure them with aggregation pipelines.

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

Overview
--------

In this guide, you can learn about **change streams** and how they are
used in a {+source-connector+}.

Change Streams
--------------

Change streams are a MongoDB feature that allow you to receive real-time
updates on data changes. Change streams return **change event documents**.

A change event document contains idempotent instructions to describe a change
that occurred in your MongoDB deployment and metadata related to that change.
Change event documents are generated from data in the :term:`oplog`.

.. important:: Change streams only run on MongoDB replica sets and sharded clusters

   A standalone MongoDB instance cannot produce a change stream.

To view a list of all configuration options for change streams, see the
:ref:`<source-configuration-change-stream>` page.

To learn more about change streams, see the following resources:

- :manual:`Change Streams </changeStreams>` in the MongoDB manual
- `An Introduction to Change Streams <https://www.mongodb.com/blog/post/an-introduction-to-change-streams>`__
  blog post

To learn more about the oplog, see the MongoDB manual entry on the
:manual:`Replica Set Oplog </core/replica-set-oplog/>`.

Aggregation
~~~~~~~~~~~

Use an aggregation pipeline to configure your source connector's change
stream. You can configure a connector change stream to use an
aggregation pipeline to perform tasks including the following operations:

- Filter change events by operation type
- Project specific fields
- Update the value of fields
- Add fields
- Trim the amount of data generated by the change stream

To learn which aggregation operators you can use with a change stream, see
the :manual:`Modify Change Stream Output </changeStreams/#modify-change-stream-output>`
guide in the MongoDB manual.

To view examples that use an aggregation pipeline to modify a change stream,
see the following pages:

- :ref:`<source-usage-example-custom-pipeline>` Usage Example
- :ref:`<source-usage-example-copy-existing-data>` Usage Example

.. _source-connector-fundamentals-change-event:

Change Event Structure
~~~~~~~~~~~~~~~~~~~~~~

Find the complete structure of change event documents, including
descriptions of all fields,
:ref:`in the MongoDB manual <change-stream-output>`.

.. note:: The Full Document Option

   If you want Kafka Connect to receive just the document created or modified
   from your change operation, use the ``publish.full.document.only=true``
   option. For more information, see the :ref:`<source-configuration-change-stream>`
   page.

Performance
~~~~~~~~~~~

The oplog is a special capped collection which cannot use indexes.  For more
information on this limitation, see
:manual:`Change Streams Production Recommendations </administration/change-streams-production-recommendations/#indexes>`.

If you want to improve change stream performance, use a faster disk for
your MongoDB cluster and increase the size of your WiredTiger cache. To
learn how to set your WiredTiger cache, see the guide on the
:manual:`WiredTiger Storage Engine </core/wiredtiger/#memory-use>`.

Source Connectors
-----------------

.. include:: /includes/source-connector-change-stream.rst

To view the available options to configure your source connector's change stream,
see the :ref:`<source-configuration-change-stream>` page.

Resume Tokens
~~~~~~~~~~~~~

Your connector uses a **resume token** as its **offset**. An offset is a value
your connector stores in an {+kafka+} topic to keep track of what source data it
has processed. Your connector uses its offset value when it must recover from
a restart or crash. A resume token is a piece of data that references the
``_id`` field of a change event document in your MongoDB oplog.

If your source connector does not have an offset, such as when you start
the connector for the first time, your connector starts a new change stream.
Once your connector receives its first change event document and publishes that
document to {+kafka+}, your connector stores the resume token of that document as
its offset.

If the resume token value of your source connector's offset does not correspond to
any entry in your MongoDB deployment's oplog, your connector has an invalid resume
token. To learn how to recover from an invalid resume token, see the
:ref:`invalid token troubleshooting guide <kafka-troubleshoot-invalid-resume-token>`.

To learn more about resume tokens, see the following resources:

- :manual:`Resume a Change Stream </changeStreams/#resume-a-change-stream>`
  in the MongoDB manual
- :manual:`Change Events </reference/change-events/#std-label-change-stream-event-id>`
  in the MongoDB manual

To learn more about offsets, see the following resources:

- {+kafka-connect+} ``offset.storage.topic``
  `configuration option documentation <https://docs.confluent.io/platform/current/installation/configuration/connect/index.html#offset-storage-topic>`__
- {+kafka-connect+} ``OffsetStorageReader``
  `API documentation  <https://kafka.apache.org/0110/javadoc/org/apache/kafka/connect/storage/OffsetStorageReader.html>`__
