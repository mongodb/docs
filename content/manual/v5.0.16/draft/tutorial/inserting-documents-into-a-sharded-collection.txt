=============================================
Inserting Documents into a Sharded Collection
=============================================

Synopsis
--------

When inserting documents into a :term:`sharded <sharding>`
:term:`collection`, you must consider how MongoDB will distribute the
inserted data and how the distribution will affect performance. You must
also consider whether you need first to :term:`pre-split
<pre-splitting>` the data. This document describes types of distribution
and how to pre-split data.

.. seealso::

   - :ref:`chunk-management`
   - :doc:`/core/sharding-internals`
   - :doc:`/core/sharding`

.. _sharding-distribution-types:

Types of Distribution
---------------------

MongoDB distributes inserted data in one of three ways, depending on
the following factors: the distribution of the :term:`collection's <collection>`
existing :term:`chunks <chunk>`, the existence of
:term:`shard keys <shard key>` on the write operation (i.e., whether
data is :term:`pre-split <pre-splitting>`), the distribution of the
inserted data, and the volume of the inserted data.

Depending on the above, MongoDB distributes inserted data in one of the
following ways:

- MongoDB distributes the data evenly around the cluster. For details
  see :ref:`sharding-even-distribution`.

- MongoDB distributes data unevenly around the cluster. For details see
  :ref:`sharding-uneven-distribution`.

- MongoDB inserts all data into the last chunk in the cluster. For
  details see :ref:`sharding-monotonic-distribution`.

.. _sharding-even-distribution:

Even Distribution
~~~~~~~~~~~~~~~~~

In even distribution, MongoDB balances writes among all :term:`chunks <chunk>`
and :term:`shards` in the cluster. Even distribution provides the best
performance and occurs in the following cases:

- The insert data has been :term:`pre-split <pre-splitting>` by specifying
  :term:`shard keys <shard key>` in the write operation. When you insert
  the data, MongoDB uses the keys to distribute writes evenly. It does
  not matter whether the existing :term:`collection` is evenly distributed. For
  details on pre-splitting data, see the :ref:`sharding-pre-splitting`
  section below.

- The :term:`sharded <sharding>` collection contains existing documents
  balanced over multiple chunks *and* the inserted data is either low
  volume or already evenly distributed. If the data is large and
  unevenly distributed, the write operation becomes imbalanced and
  monopolizes certain chunks.

.. _sharding-uneven-distribution:

Uneven Distribution
~~~~~~~~~~~~~~~~~~~

In uneven distribution, MongoDB focuses write operations on a small
number of :term:`chunk <chunks>` instead of balancing writes across
chunks. This increases the likelihood that chunks will fill and that
MongoDB must move chunks between :term:`shards`, an operation that slows
performance.

To avoid uneven distribution, :term:`pre-split` your data, as described
in the :ref:`sharding-pre-splitting` section below.

Uneven distribution occurs in the following cases:

- You insert a large volume of data that is not evenly distributed. Even
  if the :term:`sharded cluster` contains existing
  documents balanced over multiple chunks, the inserted data might
  include values that write disproportionately to a small number of
  chunks.

- The collection is empty, *and* the inserted data is not evenly
  distributed. MongoDB fills one chunk before creating the next and
  eventually must rebalance and move chunks between shards, slowing
  performance.

- Neither the collection nor the inserted data are evenly distributed.
  MongoDB fills certain chunks too soon and eventually must rebalance
  and move chunks between shards, slowing performance.

.. _sharding-monotonic-distribution:

All Writes Go to Last Chunk (Monotonic)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When you insert documents with monotonically increasing
:term:`shard keys <shard key>`, such as the BSON ObjectID, MongoDB
inserts all data into the last :term:`chunk` in the collection.

For example, consider a :term:`sharded <sharding>` collection with
two chunks, the second of which has an unbounded upper limit. The "("
and ")" symbols indicate a non-inclusive value. The "[" and "]" symbols
indicate an inclusive value:

.. code-block:: sh

   (-infinity, 100)
   [100, +infinity)

If the data being inserted has an increasing key, then writes always
direct to the shard containing the chunk with the unbounded upper limit.

Inserting to the last chunk can hinder the cluster's performance by placing a
significant load on a single :term:`shard <shards>`.

If, however, a single shard can handle the write volume, an increasing
shard key may have some advantages. For example, if you need to do
queries based on document-insertion time, sharding on the ObjectID
ensures that documents created around the same time exist on the same
shard. Data locality helps to improve query performance.

If you decide to use a monotonically increasing shard key and
anticipate large inserts, one solution may be to store the hash of the
shard key as a separate field. Hashing may prevent the need to balance
chunks by distributing data equally around the cluster. You can create a
hash client-side.

.. _sharding-pre-splitting:

Operation
---------

Pre-Splitting
~~~~~~~~~~~~~

Pre-splitting is the process of specifying :term:`shard key` ranges for
:term:`chunks <chunk>` prior to data insertion in order to ensure
MongoDB distributes the data evenly around the cluster. You should
consider pre-splitting if:

- You are doing high volume inserts.

- The :term:`sharded <sharding>` collection is empty.

- Either the collection's data or the data being inserted is not evenly distributed.

- The shard key is monotonically increasing.

For more information on how MongoDB distributes inserted data, see
:ref:`sharding-distribution-types`.

As an example of when you might choose to pre-split, consider a
collection sharded by last name with the following key distribution. The
"(" and ")" symbols indicate a non-inclusive value. The "[" and "]"
symbols indicate an inclusive value:

.. code-block:: sh

   ["A", "Jones")
   ["Jones", "Smith")
   ["Smith", "zzz")

Although the chunk ranges in the collection may be evenly split,
inserting a large number of new users with a common last name, such as
"Smith" or "Jones", will write disproportionately to a single shard,
monopolizing writes and reads to that shard.

To avoid this situation, you would make the chunk range more granular:

.. code-block:: sh

   ["A", "Jones")
   ["Jones", "Parker")
   ["Parker", "Smith")
   ["Smith", "Tyler")
   ["Tyler", "zzz"]

Procedures
----------

Pre-Splitting
~~~~~~~~~~~~~

This procedure assumes:

- You have a database "foo".

- The "foo" database has a collection "bar".

- The "bar" collection has a unique index.

- The "bar" collection, which contains no data, is sharded on _id : 99.

Note that a key need not exist for a chunk to use it in its range. The
chunk may even be empty.

Once the key range is specified, chunks can be moved around the cluster
using the moveChunk command.

.. code-block:: javascript

   db.runCommand( { moveChunk : "test.foo" , find : { _id : 99 } , to : "shard1" } )

You can repeat these steps as many times as necessary to create or move
chunks around the cluster. To get information about the two chunks
created in this example:

.. code-block:: javascript

   db.printShardingStatus()
   --- Sharding Status ---
     sharding version: { "_id" : 1, "version" : 3 }
     shards:
         { "_id" : "shard0000", "host" : "localhost:30000" }
         { "_id" : "shard0001", "host" : "localhost:30001" }
     databases:
       { "_id" : "admin", "partitioned" : false, "primary" : "config" }
       { "_id" : "test", "partitioned" : true, "primary" : "shard0001" }
           test.foo chunks:
                   shard0001    1
                   shard0000    1
               { "_id" : { "$MinKey" : true } } -->> { "_id" : "99" } on : shard0001 { "t" : 2000, "i" : 1 }
               { "_id" : "99" } -->> { "_id" : { "$MaxKey" : true } } on : shard0000 { "t" : 2000, "i" : 0 }

Once the chunks and the key ranges are evenly distributed, you can proceed with a
high volume insert.

.. _sharding-changing-shard-key:

Changing Shard Key
~~~~~~~~~~~~~~~~~~

There is no automatic support for changing the shard key for a
collection. In addition, since a document's location within the cluster
is determined by its shard key value, changing the shard key could force
data to move from machine to machine, potentially a highly expensive
operation.

Thus it is very important to choose the right shard key up front.

If you do need to change a shard key, an export and import is likely the
best solution. Create a new pre-sharded collection, and then import the
exported data to it. If desired use a dedicated :program:`mongos` for
the export and the import.

.. :issue:`SERVER-4000`

.. _sharding-pre-allocating-documents:

Pre-allocating Documents
~~~~~~~~~~~~~~~~~~~~~~~~

.. http://docs.mongodb.org/manual/use-cases/pre-aggregated-reports/#pre-allocate

