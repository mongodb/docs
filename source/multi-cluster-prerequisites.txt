.. _multi-cluster-prereqs:

=============
Prerequisites
=============

.. default-domain:: mongodb

.. meta::
   :keywords: multicluster, multi-Kubernetes-cluster MongoDB, mongoDBmultiCluster resource, kubectl mongodb plugin, central cluster, member clusters
   :description: Review and complete the prerequisite tasks before you deploy multi-Kubernetes-cluster MongoDB resources.

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol


Before you create a |multi-cluster| using either the quick start or a
deployment procedure, complete the following tasks:

Review Supported Hardware Architectures
---------------------------------------

See :ref:`supported hardware architectures <k8s-supported-hardware-arch-compatibility>`.

Set Environment Variables and GKE Zones
----------------------------------------

Set the environment variables with cluster names and the
`available GKE zones <https://cloud.google.com/compute/docs/regions-zones#available>`__
where you deploy the clusters, as in this example:

.. code-block:: sh

   export MDB_GKE_PROJECT={GKE project name}

   export MDB_CENTRAL_CLUSTER="mdb-central"
   export MDB_CENTRAL_CLUSTER_ZONE="us-west1-a"

   export MDB_CLUSTER_1="mdb-1"
   export MDB_CLUSTER_1_ZONE="us-west1-b"

   export MDB_CLUSTER_2="mdb-2"
   export MDB_CLUSTER_2_ZONE="us-east1-b"

   export MDB_CLUSTER_3="mdb-3"
   export MDB_CLUSTER_3_ZONE="us-central1-a"

   export MDB_CENTRAL_CLUSTER_FULL_NAME="gke_${MDB_GKE_PROJECT}_${MDB_CENTRAL_CLUSTER_ZONE}_${MDB_CENTRAL_CLUSTER}"

   export MDB_CLUSTER_1_FULL_NAME="gke_${MDB_GKE_PROJECT}_${MDB_CLUSTER_1_ZONE}_${MDB_CLUSTER_1}"
   export MDB_CLUSTER_2_FULL_NAME="gke_${MDB_GKE_PROJECT}_${MDB_CLUSTER_2_ZONE}_${MDB_CLUSTER_2}"
   export MDB_CLUSTER_3_FULL_NAME="gke_${MDB_GKE_PROJECT}_${MDB_CLUSTER_3_ZONE}_${MDB_CLUSTER_3}"

Set up GKE Clusters
-------------------

Set up |gke| clusters:

1. Set up your Google Cloud account and the ``gcloud`` tool, using the
   `Google Kubernetes Engine Quickstart <https://cloud.google.com/kubernetes-engine/docs/quickstart>`__.

2. Create :ref:`one central cluster and one or more member clusters
   <central-and-member-clusters>`, specifying the GKE zones, the number
   of nodes, and the instance types, as in these examples:

   .. code-block:: sh

      gcloud container clusters create $MDB_CENTRAL_CLUSTER \
        --zone=$MDB_CENTRAL_CLUSTER_ZONE \
        --num-nodes=5 \
        --machine-type "e2-standard-2"

   .. code-block:: sh

      gcloud container clusters create $MDB_CLUSTER_1 \
        --zone=$MDB_CLUSTER_1_ZONE \
        --num-nodes=5 \
        --machine-type "e2-standard-2"

   .. code-block:: sh

      gcloud container clusters create $MDB_CLUSTER_2 \
        --zone=$MDB_CLUSTER_2_ZONE \
        --num-nodes=5 \
        --machine-type "e2-standard-2"

   .. code-block:: sh

      gcloud container clusters create $MDB_CLUSTER_3 \
        --zone=$MDB_CLUSTER_3_ZONE \
        --num-nodes=5 \
        --machine-type "e2-standard-2"

.. _multi-cluster-user-auth-clusters-ref:

Obtain User Authentication Credentials for Central and Member Clusters
----------------------------------------------------------------------

Obtain user authentication credentials for the central and member |k8s|
clusters and save the credentials. You will later use these credentials
for running ``kubectl`` commands on these clusters.

Run the following commands:

.. code-block:: sh

   gcloud container clusters get-credentials $MDB_CENTRAL_CLUSTER \
     --zone=$MDB_CENTRAL_CLUSTER_ZONE

   gcloud container clusters get-credentials $MDB_CLUSTER_1 \
     --zone=$MDB_CLUSTER_1_ZONE

   gcloud container clusters get-credentials $MDB_CLUSTER_2 \
     --zone=$MDB_CLUSTER_2_ZONE

   gcloud container clusters get-credentials $MDB_CLUSTER_3 \
     --zone=$MDB_CLUSTER_3_ZONE

.. _install_go:
.. _install-helm:
.. _install-tools-needed-for-multi-clusters:

Install Go and Helm
---------------------

Install the following tools:

1. `Install Go <https://golang.org/dl/>`__ v1.17 or later.

2. `Install Helm <https://helm.sh/docs/intro/install/>`__.

.. _mc_plan_external_connectivity:

Plan for External Connectivity: Should You Use a Service Mesh?
---------------------------------------------------------------

A service mesh enables inter-cluster communication between the replica set
members deployed in different |k8s| clusters. Using a service mesh greatly
simplifies creating |multi-clusters| and is the recommended way of deploying
MongoDB across multiple |k8s| clusters. However, if your IT organization doesn't
use a service mesh, you can deploy a replica set in a |multi-cluster| without it.

Depending on your environment, do the following:

- If you can use a service mesh, :ref:`install Istio <install-istio>`.
- If you can't use a service mesh:

  1. Review the section :ref:`external_connectivity_how`
  2. :ref:`Enable external connectivity through external domains and DNS zones <dns-and-external-domain>`.
  3. :ref:`Deploy a multi-Kubernetes cluster without a service mesh <multi-cluster-no-service-mesh-deploy-rs>`.

.. _external_connectivity_how:

How Does the |k8s-op-short| Establish Connectivity?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Regardless of the deployment type, a MongoDB deployment in |k8s| must
establish the following connections:

- From the |onprem| Automation Agent in the Pod to its ``mongod`` process,
  to enable MongoDB deployment's lifecycle management and monitoring.
- From the |onprem| Automation Agent in the Pod to the |onprem| instance,
  to enable automation.
- Between all ``mongod`` processes, to allow replication.

When the |k8s-op-short| deploys the MongoDB resources, it treats these
connectivity requirements in the following ways, depending on the type of deployment:

- In a single |k8s| cluster deployment, the |k8s-op-short| configures hostnames
  in the replica set as |fqdn|\s of a |k8s-headless-service|. This is a single
  service that resolves the |dns| of a direct IP address of each Pod hosting
  a MongoDB instance by the Pod's |fqdn|, as follows:
  ``<pod-name>.<replica-set-name>-svc.<namespace>.svc.cluster.local``.

- In a |multi-cluster| that uses a service mesh, the |k8s-op-short| creates
  a separate StatefulSet for each MongoDB replica set member in the |k8s|
  cluster. A service mesh allows communication between ``mongod`` processes
  across distinct |k8s| clusters.

  Using a service mesh allows the |multi-cluster| to:

  - Achieve global |dns| hostname resolution across |k8s| clusters and
    establish connectivity between them. For each MongoDB deployment Pod
    in each |k8s| cluster, the |k8s-op-short| creates a ClusterIP service
    through the ``spec.duplicateServiceObjects: true`` configuration in
    the |mongodb-multi|. Each process has a hostname defined to this
    service's |fqdn|: ``<pod-name>-svc.<namespace>.svc.cluster.local``.
    These hostnames resolve from DNS to a service's ClusterIP in each member cluster.

  - Establish communication between Pods in different |k8s| clusters.
    As a result, replica set members hosted on different clusters form a
    single replica set across these clusters.

- In a |multi-cluster| without a service mesh, the |k8s-op-short| uses the
  following |mongodb-multi| settings to expose all its ``mongod`` processes
  externally. This enables DNS resolution of hostnames between distinct
  |k8s| clusters, and establishes connectivity between Pods routed through
  the networks that connect these clusters.

  - :ref:`spec.clusterSpecList.externalAccess.externalService <multi-spec-clusterspeclist-externalservice>`
  - :ref:`spec.externalAccess <multi-spec-externalaccess>`
  - :ref:`spec.clusterSpecList.externalAccess.externalService.annotations <multi-spec-clusterspeclist-annotations>`
  - :ref:`spec.clusterSpecList.externalAccess.externalDomain <multi-spec-clusterspeclist-externaldomain>`

.. _install-istio:

Install Istio
~~~~~~~~~~~~~

Istio is a service mesh that simplifies |dns| resolution and helps establish
inter-cluster communication between the member |k8s| clusters in a |multi-cluster|.
If you choose to use a service mesh, you need to install it. If you can't
utilize a service mesh, skip this section and :ref:`use external domains
and configure DNS to enable external connectivity <dns-and-external-domain>`.

To install Istio:

1. Install |istio| in a `multi-primary mode on different networks
   <https://istio.io/latest/docs/setup/install/multicluster/multi-primary_multi-network/>`__,
   using the Istio documentation.

   .. include:: /includes/facts/fact-istio.rst

.. _dns-and-external-domain:

Enable External Connectivity through External Domains and DNS Zones
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you don't use a service mesh, do the following to enable external
connectivity to and between ``mongod`` processes and the
|onprem| Automation Agent:

- Register ``mongod`` processes on externally-available hostnames using
  one of the following approaches:

  - When you create a |k8s| cluster for your |multi-cluster|, use the
    :ref:`spec.clusterDomain <multi-spec-clusterdomain>` setting to specify
    an externally-available custom domain instead of the default domain.
    With the default cluster domain, ``mongod`` processes use ``*.cluster.local``
    hostnames. However, if you specify an externally-available custom domain
    for each |k8s| cluster in a |multi-cluster|, ``mongod`` processes use
    hostnames in the following pattern:
  
    .. code-block:: sh

       <pod-name>.<replica-set-name>-svc.<namespace>.svc.<externally-available-cluster-domain>

    .. note::

       You can set a custom cluster domain **only**  when creating a |k8s| cluster
       for a |multi-cluster|.

  - When you create a |multi-cluster|, use
    the :ref:`spec.clusterSpecList.externalAccess.externalDomain <multi-spec-clusterspeclist-externaldomain>`
    setting to specify an external domain and instruct the |k8s-op-short| to
    configure hostnames for ``mongod`` processes in the following pattern:
  
    .. code-block:: sh
  
       <pod-name>.<externalDomain>

    .. note::

       You can specify external domains **only** for new deployments. You
       can't change external domains after you configure a |multi-cluster|.

    After you configure an external domain in this way, the |onprem|
    Automation Agents and ``mongod`` processes use this domain to connect
    to each other.

- Customize external services that the |k8s-op-short| creates for each Pod
  in the |k8s| cluster. Use the global configuration in the :ref:`spec.externalAccess <multi-spec-externalaccess>`
  settings and |k8s| cluster-specific overrides in the :ref:`spec.clusterSpecList.externalAccess.externalService
  <multi-spec-clusterspeclist-externalservice>` settings.

- Configure Pod hostnames in a |dns| zone to ensure that each |k8s| Pod
  hosting a ``mongod`` process allows establishing an external connection
  to the other ``mongod`` processes in a |multi-cluster|. A Pod is considered
  "exposed externally" when you can connect to a ``mongod`` process by
  using the ``<pod-name>.<externalDomain>`` hostname on ports 27017
  (this is the default database port) and 27018 (this is the database port + 1).
  You may also need to configure firewall rules to allow TCP traffic on
  ports 27017 and 27018.

After you complete these prerequisites, you can
:ref:`deploy a multi-Kubernetes cluster without a service mesh <multi-cluster-no-service-mesh-deploy-rs>`.

.. _multi-cluster-rbac-manual:

Configure |k8s| Roles and Role Bindings
--------------------------------------------

To use a |multi-cluster|, you must configure a specific set of 
|k8s| :k8sdocs:`Roles, ClusterRoles </reference/access-authn-authz/rbac/#role-and-clusterrole>`, 
:k8sdocs:`RoleBindings, ClusterRoleBindings </reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding>`, 
and :k8sdocs:`ServiceAccounts </tasks/configure-pod-container/configure-service-account/>`.

You should use the
:ref:`setup Subcommand <kubectl-mongodb-mc-setup>` provided by the 
:ref:`MongoDB Plugin <kubectl-plugin-ref>` to automatically create 
these resources and apply them to the appropriate clusters within 
your |multi-cluster|.

Alternatively, you can manually create |k8s| object ``.yaml`` files and
add these objects to your |multi-cluster| with the ``kubcetl apply`` 
command. This may be necessary for certain highly automated workflows. 
MongoDB provides sample configuration files.

For namespace-scoped resources:

- :github-raw:`Roles, Role Bindings, and Service Accounts for your Central Cluster </mongodb/mongodb-enterprise-kubernetes/master/samples/multi-cluster-cli-gitops/resources/rbac/namespace_scoped_central_cluster.yaml>`
- :github-raw:`Roles, Role Bindings, and Service Accounts for your Member Clusters </mongodb/mongodb-enterprise-kubernetes/master/samples/multi-cluster-cli-gitops/resources/rbac/namespace_scoped_member_cluster.yaml>`

For cluster-scoped resources:

- :github-raw:`ClusterRoles, ClusterRoleBindings, and ServiceAccounts for your Central Cluster </mongodb/mongodb-enterprise-kubernetes/master/samples/multi-cluster-cli-gitops/resources/rbac/cluster_scoped_central_cluster.yaml>`
- :github-raw:`ClusterRoles, ClusterRoleBindings, and ServiceAccounts for your Member Cluster </mongodb/mongodb-enterprise-kubernetes/master/samples/multi-cluster-cli-gitops/resources/rbac/cluster_scoped_member_cluster.yaml>`

Each file defines multiple resources. To support your deployment, you
must replace the placeholder values in the following fields:

- ``subjects.namespace`` in each ``RoleBinding`` or 
  ``ClusterRoleBinding`` resource 
- ``metadata.namespace`` in each ``ServiceAccount`` resource

After modifying the definitions, apply them by running the following 
command for each file:

   .. code-block:: sh

      kubectl apply -f <fileName>

.. _install-kubectl-mongodb-plugin:

Install the kubectl MongoDB Plugin
-----------------------------------

.. include:: /includes/facts/fact-multi-cluster-plugin-about.rst

To install the |kubectl-mongodb|:

.. include:: /includes/steps/install-kubectl-mongodb-plugin.rst

.. _mc-namespace-scope-ref:

Set the Deployment's Scope
--------------------------

By default, the multi-cluster |k8s-op-short| is scoped to the |k8s-ns|
in which you install it. The |k8s-op-short| reconciles the |mongodb-multi|
deployed in the same namespace as the |k8s-op-short|.

When you run the :ref:`MongoDB kubectl plugin <kubectl-plugin-ref>`
as part of the :ref:`multi-cluster quick start 
<multi-cluster-quick-start-ref>`, and don't modify the |kubectl-mongodb|
settings, the plugin:

.. include:: /includes/facts/fact-multi-cluster-plugin-actions-setup.rst

Once the |k8s-op-short| creates the |multi-cluster|, the |k8s-op-short|
starts watching |k8s-mdbrscs| in the ``mongodb`` |k8s-ns|.

To configure the |k8s-op-short| with the correct permissions to deploy
in multiple or all namespaces, run the following command and specify the
namespaces that you would like the |k8s-op-short| to watch.

.. code-block:: sh

   kubectl mongodb multicluster setup \
     --central-cluster="${MDB_CENTRAL_CLUSTER_FULL_NAME}" \
     --member-clusters="${MDB_CLUSTER_1_FULL_NAME},${MDB_CLUSTER_2_FULL_NAME},${MDB_CLUSTER_3_FULL_NAME}" \
     --member-cluster-namespace="mongodb2" \
     --central-cluster-namespace="mongodb2" \
     --cluster-scoped="true"

When you install the |multi-cluster| to multiple or all |k8s-nss|, you
can configure the |k8s-op-short| to:

- :ref:`Watch Resources in Multiple Namespaces <mc-cluster-many-namespaces-ref>`
- :ref:`Watch Resources in All Namespaces <mc-cluster-all-namespaces-ref>`

.. _mc-cluster-many-namespaces-ref:

Watch Resources in Multiple Namespaces
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you set the scope for the |multi-cluster| to many |k8s-nss|, you can
configure the |k8s-op-short| to watch |k8s-mdbrscs| in these namespaces
in the |multi-cluster|.

.. tabs::

   .. tab:: Using kubectl
      :tabid: mc-kubectl

      Set the ``spec.template.spec.containers.name.env.name:WATCH_NAMESPACE``
      in the :github:`mongodb-enterprise.yaml </mongodb/mongodb-enterprise-kubernetes/blob/master/mongodb-enterprise.yaml>`
      file from the |k8s-op| GitHub Repository to the comma-separated list
      of namespaces that you would like the |k8s-op-short| to watch:

         .. code-block:: sh

            WATCH_NAMESPACE: "$namespace1,$namespace2,$namespace3"


   .. tab:: Using Helm
      :tabid: mc-with-helm

      Run the following command and replace the values in the last line
      with the namespaces that you would like the |k8s-op-short| to
      watch.

      .. code-block:: sh

         helm upgrade \
           --install \
           mongodb-enterprise-operator-multi-cluster \
           mongodb/enterprise-operator \
           --namespace mongodb \
           --set namespace=mongodb \
           --version <mongodb-kubernetes-operator-version>\
           --set operator.name=mongodb-enterprise-operator-multi-cluster \
           --set operator.createOperatorServiceAccount=false \
           --set "multiCluster.clusters={$MDB_CLUSTER_1_FULL_NAME,$MDB_CLUSTER_2_FULL_NAME,$MDB_CLUSTER_3_FULL_NAME}" \
           --set operator.watchNamespace="$namespace1,$namespace2,$namespace3"

.. _mc-cluster-all-namespaces-ref:

Watch Resources in All Namespaces
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you set the scope for the |multi-cluster| to all |k8s-nss| instead 
of the default ``mongodb`` namespace, you can configure the |k8s-op-short|
to watch |k8s-mdbrscs| in all namespaces in the |multi-cluster|.

.. tabs::

   .. tab:: Using kubectl
      :tabid: mc-kubectl


      Set the ``spec.template.spec.containers.name.env.name:WATCH_NAMESPACE``
      in :github:`mongodb-enterprise.yaml </mongodb/mongodb-enterprise-kubernetes/blob/master/mongodb-enterprise.yaml>`
      to ``"*"``. You must include the double quotation marks (``"``)
      around the asterisk (``*``) in the YAML file.

      .. code-block:: sh

         WATCH_NAMESPACE: "*"

   .. tab:: Using Helm
      :tabid: mc-with-helm

      Run the following command:

      .. code-block:: sh

         helm upgrade \
           --install \
           mongodb-enterprise-operator-multi-cluster \
           mongodb/enterprise-operator \
           --namespace mongodb \
           --set namespace=mongodb \
           --version <mongodb-kubernetes-operator-version>\
           --set operator.name=mongodb-enterprise-operator-multi-cluster \
           --set operator.createOperatorServiceAccount=false \
           --set "multiCluster.clusters={$MDB_CLUSTER_1_FULL_NAME,$MDB_CLUSTER_2_FULL_NAME,$MDB_CLUSTER_3_FULL_NAME}" \
           --set operator.watchNamespace="*"

.. _mc-cluster-review-onprem-diffs:

Review the Requirements for Deploying |onprem|
-----------------------------------------------

As part of the Quick Start, you deploy an |onprem| resource on the central
cluster. To learn more, see :ref:`multi-cluster-deploy-om-summary`.

.. _mc-cluster-check-connectivity-ref:

Check Connectivity Across Clusters
----------------------------------

Follow the steps in this procedure to verify that service |fqdn|\s are
reachable across |k8s| clusters.

In this example, you deploy a sample application defined in
:github:`sample-service.yaml </mongodb/mongodb-enterprise-kubernetes/blob/master/multi_cluster_verify/sample-service.yaml>`
across two |k8s| clusters.

1. Create a namespace in each of the |k8s| clusters to deploy the ``sample-service.yaml``.

   .. code-block:: sh

      kubectl create --context="${CTX_CLUSTER_1}" namespace sample
      kubectl create --context="${CTX_CLUSTER_2}" namespace sample

   .. note::

      In certain service mesh solutions, you might need to annotate
      or label the namespace.

#. Deploy the ``sample-service.yaml`` in both |k8s| clusters.

   .. code-block:: sh
      
      kubectl apply --context="${CTX_CLUSTER_1}" \
         -f sample-service.yaml \
         -l service=helloworld1 \
         -n sample

      kubectl apply --context="${CTX_CLUSTER_2}" \
         -f sample-service.yaml \
         -l service=helloworld2 \
         -n sample

#. Deploy the sample application on ``CLUSTER_1``.

   .. code-block:: sh

      kubectl apply --context="${CTX_CLUSTER_1}" \
        -f sample-service.yaml \
        -l version=v1 \
        -n sample

#. Check that the ``CLUSTER_1`` hosting Pod is in the ``Running`` state.

   .. code-block:: sh

      kubectl get pod --context="${CTX_CLUSTER_1}" \
        -n sample \
        -l app=helloworld

#. Deploy the sample application on ``CLUSTER_2``.

   .. code-block:: sh

      kubectl apply --context="${CTX_CLUSTER_2}" \
        -f sample-service.yaml \
        -l version=v2 \
        -n sample

#. Check that the ``CLUSTER_2`` hosting Pod is in the ``Running`` state.

   .. code-block:: sh

      kubectl get pod --context="${CTX_CLUSTER_2}" \
        -n sample \
        -l app=helloworld

#. Deploy the Pod in ``CLUSTER_1`` and check that you can reach the sample application in ``CLUSTER_2``.

   .. code-block:: sh
         
      kubectl run  --context="${CTX_CLUSTER_1}" \
        -n sample \
        curl --image=radial/busyboxplus:curl \
        -i --tty \
        curl -sS helloworld2.sample:5000/hello

   You should see output similar to this example:

   .. code-block:: sh
      :copyable: false

      Hello version: v2, instance: helloworld-v2-758dd55874-6x4t8

#. Deploy the Pod in ``CLUSTER_2`` and check that you can reach the sample application in ``CLUSTER_1``.

   .. code-block:: sh

      kubectl run --context="${CTX_CLUSTER_2}" \
        -n sample \
        curl --image=radial/busyboxplus:curl \
        -i --tty \
        curl -sS helloworld1.sample:5000/hello


   You should see output similar to this example:

   .. code-block:: sh
      :copyable: false

      Hello version: v1, instance: helloworld-v1-758dd55874-6x4t8

.. _multi-cluster-tls-prereqs:

Prepare for TLS-Encrypted Connections
-------------------------------------

If you plan to secure your |multi-cluster| using |tls| encryption,
complete the following tasks:

.. include:: /includes/prereqs/custom-ca-prereqs-multi-cluster-rs-tls-only.rst


