=============================================
Inserting Documents into a Sharded Collection
=============================================

Synopsis
--------

When inserting documents into a :term:`sharded <sharding>`
:term:`collection`, you must consider how MongoDB will distribute the
inserted data and how the distribution will affect performance. You must
also consider whether you need first to :term:`pre-split
<pre-splitting>` the data. This document describes types of distribution
and how to pre-split data.

Types of Distribution
---------------------

MongoDB distributes inserted data in one of three ways, depending on
the following factors: the distribution of the :term:`collection's <collection>`
existing :term:`chunks <chunk>`, the existence of
:term:`shard keys <shard key>` on the write operation (i.e., whether
data is :term:`pre-split <pre-splitting>`), the distribution of the
inserted data, and the volume of the inserted data.

Depending on the above, MongoDB distributes inserted data in one of the
following ways:

- MongoDB distributes the data evenly around the cluster. For details
  see :ref:`sharding-even-distribution`.

- MongoDB distributes data unevenly around the cluster. For details see
  :ref:`sharding-uneven-distribution`.

- MongoDB inserts all data into the last chunk in the cluster. For
  details see :ref:`sharding-monotonic-distribution`.

.. _sharding-even-distribution:

Even Distribution
~~~~~~~~~~~~~~~~~

In even distribution, MongoDB balances writes among all :term:`chunks <chunk>`
and :term:`shards` in the cluster. Even distribution provides the best
performance and occurs in the following cases:

- The insert data has been :term:`pre-split <pre-splitting>` by specifying
  :term:`shard keys <shard key>` in the write operation. When you insert
  the data, MongoDB uses the keys to distribute writes evenly. It does
  not matter whether the existing :term:`collection` is evenly distributed. For
  details on pre-splitting data, see the :ref:`sharding-pre-splitting`
  section below.

- The :term:`sharded <sharding>` collection contains existing documents
  balanced over multiple chunks *and* the inserted data is either low
  volume or already evenly distributed. If the data is large and
  unevenly distributed, the write operation becomes imbalanced and
  monopolizes certain chunks.

.. _sharding-uneven-distribution:

Uneven Distribution
~~~~~~~~~~~~~~~~~~~

In uneven distribution, MongoDB focuses write operations on a small
number of :term:`chunk <chunks>` instead of balancing writes across
chunks. This increases the likelihood that chunks will fill and that
MongoDB must move chunks between :term:`shards`, an operation that slows
performance.

To avoid uneven distribution, :term:`pre-split` your data, as described
in the :ref:`sharding-pre-splitting` section below.

Uneven distribution occurs in the following cases:

- You insert a large volume of data that is not evenly distributed. Even
  if the :term:`sharded cluster <shard cluster>` contains existing
  documents balanced over multiple chunks, the inserted data might
  include values that write disproportionately to a small number of
  chunks.

- The collection is empty, *and* the inserted data is not evenly
  distributed. MongoDB fills one chunk before creating the next and
  eventually must rebalance and move chunks between shards. Moving
  chunks between shards affects performance.

- Neither the collection nor the inserted data are evenly distributed.
  MongoDB fills certain chunks too soon and eventually must rebalance
  and move chunks between shards. Moving chunks between shards affects
  performance.

.. _sharding-monotonic-distribution:

All Inserts are to Last Chunk (Monotonic)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When you insert documents with monotonically increasing
:term:`shard keys <shard key>`, such as the BSON ObjectID, MongoDB
inserts all data into the last :term:`chunk` in the collection.

For an example, consider a :term:`sharded <sharding>` collection with
two chunks, the second of which has an unbounded upper limit. The "("
and ")" symbols indicate a non-inclusive value. The "[" and "]" symbols
indicate an inclusive value:

(-infinity, 100)
[100, +infinity)

If the data being inserted has an increasing key, then writes always
direct to the shard containing the chunk with the unbounded upper limit.

Inserting to the last chunk can hinder the cluster's performance by placing a
significant load on a single :term:`shard <shards>`.

If, however, a single shard can handle the write volume, an increasing
shard key may have some advantages. For example, if you need to do
queries based on document-insertion time, sharding on the ObjectID
ensures that documents created around the same time exist on the same
shard. Data locality helps to improve query performance.

If you decide to use a monotonically increasing shard key and
anticipate large inserts, one solution may be to store the hash of the
shard key as a separate field. Hashing may prevent the need to balance
chunks by distributing data equally around the cluster. You can create a
hash client-side.

Operations
----------

.. TODO
.. outline the procedures and rationale for each process.

.. _sharding-pre-splitting:

Pre-Splitting
~~~~~~~~~~~~~

.. TODO
.. procedure for this process

Pre-splitting is the process of specifying shard key ranges for :term:`chunks <chunk>`
prior to data insertion. Pre-splitting ensures the write operation is
evenly spread around the cluster. You should consider pre-splitting if:

- You are doing high volume inserts.

- The sharded collection is empty.

- Either the collection's data or the data being inserted is not evenly distributed.

- The shard key is monotonically increasing.

As an example of pre-splitting, consider a collection sharded by last
name with the following key distribution. The "(" and ")" symbols
indicate a non-inclusive value. The "[" and "]" symbols indicate an
inclusive value:

["A", "Jones")
["Jones", "Smith")
["Smith", "zzz")

Although the chunk ranges may be split evenly, inserting lots of users
with with a common last name, such as "Jones" or "Smith", will
potentially monopolize a single shard. Making the chunk range more
granular in these portions of the alphabet may improve write
performance.

["A", "Jones")
["Jones", "Peters")
["Peters", "Smith")
["Smith", "Tyler")
["Tyler", "zzz"]

Procedure
---------

In the example below the pre-split command splits the chunk where the
_id 99 would reside using that key as the split point. Note that a key
need not exist for a chunk to use it in its range. The chunk may even be
empty.

The first step is to create a sharded collection to contain the data,
which can be done in three steps:

> use admin
> db.runCommand({ enableSharding : "foo" })

Next, we add a unique index to the collection "foo.bar" which is
required for the shard key.

> use foo
> db.bar.ensureIndex({ _id : 1 }, { unique : true })

Finally we shard the collection (which contains no data) using the _id
value.

> use admin
switched to db admin
> db.runCommand( { split : "test.foo" , middle : { _id : 99 } } )

Once the key range is specified, chunks can be moved around the cluster
using the moveChunk command.

> db.runCommand( { moveChunk : "test.foo" , find : { _id : 99 } , to : "shard1" } )

You can repeat these steps as many times as necessary to create or move
chunks around the cluster. To get information about the two chunks
created in this example:

> db.printShardingStatus()
--- Sharding Status ---
  sharding version: { "_id" : 1, "version" : 3 }
  shards:
      { "_id" : "shard0000", "host" : "localhost:30000" }
      { "_id" : "shard0001", "host" : "localhost:30001" }
  databases:
    { "_id" : "admin", "partitioned" : false, "primary" : "config" }
    { "_id" : "test", "partitioned" : true, "primary" : "shard0001" }
        test.foo chunks:
                shard0001    1
                shard0000    1
            { "_id" : { "$MinKey" : true } } -->> { "_id" : "99" } on : shard0001 { "t" : 2000, "i" : 1 }
            { "_id" : "99" } -->> { "_id" : { "$MaxKey" : true } } on : shard0000 { "t" : 2000, "i" : 0 }

Once the chunks and the key ranges are evenly distributed, you can proceed with a
high volume insert.

.. _sharding-changing-shard-key:

Changing Shard Key
~~~~~~~~~~~~~~~~~~

There is no automatic support for changing the shard key for a
collection. In addition, since a document's location within the cluster
is determined by its shard key value, changing the shard key could force
data to move from machine to machine, potentially a highly expensive
operation.

Thus it is very important to choose the right shard key up front.

If you do need to change a shard key, an export and import is likely the
best solution. Create a new pre-sharded collection, and then import the
exported data to it. If desired use a dedicated mongos for the export
and the import.

.. :issue:`SERVER-4000`

.. _sharding-pre-allocating-documents:

Pre-allocating Documents
~~~~~~~~~~~~~~~~~~~~~~~~

.. http://docs.mongodb.org/manual/use-cases/pre-aggregated-reports/#pre-allocate

