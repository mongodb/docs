.. _backup-cloud-provider-ea:

====================================
Back Up Your {+Database-Deployment+}
====================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

{+Cloud-Backup+}s
-----------------

.. include:: /includes/fact-atlas-free-tier-only-limits.rst

|service| {+Cloud-Backup+}s provide localized backup storage using the
native snapshot functionality of the cluster's cloud service provider.

.. include:: /includes/admonitions/notes/backup-roles.rst

|service| supports {+cloud-backup+} for clusters served on:

- :ref:`Microsoft Azure <microsoft-azure>`
- :ref:`Amazon Web Services (AWS) <amazon-aws>`
- :ref:`Google Cloud Platform (GCP) <google-gcp>`

You can enable {+cloud-backup+} during the
:doc:`cluster creation </tutorial/create-new-cluster>` or during the
:doc:`modification of an existing cluster </scale-cluster>`.
From the cluster configuration modal, toggle
:guilabel:`Turn on Cloud Backup` to :guilabel:`Yes`.

If you need to retain any {+old-backup+} snapshots for archival
purposes, download them before you switch to {+Cloud-Backup+} from
{+old-backup+}s. To learn how to download a snapshot, see
:doc:`/backup/legacy-backup/restore`.

When you change from {+Old-Backup+}s to {+Cloud-Backup+}s, |service| retains
your {+Old-Backup+} snapshots in accordance with your
:ref:`legacy backup retention policy <retention-policy>`.

.. important:: Limitations of {+Cloud-Backup+}

   {+Cloud-Backup+}s:

   - Can support sharded clusters running MongoDB version 4.0 or later.

   - Cannot restore an existing snapshot to a cluster after you add or
     remove a shard from it. You may restore an existing snapshot to
     another cluster with a matching topology.

.. note:: Sharded Cluster Balancer, FCV 4.0 databases and {+Cloud-Backup+}

   With databases running |fcv-link| 4.0 or earlier, {+Cloud-Backup+}
   automatically disables the balancer for snapshots if it's running.
   This ensures an inactive balancer during the backup operation. When
   the snapshot completes, {+Cloud-Backup+} returns the balancer to its
   previous state.

.. _single-region-cloud-backup-ea:

Single-Region Cluster Backups
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With single-region cluster backups, |service|:

- Determines the order of nodes to try to snapshot using the following 
  algorithm:

  i. Snapshots on a secondary. :sup:`1` Then,
  #. Snapshots the node with the lowest priority if possible. :sup:`2` 
     Then,
  #. Snapshots incrementally from one snapshot to the next if possible. 
     :sup:`3` Then,
  #. Snapshots node lexically first by hostname.

  :sup:`1` If there is a tie, |service| skips to the next step to determine the node to snapshot.

  :sup:`2` If there is a tie, |service| then favors the node that can be snapshotted incrementally from the previous snapshot (i.e., node using the same disk). 
  
  :sup:`3` If there is a tie, |service| then favors the node with the lexicographically smallest hostname.

- Once the node order is determined, tries to snapshot a node. If a 
  selected node is unhealthy, |service| tries to snapshot the next 
  node that it favors.

- Stores the snapshots in the same cloud region as the cluster.
- Retains snapshots based on your
  :ref:`retention policy <cloud-provider-retention-policy>`.

.. figure:: /images/cloud-provider-snapshot-single-region-primary.svg
   :alt: {+Cloud-Backup+} of the Primary
   :figwidth: 400px
   :align: center

.. figure:: /images/cloud-provider-snapshot-single-region-secondary.svg
   :alt: A {+Cloud-Backup+} of the Secondary
   :figwidth: 400px
   :align: center

|service| automatically creates a new snapshot storage volume if the
existing snapshot storage volume becomes invalid. |service| creates the
new volume in the same region as the cluster's current primary.
|service| then takes a full-copy snapshot to maintain backup
availability and continues using that member and its corresponding
region for further incremental snapshots.

Events that can cause an election to select a new node for the snapshot
storage volume include:

- Changing the |service| cluster tier,
- Modifying the |service| cluster's storage volume or speed,
- Changing the |service| cluster's region, and
- Maintenance performed by |service| or the cluster's cloud provider.

.. seealso::

   To learn more about snapshot retention, see
   :ref:`cloud-provider-retention-policy`.

.. _multi-region-cloud-backup-ea:

Multi-Region Cluster Backups
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With multi-region cluster backups, |service|:

- Determines the order of nodes to snapshot using the following 
  algorithm:

  i. Snapshots in the highest priority region if possible. :sup:`1` 
     Then,  
  #. Snapshots on a secondary. :sup:`2` Then,
  #. Snapshots the node with the lowest priority if possible. :sup:`3` 
     Then,
  #. Snapshots incrementally from one snapshot to the next if possible. 
     :sup:`4` Then,
  #. Snapshots node lexically first by hostname.

  :sup:`1` If there is a tie, |service| then compares based on the descending order of priority. 

  :sup:`2` If there is a tie, |service| skips to the next step to determine the node to snapshot.

  :sup:`3` If there is a tie, |service| then favors the node that can be snapshotted incrementally from the previous snapshot (i.e., node using the same disk). 
  
  :sup:`4` If there is a tie, |service| then favors the node with the lexicographically smallest hostname.

- Tries to snapshot a node once the node order is determined. If a 
  selected node is unhealthy, |service| tries to snapshot the next 
  node that it favors.

- Retains snapshots based on your :ref:`retention policy 
  <cloud-provider-retention-policy>`.

.. figure:: /images/cloud-provider-snapshot-multi-region-primary.svg
   :alt: {+Cloud-Backup+} of the Primary
   :figwidth: 400px
   :align: center

.. figure:: /images/cloud-provider-snapshot-multi-region-secondary.svg
   :alt: A {+Cloud-Backup+} of the Secondary
   :figwidth: 400px
   :align: center

|service| automatically creates a new snapshot storage volume if the
existing snapshot storage volume becomes invalid. |service| creates the
new volume in the same region as the cluster's current primary.
|service| then takes a full-copy snapshot to maintain backup
availability and continues using that member and its corresponding
region for further incremental snapshots.

Events that can cause an election to select a new node for the snapshot
storage volume include:

- Changing the |service| cluster tier,
- Modifying the |service| cluster's storage volume or speed,
- Changing the |service| cluster's highest priority region, and
- Maintenance performed by |service| or the cluster's cloud provider.

.. seealso::

   To learn more about snapshot retention, see
   :ref:`cloud-provider-retention-policy`.

.. _sharded-global-cluster-backup-ea:

Global Cluster Backups
~~~~~~~~~~~~~~~~~~~~~~

|service| can back up :doc:`Global Clusters </global-clusters>` using
{+Cloud-Backup+}s as their backup method. |service| restores the shards
in the source cluster to the corresponding shards in the target cluster
using the same order as specified in the cluster configuration.

.. example::

   ``shard0`` in the source cluster is restored to ``shard0`` in the
   target cluster.

.. note::

   If you used the |api| to create your Global Cluster, the zones are
   defined in the ``replicationSpecs`` parameter in the
   :oas-atlas-op:`Create a Cluster </createOneCluster>` and
   :oas-atlas-op:`Modify a Cluster </updateConfigurationOfOneCluster>`
   |api| endpoints.

If the cluster configurations of the source and target clusters do not
match, shard data may migrate to a different cloud provider zone than
where it resided in the source cluster. After |service| completes the
restore operation, the MongoDB :term:`balancer` for the target cluster
migrates the data back to the zone where it resided in the source
cluster if your clusters meet the following requirements:

- Both clusters have enabled a |global-write-cluster| on the same
  collection

- Both clusters use the same :term:`shard key` for the
  |global-write|-enabled collection

.. note::

   If the |global-write|-enabled collection on the target cluster does
   not contain any data, the MongoDB balancer for the cluster
   automatically distributes any data that you later add to the
   collection among the target cluster's shards.

To enable global writes on the target cluster:

1. Click :guilabel:`Database` in the top-left corner of |service|.
   
#. Click :guilabel:`Browse Collections` beneath the target cluster on the
   :guilabel:`{+Database-Deployments+}` page.

#. Click :guilabel:`Enable Global Writes`.

.. _pit-restore-ea:

{+PIT-Restore+}s
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

{+PIT-Restore+}s replay the :term:`oplog` to restore a cluster from a
particular point in time within a window specified in the Backup
Policy.

You may opt to
:ref:`enable {+PIT-Restore+} restores <create-cluster-backups>`.
Configure your {+pit-restore+} window with the
:ref:`Backup Policy Editor <cps-backup-policies>`.

.. note::

   Enabling {+pit-restore+}s increases the monthly cost of your
   cluster.

   To learn more about the cost implications, see
   :ref:`billing <billing-backup-cloud-provider-snapshots>`.

Your cluster's snapshots stay within the cloud provider's storage
service under the cluster or shard's
:ref:`highest priority region <deploy-across-multiple-regions>`. Oplog
backups on |aws| clusters use standard |aws| |s3| encryption.

.. note::

   All clusters with {+pit-restore+}s enabled store :term:`oplog` data
   on |aws| |s3|, including clusters backed by Azure and |gcp|.

The following actions cause all existing oplog backups to be deleted.
All existing snapshots remain intact, but |service| removes previously
preserved oplog data when:

- You disable {+pit-restore+}s for your cluster.
- The cluster receives an excessive number of writes. The cluster 
  processes a large number of writes that causes the oplog to roll over 
  before backup collects it.

  .. example::

     1. You sized your oplog for one hour of its usual write traffic,
        say 1,000 operations.

     #. Database activity results in a large number of writes to the
        oplog, say 2,000 operations.

     #. The number of writes result in the oplog dropping older
        records. This example would lose 1,000 operations.

     #. Backup should collect operation #1, but it collects #1,001
        instead.

If you :ref:`change <move-cluster>` your cluster's :ref:`highest 
priority region <deploy-across-multiple-regions>` or if MongoDB 
migrates oplog data to a different region:

- |service| retains data in both the old and new regions until your 
  {+pit-restore+} window is represented in the new region. Once 
  the {+pit-restore+} window is represented in the new region, 
  |service| deletes the data in the old region.
- You will be billed for storage in both the old and new regions for   
  the days following the region change. You must disable 
  {+pit-restore+} and reenable it to prevent billing in both regions. 

  .. note:: 

   If you disable {+pit-restore+}, |service| will delete the 
   {+pit-restore+} history.

When you use {+pit-restore+}s to restore a {+cluster+} from a previous 
point in time, |service| retains the {+cluster+}'s :term:`oplog`. You 
can use {+pit-restore+}s repeatedly to restore the {+cluster+} to any point
in its {+pit-restore+} window **except** between when you initiated a
restore and when |service| completes a snapshot after the restore.

Consistency and Snapshots
~~~~~~~~~~~~~~~~~~~~~~~~~

For {+clusters+} running MongoDB version 4.2 or and later: 

- |service| maintains
  :manual:`causal consistency </core/read-isolation-consistency-recency/#std-label-sessions>` 
  when taking snapshots except for size statistics reported by :manual:`collStats </reference/command/collStats/#mongodb-dbcommand-dbcmd.collStats>`
  and ``db.[collection].count()``. Size statistics reported
  by :manual:`collStats </reference/command/collStats/#mongodb-dbcommand-dbcmd.collStats>` and ``db.[collection].count()`` may be inccurate.
- |service| coordinates the time across all
  shards for sharded {+clusters+}. This ensures that snapshots include all documents written to every
  shard and node as of the scheduled snapshot time.

For {+clusters+} running MongoDB version 4.0 and earlier:

- |service| maintains crash-consistent snapshots.
- |service| takes snapshots from each of the shards for sharded {+clusters+}
  and the Config Server Replica Sets at approximately the same time.

.. _cloud-provider-backup-schedule-ea:
.. _cloud-provider-retention-policy-ea:
.. _cps-backup-policies-ea:

Backup Scheduling, Retention, and On-Demand Backup Snapshots
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. tabs::

   .. tab:: {+atlas-cli+}
      :tabid: atlascli

      .. include:: /includes/extracts/atlas-backups-schedule-describe.rst

   .. tab:: {+atlas-ui+}
      :tabid: ui

      Use the Backup Policy Editor to configure a backup policy for 
      {+Cloud-Backup+}s.

      1. Click :guilabel:`Database` in the top-left corner of 
         |service|.
   
      #. From the :guilabel:`{+Database-Deployments+}` view, click the cluster name.

      #. Click the :guilabel:`Backup` tab.

      #. Click :guilabel:`Backup Policy`.

A backup policy has the following sections:

- A time of day, in |utc|, at which to create snapshots.

- A frequency interval and duration of retention.

- If |pit| Restores are enabled, a |pit| window that allows you
  to restore to any point in time in the last X days where X is the window.

.. example::

   The default backup policy specifies a snapshot time of ``18:00``
   |utc| and the following four policy items:

   .. list-table::
      :widths: 15 15 20 30 20
      :header-rows: 1

      * - Policy Type
        - Tier
        - {+PIT-Restore+}
        - Snapshot Taken
        - Snapshot Retained

      * - Hourly
        - |nvme|
        - Enabled
        - Every 12 hours
        - 2 days

      * - Hourly
        - non-|nvme|
        - Enabled
        - Every 6 hours
        - 2 days

      * - Daily
        - All
        - Either
        - Every day
        - 7 days

      * - Weekly
        - All
        - Either
        - Every Saturday
        - 4 weeks

      * - Monthly
        - All
        - Either
        - Last day of the month
        - 12 months

To learn more about {+Cloud-Backup+} billing, see
:ref:`billing-backup-cloud-provider-snapshots`.

.. _changine-backup-policy-time-ea:

Change the Backup Policy Time
`````````````````````````````
.. tabs::

   .. tab:: {+atlas-cli+}
      :tabid: atlascli

      .. include:: /includes/extracts/atlas-backups-schedule-delete-and-update.rst

   .. tab:: {+atlas-ui+}
      :tabid: ui

      To modify the backup policy time:

      1. In the :guilabel:`Backup Policy Editor`, select the hour 
         which |service| takes a snapshot each day from :guilabel:`hr`
         beneath :guilabel:`Snapshot Time (UTC)`.

      2. Select the number of minutes after :guilabel:`hr` at which 
         |service| takes a snapshot from :guilabel:`min` beneath
         :guilabel:`Snapshot Time (UTC)`.

      3. Click :guilabel:`Save Changes`.

.. _creating-backup-policy-ea:

Configure the Backup Policy
```````````````````````````

Each row in the :guilabel:`Backup Policy Frequency and Retention` table
represents a backup policy item. Configure the policy items and,
optionally, add new policy items to configure a new backup policy.

.. tip::

   |service| displays the estimated number of snapshots
   associated with your changes below the
   :guilabel:`Backup Policy Frequency and Retention` table.

To specify a backup policy item using the {+atlas-ui+}:

1. Select the frequency unit from :guilabel:`Frequency Unit` for a
   policy item.

   Alternatively, click :guilabel:`Add Frequency Unit` to add a new
   policy item to the backup policy.

   .. note::

      You cannot specify multiple :guilabel:`Hourly` and
      :guilabel:`Daily` backup policy items.

2. Select the frequency for the frequency unit from :guilabel:`Every`.

   .. note::

      If you delete an existing backup frequency unit, the snapshots
      for which the frequency was specified remain intact until they
      expire or you delete them.

3. Specify the retention time for the policy item in
   :guilabel:`Retention Time` and the units for the retention time from
   the list to the right.

   .. note::

      |service| requires that the value specified for
      :guilabel:`Retention Time` for items that are less frequent is
      equal to or larger than the value specified for items that are
      more frequent. For example, if the hourly policy item specifies
      a retention of two days or greater, the retention for the weekly
      snapshot must be two weeks or greater.

      You can't configure a :ref:`restore window <create-pit-policy>` 
      that is longer than the Hourly Snapshot 
      :guilabel:`Retention Time`.

4. (Optional) To apply the retention changes in the updated backup
   policy to snapshots that |service| took previously, check
   :guilabel:`Apply policy retention changes to existing snapshots`.

   .. note::

      This option affects only snapshots created by the updated policy
      items and whose retention has not been updated individually with
      the :oas-atlas-op:`Update Cloud Backup Schedule 
      </updateCloudBackupBackupPolicyForOneCluster>` API.

5. Click :guilabel:`Save Changes`.

.. note::

   To take a snapshot sooner than the next scheduled snapshot,
   use the :oas-atlas-op:`Take One On-Demand Snapshot 
   </takeOneOn-DemandSnapshot>` API.

.. note::

   .. include:: /includes/fact-overlapping-backup-policy-items.rst

.. important::

   If you disable {+Cloud-Backup+}s for a cluster or terminate a
   cluster that had snapshots enabled, |service| immediately
   deletes the backup snapshots for that cluster. For clusters not
   using :ref:`security-kms-encryption`
   you can :ref:`download the latest snapshot
   <restore-cloud-provider-snapshot-download>` to preserve any data
   stored in the cluster.

.. _create-pit-policy-ea:

Configure the Restore Window
````````````````````````````

You can replay the :term:`oplog` to restore a cluster from any point in 
time within a specified restore window.

To specify the restore window duration, select how long you want
|service| to retain the oplog for point-in-time restores from
the :guilabel:`Restore Window` list.

You can't configure a restore window that is longer than the 
:ref:`Hourly Snapshot Retention Time <creating-backup-policy>`.

.. _snapshot-distribution-ea:

Configure |service| to Automatically Copy Snapshots to Other Regions
````````````````````````````````````````````````````````````````````

You can configure |service| to automatically create copies of your
snapshots and oplogs and store them in other regions. With snapshots
distributed across regions, you can still restore your {+cluster+} if
the primary region goes down, enhancing your disaster recovery
capabilities to bring them in line with internal or regulatory 
requirements. Additionally, you can perform direct attach restores 
rather than slower streaming restores for {+clusters+} based in the 
same project and region where you store copies. This reduces system 
recovery time.

You can use copied oplogs to perform {+pit-restore+} from secondary
regions in the event the primary region of your {+cluster+} goes down.

.. include:: /includes/atlas-backup-note.rst

To configure automatic snapshot distribution in other regions:

1. Navigate to the :guilabel:`Backup Policy` tab. In the 
   :guilabel:`Additional Backup Copies` section, click the arrow button 
   to expand the pane.

#. Toggle :guilabel:`Copy to other regions` on. You will see a table of 
   distribution policies pre-populated with the cluster's current 
   regions.
   
   .. note::

      For a global {+cluster+}, you can enable or disable this setting
      independently for each zone.

#. In the :guilabel:`Copy to other regions` table, click 
   :guilabel:`Choose a region` in the :guilabel:`Region` column of the 
   row under your current region and select the region to distribute 
   snapshots to.

   .. note::

      You can select regions only for the same cloud provider as the 
      primary region. For example, if your {+cluster+} is in the |aws| 
      ``us-east-1`` region, you can distribute snapshots only to other 
      |aws| regions supported by |service|. For global {+clusters+}, 
      each zone may have nodes in multiple cloud providers. When you 
      select regions in which to store copied snapshots, you can select 
      only regions in the same cloud provider as the zone's primary
      region.

#. In the :guilabel:`Snapshots` column, click the arrow to view the 
   available snapshot policies for the cluster. |service| displays
   options based on the backup policies for the cluster.

   For example, if you set the {+cluster+} with a :guilabel:`Daily`
   backup policy of taking daily 5:00 PM snapshots, checking
   :guilabel:`Daily` in the :guilabel:`Snapshots` column enables 
   snapshot distribution of daily 5:00 PM snapshots. You can't set the 
   snapshot distribution policy to a different frequency or timing than 
   its underlying backup policy.

   You can enable or disable any combination of the available policies
   on a region-by-region basis.

   .. example::

      The following table describes the snapshot distribution policy
      for a {+cluster+} with enabled storage nodes in the |aws| regions
      ``us-east-2``, ``us-west-1``, and ``us-west-2``:

      .. list-table::
         :widths: 16 14 14 14 14 14 14
         :header-rows: 1

         * - Region
           - Hourly
           - Daily
           - Weekly
           - Monthly
           - On Demand
           - Point-in-Time

         * - ``us-east-1`` (Original)
           - Every 6 hours
           - Every 2 days
           - Every Sunday
           - Last day of the month
           - Yes
           - Yes

         * - ``us-east-2`` (Copy)
           - Disabled
           - Disabled
           - Enabled
           - Enabled
           - Disabled
           - No

         * - ``us-west-1`` (Copy)
           - Enabled
           - Enabled
           - Enabled
           - Enabled
           - Enabled
           - Yes

         * - ``us-west-2`` (Copy)
           - Enabled
           - Enabled
           - Disabled
           - Disabled
           - Enabled
           - Yes

#. If you enabled {+PIT-Restore+} for the cluster, you can
   enable oplog distribution for each additional region. If you 
   disabled {+PIT-Restore+} for the cluster, you can't enable oplog
   distribution for additional regions.

To delete a policy for snapshot distribution to other regions:

1. In the :guilabel:`Action` column, click :icon:`trash-alt`.
   
2. In the dialog box, select either
   :guilabel:`Keep all snapshots in this region` or
   :guilabel:`Delete all snapshots in this region`, and click
   :guilabel:`Confirm`. If you choose to keep snapshots in
   a disabled region, the snapshots persist until their
   scheduled expiration date.

Alternatively, to disable all snapshot distribution policies
within a single-region {+cluster+} or within a global {+cluster+} zone,
click the :guilabel:`Copy to other regions` toggle. As with deleting
an individual policy, you can choose to keep or delete all copied
snapshots for that {+cluster+} or zone.

.. note::
   
   In the event of a cloud provider service outage in a region 
   configured for automatic snapshot distribution, |service| will 
   attempt restore using snapshots stored in copy regions.

.. _viewing-snapshots-ea:

View Backup Snapshots
`````````````````````

.. tabs::

   .. tab:: {+atlas-cli+}
      :tabid: atlascli

      .. include:: /includes/extracts/atlas-backups-snapshots-list-and-describe.rst

   .. tab:: {+atlas-ui+}
      :tabid: ui

      .. include:: /includes/view-snapshots-cluster-ui.rst

By default, |service| displays both on-demand and policy-based
snapshots. To view only policy-based snapshots:

1. Click :guilabel:`Policy` under :guilabel:`View Snapshots by`.

   Alternatively, click :guilabel:`On-demand` to display only snapshots
   taken by clicking :guilabel:`Take Snapshot Now`.

Snapshots taken according to the backup policy display the frequency of
the policy item that generated the snapshot in the
:guilabel:`Frequency` column: ``Monthly``, ``Weekly``, ``Daily``, or
``Hourly``.

.. note::

   .. include:: /includes/fact-overlapping-backup-policy-items.rst

.. include:: /includes/fact-backup-zone-renaming.rst

.. _cloud-provider-snapshots-on-demand-ea:
.. _on-demand-snapshots-ea:

Take On-Demand Snapshots
````````````````````````

|service| takes on-demand snapshots immediately, unlike scheduled
snapshots which occur at :oas-atlas-tag:`regular intervals 
</Cloud-Backup-Schedule>`. If there is already an on-demand snapshot 
with a status of ``queued`` or ``inProgress``, you must wait until 
|service| has completed the on-demand snapshot before taking another. 
If there is already a scheduled snapshot with a status of ``queued`` or 
``inProgress``, you may queue an on-demand snapshot. You must have the
:authrole:`Organization Owner` or :authrole:`Project Owner` role to
successfully call this endpoint.

.. tabs::

   .. tab:: {+atlas-cli+}
      :tabid: atlascli

      .. include:: /includes/extracts/atlas-backups-snapshots-create-and-watch.rst

   .. tab:: {+atlas-ui+}
      :tabid: ui

      .. include:: /includes/take-on-demand-snapshot-ui.rst

.. _m2-m5-snapshots-ea:

{+Shared-Cluster+} Backups
--------------------------

|service| takes daily snapshots of ``M2`` and ``M5`` 
{+shared-clusters+}. |service| takes these snapshots automatically 
starting 24 hours after you create your {+cluster+}.

.. include:: /includes/admonitions/notes/backup-roles.rst

|service| always takes ``M2`` and ``M5`` snapshots from a
:term:`secondary` node to minimize performance impact.

|service| retains the last 8 daily snapshots, which you can
:ref:`download or restore to an Atlas {+cluster+} 
<restore-from-snapshot>`.

Limitations
~~~~~~~~~~~

.. include:: /includes/fact-backup-shared-tier-limitations.rst

Backup Snapshot Storage Locations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following table indicates where |service| stores ``M2`` and ``M5`` 
{+cluster+} snapshots:

.. list-table::
   :header-rows: 1

   * - {+Cluster+} Location
     - Snapshot Storage Location

   * - Australia
     - Australia

   * - Germany
     - Germany

   * - Hong Kong
     - Australia

   * - India, Singapore, Taiwan
     - Asia

   * - USA
     - USA

   * - All other {+cluster+} locations
     - Ireland


View ``M2``/``M5`` Backup Snapshots
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. tabs::

   .. tab:: {+atlas-cli+}
      :tabid: atlascli

      .. include:: /includes/extracts/atlas-backups-snapshots-list-and-describe.rst

   .. tab:: {+atlas-ui+}
      :tabid: ui

      .. include:: /includes/view-snapshots-cluster-ui.rst

|service| displays existing snapshots in the
:guilabel:`All Daily Snapshots` table. From this table, you can
:ref:`restore <restore-from-snapshot>` or
:ref:`download <restore-from-local-file>` your 
existing snapshots.

.. _serverless-snapshots-ea:

{+Serverless-Instance+} Backups
-------------------------------

|service| takes snapshots of {+serverless-instances+} using the native
snapshot capabilities of the {+serverless-instances+}'s cloud service
provider.

.. important::

   You can't disable backup of {+serverless-instances+}.

|service| offers the following backup options for
{+serverless-instances+}:

.. include:: /includes/list-table-serverless-backup-options.rst

To learn more, see :ref:`config-serverless-backup`.

Limitations
~~~~~~~~~~~

.. include:: /includes/fact-backup-serverless-limitations.rst

View {+Serverless-Instance+} Snapshots
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

|service| displays existing snapshots on the :guilabel:`Snapshots`
page. 

To view your snapshots:

.. tabs::

   .. tab:: {+atlas-cli+}
      :tabid: atlascli

      View Snapshots
      ``````````````

      .. include:: /includes/extracts/atlas-serverless-backups-snapshots-list-and-describe.rst

      View Snapshot Status
      ````````````````````

      .. include:: /includes/extracts/atlas-serverless-backups-snapshots-watch.rst
   
   .. tab:: {+atlas-ui+}
      :tabid: ui      

      .. include:: /includes/view-serverless-backups-snapshots-ui.rst
