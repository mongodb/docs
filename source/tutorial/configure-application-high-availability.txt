==========================================
Configure a Highly Available |application|
==========================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

The |application| provides high availability through use of multiple
|application| servers behind a load balancer and through use of a
:manual:`replica set </reference/glossary/#std-term-replica-set>` to host the :ref:`mms-application-database`.

Considerations
--------------

Load Balancer
~~~~~~~~~~~~~

The |application|'s :ref:`components <mms-application-package>` are
stateless between requests. Any |application| server can handle
requests as long as all the servers read from the same |application|
Database. If one |application| becomes unavailable, another
fills requests.

To take advantage of this for high availability, configure a load
balancer of your choice to balance between the pool of |application|
hosts. To do this in |mms|, perform the following actions:

- Set the :setting:`URL to Access Ops Manager` property to the load
  balancer |url|.
  
- Set the :setting:`Load Balancer Remote IP Header` property to
  ``X-Forwarded-For``,
  which is the |http| header field the load balancer uses to 
  identify the originating client's IP address. 

  .. note::

     If you are using a Layer-4 load balancer that does not support
     ``X-Forwarded-For`` by default, either enable ``X-Forwarded-For``,
     or use `Proxy Protocol <https://www.haproxy.com/blog/haproxy/proxy-protocol/>`_.


The |application| uses the client's IP address for auditing, logging,
and :ref:`setting an access list <access-list-for-api-operations>` for
the |api|.

After the load balancer is configured and started, you should not log 
in to the |application| from its individual host |url|\s. 

.. note::

   To disallow access to each |application| server, configure your 
   firewall rules accordingly.

.. example::

   If you have two |onprem| hosts serving the following |url|\s:

   - ``ops1.example.com``
   - ``ops2.example.com``

   and put them behind a load balancer at the following |url|:

   - ``opsmanager.example.com``

   After you configure and start that load balancer, you should not log 
   in to ``ops1.example.com``. Log in to ``opsmanager.example.com`` 
   instead.

.. note::

   If you set these parameters using the configuration file, change
   :setting:`mms.remoteIp.header` to the |url| for the load balancer
   and :setting:`mms.centralUrl` to the |url| for the |onprem| host
   and port.

File System Snapshots Require Shared File System
++++++++++++++++++++++++++++++++++++++++++++++++

.. include:: /includes/fact-shared-file-system-reqs.rst

Diagnostic Archive 
++++++++++++++++++

To give your |onprem| diagnostic archive time to generate, set the
:guilabel:`HTTP idle timeout` parameter for the load balancer to 180
seconds.

.. _opsmgr-appliance:

Appliance Network Layer Support
+++++++++++++++++++++++++++++++

Any load balancing appliance must support
:wikipedia:`Layer 7 </Application_layer>` (the Application Layer) of
the :abbr:`OSI (Open Systems Interconnection)` model.

Replica Set for the |application| Database
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Deploy a :manual:`replica set </reference/glossary/#std-term-replica-set>` rather than a standalone to host the
:ref:`mms-application-database`. Replica sets have automatic
:manual:`failover </core/replica-set-high-availability>` if the
:manual:`primary </reference/glossary/#std-term-primary>` becomes unavailable.

If the replica set has members in multiple facilities, ensure that a
single facility has enough votes to elect a :manual:`primary </reference/glossary/#std-term-primary>` if needed.
Choose the facility that hosts the core application systems. Place a
majority of voting members and all the members that can become primary
in this facility. Otherwise, network partitions could prevent the set
from being able to form a majority. For details on how replica sets
elect primaries, see
:manual:`Replica Set Elections </core/replica-set-elections>`.

You can back up the replica set using
:manual:`file system snapshots </tutorial/backup-with-filesystem-snapshots>`.
File system snapshots use system-level tools to create copies of the
device that holds replica set's data files.

To deploy the replica set that hosts the |application| Database, see
:doc:`backing MongoDB instance </tutorial/prepare-backing-mongodb-instances>`.

.. _gen-key:

The ``gen.key`` File
~~~~~~~~~~~~~~~~~~~~

``gen.key`` file is a 24-byte binary file used to encrypt and decrypt
|onprem|\'s backing databases and user credentials. An identical
``gen.key`` file must be stored on every server that is part of a
highly available |onprem| deployment.

The ``gen.key`` file can be generated automatically or manually.

To have |onprem| generate the file:
   Start *one* |onprem| server. |onprem| will create a ``gen.key`` file
   if none exists.

To create the file manually:
   Generate a 24-byte binary file.

   .. example::

      The following creates the ``gen.key`` file using ``openssl``:

      .. code-block:: sh

         openssl rand 24 > /<keyPath>/gen.key

   Protect the ``gen.key`` file like any sensitive file. Change the
   owner to the user running |onprem| and set the file permission to
   read and write for the owner only.

Once you have the ``gen.key`` file (either created automatically or
manually), *before* starting the other |onprem| servers, copy the file
to the appropriate directory on the current server and to the
appropriate directory on the other |onprem| servers:

* ``/etc/mongodb-mms/`` for RPM or Ubuntu installations

* ``${HOME}/.mongodb-mms/`` for archive (``.tar``) file
  installations

.. important::

   * Any shared storage resource that stores the ``gen.key`` file
     should be configured for high availability so as not to introduce
     a potential single point of failure.

   * Any |onprem| server that does not have the ``gen.key`` file
     installed cannot connect to the backing databases and become part
     of an HA |onprem| instance.

   * Once you have generated the ``gen.key`` for your |onprem| instance
     on the first |onprem| server, back up the ``gen.key`` file to a
     secure location.

.. _opsmgr-upgrade-mode:

Upgrade Mode
~~~~~~~~~~~~

If you have an |onprem| installation with more than one |onprem| host
pointing to the same Application Database, you can upgrade |onprem| to
a newer version without incurring monitoring downtime. After you
complete the upgrade of one |onprem| host of a highly available
|onprem| deployment, that deployment enters a state known as **Upgrade
Mode**. In this state, |onprem| is available during an upgrade. The
benefits of this mode are that throughout the upgrade process:

- Alerts and monitoring operate
- |onprem| instances remain live
- |application| may be accessed in read-only mode
- |onprem| |api|\s that write or delete data are disabled

Your |onprem| instance stays in **Upgrade Mode** until all |onprem|
hosts have been upgraded and restarted. You should not upgrade more
than one |onprem| host at a time.

.. _opsmgr-multi-regional-perf-considerations:

Performance in Multi-Region Deployments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The geographical distribution of the Application Database and |onprem|
instances might impact the performance of the |application|.

Multi-region Application Database Performance
+++++++++++++++++++++++++++++++++++++++++++++++

If you plan to replicate the Application Database across multiple regions,
consider that many of the |onprem| write workload operations use
``w:2`` :manual:`write concern </reference/write-concern>`, which requires
acknowledgement from the primary member and one secondary member of the
replica set for each write operation.

Therefore, having a secondary replica member of the Application Database in
the **same** region as the primary member can lead to better read and write
performance.

For example, deploying three Application Database replica set members in
three regions in a 1-1-1 fashion might result in worse performance
compared with deploying three Application Database replica members in
two regions in a 2-1 fashion, where one region hosts two Application Database
replica set members and another region hosts the third replica set member.

Performance of the |mms| UI
+++++++++++++++++++++++++++++++++++

The |mms| UI is more performant if you connect to the |application| instance
deployed it in the same region as the Application Database primary member
of the Application Database replica set.

In other words, you may achieve a better user experience connecting to
the |application| instance hosted in the same region as the
Application Database primary member of the replica set, rather than
connecting to a closer |application| instance where the instance itself
must connect to a primary Application Database replica set member hosted in
a different, distant region with a high latency.

Prerequisites
-------------

Deploy the replica set that serves the :ref:`mms-application-database`.
To deploy a replica set, see
:manual:`Deploy a Replica Set </tutorial/deploy-replica-set>` in the
MongoDB manual.

Procedure
---------

The following procedure assumes you generated the first ``gen.key`` 
using one of the |application| hosts. If you instead create your
own ``gen.key``, distribute it to the |onprem| hosts before starting
any of the |application|\ s.

.. important::

   The load balancer placed in front of the |application| servers must
   not return cached content. The load balancer must have caching
   disabled.

To configure multiple |application|\ s with load balancing:

.. include:: /includes/steps/configure-application-high-availability.rst

Additional Information
----------------------

For information on making |mms| Backup highly available, see
:doc:`/tutorial/configure-backup-high-availability`.
