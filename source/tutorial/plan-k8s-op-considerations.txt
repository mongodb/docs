.. _k8s-considerations:

==============
Considerations
==============

.. meta::
   :description: Explore best practices and system configuration recommendations for deploying MongoDB Controllers for Kubernetes Operator in production environments.

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

This page details best practices and system configuration
recommendations for the |k8s-op-full| when running in production.

.. _deploy_recommended-number-sets:

Deploy Multiple MongoDB Replica Sets
------------------------------------

We recommend that you use a single instance of the |k8s-op-short|
to deploy and manage your MongoDB replica sets.

To deploy more than 10 MongoDB replica sets in parallel,
you can :ref:`increase the thread count of your Kubernetes Operator instance <increase-thread-count-ops-manager>`. 

Specify CPU and Memory Resource Requirements
--------------------------------------------

.. note:: 

   The following considerations apply:

   - All sizing and performance recommendations for common MongoDB deployments
     through the |k8s-op-short| in this section are subject to change. Do
     not treat these recommendations as guarantees or limitations of any kind.

   - These recommendations reflect performance testing findings and represent
     our suggestions for production deployments. We ran the tests on a cluster
     comprised of seven AWS EC2 instances of type ``t2.2xlarge`` and a
     master node of type ``t2.medium``.

   - The recommendations in this section don't discuss characteristics of
     any specific deployment. Your deployment's characteristics may differ
     from the assumptions made to create these recommendations. Contact
     |mdb-support| for further help with sizings.

In |k8s|, each Pod includes parameters that allow you
to specify :k8sdocs:`CPU resources </tasks/configure-pod-container/assign-cpu-resource/>`
and :k8sdocs:`memory resources
</tasks/configure-pod-container/assign-memory-resource/>` for each
container in the Pod.

To indicate resource bounds, |k8s| uses the :k8sdocs:`requests and limits
</concepts/configuration/manage-resources-containers/#requests-and-limits>`
parameters, where:
  
- *request* indicates a lower bound of a resource.
- *limit* indicates an upper bound of a resource.

The following sections illustrate how to:

- :ref:`set CPU and Memory for the Operator Pod <operator_pod_resources>`.
- :ref:`set CPU and Memory for MongoDB Pods <mdb_pods_resources>`.

For the Pods hosting |onprem|, use the
:github:`default resource limits configurations
</mongodb/mongodb-kubernetes/blob/master/public/samples/ops-manager/ops-manager-pod-spec.yaml#L38-L46>`.

Consider Recommended Practices for Storage
--------------------------------------------

The following recommendations are applicable to MongoDB deployments and |application| database 
deployments managed by the {+k8s-op-short+}.

.. note::
   
   - Redundancy in the storage class is not required because MongoDB automatically replicates data between 
     the members of a replica set or shard.

   - You can't share storage across members of a replica set or shard.

- Use the storage class that provides the best performance given your constraints. See 
  :ref:`scale a deployment <scale-k8s-resources>` to learn more.
- Provision |k8s-pvs| that support ``ReadWriteOnce`` for your storage class.
- On a worker node level, apply the best practices from :manual:`MongoDB on Linux </administration/production-notes/>`.
- Ensure that you use a storage class that supports `volume expansion <https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion>`__ 
  to enable database volume resizing.

Use Static Containers (Public Preview)
--------------------------------------

.. include:: /includes/static-containers-description.rst

To learn more, see :ref:`static-containers`.

Co-locate ``mongos`` Pods with Your Applications
------------------------------------------------

You can run the lightweight ``mongos`` instance on the same |k8s-node|
as your apps using MongoDB. The |k8s-op-short| supports standard |k8s|
:k8sdocs:`node affinity and anti-affinity </concepts/configuration/assign-pod-node/#affinity-and-anti-affinity>`
features. Using these features, you can force install the ``mongos``
on the same node as your application.

The following abbreviated example shows affinity and multiple
availability zones configuration.

The ``podAffinity`` key determines whether to install an application
on the same Pod, node, or data center as another application.

To specify Pod affinity:

1. Add a label and value in the ``spec.podSpec.podTemplate.metadata.labels``
   |yaml| collection to tag the deployment. See
   :setting:`spec.podSpec.podTemplate.metadata`,
   and the
   :k8sdocs:`Kubernetes PodSpec v1 core API </reference/generated/kubernetes-api/{+k8s-api-version+}/#podspec-v1-core>`.

#. Specify which label the ``mongos`` uses in the
   ``spec.mongosPodSpec.podAffinity``
   ``.requiredDuringSchedulingIgnoredDuringExecution.labelSelector``
   |yaml| collection. The ``matchExpressions`` collection defines the
   ``label`` that the |k8s-op-short| uses to identify the Pod for hosting
   the ``mongos``.

.. example::

   .. code-block:: yaml
      :copyable: false
      :linenos:

      apiVersion: mongodb.com/v1
      kind: MongoDB
      metadata:
        name: my-replica-set
      spec:
        members: 3
        version: 8.0.0
        service: my-service

        ...
          podTemplate:
            spec:
              affinity:
                podAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    - labelSelector:
                        matchExpressions:
                          - key: security
                            operator: In
                            values:
                              - S1
                      topologyKey: failure-domain.beta.kubernetes.io/zone
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                      - matchExpressions:
                          - key: kubernetes.io/e2e-az-name
                            operator: In
                            values:
                              - e2e-az1
                              - e2e-az2
                podAntiAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                  - podAffinityTerm:
                      topologyKey: nodeId

See the full example of multiple availability zones and node affinity
configuration in
:github:`replica-set-affinity.yaml </mongodb/mongodb-kubernetes/blob/master/public/samples/mongodb/affinity/replica-set-affinity.yaml>`
in the :github:`Affinity Samples </mongodb/mongodb-kubernetes/tree/master/public/samples/mongodb/persistent-volumes>`
directory.

This directory also contains sample affinity and multiple
zones configurations for sharded clusters and standalone
MongoDB deployments.

.. seealso::

   - :k8sdocs:`Assigning Pods to Nodes </concepts/scheduling-eviction/assign-pod-node/#nodeselector>`
   - :k8sdocs:`Node affinity and anti-affinity </concepts/configuration/assign-pod-node/#affinity-and-anti-affinity>`
   - :k8sdocs:`Kubernetes PodSpec v1 core API </reference/generated/kubernetes-api/{+k8s-api-version+}/#podspec-v1-core>`

Name Your MongoDB Service with its Purpose
------------------------------------------

Set the :setting:`spec.service` parameter to a value that identifies
this deployment's purpose, as illustrated in the following example.

.. code-block:: yaml
   :copyable: false
   :linenos:
   :emphasize-lines: 8

   apiVersion: mongodb.com/v1
   kind: MongoDB
   metadata:
     name: my-replica-set
   spec:
     members: 3
     version: "8.0.0"
     service: drilling-pumps-geosensors
     featureCompatibilityVersion: "8.0"

.. seealso::

   :setting:`spec.service`

Use Labels to Differentiate Between Deployments
-----------------------------------------------

Use the :k8sdocs:`Pod affinity
</concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity>`
|k8s| feature to:

- Separate different MongoDB resources, such as ``test``, ``staging``,
  and ``production`` environments.

- Place |k8s-pods| on some specific nodes to take advantage of
  features such as |ssd| support.

.. code-block:: yaml
   :copyable: false
   :linenos:

   mongosPodSpec:
     podAffinity:
       requiredDuringSchedulingIgnoredDuringExecution:
         - labelSelector:
           matchExpressions:
           - key: security
             operator: In
             values:
             - S1
           topologyKey: failure-domain.beta.kubernetes.io/zone

.. seealso::

  :k8sdocs:`Pod affinity
  </concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity>`

Customize the CustomResourceDefinitions that the |k8s-op-short| Watches
-----------------------------------------------------------------------

You can specify which custom resources you want the |k8s-op-short|
to watch. This allows you to install the CustomResourceDefinition for
only the resources that you want the |k8s-op-short| to manage.

You must use ``helm`` to configure the |k8s-op-short| to watch only the
custom resources you specify. Follow the relevant ``helm``
:ref:`installation instructions <install-k8s-operator>`,
but make the following adjustments:

1. Decide which CustomResourceDefinitions you want to install. You can
   install any number of the following:

   .. include:: /includes/list-tables/crds.rst

#. Install the Helm Chart and specify which 
   CustomResourceDefinitions you want the 
   |k8s-op-short| to watch.
   
   Separate each custom resource with a comma:

   .. code-block:: sh

      helm install <deployment-name> mongodb/mongodb-kubernetes \
        --set operator.watchedResources="{mongodb,mongodbusers}" \
           --skip-crds

Ensure Proper Persistence Configuration
---------------------------------------

The |k8s| deployments orchestrated by the |k8s-op-short| are
stateful. The |k8s| container uses |k8s-pvs| to maintain the
cluster state between restarts.

To satisfy the statefulness requirement, the |k8s-op-short| performs
the following actions:

- Creates |k8s-pvs| for your MongoDB deployment.
- Mounts storage devices to one or more directories
  called mount points.
- Creates one persistent volume for each MongoDB mount point.
- Sets the default path in each |k8s| container to ``/data``.

To meet your MongoDB cluster's storage needs, make the following
changes in your configuration for each replica set deployed with
the |k8s-op-short|:

- Verify that persistent volumes are enabled in
  :setting:`spec.persistent`. This setting defaults to ``true``.
- Specify a sufficient amount of storage for the |k8s-op-short|
  to allocate for each of the volumes. The volumes store the data
  and the logs.

  - To set multiple volumes, each for data, logs, and the ``oplog``, use
    :setting:`spec.podSpec.persistence.multiple.data`.
  - To set a single volume to store data, logs, and the ``oplog``,
    use :setting:`spec.podSpec.persistence.single`.

The following abbreviated example shows recommended persistent storage
sizes.

.. code-block:: yaml
   :linenos:
   :emphasize-lines: 8, 13-19
   

   apiVersion: mongodb.com/v1
   kind: MongoDB
   metadata:
     name: my-replica-cluster
   spec:
     
     ...
     persistent: true
     
     
     shardPodSpec:
     ...
       persistence:
         multiple:
           data:
             storage: "20Gi"
           logs:
             storage: "4Gi"
             storageClass: standard

For a full example of persistent volumes configuration, see
:github:`replica-set-persistent-volumes.yaml 
</mongodb/mongodb-kubernetes/blob/master/public/samples/mongodb/persistent-volumes/replica-set-persistent-volumes.yaml>`
in the :github:`Persistent Volumes Samples
</mongodb/mongodb-kubernetes/tree/master/public/samples/mongodb/persistent-volumes>` directory. This
directory also contains sample persistent volumes configurations for
sharded clusters and standalone deployments.

.. seealso::

   - :setting:`spec.persistent`
   - :setting:`spec.podSpec.persistence.single`
   - :setting:`spec.podSpec.persistence.multiple.data`

.. _operator_pod_resources:

Set CPU and Memory Utilization Bounds for the |k8s-op-short| Pod
----------------------------------------------------------------

When you deploy MongoDB replica sets with the |k8s-op-short|, the initial 
reconcilliation process increases CPU usage for the Pod running the 
|k8s-op-short|. However, when the replica set deployment process completes, 
the CPU usage by the |k8s-op-short| reduces considerably. 

.. note:: 

   The severity of CPU usage spikes in the |k8s-op-short| is directly impacted
   by :ref:`the thread count <increase-thread-count-ops-manager>` of the 
   |k8s-op-short|, as the thread count (defined by the :ref:`MDB_MAX_CONCURRENT_RECONCILES <mdb-max-concurrent-reconciles>` value) 
   is equal to the number of reconcilliation processes that can be running in 
   parallel at any given time.

For production deployments, to satisfy deploying up to 50 MongoDB
replica sets or sharded clusters in parallel with the |k8s-op-short|,
set the CPU and memory resources and limits for the |k8s-op-short| Pod
as follows:

- ``spec.template.spec.containers.resources.requests.cpu`` to 500m
- ``spec.template.spec.containers.resources.limits.cpu`` to 1100m
- ``spec.template.spec.containers.resources.requests.memory`` to 200Mi
- ``spec.template.spec.containers.resources.limits.memory`` to 1Gi


If you use Helm to deploy resources, define these values in 
the :ref:`values.yaml file <k8s-op-resources-setting>`. 
  
The following abbreviated example shows the configuration with
recommended CPU and memory bounds for the |k8s-op-short| Pod in your
deployment of 50 replica sets or sharded clusters. If you are
deploying fewer than 50 MongoDB clusters, you may use lower
numbers in the configuration file for the |k8s-op-short| Pod.

.. example::

   .. code-block:: yaml
      :copyable: false
      :linenos:
      :emphasize-lines: 24-25, 34-40

      apiVersion: apps/v1
      kind: Deployment
      metadata:
       name: mongodb-kubernetes-operator
       namespace: mongodb
      spec:
       replicas: 1
       selector:
        matchLabels:
           app.kubernetes.io/component: controller
           app.kubernetes.io/name: mongodb-kubernetes-operator
           app.kubernetes.io/instance: mongodb-kubernetes-operator
       template:
        metadata:
         labels:
           app.kubernetes.io/component: controller
           app.kubernetes.io/name: mongodb-kubernetes-operator
           app.kubernetes.io/instance: mongodb-kubernetes-operator
         spec:
           serviceAccountName: mongodb-kubernetes-operator
           securityContext:
             runAsNonRoot: true
             runAsUser: 2000
           containers:
           - name: mongodb-kubernetes-operator
             image: quay.io/mongodb/mongodb-kubernetes-operator:1.9.2
             imagePullPolicy: Always
             args:
              - "-watch-resource=mongodb"
              - "-watch-resource=opsmanagers"
              - "-watch-resource=mongodbusers"
             command:
              - "/usr/local/bin/mongodb-kubernetes-operator"
             resources:
               limits:
                 cpu: 1100m
                 memory: 1Gi
               requests:
                 cpu: 500m
                 memory: 200Mi

For a full example of CPU and memory utilization resources and limits
for the |k8s-op-short| Pod that satisfy parallel deployment of up to
50 MongoDB replica sets, see the :github:`mongodb-kubernetes.yaml
</mongodb/mongodb-kubernetes/blob/master/public/mongodb-kubernetes.yaml#L219-L235>`
file.

.. seealso::

   - :k8sdocs:`Requests and Limits </concepts/configuration/manage-resources-containers/#requests-and-limits>`
   - :k8sdocs:`Assign CPU Resources to Containers and Pods </tasks/configure-pod-container/assign-cpu-resource/>`

.. _mdb_pods_resources:

Set CPU and Memory Utilization Bounds for MongoDB Pods
-------------------------------------------------------

The values for Pods hosting replica sets or sharded clusters map
to the :k8sdocs:`requests field </reference/generated/kubernetes-api/{+k8s-api-version+}/#resourcerequirements-v1-core>`
for CPU and memory for the created Pod. These values are consistent
with :manual:`considerations </administration/production-notes#allocate-sufficient-ram-and-cpu>`
stated for MongoDB hosts.

The |k8s-op-short| uses its allocated memory for processing, for the
WiredTiger cache, and for storing packages during the deployments.

For production deployments, set the CPU and memory resources and limits
for the MongoDB Pod as follows:

- ``spec.podSpec.podTemplate.spec.containers.resources.requests.cpu`` to 0.25
- ``spec.podSpec.podTemplate.spec.containers.resources.limits.cpu`` to 0.25
- ``spec.podSpec.podTemplate.spec.containers.resources.requests.memory`` to 512M
- ``spec.podSpec.podTemplate.spec.containers.resources.limits.memory`` to 512M

If you use Helm to deploy resources, define these values in 
the :ref:`values.yaml file <k8s-op-resources-setting>`. 

The following abbreviated example shows the configuration with
recommended CPU and memory bounds for each Pod hosting a MongoDB
replica set member in your deployment.

.. example::

  .. code-block:: yaml
     :linenos:
     :emphasize-lines: 5, 12-20
     :copyable: false

     apiVersion: mongodb.com/v1
     kind: MongoDB
     metadata:
     name: my-replica-set
     spec:
       members: 3
       version: 8.0.0
       service: my-service
       ...

       persistent: true
       podSpec:
         podTemplate:
           spec:
             containers:
             - name: mongodb-enterprise-database
               resources:
                 limits:
                   cpu: "0.25"
                   memory: 512M

For a full example of CPU and memory utilization resources and limits
for Pods hosting MongoDB replica set members, see the
:github:`replica-set-podspec.yaml </mongodb/mongodb-kubernetes/blob/master/public/samples/mongodb/podspec/replica-set-podspec.yaml#L38-L45>`
file in the :github:`MongoDB Podspec Samples </mongodb/mongodb-kubernetes/tree/master/public/samples/mongodb/podspec>` directory.

This directory also contains sample CPU and memory limits
configurations for Pods used for:

- A sharded cluster, in the :github:`sharded-cluster-podspec.yaml </mongodb/mongodb-kubernetes/blob/master/public/samples/mongodb/podspec/sharded-cluster-podspec.yaml#L62-91>`.
- Standalone MongoDB deployments, in the :github:`standalone-podspec.yaml </mongodb/mongodb-kubernetes/blob/master/public/samples/mongodb/podspec/standalone-podspec.yaml#L36-39>`.

.. seealso::

   - :setting:`spec.podSpec.podTemplate.spec`
   - :k8sdocs:`Requests and Limits </concepts/configuration/manage-resources-containers/#requests-and-limits>`
   - :k8sdocs:`Assign CPU Resources to Containers and Pods </tasks/configure-pod-container/assign-cpu-resource/>`

Use Multiple Availability Zones
-------------------------------

Set the |k8s-op-short| and |k8s-statefulsets| to distribute all members
of one replica set to different |k8s-nodes| to ensure high
availability.

The following abbreviated example shows affinity and multiple
availability zones configuration.

.. example::

   .. code-block:: yaml
      :copyable: false
      :linenos:

      apiVersion: mongodb.com/v1
      kind: MongoDB
      metadata:
        name: my-replica-set
      spec:
        members: 3
        version: 8.0.0
        service: my-service
        ...
          podAntiAffinityTopologyKey: nodeId
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
              matchExpressions:
              - key: security
                operator: In
                values:
                - S1
              topologyKey: failure-domain.beta.kubernetes.io/zone

          nodeAffinity:
             requiredDuringSchedulingIgnoredDuringExecution:
               nodeSelectorTerms:
               - matchExpressions:
                 - key: kubernetes.io/e2e-az-name
                 operator: In
                 values:
                 - e2e-az1
                 - e2e-az2

In this example, the |k8s-op-short| schedules the Pods deployment to
the nodes which have the label ``kubernetes.io/e2e-az-name`` in ``e2e-az1`` or
``e2e-az2`` availability zones. Change ``nodeAffinity`` to
schedule the deployment of Pods to the desired availability zones.

See the full example of multiple availability zones configuration in
:github:`replica-set-affinity.yaml </mongodb/mongodb-kubernetes/blob/master/public/samples/mongodb/affinity/replica-set-affinity.yaml>`
in the :github:`Affinity Samples </mongodb/mongodb-kubernetes/tree/master/public/samples/mongodb/persistent-volumes>`
directory.

This directory also contains sample affinity and multiple zones
configurations for sharded clusters and standalone MongoDB deployments.

.. seealso::

   - :k8sdocs:`Running in Multiple Zones </setup/best-practices/multiple-zones/>`
   - :k8sdocs:`Node affinity </concepts/scheduling-eviction/assign-pod-node/#node-affinity>`

.. _increase-thread-count-ops-manager:

Increase Thread Count to Run multiple Reconciliation Processes in Parallel
--------------------------------------------------------------------------

If you plan to deploy more than 10 MongoDB replica sets in parallel, 
you can configure the |k8s-op-short| to run multiple reconciliation processes 
in parallel by setting :ref:`MDB_MAX_CONCURRENT_RECONCILES <mdb-max-concurrent-reconciles>` environment variable in your |k8s-op-short| 
deployment or or through the :ref:`operator.maxConcurrentReconciles <mdb-max-concurrent-reconciles-helm>` field in your Helm 
``values.yaml`` file to configure a higher thread count. 

Increasing the thread count of the |k8s-op-short| allows you to vertically scale your |k8s-op-short| 
deployment to hundreds of |k8s-mdbrscs| running within your |k8s| cluster  
and optimize CPU utilization.

Please monitor |k8s| API server and |k8s-op-short| resource usage and adjust their respective 
resource allocation if necessary.

.. note:: 

   - Proceed with caution when increasing the :ref:`MDB_MAX_CONCURRENT_RECONCILES <mdb-max-concurrent-reconciles>` beyond 10.
     In particular, you must monitor the |k8s-op-short|, and the |k8s| API 
     closely to avoid downtime resulting from increased load on those components. 

     To determine the thread count that suits your deployment's needs, 
     use the following guidelines:

     - Your requirements for how responsive the |k8s-op-short| must be when 
       reconciling many resources

     - The compute resources available within your |k8s| environment and 
       the total processing load your |k8s| compute resources are under, including 
       resources that may be unrelated to MongoDB
   
   - An alternative to increasing the thread count of a single |k8s-op-short| 
     instance, while still increasing the number of |k8s-mdbrscs| you can support 
     in your |k8s| cluster, is to deploy multiple |k8s-op-short| instances within 
     your |k8s| cluster. However, deploying multiple |k8s-op-short| 
     instances requires that you ensure that no two |k8s-op-short| instances
     are monitoring the same |k8s-mdbrscs|.

     Running more than one instance of the |k8s-op-short| should be done with care, 
     as more |k8s-op-short| instances (especially with parallel reconciliation enabled) 
     put the API server at greater risk of being overwhelmed. 

   - Scaling of the |k8s| API server is not a valid reason to run 
     more than one instance of the |k8s-op-short|. If you observe that performance of 
     the API server is affected, adding more instances of the |k8s-op-short| is 
     likely to compound the problem.
