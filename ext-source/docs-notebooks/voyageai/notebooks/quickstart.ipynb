{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voyage AI Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a companion to the [Voyage AI Quick Start](https://www.mongodb.com/docs/voyageai/quickstart/) tutorial. Refer to the page for set-up instructions and detailed explanations.\n",
    "\n",
    "In this guide, you learn how to generate your first vector embeddings with Voyage AI and build a basic application.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mongodb/docs-notebooks/blob/main/voyageai/notebooks/quickstart.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model API Key\n",
    "\n",
    "To access Voyage AI models, create a Model API Key in the MongoDB Atlas UI.\n",
    "\n",
    "1. [Sign up for a free Atlas account or log in.](https://www.mongodb.com/cloud/atlas/register)\n",
    "2. Create a model API Key for your project:\n",
    "   - In your Atlas project, select **AI Models** from the navigation bar.\n",
    "   - Click **Create Model API Key**.\n",
    "   - Give the API key a name and then click **Create**.\n",
    "3. Set the API key in your environment:\n",
    "   - Copy the API key and store it in a safe location.\n",
    "   - Export the API key as an environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade voyageai numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your API key here or export it as an environment variable\n",
    "os.environ[\"VOYAGE_API_KEY\"] = \"<your-model-api-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Your First Embeddings\n",
    "\n",
    "In this section, you generate vector embeddings using a Voyage AI embedding model and the Python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import voyageai\n",
    "\n",
    "# Initialize Voyage client\n",
    "vo = voyageai.Client()\n",
    "\n",
    "# Sample texts\n",
    "texts = [\n",
    "    \"hello, world\",\n",
    "    \"welcome to voyage ai!\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "result = vo.embed(\n",
    "    texts,\n",
    "    model=\"voyage-4-large\"\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(result.embeddings)} embeddings\")\n",
    "print(f\"Each embedding has {len(result.embeddings[0])} dimensions\")\n",
    "print(f\"First embedding (truncated): {result.embeddings[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Basic RAG Application\n",
    "\n",
    "Now that you know how to generate vector embeddings, build a basic RAG application to learn how to use Voyage AI models to implement AI search and retrieval. RAG enables LLMs to generate context-aware responses by retrieving relevant information from your data before generating answers.\n",
    "\n",
    "> **Note:** RAG applications require access to an LLM. This tutorial provides examples using Anthropic or OpenAI, but you can use any LLM provider of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import voyageai\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Voyage client\n",
    "vo = voyageai.Client()\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This quarter, our company is focused on building new products, increasing market share, and cutting costs.\",\n",
    "    \"20th-century innovations, from radios to smartphones, centered on electronic advancements.\",\n",
    "    \"Photosynthesis in plants converts light energy into glucose and produces essential oxygen.\"\n",
    "]\n",
    "\n",
    "query = \"What are my company's goals this quarter?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings and Perform Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for documents\n",
    "doc_embeddings = vo.embed(\n",
    "    texts=documents,\n",
    "    model=\"voyage-4-large\",\n",
    "    input_type=\"document\"\n",
    ").embeddings\n",
    "\n",
    "# Generate embedding for query\n",
    "query_embedding = vo.embed(\n",
    "    texts=[query],\n",
    "    model=\"voyage-4-large\",\n",
    "    input_type=\"query\"\n",
    ").embeddings[0]\n",
    "\n",
    "# Calculate similarity scores using dot product\n",
    "similarities = np.dot(doc_embeddings, query_embedding)\n",
    "\n",
    "# Sort by similarity (np.argsort with negative sign sorts high to low)\n",
    "ranked_indices = np.argsort(-similarities)\n",
    "\n",
    "print(f\"Semantic search result: {documents[ranked_indices[0]][:50]}...\")\n",
    "print(f\"Similarity score: {similarities[ranked_indices[0]]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine Results with Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine results with reranking model\n",
    "reranked = vo.rerank(query, documents, model=\"rerank-2.5\", top_k=3)\n",
    "\n",
    "print(\"Reranked results:\")\n",
    "for i, result in enumerate(reranked.results, 1):\n",
    "    print(f\"{i}. Score: {result.relevance_score:.4f} - {result.document[:50]}...\")\n",
    "\n",
    "# Get the most relevant document as context\n",
    "context = reranked.results[0].document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Response with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install anthropic openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your LLM provider and set the appropriate API key\n",
    "LLM_PROVIDER = \"anthropic\"  # Options: \"anthropic\" or \"openai\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"<your-anthropic-api-key>\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<your-openai-api-key>\"\n",
    "\n",
    "# Initialize LLM client based on provider\n",
    "if LLM_PROVIDER == \"anthropic\":\n",
    "    import anthropic\n",
    "    llm_client = anthropic.Anthropic()\n",
    "elif LLM_PROVIDER == \"openai\":\n",
    "    from openai import OpenAI\n",
    "    llm_client = OpenAI()\n",
    "else:\n",
    "    raise ValueError(\"Choose a different LLM provider: 'anthropic' or 'openai'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answer using retrieved context\n",
    "prompt = f\"Based on this information: '{context}', answer: {query}\"\n",
    "\n",
    "if LLM_PROVIDER == \"anthropic\":\n",
    "    response = llm_client.messages.create(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=1024,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    answer = response.content[0].text\n",
    "elif LLM_PROVIDER == \"openai\":\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "else:\n",
    "    answer = \"Configure another LLM provider.\"\n",
    "\n",
    "print(f\"\\nQuestion: {query}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
