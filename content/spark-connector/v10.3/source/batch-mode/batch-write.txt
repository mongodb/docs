.. _batch-write-to-mongodb:

==============================
Write to MongoDB in Batch Mode
==============================

.. toctree::
   :caption: Batch Write Configuration Options

   Configuration </batch-mode/batch-write-config>

Overview
--------

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         .. include:: /java/write-to-mongodb.rst

     - id: python
       content: |

         .. include:: /python/write-to-mongodb.rst

     - id: scala
       content: |

         .. include:: /scala/write-to-mongodb.rst

.. warning:: Save Modes
   
   The {+connector-long+} supports the following save modes:

   -  ``append``
   - ``overwrite``

   If you specify the ``overwrite`` write mode, the connector drops the target
   collection and creates a new collection that uses the
   default collection options.
   This behavior can affect collections that don't use the default options,
   such as the following collection types:

   - Sharded collections
   - Collections with nondefault collations
   - Time-series collections

   To learn more about save modes, see the
   `Spark SQL Guide <https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes>`__.
   
.. important::

   If your write operation includes a field with a ``null`` value,
   the connector writes the field name and ``null`` value to MongoDB. You can
   change this behavior by setting the write configuration property
   ``ignoreNullValues``.
   
   For more information about setting the connector's
   write behavior, see :ref:`Write Configuration Options <spark-batch-write-conf>`.
   
API Documentation
-----------------

To learn more about the types used in these examples, see the following Apache Spark
API documentation:

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         - `Dataset<T> <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html>`__
         - `DataFrameWriter <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameWriter.html>`__

     - id: python
       content: |

         - `DataFrame <https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html>`__
         - `DataFrameReader <https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.html>`__

     - id: scala
       content: |

         - `Dataset[T] <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html>`__
         - `DataFrameReader <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameWriter.html>`__