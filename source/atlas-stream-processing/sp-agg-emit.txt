.. _streams-agg-pipeline-emit:
.. _atlas-sp-agg-emit:

=========
``$emit``
=========

.. default-domain:: mongodb

.. meta::
   :keywords: atlas stream processing, $emit aggregation pipeline stage 
   :description: Learn how to use the $emit stage to output processed data
                 to streaming data platforms.

.. facet::
   :name: genre
   :values: reference

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

.. _atlas-sp-agg-emit-def:

Definition
----------

The ``$emit`` stage specifies a connection in the
:ref:`Connection Registry <manage-spi-connection-add>` to emit
messages to. The connection must be either an {+kafka+} broker or a
:manual:`time series collection </core/timeseries-collections>`.

.. _atlas-sp-agg-emit-syntax:

Syntax
------

.. _sp-emit-kafka:

Apache Kafka Broker
~~~~~~~~~~~~~~~~~~~

To write processed data to an {+kafka+} broker, use the ``$emit``
pipeline stage with the following prototype form:

.. code-block:: json

   {
     "$emit": {
       "connectionName": "<registered-connection>",
       "topic" : "<target-topic>" | <expression>,
       }
     }  
   }

The ``$emit`` stage takes a document with the following fields: 

.. list-table:: 
   :header-rows: 1
   :widths: 10 15 15 70

   * - Field 
     - Type 
     - Necessity 
     - Description

   * - ``connectionName`` 
     - string
     - Required 
     - Name, as it appears in the 
       :ref:`Connection Registry <manage-spi-connection-add>`, of the
       connection to ingest data from.

   * - ``topic``
     - string | expression
     - Conditional 
     - Name of the {+kafka+} topic to emit messages to.

.. _sp-emit-timeseries:

{+service+} Time Series Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To write processed data to an {+service+} time series collection,
use the ``$emit`` pipeline stage with the following prototype form:

.. code-block:: json

   {
     "$emit": {
       "connectionName": "<registered-connection>",
       "db" : "<target-db>",
       "coll" : "<target-coll>",
       "timeseries" : {
	 <options>
       } 
     }  
   }


The ``$emit`` stage takes a document with the following fields: 

.. list-table:: 
   :header-rows: 1
   :widths: 10 15 15 70

   * - Field 
     - Type 
     - Necessity 
     - Description

   * - ``connectionName`` 
     - string
     - Required 
     - Name, as it appears in the 
       :ref:`Connection Registry <manage-spi-connection-add>`, of the
       connection to ingest data from.

   * - ``db``
     - string
     - Required 
     - Name of the {+service+} database that contains the target
       time series collection.

   * - ``coll``
     - string
     - Required 
     - Name of the {+service+} time series collection to write
       to.

   * - ``timeseries``
     - document
     - Required 
     - Document defining the :manual:`time series fields
       </core/timeseries/timeseries-procedures/#time-series-field-reference>`
       for the collection.

.. _atlas-sp-agg-emit-behavior:

Behavior
--------

``$emit`` must be the last stage of any pipeline it appears in. You can
use only one ``$emit`` stage per pipeline.

You can only write to a single {+service+} time series collection per
stream processor. If you specify a collection that doesn't exist,
{+service+} creates the collection with the time series fields you
specified. You must specify an existing database.

You can use a :manual:`dynamic expression
</reference/operator/aggregation/#expression-operators>` as the value
of the ``topic`` field to enable your stream processor to write to
different target {+kafka+} topics on a message-by-message basis.

.. example::

   You have a stream of transaction events that generates messages of 
   the following form:

   .. code-block:: json

      { 
        "customer": "Very Important Industries",
        "customerStatus": "VIP",
        "tenantId": 1,
        "transactionType": "subscription"
      }

      { 
        "customer": "N. E. Buddy",
        "customerStatus": "employee",
        "tenantId": 5,
        "transactionType": "requisition"
      }

      { 
        "customer": "Khan Traktor",
        "customerStatus": "contractor",
        "tenantId": 11,
        "transactionType": "billableHours"
      }

   To sort each of these into a distinct {+kafka+} topic, you can write
   the following ``$emit`` stage:

   .. code-block:: json

      $emit: {
        connectionName: "kafka1",
        topic: "$customerStatus"
      }

   This ``$emit`` stage:

   - Writes the ``Very Important Industries`` message to a topic named 
     ``VIP``.
   - Writes the ``N. E. Buddy`` message to a topic named ``employee``.
   - Writes the ``Khan Traktor`` message to a topic named 
     ``contractor``.

You can use only dynamic expressions that evaluate to strings. For 
more information on dynamic expressions, see 
:manual:`expression operators
</reference/operator/aggregation/#expression-operators>`. 

If you specify a topic that doesn't already exist, {+kafka+}
automatically creates the topic when it receives the first message
that targets it.

If you specify a topic with a dynamic expression, but 
{+atlas-sp+} cannot evaluate the expression for a given message, 
{+atlas-sp+} skips that message and continues processing subsequent 
messages.
