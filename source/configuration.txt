.. _configuration:

=================
Configuring Spark
=================

.. meta::
   :description: Configure read and write operations in Spark using `SparkConf`, options maps, or system properties for batch and streaming modes.

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

Overview
--------

You can configure read and write operations in both batch and streaming mode.
To learn more about the available configuration options, see the following
pages:

- :ref:`spark-batch-read-conf`
- :ref:`spark-batch-write-conf`
- :ref:`spark-streaming-read-conf`
- :ref:`spark-streaming-write-conf`

Specify Configuration
---------------------

.. _spark-conf:

Using ``SparkConf``
~~~~~~~~~~~~~~~~~~~

You can specify configuration options with ``SparkConf`` using any of 
the following approaches:

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         - The ``SparkConf`` constructor in your application. To learn more, see the `Java SparkConf documentation <https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/SparkConf.html>`__.

     - id: python
       content: |

         - The ``SparkConf`` constructor in your application. To learn more, see the `Python SparkConf documentation <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html>`__.

     - id: scala
       content: |

         - The ``SparkConf`` constructor in your application. To learn more, see the `Scala SparkConf documentation <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/SparkConf.html>`__.

- The ``--conf`` flag at runtime. To learn more, see 
  `Dynamically Loading Spark Properties <https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties>`__ in 
  the Spark documentation.

- The ``$SPARK_HOME/conf/spark-default.conf`` file.

The MongoDB Spark Connector will use the settings in ``SparkConf`` as 
defaults.

.. _options-map:

Using an Options Map
~~~~~~~~~~~~~~~~~~~~

In the Spark API, the ``DataFrameReader``, ``DataFrameWriter``, ``DataStreamReader``,
and ``DataStreamWriter`` classes each contain an ``option()`` method. You can use
this method to specify options for the underlying read or write operation.

.. note::
   
   Options specified in this way override any corresponding settings in ``SparkConf``.

Short-Form Syntax
`````````````````

Options maps support short-form syntax. You may omit the prefix when 
specifying an option key string.

.. example::

   The following syntaxes are equivalent to one another:

   - ``dfw.option("spark.mongodb.write.collection", "myCollection").save()``

   - ``dfw.option("spark.mongodb.collection", "myCollection").save()``

   - ``dfw.option("collection", "myCollection").save()``

To learn more about the ``option()`` method, see the following Spark
documentation pages:

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         - `DataFrameReader <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html#option-java.lang.String-java.lang.String->`__
         - `DataFrameWriter <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameWriter.html#option-java.lang.String-java.lang.String->`__
         - `DataStreamReader <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html#option-java.lang.String-java.lang.String->`__
         - `DataStreamWriter <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html#option-java.lang.String-java.lang.String-->`__

     - id: python
       content: |

         - `DataFrameReader <https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.option.html>`__
         - `DataFrameWriter <https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.option.html>`__
         - `DataStreamReader <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.option.html>`__
         - `DataStreamWriter <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.option.html>`__

     - id: scala
       content: |

         - `DataFrameReader <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameReader.html#option(key:String,value:Double):org.apache.spark.sql.DataFrameReader>`__
         - `DataFrameWriter <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameWriter.html#option(key:String,value:Double):org.apache.spark.sql.DataFrameWriter[T]>`__
         - `DataStreamReader <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamReader.html#option(key:String,value:Double):org.apache.spark.sql.streaming.DataStreamReader>`__
         - `DataStreamWriter <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html#option(key:String,value:Double):org.apache.spark.sql.streaming.DataStreamWriter[T]>`__
  
Using a System Property
~~~~~~~~~~~~~~~~~~~~~~~

The {+connector-short+} reads some configuration settings before ``SparkConf`` is
available. You must specify these settings by using a JVM system property.

For more information on Java system properties, see the `Java documentation. <https://docs.oracle.com/javase/tutorial/essential/environment/sysprop.html>`__

.. tip:: Configuration Exceptions

   If the {+connector-short+} throws a ``ConfigException``, confirm that your ``SparkConf``
   or options map uses correct syntax and contains only valid configuration options.