.. _pymongo-arrow-quick-start:

===========
Quick Start
===========

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

.. facet::
   :name: genre
   :values: reference
 
.. meta::
   :keywords: tutorial, introduction, setup, begin  
   :description: Explore how to use PyMongoArrow for data manipulation in MongoDB, including inserting, querying, and writing data to various formats like Parquet and CSV.

This tutorial is intended as an introduction to working with
**{+driver-short+}**. The tutorial assumes the reader is familiar with basic
`PyMongo <https://pymongo.readthedocs.io/en/stable/tutorial.html>`__ and
`MongoDB <https://docs.mongodb.com>`__ concepts.

Prerequisites
-------------

Ensure that you have the {+driver-short+} distribution
:ref:`installed <pymongo-arrow-install>`. In the Python shell, the following should
run without raising an exception:

.. code-block:: python

   >>> import pymongoarrow as pma

This tutorial also assumes that a MongoDB instance is running on the
default host and port. After you have `downloaded and installed
<https://docs.mongodb.com/manual/installation/>`__ MongoDB, you can start
it as shown in the following code example:

.. code-block:: bash

   $ mongod

Extending PyMongo
~~~~~~~~~~~~~~~~~

The ``pymongoarrow.monkey`` module provides an interface to patch PyMongo
in place, and add {+driver-short+} functionality directly to
``Collection`` instances:

.. code-block:: python

   from pymongoarrow.monkey import patch_all
   patch_all()

After you run the ``monkey.patch_all()`` method, new instances of
the ``Collection`` class will contain the {+driver-short+} APIs--
for example, the ``pymongoarrow.api.find_pandas_all()`` method.

.. note::
  
   You can also use any of the {+driver-short+} APIs
   by importing them from the ``pymongoarrow.api`` module. If you do,
   you must pass the instance of the ``Collection`` on which the operation is to be
   run as the first argument when calling the API method.

Test Data
~~~~~~~~~

The following code uses PyMongo to add sample data to your cluster:

.. code-block:: python

   from datetime import datetime
   from pymongo import MongoClient
   client = MongoClient()
   client.db.data.insert_many([
     {'_id': 1, 'amount': 21, 'last_updated': datetime(2020, 12, 10, 1, 3, 1), 'account': {'name': 'Customer1', 'account_number': 1}, 'txns': ['A']},
     {'_id': 2, 'amount': 16, 'last_updated': datetime(2020, 7, 23, 6, 7, 11), 'account': {'name': 'Customer2', 'account_number': 2}, 'txns': ['A', 'B']},
     {'_id': 3, 'amount': 3,  'last_updated': datetime(2021, 3, 10, 18, 43, 9), 'account': {'name': 'Customer3', 'account_number': 3}, 'txns': ['A', 'B', 'C']},
     {'_id': 4, 'amount': 0,  'last_updated': datetime(2021, 2, 25, 3, 50, 31), 'account': {'name': 'Customer4', 'account_number': 4}, 'txns': ['A', 'B', 'C', 'D']}])

Defining the Schema
-------------------

{+driver-short+} relies on a data schema to marshall
query result sets into tabular form. If you don't provide this schema, {+driver-short+}
infers one from the data. You can define the schema by
creating a ``Schema`` object and mapping the field names
to type-specifiers, as shown in the following example:

.. code-block:: python

   from pymongoarrow.api import Schema
   schema = Schema({'_id': int, 'amount': float, 'last_updated': datetime})

MongoDB uses embedded documents to represent nested data. {+driver-short+} offers
first-class support for these documents:

.. code-block:: python

   schema = Schema({'_id': int, 'amount': float, 'account': { 'name': str, 'account_number': int}})

{+driver-short+} also supports lists and nested lists:

.. code-block:: python

   from pyarrow import list_, string
   schema = Schema({'txns': list_(string())})
   polars_df = client.db.data.find_polars_all({'amount': {'$gt': 0}}, schema=schema)

.. tip::
  
   {+driver-short+} includes multiple permissible type-identifiers for each supported BSON
   type. For a full list of these data types and their associated type-identifiers, see
   :ref:`<pymongo-arrow-data-types>`.

Find Operations
---------------

The following code example shows how to load all records that have a non-zero
value for the ``amount`` field as a ``pandas.DataFrame`` object:

.. code-block:: python

   df = client.db.data.find_pandas_all({'amount': {'$gt': 0}}, schema=schema)

You can also load the same result set as a ``pyarrow.Table`` instance:

.. code-block:: python

   arrow_table = client.db.data.find_arrow_all({'amount': {'$gt': 0}}, schema=schema)

Or as a ``polars.DataFrame`` instance:

.. code-block:: python

   df = client.db.data.find_polars_all({'amount': {'$gt': 0}}, schema=schema)

Or as a NumPy ``arrays`` object:

.. code-block:: python
  
   ndarrays = client.db.data.find_numpy_all({'amount': {'$gt': 0}}, schema=schema)

When using NumPy, the return value is a dictionary where the keys are field
names and the values are the corresponding ``numpy.ndarray`` instances.

.. note::

   In all of the preceding examples, you can omit the schema as shown in the following
   example:

   .. code-block:: python

      arrow_table = client.db.data.find_arrow_all({'amount': {'$gt': 0}})

   If you omit the schema, {+driver-short+} tries to automatically apply a schema based on
   the data contained in the first batch.

Aggregate Operations
--------------------

Running an aggregate operation is similar to running a find operation, but it takes a
sequence of operations to perform.

The following is a simple example of the ``aggregate_pandas_all()`` method that outputs a
new dataframe in which all ``_id`` values are grouped together and their ``amount`` values
summed:

.. code-block:: python

   df = client.db.data.aggregate_pandas_all([{'$group': {'_id': None, 'total_amount': { '$sum': '$amount' }}}])

You can also run aggregate operations on embedded documents.
The following example unwinds values in the nested ``txn`` field, counts the number of each
value, then returns the results as a list of NumPy ``ndarray`` objects, sorted in
descending order:

.. code-block:: python

   pipeline = [{'$unwind': '$txns'}, {'$group': {'_id': '$txns', 'count': {'$sum': 1}}}, {'$sort': {"count": -1}}]
   ndarrays = client.db.data.aggregate_numpy_all(pipeline)

.. tip::
  
   For more information about aggregation pipelines, see the
   :manual:`MongoDB Server documentation </core/aggregation-pipeline/>`.

Writing to MongoDB
------------------

You can use the ``write()`` method to write objects of the following types to MongoDB:

- Arrow ``Table``
- Pandas ``DataFrame``
- NumPy ``ndarray``
- Polars ``DataFrame``

.. code-block:: python
 
   from pymongoarrow.api import write
   from pymongo import MongoClient
   coll = MongoClient().db.my_collection
   write(coll, df)
   write(coll, arrow_table)
   write(coll, ndarrays)

.. note::
  
   NumPy arrays are specified as ``dict[str, ndarray]``.

Ignore Empty Values
~~~~~~~~~~~~~~~~~~~

The ``write()`` method optionally accepts a boolean ``exclude_none`` parameter.
If you set this parameter to ``True``, the driver doesn't write empty values to
the database. If you set this parameter to ``False`` or leave it blank, the 
driver writes ``None`` for each empty field.

The code in the following example writes an Arrow ``Table`` to MongoDB twice. One
of the values in the ``'b'`` field is set to ``None``. 

The first call to the ``write()`` method omits the ``exclude_none`` parameter, 
so it defaults to ``False``. All values in the ``Table``, including ``None``, 
are written to the database. The second call to the ``write()`` method sets 
``exclude_none`` to ``True``, so the empty value in the ``'b'`` field is ignored.

.. io-code-block::
   :copyable: true

   .. input:: 
      :language: python
      :emphasize-lines: 12, 16
      
      data_a = [1, 2, 3]
      data_b = [1, None, 3]

      data = Table.from_pydict(
         {
            "a": data_a,
            "b": data_b,
         },
      )

      coll.drop()
      write(coll, data)
      col_data = list(coll.find({}))

      coll.drop()
      write(coll, data, exclude_none=True)
      col_data_exclude_none = list(coll.find({}))

      print(col_data)

      print(col_data_exclude_none)

   .. output::
      :language: json
      :visible: false
      :emphasize-lines: 2, 6

      {'_id': ObjectId('...'), 'a': 1, 'b': 1}
      {'_id': ObjectId('...'), 'a': 2, 'b': None}
      {'_id': ObjectId('...'), 'a': 3, 'b': 3}

      {'_id': ObjectId('...'), 'a': 1, 'b': 1}
      {'_id': ObjectId('...'), 'a': 2}
      {'_id': ObjectId('...'), 'a': 3, 'b': 3}

Writing to Other Formats
------------------------

Once result sets have been loaded, you can then write them to any format that the package
supports.

For example, to write the table referenced by the variable ``arrow_table`` to a Parquet
file named ``example.parquet``, run the following code:

.. code-block:: python
  
   import pyarrow.parquet as pq
   pq.write_table(arrow_table, 'example.parquet')

Pandas also supports writing ``DataFrame`` instances to a variety
of formats, including CSV and HDF. To write the data frame
referenced by the variable ``df`` to a CSV file named ``out.csv``, run the following
code:

.. code-block:: python
  
   df.to_csv('out.csv', index=False)

The Polars API is a mix of the two preceding examples:

.. code-block:: python
 
   import polars as pl
   df = pl.DataFrame({"foo": [1, 2, 3, 4, 5]})
   df.write_parquet('example.parquet')

.. note::

   Nested data is supported for parquet read and write operations, but is not well
   supported by Arrow or Pandas for CSV read and write operations.
