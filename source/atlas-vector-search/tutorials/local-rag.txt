:tabs-selector-position: main

.. _local-rag:

=========================================================
Build a Local RAG Implementation with {+avs+}
=========================================================

.. default-domain:: mongodb

.. facet::
   :name: genre
   :values: tutorial

.. facet::
   :name: programming_language
   :values: javascript/typescript, python, go

.. meta::
   :description: How to implement retrieval-augmented generation (RAG) for Atlas Vector Search using local embedding models and chat models.

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

This tutorial demonstrates how to implement
retrieval-augmented generation (RAG)
locally, without the need for |api| keys or credits. 
To learn more about |rag|, see :ref:`ai-key-concepts`. 

Specifically, you perform the following actions:

#. Create a local |service| {+deployment+}.
#. Set up the environment.
#. Use a local embedding model to generate vector embeddings.
#. Create an {+avs+} index on your data.
#. Use a local |llm| to answer questions on your data.

----------

.. |arrow| unicode:: U+27A4

|arrow| Use the **Select your language** drop-down menu to set the 
language of the examples on this page.

.. tabs-selector:: drivers

----------

Background
----------

To complete this tutorial, you can either create a local |service| 
{+deployment+} by using the :ref:`{+atlas-cli+} 
<atlas-programmatic-access-cli-overview>` or :ref:`deploy a 
{+cluster+} <create-new-cluster>` on the cloud. The {+atlas-cli+} 
is the command-line interface for |service-fullname|, and you can use 
the {+atlas-cli+} to interact with |service| from the terminal for 
various tasks, including creating local |service| {+deployments+}. 
To learn more, see 
:atlascli:`Manage Local and Cloud Deployments from the Atlas CLI </atlas-cli-local-cloud>`.

.. note::
   
   Local |service| {+deployments+} are intended for testing only. 
   For production environments, :ref:`deploy a {+cluster+} <create-new-cluster>`.

.. tabs-drivers::
   
   .. tab::
      :tabid: go

      You also use the following open-source models in this tutorial:

      - `Nomic Embed Text <https://ollama.com/library/nomic-embed-text>`__ embedding model
      - `Mistral 7B <https://ollama.com/library/mistral>`__ generative model

      There are several ways to download and deploy |llm|\s locally.
      In this tutorial, you download `Ollama <https://ollama.com/>`__ and pull
      the open source models listed above to perform |rag| tasks.

      This tutorial also uses the `Go language port of LangChain
      <https://tmc.github.io/langchaingo/docs/>`__, a popular 
      open-source |llm| framework, to connect to these models and 
      integrate them with {+avs+}. If you prefer different models or a 
      different framework, you can adapt this tutorial by replacing 
      the Ollama model names or LangChain library components 
      with their equivalents for your preferred setup.

   .. tab::
      :tabid: nodejs

      You also use the following open-source models in this tutorial:

      - `mxbai-embed-large-v1 <https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1>`__ embedding model 
      - `Mistral 7B <https://docs.mistral.ai/getting-started/models/>`__ generative model

      There are several ways to download and deploy |llm|\s locally.
      In this tutorial, you download the Mistral 7B model
      by using `GPT4All <https://gpt4all.io/index.html>`__,
      an open-source ecosystem for local |llm| development.

   .. tab::
      :tabid: python

      When working through this tutorial, you use an interactive Python notebook.
      This environment allows you to create and execute individual code blocks without
      running the entire file each time.

      You also use the following open-source models in this tutorial:

      - `mxbai-embed-large-v1 <https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1>`__ embedding model 
      - `Mistral 7B <https://docs.mistral.ai/getting-started/models/>`__ generative model

      There are several ways to download and deploy |llm|\s locally.
      In this tutorial, you download the Mistral 7B model
      by using `GPT4All <https://gpt4all.io/index.html>`__,
      an open-source ecosystem for local |llm| development. 

Prerequisites
-------------

.. tabs-drivers::

   .. tab::
      :tabid: go

      To complete this tutorial, you must have the following:

      - The :atlascli:`{+atlas-cli+} </>` installed and running v1.14.3 or later.

      - A terminal and code editor to run your Go project.

      - `Go <https://go.dev/doc/install>`__ installed.

      - `Ollama <https://ollama.com/>`__ installed.

   .. tab::
      :tabid: nodejs

      To complete this tutorial, you must have the following:

      - The :atlascli:`{+atlas-cli+} </>` installed and running v1.14.3 or later.

      - A `Hugging Face Access Token <https://huggingface.co/docs/hub/en/security-tokens>`__
        with read access.

      - `Git Large File Storage <https://git-lfs.com/>`__ installed.

      - A terminal and code editor to run your Node.js project.

      - `npm and Node.js <https://docs.npmjs.com/downloading-and-installing-node-js-and-npm>`__ installed.

   .. tab::
      :tabid: python

      To complete this tutorial, you must have the following:

      - The :atlascli:`{+atlas-cli+} </>` installed and running v1.14.3 or later.

      - An interactive Python notebook that you can run locally. 
        You can run interactive Python notebooks in `VS Code 
        <https://code.visualstudio.com/docs/datascience/jupyter-notebooks>`__.
        Ensure that your environment runs Python v3.10 or later.

      .. note::

         If you use a hosted service such as `Colab 
         <https://colab.research.google.com>`__, ensure that 
         you have enough RAM to run this tutorial. Otherwise,
         you might experience performance issues.

Create a Local |service| Deployment
-----------------------------------

In this section, you create a local |service| {+deployment+} to
use as a vector database. If you have an |service| {+cluster+} 
running MongoDB version 6.0.11, 7.0.2, or later with the 
:ref:`sample data <sample-data>` loaded, you can skip this step.

To create the local {+deployment+}:

.. include:: /includes/steps-create-local-deployment-atlas-cli.rst

Set Up the Environment
----------------------

.. tabs-drivers::

   .. tab::
      :tabid: go

      In this section, you set up the environment for this tutorial. Create
      a project, install the required packages, and define a connection
      string:

      .. include:: /includes/avs-local-rag-set-up-environment-go.rst

   .. tab::
      :tabid: nodejs

      In this section, you set up the environment for this tutorial. Create
      a project, install the required packages, and define a connection
      string:

      .. include:: /includes/avs-local-rag-set-up-environment-javascript.rst

   .. tab::
      :tabid: python

      In this section, you set up the environment for this tutorial.

      .. include:: /includes/ai-integrations/langchain/local-rag-set-up-environment.rst

Generate Embeddings with a Local Model
--------------------------------------

In this section, you load an embedding model locally and 
generate vector embeddings by using data from the 
:ref:`sample_airbnb <sample-airbnb>` database, 
which contains a single collection called ``listingsAndReviews``.

.. tabs-drivers::

   .. tab::
      :tabid: go

      .. include:: /includes/avs-local-rag-generate-embeddings-with-local-model-go.rst

   .. tab::
      :tabid: nodejs

      .. include:: /includes/avs-local-rag-generate-embeddings-with-local-model-javascript.rst

   .. tab::
      :tabid: python

      .. include:: /includes/avs-local-rag-generate-embeddings-python.rst

.. tabs::
   :hidden: true

   .. tab:: Local {+Deployment+}
      :tabid: local

      This code takes some time to run. After it's finished, you can
      connect to your local {+deployment+} 
      from {+mongosh+} or your application by using your {+deployment+}\'s 
      connection string. Then, to view your vector embeddings,
      run :manual:`read operations 
      </crud/#read-operations>` on the 
      ``sample_airbnb.listingsAndReviews`` collection. 
      
   .. tab:: Cloud {+Deployment+}
      :tabid: cloud

      You can view your vector embeddings :ref:`in the {+atlas-ui+} <atlas-ui-view-collections>`
      by navigating to the ``sample_airbnb.listingsAndReviews`` collection in your 
      {+cluster+} and expanding the fields in a document.

.. note:: 

   You can convert the embeddings in the sample data to |bson| vectors for
   efficient storage and ingestion of vectors in |service|. To learn
   more, see :ref:`how to convert native embeddings to BSON vectors
   <avs-bindata-vector-subtype>`. 

Create the {+avs+} Index
------------------------------------

To enable vector search on the ``sample_airbnb.listingsAndReviews`` 
collection, create an {+avs+} index.

.. tabs::

   .. tab:: Local {+Deployment+}
      :tabid: local

      If you're using a local |service| {+deployment+}, complete the following steps:

      .. tabs-drivers::

         .. tab::
            :tabid: go

            .. include:: /includes/avs-examples/local-rag/local-rag-create-index-cli-nomic-dims.rst

         .. tab::
            :tabid: nodejs

            .. include:: /includes/ai-integrations/langchain/local-rag-create-index-cli.rst

         .. tab::
            :tabid: python

            .. include:: /includes/ai-integrations/langchain/local-rag-create-index-cli.rst

   .. tab:: Cloud {+Deployment+}
      :tabid: cloud
      
      .. note:: 

         To create an {+avs+} index, you must have :authrole:`Project Data Access 
         Admin` or higher access to the |service| project.

      If you're using an |service| {+cluster+}, complete the following steps:

      .. tabs-drivers::

         .. tab::
            :tabid: go

            .. include:: /includes/avs-examples/local-rag/create-index-programmatically-go.rst

         .. tab::
            :tabid: nodejs

            .. include:: /includes/avs-examples/local-rag/create-index-programmatically-javascript.rst

         .. tab::
            :tabid: python

            .. include:: /includes/avs-examples/local-rag/create-index-programmatically-python.rst

Answer Questions with a Local LLM
---------------------------------

.. tabs-drivers::

   .. tab::
      :tabid: go

      This section demonstrates a sample |rag| implementation 
      that you can run locally by using {+avs+} and Ollama.

      .. include:: /includes/avs-examples/local-rag/perform-qa-go.rst

   .. tab::
      :tabid: nodejs

      This section demonstrates a sample |rag| implementation 
      that you can run locally by using {+avs+} and GPT4All.

      .. include:: /includes/avs-examples/local-rag/perform-qa-javascript.rst

   .. tab::
      :tabid: python

      This section demonstrates a sample |rag| implementation 
      that you can run locally by using {+avs+} and GPT4All.

      In your notebook, run the following code snippets:

      .. include:: /includes/ai-integrations/langchain/local-rag-perform-qa.rst
