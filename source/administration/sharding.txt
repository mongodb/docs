.. index:: administration; sharding
.. _sharding-administration:

============================
Shard Cluster Administration
============================

.. default-domain:: mongodb

This document provides a collection of basic operations and procedures
for administering :term:`shard clusters <shard cluster>`.

For a full introduction to sharding in MongoDB see
":doc:`/core/sharding`," and for a complete overview of all sharding
documentation in the MongoDB Manual, see ":doc:`/sharding`." The
":doc:`/administration/sharding-architectures`" document provides an
overview of deployment possibilities to help deploy a shard
cluster. Finally, the ":doc:`/core/sharding-internals`" document
provides a more detailed introduction to sharding when troubleshooting
issues or understanding your cluster's behavior.

.. contents:: Sharding Procedures:
   :backlinks: none
   :local:

.. _sharding-procedure-setup:

Setup
-----

If you have an existing replica set, you can use the
":doc:`/tutorial/convert-replica-set-to-replicated-shard-cluster`"
tutorial as a guide. If you're deploying a :term:`shard cluster` from
scratch, see the ":doc:`/tutorial/deploy-shard-cluster`" tutorial for
more detail or use the following procedure as a quick starting point:

#. Provision the required hardware.

   The ":ref:`sharding-requirements`" section describes what you'll
   need to get started.

   .. warning:: Sharding and "localhost" Addresses

      If you use either "localhost" or "``127.0.0.1``" as the host
      identifier, then you must use "localhost" or "``127.0.0.1``" for
      *all* host settings for any MongoDB instances in the cluster. If
      you mix localhost addresses with remote host address, MongoDB will
      produce errors.

#. On all three (3) config server instances, issue the following
   command to start the :program:`mongod` process:

   .. code-block:: sh

      mongod --configsvr

   This starts a :program:`mongod` instance running on TCP port
   ``27018``, with the data stored in the ``/data/configdb`` path. All other
   :doc:`command line </reference/mongod>` and :doc:`configuration
   file </reference/configuration-options>` options are available for config
   server instances.

#. Start a :program:`mongos` instance issuing the following command:

   .. code-block:: sh

      mongos --configdb config0.mongodb.example.net,config1.mongodb.example.net,config2.mongodb.example.net --port 27017

#. Connect to the :program:`mongos` instance using the :program:`mongo`
   shell.

   .. code-block:: sh

      mongo mongos.mongodb.example.net

#. Add shards to the cluster.

   Run the following commands while connected to a :program:`mongos` to
   initialize the cluster.

   First, you need to tell the cluster where to find the individual
   shards. You can do this using the :dbcommand:`addShard` command.

   .. code-block:: javascript

      sh.addShard( "[hostname]:[port]" )

   For example:

   .. code-block:: javascript

      sh.addShard( "mongodb0.example.net:27027" )

   MongoDB will discover all other members of the replica set, if
   ``mongodb0.example.net:27027`` is a member of a replica set.

   .. note:: In production deployments, all shards should be replica sets.

   Repeat this step for each new shard in your cluster.

   .. optional::

      You may specify a "name" as an argument to the
      :dbcommand:`addShard`, as follows:

      .. code-block:: javascript

         db.runCommand( { addShard: mongodb0.example.net, name: "mongodb0" } )

      Or:

      .. code-block:: javascript

         sh.addShard( mongodb0.example.net, name: "mongodb0" )

      If you do not specify a shard name, then MongoDB will assign a
      name upon creation.

   .. note::

      .. versionchanged:: 2.0.3

      Before version 2.0.3, you must specify the shard in the following
      form:

      .. code-block:: sh

         replicaSetName/<seed1>,<seed2>,<seed3>

      For example, if the name of the replica set is "``repl0``", then
      your :method:`sh.addShard` command would be:

      .. code-block:: javascript

         sh.addShard( "repl0/mongodb0.example.net:27027,mongodb1.example.net:27017,mongodb2.example.net:27017" )

#. Enable sharding for any database that you want to shard.

   MongoDB enables sharding on a per-database basis. This is only a
   meta-data change and will not redistribute your data. To enable
   sharding for a given database, use the :dbcommand:`enableSharding`
   command or the :method:`sh.enableSharding()` shell function.

   .. code-block:: javascript

      db.runCommand( { enableSharding: [database] } )

   Or:

   .. code-block:: javascript

      sh.enableSharding([database])

   Replace ``[database]`` with the name of the database you wish to
   enable sharding on.

   .. note::

      MongoDB creates databases automatically upon their first use.

      Once you enable sharding for a database, MongoDB assigns a
      "primary" shard for that database, where MongoDB stores all data
      before sharding begins.

#. Enable sharding on a per-collection basis.

   Finally, you must explicitly specify collections to shard.  The
   collections must belong to a database for which you have enabled
   sharding. When you shard a collection, you also choose the shard
   key. To shard a collection, run the :dbcommand:`shardCollection`
   command or the :method:`sh.shardCollection()` shell helper.

   .. code-block:: javascript

      db.runCommand( { shardCollection: "[database].[collection]", key: "[shard-key]" } )

   Or:

   .. code-block:: javascript

      sh.shardCollection("[database].[collection]", "key")

   For example:

   .. code-block:: javascript

      db.runCommand( { shardCollection: "myapp.users", key: {username: 1} } )

   Or:

   .. code-block:: javascript

      sh.shardCollection("myapp.users", {username: 1})

   The choice of shard key is incredibly important: it affects
   everything about the cluster from the efficiency of your queries to
   the distribution of data. Furthermore, you cannot change a
   collection's shard key once it has been set.

   See the ":ref:`Shard Key Overview <sharding-shard-key>`" and the
   more in depth documentation of ":ref:`Shard Key Qualities
   <sharding-internals-shard-keys>`" to help you select better shard
   keys.

   If you do not specify a shard key, MongoDB will shard the
   collection using the ``_id`` field.

Cluster Management
------------------

Once you have a running shard cluster, you will need to maintain it.
This section describes common maintenance procedure, including: how to
add and remove nodes, how to manually split chunks, and how to disable
the balancer for backups.

.. _sharding-procedure-add-shard:

Adding a Shard to a Cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~

To add a shard to an *existing* shard cluster, use the following
procedure:

#. Connect to a :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. First, you need to tell the cluster where to find the individual
   shards. You can do this using the :dbcommand:`addShard` command or
   the :method:`sh.addShard()` helper:

   .. code-block:: javascript

      sh.addShard( "[hostname]:[port]" )

   Replace ``[hostname]`` and ``[port]`` with the hostname and TCP
   port number of where the shard is accessible.

   For example:

   .. code-block:: javascript

      sh.addShard( "mongodb0.example.net:27027" )

   .. note:: In production deployments, all shards should be replica sets.

   Repeat for each shard in your cluster.

   .. optional::

      You may specify a "name" as an argument to the
      :dbcommand:`addShard`, as follows:

      .. code-block:: javascript

         db.runCommand( { addShard: mongodb0.example.net, name: "mongodb0" } )

      Or:

      .. code-block:: javascript

         sh.addShard( mongodb0.example.net, name: "mongodb0" )

      If you do not specify a shard name, then MongoDB will assign a
      name upon creation.

   .. versionchanged:: 2.0.3

      Before version 2.0.3, you must specify the shard in the
      following form: the replica set name, followed by a forward
      slash, followed by a comma-separated list of seeds for the
      replica set. For example, if the name of the replica set is
      "myapp1", then your :method:`sh.addShard` command might resemble:

      .. code-block:: javascript

         sh.addShard( "repl0/mongodb0.example.net:27027,mongodb1.example.net:27017,mongodb2.example.net:27017" )

.. note::

   It may take some time for :term:`chunks <chunk>` to migrate to the
   new shard.

   See the ":ref:`Balancing and Distribution <sharding-balancing>`"
   section for an overview of the balancing operation and the ":ref:`Balancing Internals
   <sharding-balancing-internals>`" section for additional information.

.. _sharding-procedure-remove-shard:

Removing a Shard from a Cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To remove a :term:`shard` from a :term:`shard cluster`, you must:

- Migrate :term:`chunks <chunk>` to another shard or database.

- Ensure that this shard is not the "primary" shard for any databases in
  the cluster. If it is, move the "primary" status for these databases
  to other shards.

- Finally, remove the shard from the cluster's configuration.

.. note::

   To successfully migrate data from a shard, the :term:`balancer`
   process **must** be active.

The procedure to remove a shard is as follows:

#. Connect to a :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Determine the name of the shard you will be removing.

   You must specify the name of the shard. You may have specified this
   shard name when you first ran the :dbcommand:`addShard` command. If not,
   you can find out the name of the shard by running the
   :dbcommand:`listshards` or :dbcommand:`printShardingStatus`
   commands or the :method:`sh.status()` shell helper.

   The following examples will remove a shard named ``mongodb0`` from the cluster.

#. Begin removing chunks from the shard.

   Start by running the :dbcommand:`removeShard` command. This will
   start "draining" or migrating chunks from the shard you're removing
   to another shard in the cluster.

   .. code-block:: javascript

      db.runCommand( { removeshard: "mongodb0" } )

   This operation will return the following response immediately:

   .. code-block:: javascript

      { msg : "draining started successfully" , state: "started" , shard :"mongodb0" , ok : 1 }

   Depending on your network capacity and the amount of data in the
   shard, this operation can take anywhere from a few minutes to several
   days to complete.

#. View progress of the migration.

   You can run the :dbcommand:`removeShard` again at any stage of the
   process to view the progress of the migration, as follows:

   .. code-block:: javascript

      db.runCommand( { removeShard: "mongodb0" } )

   The output should look something like this:

   .. code-block:: javascript

      { msg: "draining ongoing" , state: "ongoing" , remaining: { chunks: 42, dbs : 1 }, ok: 1 }

   In the ``remaining`` sub-document ``{ chunks: xx, dbs: y }``, a
   counter displays the remaining number of chunks that MongoDB must
   migrate to other shards and the number of MongoDB databases that have
   "primary" status on this shard.

   Continue checking the status of the :dbcommand:`removeShard` command
   until the remaining number of chunks to transfer is 0.

#. Move any databases to other shards in the cluster as needed.

   This is only necessary when removing a shard that is also the
   "primary" shard for one or more databases.

   Issue the following command at the :program:`mongo` shell:

   .. code-block:: javascript

      db.runCommand( { movePrimary: "myapp", to: "mongodb1" })

   This command will migrate all remaining non-sharded data in the
   database named ``myapp`` to the shard named ``mongodb1``.

   .. warning::

      Do not run the :dbcommand:`movePrimary` until you have *finished*
      draining the shard.

   The command will not return until MongoDB completes moving all
   data. The response from this command will resemble the following:

   .. code-block:: javascript

      { "primary" : "mongodb1", "ok" : 1 }

#. Run :dbcommand:`removeShard` again to clean up all metadata
   information and finalize the shard removal, as follows:

   .. code-block:: javascript

      db.runCommand( { removeshard: "mongodb0" } )

   When successful, this command will return a document like this:

   .. code-block:: javascript

      { msg: "remove shard completed successfully" , stage: "completed", host: "mongodb0", ok : 1 }

Once the value of the ``stage`` field is "completed," you may safely
stop the processes comprising the ``mongodb0`` shard.

Chunk Management
----------------

This section describes various operations on
:term:`chunks <chunk>` in :term:`shard clusters <shard cluster>`. In
most cases MongoDB automates these processes; however, in some cases,
particularly when you're setting up a shard cluster, you may
need to create and manipulate chunks directly.

.. _sharding-procedure-create-split:

Splitting Chunks
~~~~~~~~~~~~~~~~

Normally, MongoDB splits a :term:`chunk` following inserts or updates
when a chunk exceeds the :ref:`chunk size <sharding-chunk-size>`.

You may want to split chunks manually if:

- you have a large amount of data in your cluster that is *not* split,
  as is the case after creating a shard cluster with existing data.

- you expect to add a large amount of data that would
  initially reside in a single chunk or shard.

.. example::

   You plan to insert a large amount of data as the result of an
   import process with :term:`shard key` values between ``300`` and
   ``400``, *but* all values of your shard key between ``250`` and
   ``500`` are within a single chunk.

Use :method:`sh.status()` to determine the current chunks ranges across
the cluster.

To split chunks manually, use either the :method:`sh.splitAt()` or
:method:`sh.splitFind()` helpers in the :program:`mongo` shell.
These helpers wrap the :dbcommand:`split` command.

.. example::

   The following command will split the chunk that contains
   the value of ``63109`` for the ``zipcode`` field:

   .. code-block:: javascript

      sh.splitFind( { "zipcode": 63109 } )

:method:`sh.splitFind()` will split the chunk that contains the *first* document returned
that matches this query into two equal components. MongoDB will split
the chunk so that documents that have half of the shard keys in will
be in one chunk and the documents that have other half of the shard
keys will be a second chunk. The query in :method:`sh.splitFind()` need
not contain the shard key, though it almost always makes sense to
query for the shard key in this case, and including the shard key will
expedite the operation.

However, the location of the document that this query finds with
respect to the other documents in the chunk does not affect how the
chunk splits.

Use :method:`sh.splitAt()` to split a chunk in two using the queried
document as the partition point:

.. code-block:: javascript

   sh.splitAt( { "zipcode": 63109 } )

.. warning::

   You cannot merge or combine chunks once you have split them.

.. _sharding-balancing-modify-chunk-size:

Modifying Chunk Size
~~~~~~~~~~~~~~~~~~~~

When you initialize a shard cluster, the default chunk size is 64
megabytes. This default chunk size works well for most deployments. However, if you
notice that automatic migrations are incurring a level of I/O that
your hardware cannot handle, you may want to reduce the chunk
size. For the automatic splits and migrations, a small chunk size
leads to more rapid and frequent migrations.

To modify the chunk size, use the following procedure:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the config database:

   .. code-block:: javascript

      use config

#. Issue the following :method:`save() <db.collection.save()>`
   operation:

   .. code-block:: javascript

      db.settings.save( { _id:"chunksize", value: <size> } )

   Where the value of ``<size>`` reflects the new chunk size in
   megabytes. Here, you're essentially writing a document whose values
   store the global chunk size configuration value.

.. note::

   The :setting:`chunkSize` and :option:`--chunkSize <mongos --chunkSize>`
   options, passed at runtime to the :program:`mongos` **do not**
   affect the chunk size after you have initialized the cluster.

   To eliminate confusion you should *always* set chunk size using the
   above procedure and never use the runtime options.

Modifying the chunk size has several limitations:

- Automatic splitting only occurs when inserting :term:`documents
  <document>` or updating existing documents.

- If you lower the chunk size it may take time for all chunks to split to
  the new size.

- Splits cannot be "undone."

If you increase the chunk size, existing chunks must grow through
insertion or updates until they reach the new size.

.. _sharding-balancing-manual-migration:

Migrating Chunks
~~~~~~~~~~~~~~~~

In most circumstances, you should let the automatic balancer
migrate :term:`chunks <chunk>` between :term:`shards <shard>`.
However, you may want to migrate chunks manually in a few cases:

- If you create chunks by presplitting the data in your collection,
  you will have to migrate chunks manually to distribute chunks
  evenly across the shards.

- If the balancer in an active cluster cannot distribute chunks within
  the balancing window, then you will have to migrate chunks manually.

To migrate chunks, use the :dbcommand:`moveChunk` command. 

.. note::

   To return a list of shards, use the :dbcommand:`listshards`
   command.

   Specify shard names using the :dbcommand:`addShard` command
   using the ``name`` argument. If you do not specify a name in the
   :dbcommand:`addShard` command, MongoDB will assign a name
   automatically.

The following example assumes that the field ``username`` is the
:term:`shard key` for a collection named ``users`` in the ``myapp``
database, and that the value ``smith`` exists within the :term:`chunk`
you want to migrate.

To move this chunk, you would issue the following command from a :program:`mongo`
shell connected to any :program:`mongos` instance.

.. code-block:: javascript

   db.adminCommand({moveChunk : "myapp.users", find : {username : "smith"}, to : "mongodb-shard3.example.net"})

This command moves the chunk that includes the shard key value "smith" to the
:term:`shard` named ``mongodb-shard3.example.net``. The command will
block until the migration is complete.

.. versionadded:: 2.2
   :dbcommand:`moveChunk` command now has the : ``_secondaryThrottle``
   paramenter. When set to ``true``, MongoDB ensures that
   :term:`secondary` members have replicated operations before allowing
   new chunk migrations.

.. warning::

   The :dbcommand:`moveChunk` command may produce an error message:

   .. code-block:: none

      The collection's metadata lock is already taken.

   when there are open :term:`cursors <cursor>` accessing the chunk to
   be moved. To fix this situation, either wait until the cursors
   complete their operation or remove the cursors.

.. index:: balancing; operations
.. _sharding-balancing-operations:

Balancer Operations
-------------------

This section provides an overview of common administrative procedures
related to balancing and the balancing process.

.. seealso:: ":ref:`sharding-balancing`" and the
   :dbcommand:`moveChunk` that provides manual :term:`chunk`
   migrations.

.. _sharding-balancing-check-lock:

Check the Balancer Lock
~~~~~~~~~~~~~~~~~~~~~~~

To see if the balancer process is active in your :term:`shard
cluster`, do the following:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the config database:

   .. code-block:: javascript

      use config

#. Use the following query to return the balancer lock:

   .. code-block:: javascript

      db.locks.find( { _id : "balancer" } ).pretty()

   You can also use the following shell helper to return the same
   information:

   .. code-block:: javascript

      sh.getBalancerState().pretty()

When this command returns, you will see output like the following:

.. code-block:: javascript

   {   "_id" : "balancer",
   "process" : "mongos0.example.net:1292810611:1804289383",
     "state" : 2,
        "ts" : ObjectId("4d0f872630c42d1978be8a2e"),
      "when" : "Mon Dec 20 2010 11:41:10 GMT-0500 (EST)",
       "who" : "mongos0.example.net:1292810611:1804289383:Balancer:846930886",
       "why" : "doing balance round" }


Here's what this tells you:

- The balancer originates from the :program:`mongos` running on the
  system with the hostname ``mongos0.example.net``.

- The value in the ``state`` field indicates that a :program:`mongos`
  has the lock. For version 2.0 and later, the value of an active lock
  is ``2``; for earlier versions the value is ``1``.

  .. note::

     Use the :method:`sh.isBalancerRunning()` helper in the
     :program:`mongo` shell to determine if the balancer is
     running, as follows:

     .. code-block:: javascript

        sh.isBalancerRunning()

.. _sharding-schedule-balancing-window:

Schedule the Balancing Window
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some situations, particularly when your data set grows slowly and a
migration can impact performance, it's useful to be able to ensure
that the balancer is active only at certain times.  Use the following
procedure to specify a window during which the :term:`balancer` will
be able to migrate chunks:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the config database:

   .. code-block:: javascript

      use config

#. Use an operation modeled on the following example :method:`update()
   <db.collection.update()>` operation to modify the balancer's
   window:

   .. code-block:: javascript

      db.settings.update({ _id : "balancer" }, { $set : { activeWindow : { start : "<start-time>", stop : "<stop-time>" } } }, true )

   Replace ``<start-time>`` and ``<end-time>`` with time values using
   two digit hour and minute values (e.g ``HH:MM``) that describe the
   beginning and end boundaries of the balancing window.
   These times will be evaluated relative to the time zone of each individual
   :program:`mongos` instance in the shard cluster.
   For instance, running the following
   will force the balancer to run between 11PM and 6AM local time only:

   .. code-block:: javascript

      db.settings.update({ _id : "balancer" }, { $set : { activeWindow : { start : "23:00", stop : "6:00" } } }, true )

.. note::

   The balancer window must be sufficient to *complete* the migration
   of all data inserted during the day.

   As data insert rates can change based on activity and usage
   patterns, it is important to ensure that the balancing window you
   select will be sufficient to support the needs of your deployment.

Remove Balancing Window Schedule
--------------------------------

If you have :ref:`set the balancing window
<sharding-schedule-balancing-window>` and wish to remove the schedule
so that the balancer is always running, issue the following sequence
of operations:

.. code-block:: javascript

   use config
   db.settings.update({ _id : "balancer" }, { $unset : { activeWindow : true })

.. _sharding-balancing-disable-temporally:

Disable the Balancer
~~~~~~~~~~~~~~~~~~~~

By default the balancer may run at any time and only moves chunks as
needed. To disable the balancer for a short period of time and prevent
all migration, use the following procedure:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to disable the balancer:

   .. code-block:: javascript

      sh.setBalancerState(false)

#. Later, issue the following command to enable the balancer:

   .. code-block:: javascript

      sh.setBalancerState(true)

.. note::

   If a balancing round is in progress, the system will complete the
   current round before the balancer is officially disabled.  After
   disabling, you can use the :method:`sh.getBalancerState()` shell
   function to determine whether the balancer is in fact disabled.

The above process and the :method:`sh.setBalancerState()` helper provide a
wrapper on the following process, which may be useful if you need to
run this operation from a driver that does not have helper functions:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the config database:

   .. code-block:: javascript

      use config

#. Issue the following update to disable the balancer:

   .. code-block:: javascript

      db.settings.update( { _id: "balancer" }, { $set : { stopped: true } } , true );

#. To enable the balancer again, alter the value of "stopped" as follows:

   .. code-block:: javascript

      db.settings.update( { _id: "balancer" }, { $set : { stopped: false } } , true );

.. index:: config servers; operations
.. _sharding-procedure-config-server:

Config Server Maintenance
-------------------------

Config servers store all shard cluster metadata, perhaps most notably,
the mapping from :term:`chunks <chunk>` to :term:`shards <shard>`.
This section provides an overview of the basic
procedures to migrate, replace, and maintain these servers.

.. seealso:: :ref:`sharding-config-server`

Upgrading from One Config Server to Three Config Servers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For redundancy, all production :term:`shard clusters <shard cluster>` should
deploy three config servers processes on three different machines.

Do not use only a single config server for production deployments.
Consider deploying a single config server for testing.
immediately. The following process shows how to upgrade from one to
three config servers.

#. Shut down all existing MongoDB processes. This includes:

   - all :program:`mongod` instances or :term:`replica sets <replica set>`
     that provide your shards.

   - the :program:`mongod` instance that provides your existing config
     database.

   - all :program:`mongos` instances in your cluster.

#. Copy the entire :setting:`dbpath` file system tree from the
   existing config server to the two machines that will provide the
   additional config servers. These commands, issued on the system
   with the existing config database, ``mongo-config0.example.net`` may
   resemble the following:

   .. code-block:: sh

      rsync -az /data/configdb mongo-config1.example.net:/data/configdb
      rsync -az /data/configdb mongo-config2.example.net:/data/configdb

#. Start all three config servers, using the same invocation that you
   used for the single config server.

   .. code-block:: sh

      mongod --configsvr

#. Restart all shard :program:`mongod` and :program:`mongos` processes.

.. _sharding-process-config-server-migrate-same-hostname:

Migrating Config Servers with the Same Hostname
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use this process when you need to migrate a config server to a new
system but the new system will be accessible using the same host
name.

#. Shut down the config server that you're moving.

   This will render all config data for your cluster :ref:`read only
   <sharding-config-server>`."

#. Change the DNS entry that points to the system that provided the old
   config server, so that the *same* hostname points to the new
   system.

   How you do this depends on how you organize your DNS and
   hostname resolution services.

#. Move the entire :setting:`dbpath` file system tree from the old
   config server to the new config server. This command, issued on the
   old config server system, may resemble the following:

   .. code-block:: sh

      rsync -az /data/configdb mongo-config0.example.net:/data/configdb

#. Start the config instance on the new system. The default invocation
   is:

   .. code-block:: sh

      mongod --configsrv

When you start the third config server, your cluster will become
writable and it will be able to create new splits and migrate chunks
as needed.

.. _sharding-process-config-server-migrate-different-hostname:

Migrating Config Servers with Different Hostnames
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use this process when you need to migrate a config database to a new
server and it *will not* be accessible via the same hostname. If
possible, avoid changing the hostname so that you can use the
:ref:`previous procedure <sharding-process-config-server-migrate-same-hostname>`.

#. Shut down the :ref:`config server <sharding-config-server>` you're moving.

   This will render all config data for your cluster "read only:"

   .. code-block:: sh

      rsync -az /data/configdb mongodb.config2.example.net:/data/configdb

#. Start the config instance on the new system. The default invocation
   is:

   .. code-block:: sh

      mongod --configsrv

#. Shut down all existing MongoDB processes. This includes:

   - all :program:`mongod` instances or :term:`replica sets <replica set>`
     that provide your shards.

   - the :program:`mongod` instances that provide your existing
     config databases.

   - all :program:`mongos` instances in your cluster.

#. Restart all :program:`mongod` processes that provide the shard
   servers.

#. Update the :option:`--configdb <mongos --configdb>` parameter (or
   :setting:`configdb`) for all :program:`mongos` instances and
   restart all :program:`mongos` instances.

Replacing a Config Server
~~~~~~~~~~~~~~~~~~~~~~~~~

Use this procedure only if you need to replace one of your config
servers after it becomes inoperable (e.g. hardware failure.) This
process assumes that the hostname of the instance will not change. If
you must change the hostname of the instance, use the process for
:ref:`migrating a config server to a different hostname
<sharding-process-config-server-migrate-different-hostname>`.

#. Provision a new system, with the same hostname as the previous
   host.

   You will have to ensure that the new system has the same IP address
   and hostname as the system it's replacing *or* you will need to
   modify the DNS records and wait for them to propagate.

#. Shut down *one* (and only one) of the existing config servers. Copy
   all this host's :setting:`dbpath` file system tree from the current system
   to the system that will provide the new config server. This
   command, issued on the system with the data files, may resemble the
   following:

   .. code-block:: sh

      rsync -az /data/configdb mongodb.config2.example.net:/data/configdb

#. Restart the config server process that you used in the previous
   step to copy the data files to the new config server instance.

#. Start the new config server instance. The default invocation is:

   .. code-block:: sh

      mongod --configsrv

Backup Cluster Metadata
~~~~~~~~~~~~~~~~~~~~~~~

The shard cluster will remain operational [#read-only]_ without one
of the config databases :program:`mongod` instances, creating a backup
of the cluster metadata from the config database is straight forward:

#. Shut down one of the :term:`config databases <config database>`.

#. Create a full copy of the data files (i.e. the path specified by
   the :setting:`dbpath` option for the config instance.)

#. Restart the original configuration server.

.. seealso:: :doc:`backups`.

.. [#read-only] While one of the three config servers unavailable, no
   the cluster cannot split any chunks nor can it migrate chunks
   between shards. Your application will be able to write data to the
   cluster. The ":ref:`sharding-config-server`" section of the
   documentation provides more information on this topic.

.. index:: troubleshooting; sharding
.. index:: sharding; troubleshooting
.. _sharding-troubleshooting:

Troubleshooting
---------------

The two most important factors in maintaining a successful shard cluster are:

- :ref:`choosing an appropriate shard key <sharding-internals-shard-keys>` and

- :ref:`sufficient capacity to support current and future operations
  <sharding-capacity-planning>`.

You can prevent most issues encountered with sharding by ensuring that
you choose the best possible :term:`shard key` for your deployment and
ensure that you are always adding additional capacity to your cluster
well before the current resources become saturated. Continue reading
for specific issues you may encounter in a production environment.

.. _sharding-troubleshooting-not-splitting:

All Data Remains on One Shard
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Your cluster must have sufficient data for sharding to make
sense. Sharding works by migrating chunks between the shards until
each shard has roughly the same number of chunks.

The default chunk size is 64 megabytes. MongoDB will not begin
migrations until the imbalance of chunks in the cluster exceeds the
:ref:`migration threshold <sharding-migration-thresholds>`. While the
default chunk size is configurable with the :setting:`chunkSize`
setting, these behaviors help prevent unnecessary chunk migrations,
which can degrade the performance of your cluster as a whole.

If you have just deployed a shard cluster, make sure that you have
enough data to make sharding effective. If you do not have sufficient
data to create more than eight 64 megabyte chunks, then all data will
remain on one shard. Either lower the :ref:`chunk size
<sharding-chunk-size>` setting, or add more data to the cluster.

As a related problem, the system will split chunks only on
inserts or updates, which means that if you configure sharding and do not
continue to issue insert and update operations, the database will not
create any chunks. You can either wait until your application inserts
data *or* :ref:`split chunks manually <sharding-procedure-create-split>`.

Finally, if your shard key has a low :ref:`cardinality
<sharding-shard-key-cardinality>`, MongoDB may not be able to create
sufficient splits among the data.

One Shard Receives too much Traffic
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some situations, a single shard or a subset of the cluster will
receive a disproportionate portion of the traffic and workload. In
almost all cases this is the result of a shard key that does not
effectively allow :ref:`write scaling <sharding-shard-key-write-scaling>`.

It's also possible that you have "hot chunks." In this case, you may
be able to solve the problem by splitting and then migrating parts of
these chunks.

In the worst case, you may have to consider re-sharding your data
and :ref:`choosing a different shard key <sharding-internals-choose-shard-key>`
to correct this pattern.

The Cluster does not Balance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you have just deployed your shard cluster, you may want to
consider the :ref:`troubleshooting suggestions for a new cluster where
data remains on a single shard <sharding-troubleshooting-not-splitting>`.

If the cluster was initially balanced, but later developed an uneven
distribution of data, consider the following possible causes:

- You have deleted or removed a significant amount of data from the
  cluster. If you have added additional data, it may have a
  different distribution with regards to its shard key.

- Your :term:`shard key` has low :ref:`cardinality <sharding-shard-key-cardinality>`
  and MongoDB cannot split the chunks any further.

- Your data set is growing faster than the balancer can distribute
  data around the cluster. This is uncommon and
  typically is the result of:

  - a :ref:`balancing window <sharding-schedule-balancing-window>` that
    is too short, given the rate of data growth.

  - an uneven distribution of :ref:`write operations
    <sharding-shard-key-write-scaling>` that requires more data
    migration. You may have to choose a different shard key to resolve
    this issue.

  - poor network connectivity between shards, which may lead to chunk
    migrations that take too long to complete. Investigate your
    network configuration and interconnections between shards.

Migrations Render Cluster Unusable
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If migrations impact your cluster or application's performance,
consider the following options, depending on the nature of the impact:

#. If migrations only interrupt your clusters sporadically, you can
   limit the :ref:`balancing window
   <sharding-schedule-balancing-window>` to prevent balancing activity
   during peak hours. Ensure that there is enough time remaining to
   keep the data from becoming out of balance again.

#. If the balancer is always migrating chunks to the detriment of
   overall cluster performance:

   - You may want to attempt :ref:`decreasing the chunk size <sharding-balancing-modify-chunk-size>`
     to limit the size of the migration.

   - Your cluster may be over capacity, and you may want to attempt to
     :ref:`add one or two shards <sharding-procedure-add-shard>` to
     the cluster to distribute load.

It's also possible, that your shard key causes your
application to direct all writes to a single shard. This kind of
activity pattern can require the balancer to migrate most data soon after writing
it. Consider redeploying your cluster  with a shard key that provides
better :ref:`write scaling <sharding-shard-key-write-scaling>`.

Disable Balancing During Backups
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If MongoDB migrates a :term:`chunk` during a :doc:`backup
</administration/backups>`, you can end with an inconsistent snapshot
of your :term:`shard cluster`. Never run a backup while the balancer is
active. To ensure that the balancer is inactive during your backup
operation:

- Set the :ref:`balancing window <sharding-schedule-balancing-window>`
  so that the balancer is inactive during the backup. Ensure that the
  backup can complete while you have the balancer disabled.

- :ref:`manually disable the balancer <sharding-balancing-disable-temporally>`
  for the duration of the backup procedure.

Confirm that the balancer is not active using the
:method:`sh.getBalancerState()` method before starting a backup
operation. When the backup procedure is complete you can reactivate
the balancer process.
