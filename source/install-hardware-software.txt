==================================
Hardware and Software Requirements
==================================

.. default-domain:: mongodb

Sizing Guide
------------

MMS Package (front end)
~~~~~~~~~~~~~~~~~~~~~~~

This package requires a minimum of 4 x 2ghz+ CPU cores and 16GB of RAM to get
started. This setup has enough capacity to monitor and backup approximately 200
servers, including all replica set members, config servers, and
:program:`mongos` instances.

There are no specific hard disk requirements as all data used by this package
persists in the configured MongoDB databases. Additional servers increases the
number of backed up front-end instances.

.. _backup-hardware-requirements:

Backup Daemon Package (back end)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A server running the Daemon package will act as a hidden secondary for every
replica set assigned to it. 4 x 2ghz+ CPU cores and 16GB of RAM will be
adequate for most loads generated by this activity.

Since it will not deal with read traffic, a server running a Backup Daemon will
typically be able to handle more replica sets then a server with production
traffic.

Disk size limits the number of backups assigned to a daemon. The server running
this package needs enough disk to hold a full copy of every database it backs
up. It also needs enough write I/O throughput to apply oplogs to each backup.

For example, imagine a sharded cluster with four 200GB shards. Looking at a
secondary for each one of the shards it appears the disk averages 15MB/sec of
write traffic. A Backup Daemon assigned these four shards would need at least
800GB of disk space (in reality more to handle growth) and that disk partition
would need to be able to write more than 60MB/sec.

Point in time restore capability requires enough space to reconstruct a
snapshot of a backup on the daemon. In the example above, a point in time
restore of this cluster would required another 800GB of temporary space on the
daemon during the restore. Snapshot restores do not require additional disk
space.

MMS Metadata Database
~~~~~~~~~~~~~~~~~~~~~

Each replica set member should have 4 x 2ghz+ CPU cores and 16GB of RAM. 200GB
of disk space will be adequate for the first 200 servers.

Because this data updates frequently, we recommend use of a high-end disk,
preferably SSDs. If the system reaches the capacity of this server, upgrade
memory or bring additional replica sets online and reconfigure the MMS
application to split different types of MMS data between these replica sets.

Snapshot Storage / Blockstore Database
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To calculate the amount of storage needed to store a replica set backup in the
Blockstore database, look at the file size of the replica set to back up,
gigabytes of oplog per hour generated by the replica set, compression ratio of
the data, and the configured snapshot retention schedule.

Using the four shard, 200GB per shard cluster example from above, also
add 2GB/day of oplogs generated per shard or 8GB/day total across the
cluster.

If the longest stored snapshot has a one year retention
period, the approximate amount of data in the blockstore will be 
800GB + (8GB * 365 days) or 3720GB. If the data gets a 4:1 compression
ratio, which is an average seen in the hosted MMS, the blockstore
space required will actually be 930GB.

930GB is a conservative estimate because it assumes 8GB of
oplog in one day changes 8GB of data on disk. The other extreme is
that 8GB of oplog could all be ``$inc`` operations on the same
document. In that case, 8GB of oplog could only change 4 bytes on
disk.

In practice the number will be somewhere in between depending on
the replica sets insert/update/delete patterns.

Based on looking at the MMS hosted service, a good rule of thumb is a replica
set will take up 2x - 3x its size in the Blockstore.

Medium grade HDDs will have enough I/O throughput to handle the load
of the Blockstore. Each replica set member should have 4 x 2ghz+ CPU
cores. We recommend 8GB of RAM for every 1TB disk of Blockstore to
provide good snapshot and restore speed.

Hardware Requirements
---------------------

Monitoring Server
~~~~~~~~~~~~~~~~~

To run the On-Prem Monitoring server, you must use a 64-bit server,
with requirements according to the following table:

.. list-table::
   :header-rows: 1
   :widths: 20, 12, 8, 15, 15

   * - **Number of Monitored Hosts**
     - **CPU Cores**
     - **RAM**
     - **Storage Capacity**
     - **Storage IOPS/s**
   * - Up to 400 monitored hosts
     - 4+
     - 15 GB
     - 200 GB
     - 500
   * - Up to 2000 monitored hosts
     - 8+
     - 15 GB
     - 500 GB
     - 10000+ (SSD)
   * - More than 2000 hosts
     - Contact MMS
     -
     -
     -

For reference: an AWS EC2 Standard Extra Large (i.e. m1.xlarge) with a
provisioned 500 IOP/s EBS volume supported the 400-host configuration
above. An AWS EC2 High I/O Quadruple Extra Large (hi1.4xlarge)
supported the 2000 host configuration above.

For the best results, On-Prem MMS instances require SSD-backed
storage.

Backup Server
~~~~~~~~~~~~~

Refer to the :ref:`Sizing Guide <backup-hardware-requirements>` above for the
backup server hardware requirements.

Combining Components
~~~~~~~~~~~~~~~~~~~~

Each component does not require a dedicated server. Combine CPU and
RAM requirements based on your environment. Each component should 
still have its own disk partition with the recommended amount of
storage space and I/O throughput.

One possible configuration used in the MMS hosted environment has multiple
RAIDs attached to each high-end physical server. Each server may 
run a combination of a Blockstore primary/secondary, a Backup
Daemon, and a MMS Metadata primary/secondary. The front-end package
can run on a much smaller server as it only has modest CPU and RAM
requirements.

Software Requirements
---------------------

Operating System
~~~~~~~~~~~~~~~~

|monitoring| has the following *required* dependencies:

MMS requires 64-bit Linux. MMS supports the following distributions:

- CentOS 5 or later,

- Red Hat Enterprise Linux 5, or later, or

- SUSE 11 or Later,

- Amazon Linux AMI (latest version only,)

- Ubuntu 12.04 or later.

MongoDB
~~~~~~~

The MongoDB databases backing MMS best be MongoDB 2.4.6 or later.

The MongoDB replica sets and sharded clusters tobe backed up must be
running MongoDB 2.4.3 or later.

SMTP
~~~~

While many Linux server-oriented distributions include a local SMTP
server by default (e.g. Postfix, Exim, Sendmail,) you may also configure
MMS to send mail via 3rd party providers including Gmail and Sendgrid.

MMS requires email for fundamental server functionality such as
password reset and alerts.

SNMP
~~~~



Authentication
~~~~~~~~~~~~~~

|mms| has the following *optional* dependencies.

- A Twilio API account for SMS alerting integration.
- A Graphite hostname / port for charting the MMS server's internal health.
- An SNMP trap receiver for periodic heartbeat traps about MMS
  server's internal health.

Web Browsers
~~~~~~~~~~~~

|mms| supports the following browsers:

- recent versions of Firefox, Chrome, and Safari
- Internet Explorer, Versions 9 and later.

The MMS application will display a warning on non-supported browsers.

Configure a Highly Available MMS Application Server
---------------------------------------------------

The On-Prem MMS app server is horizontally scalable. To take advantage of this
capability, configure each of the MMS servers ``conf-mms.properties`` file to
point their ``mms.centralUrl`` property to a load balancer. Then configure the
load balancer to balance between the pool of MMS app servers.
   
The MMS app servers are stateless between requests. Any app server can handle
requests as long as they read from the same backing MongoDB replica set. Where
multiple MMS app servers are present for high availability, we recommend a
replica set as the backing MongoDB (rather than a stand-alone).

Procedure
~~~~~~~~~

To configure a highly available MMS deployment:

1. Configure a load balancer with the pool of MMS app server locations. Because
   load balancers are specific to their environment and can have different
   configurations, we don't have a recommendation for load balancer
   configurations.

#. Configure the ``mms.centralUrl`` property of each MMS app server's
   ``conf-mms.properties`` file to point to the load balanced URL.

#. Edit the ``conf-mms.properties`` file on each MMS app server to deÔ¨Åne the
   replication hosts for the backing MongoDB.

#. Synchronize the ``/etc/mongodb-mms/gen.key`` file across all application
   servers. In MMS 1.3 and later, the application uses this file to encrypt
   sensitive information before storing the data in a database.

MMS Package (Front-end)
~~~~~~~~~~~~~~~~~~~~~~~

Use this stateless component with a load balancer (either layer 4 or 
layer 7) in front of the instances of this package to distribute requests.

To handle N servers for the desired capacity, more than N servers should be 
behind the load balancer to ensure a certain number of losses does not 
degrade performance. N+1 or N+2 are common configurations.

Backup Daemon Package (Back-end)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

While the system protects the daemon against data loss, a lost daemon 
requires manaual intervention.

A daemon exclusively owns a set of backups. 
If it is down, those backups do not continue. Built-in MMS alerts notify 
the system administrator backups have fallen behind. If the daemon is 
repairable, backups will continue where they left off. If the daemon is not 
repairable, assign the backups manually to a different daemon with the Admin 
section of the MMS service.

Moving a backup from one daemon to another involves
a self restore of the previous snapshot from the 
Blockstore. This makes the daemon is somewhat ephemeral and the
durability of the backup data lives in the Blockstore replica set.

MMS Metadata Database / Blockstore Database
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

These should be standard MongoDB replica sets. Follow all MongoDB best 
practices to make these replica sets highly available and durable.

Firewall
~~~~~~~~

The front-end package will default to running web servers on ports
8080 and 8081.

