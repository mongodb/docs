.. _k8s-troubleshooting:

==================================
Troubleshooting the |k8s-op-short|
==================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

.. _get-resource-status:

Get Status of a Deployed Resource
---------------------------------

To find the status of a resource deployed with the |k8s-op-short|,
invoke one of the following commands:

- For |onprem| resource deployments:

  .. code-block:: sh

     kubectl get <resource-name> -n <namespace> -o yaml

  The ``status.applicationDatabase.phase`` field displays the
  Application Database resource deployment status. The
  ``status.opsManager.phase`` field displays the |onprem| resource
  deployment status.

- For MongoDB resource deployments:

  .. code-block:: sh

     kubectl get mdb <resource-name> -n <namespace> -o yaml

  The ``status.phase`` field displays the MongoDB resource deployment
  status.

The following key-value pairs describe the resource deployment statuses:

.. list-table::
   :header-rows: 1
   :widths: 20 80

   * - Key
     - Value

   * - ``message``
     - Message explaining why the resource is in a ``Pending`` or
       ``Failed`` state.
   * - ``phase``
     -
       .. list-table::
          :header-rows: 1
          :widths: 20 80

          * - Status
            - Meaning

          * - ``Pending``
            - The |k8s-op-short| is unable to reconcile the resource
              deployment state. This happens when a reconciliation
              times out or if the |k8s-op-short| requires you to take
              action for the resource to enter a running state.

              If a resource is pending because a reconciliation timed
              out, the |k8s-op-short| attempts to reconcile the
              resource state in 10 seconds.

          * - ``Reconciling``
            - The |k8s-op-short| is reconciling the resource state.

              Resources enter this state after you create or update
              them or if the |k8s-op-short| is attempting to reconcile
              a resource previously in a ``Pending`` or ``Failed``
              state.

              The |k8s-op-short| attempts to reconcile the resource
              state in 10 seconds.

          * - ``Running``
            - The resource is running properly.

          * - ``Failed``
            - The resource is not running properly. The ``message``
              field provides additional details.

              The |k8s-op-short| attempts to reconcile the resource
              state in 10 seconds.

   * - ``lastTransition``
     - |iso8601-time| when the last reconciliation happened.

   * - ``link``
     - Deployment |url| in |mms|.

   * - Resource specific fields
     - For descriptions of these fields, see
       :doc:`/reference/k8s-operator-specification`.

.. example::

   If you want to see what the status of a replica set named
   ``my-replica-set`` in the ``developer`` namespace, run:

   .. code-block:: sh

      kubectl get mdb my-replica-set -n developer -o yaml

   If ``my-replica-set`` is running, you should see:

   .. code-block:: yaml

      status:
          lastTransition: "2019-01-30T10:51:40Z"
          link: http://ec2-3-84-128-187.compute-1.amazonaws.com:9080/v2/5c503a8a1b90141cbdc60a77
          members: 1
          phase: Running
          version: 4.2.2-ent

   If ``my-replica-set`` is not running, you should see:

   .. code-block:: yaml

      status:
        lastTransition: 2019-02-01T13:00:24Z
        link: http://ec2-34-204-36-217.compute-1.amazonaws.com:9080/v2/5c51c040d6853d1f50a51678
        members: 1
        message: 'Failed to create/update replica set in Ops Manager: Status: 400 (Bad Request),
          Detail: Something went wrong validating your Automation Config. Sorry!'
        phase: Failed
        version: 4.2.2-ent

Review the Logs
---------------

.. _review-k8s-op-logs:

Review Logs from the |k8s-op-short|
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To review the |k8s-op-short| logs, invoke this command:

.. code-block:: sh

   kubectl logs -f deployment/mongodb-enterprise-operator -n <metadata.namespace>

You could check the :opsmgr:`Ops Manager Logs </tutorial/view-logs>` as
well to see if any issues were reported to |onprem|.

.. _find-one-k8s-pod:

Find a Specific Pod
~~~~~~~~~~~~~~~~~~~

To find which pods are available, invoke this command first:

.. code-block:: sh

   kubectl get pods -n <metadata.namespace>

.. seealso::
   |k8s| documentation on `kubectl get <https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#get>`__.

.. _review-all-k8s-logs:
.. _review-one-k8s-pod:

Review Logs from Specific Pod
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you want to narrow your review to a specific |k8s-pod|, you can
invoke this command:

.. code-block:: sh

   kubectl logs <podName> -n <metadata.namespace>

.. example::

   If your :term:`replica set` is labeled ``myrs``, the |k8s-pod| log
   command is invoked as:

   .. code-block:: sh

      kubectl logs myrs-0 -n <metadata.namespace>

   This returns the :ref:`Automation Agent Log <agent-logs>` for this
   replica set.

View All |k8s-mdbrsc| Specifications
------------------------------------

To view all |k8s-mdbrsc| specifications in the provided
|k8s-ns|:

.. code-block:: shell

   kubectl get mdb -n <namespace>

.. example::

   To read details about the ``dublin`` standalone resource, invoke
   this command:

   .. code-block:: shell

      kubectl get mdb dublin -n <namespace> -o yaml

   This returns the following response:

   .. code-block:: yaml
      :copyable: false

      apiVersion: mongodb.com/v1
      kind: MongoDB
      metadata:
        annotations:
          kubectl.kubernetes.io/last-applied-configuration: |
            {"apiVersion":"mongodb.com/v1","kind":"MongoDB","metadata":{"annotations":{},"name":"dublin","namespace":"mongodb"},"spec":{"credentials":"credentials","persistent":false,"podSpec":{"memory":"1G"},"project":"my-om-config","type":"Standalone","version":"4.0.0-ent"}}
        clusterDomain: ""
        creationTimestamp: 2018-09-12T17:15:32Z
        generation: 1
        name: dublin
        namespace: mongodb
        resourceVersion: "337269"
        selfLink: /apis/mongodb.com/v1/namespaces/mongodb/mongodbstandalones/dublin
        uid: 7442095b-b6af-11e8-87df-0800271b001d
      spec:
        credentials: my-credentials
        type: Standalone
        persistent: false
        podSpec:
          memory: 1G
        project: my-om-config
        version: 4.2.2-ent

.. _k8s-rollback-failure:

Restore StatefulSet that Failed to Deploy
-----------------------------------------

A StatefulSet |k8s-pod| may hang with a status of ``Pending`` if it
encounters an error during deployment.

``Pending`` |k8s-pods| do not automatically terminate, even if you
make *and apply* configuration changes to resolve the error.

To return the StatefulSet to a healthy state, apply the configuration
changes to the MongoDB resource in the ``Pending`` state, then delete
those pods.

.. example::

   A host system has a number of running |k8s-pods|:

   .. code-block:: sh
      :copyable: false
      :emphasize-lines: 5

      kubectl get pods

      my-replica-set-0     1/1 Running 2 2h
      my-replica-set-1     1/1 Running 2 2h
      my-replica-set-2     0/1 Pending 0 2h

   ``my-replica-set-2`` is stuck in the ``Pending`` stage. To gather
   more data on the error, run the following:

   .. code-block:: sh
      :copyable: false
      :emphasize-lines: 1,5

      kubectl describe pod my-replica-set-2

      <describe output omitted>

      Warning FailedScheduling 15s (x3691 over 3h) default-scheduler 0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient memory.

   The output indicates an error in memory allocation.

   Updating the memory allocations in the MongoDB resource is
   insufficient, as the pod does not terminate automatically after
   applying configuration updates.

   To remedy this issue, update the configuration, apply the
   configuration, then delete the hung pod:

   .. code-block:: sh

      vi <my-replica-set>.yaml

      kubectl apply -f <my-replica-set>.yaml

      kubectl delete pod my-replica-set-2

   Once this hung pod is deleted, the other pods restart with your new
   configuration as part of rolling upgrade of the Statefulset.

.. note::

   To learn more about this issue, see
   `Kubernetes Issue 67250 <https://github.com/kubernetes/kubernetes/issues/67250>`__.

.. _replace-config-file:

Replace a ConfigMap to Reflect Changes
---------------------------------------

If you are unable to modify or redeploy an already-deployed configMap file using the *kubectl apply* command, invoke the following 
instead:

.. code-block:: shell

      kubectl replace -f <my-config-map>.yaml 

This will delete and re-create a resource. This command is useful in cases where you need to update resource files that cannot be updated
once initialized or want to make an immediate recursive change. 

.. _remove-k8s-resource:

Remove a |k8s-mdbrsc|
---------------------

To remove any instance that |k8s| deployed, you must use |k8s|.

.. important::

   You can only use the |k8s-op-short| to remove |k8s|\-deployed
   instances. If you use |mms| to remove the instance, |mms| throws an
   error.

.. example::

   To remove a single MongoDB instance you created using |k8s|:

   .. code-block:: shell

      kubectl delete mdb <name> -n <metadata.namespace>

   To remove all MongoDB instances you created using |k8s|:

   .. code-block:: shell

      kubectl delete mdb --all -n <metadata.namespace>

.. _remove-k8s-operator:

Remove the |k8s-op-short|
-------------------------

To remove the |k8s-op-short|:

1. :ref:`Remove all Kubernetes resources <remove-k8s-resource>`:

   .. code-block:: shell

      kubectl delete mdb --all -n <metadata.namespace>

2. Remove the |k8s-op-short|:

   .. code-block:: shell

      kubectl delete deployment mongodb-enterprise-operator -n <metadata.namespace>

.. _remove-k8s-namespace:

Remove the |k8s-ns|
-------------------

To remove the |k8s-ns|:

1. :ref:`Remove all Kubernetes resources <remove-k8s-resource>`:

   .. code-block:: shell

      kubectl delete mdb --all -n <metadata.namespace>

2. Remove the |k8s-ns|:

   .. code-block:: shell

      kubectl delete namespace <metadata.namespace>

.. _remove-k8s-crds:

Remove the |k8s-crds|
---------------------

To remove the |k8s-crds|:

1. :ref:`Remove all Kubernetes resources <remove-k8s-resource>`:

   .. code-block:: shell

      kubectl delete mdb --all -n <metadata.namespace>

2. Remove the |k8s-crds|:

   .. code-block:: shell

      kubectl delete crd --all

.. _k8s-disable-feature-controls:

Disable |onprem| Feature Controls
---------------------------------

When you manage an |onprem| project through the |k8s-op-short|, the
|k8s-op-short| places the ``EXTERNALLY_MANAGED_LOCK`` :opsmgr:`feature
control policy
</reference/api/controlled-features/get-all-feature-control-policies/#response>`
on the project. This policy disables certain features in the |onprem|
application that might compromise your |k8s-op-short| configuration. If
you need to use these blocked features, you can remove the policy
through the :opsmgr:`feature controls API
</reference/api/controlled-features/update-controlled-features-for-one-project/>`,
make changes in the |onprem| application, and then restore the original
policy through the :opsmgr:`API
</reference/api/controlled-features/update-controlled-features-for-one-project/>`.

.. warning::

   The following procedure enables you to use features in the |onprem|
   application that are otherwise blocked by the |k8s-op-short|.

1. :opsmgr:`Retrieve the feature control policies
   </reference/api/controlled-features/get-controlled-features-for-one-project/>`
   for your |onprem| project.

   .. code-block:: sh

      curl --user "{USERNAME}:{APIKEY}" --digest \
           --header "Accept: application/json" \
           --header "Content-Type: application/json" \
           --include \
           --request GET "https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true"      
   
   .. important::

      Save the response that the API returns. After you make changes in
      the |onprem| application, you must add these policies back to
      the project. 

   Your response should be similar to:

   .. code-block:: json

      {
       "created": "2020-02-25T04:09:42Z",
       "externalManagementSystem": {
         "name": "mongodb-enterprise-operator",
         "systemId": "6d6c139ae5528707b6e8e3b2",
         "version": "1.4.2"
       },
       "policies": [
         {
           "disabledParams": [],
           "policy": "EXTERNALLY_MANAGED_LOCK"
         },
         {
           "disabledParams": [],
           "policy": "DISABLE_AUTHENTICATION_MECHANISMS"
         }
       ],
       "updated": "2020-02-25T04:10:12Z"
     }

#. :opsmgr:`Update
   </reference/api/controlled-features/update-controlled-features-for-one-project/>`
   the ``policies`` array with an empty list: 

   .. code-block:: sh

      curl --user "{USERNAME}:{APIKEY}" --digest \
           --header "Accept: application/json" \
           --header "Content-Type: application/json" \
           --include \
           --request PUT "https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true" \
           --data 
             '{
               "externalManagementSystem": {
                 "name": "mongodb-enterprise-operator",
                 "systemId": "6d6c139ae5528707b6e8e3b2",
                 "version": "1.4.2"
               },
               "policies": []
             }'

   The previously blocked features are now available in the
   |onprem| application.

#. Make your changes in the |onprem| application.

#. :opsmgr:`Update
   </reference/api/controlled-features/update-controlled-features-for-one-project/>`
   the ``policies`` array with the original feature control policies: 

   .. code-block:: sh

      curl --user "{USERNAME}:{APIKEY}" --digest \
           --header "Accept: application/json" \
           --header "Content-Type: application/json" \
           --include \
           --request PUT "https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true" \
           --data 
             '{
               "externalManagementSystem": {
                 "name": "mongodb-enterprise-operator",
                 "systemId": "6d6c139ae5528707b6e8e3b2",
                 "version": "1.4.2"
               },
               "policies": [
                 {
                   "disabledParams": [],
                   "policy": "EXTERNALLY_MANAGED_LOCK"
                 },
                 {
                   "disabledParams": [],
                   "policy": "DISABLE_AUTHENTICATION_MECHANISMS"
                 }
               ]
             }'

   The features are now blocked again, preventing you from making
   further changes through the |onprem| application. However, the
   |k8s-op-short| retains any changes you made in the |onprem|
   application while features were available.

Tune MongoDB |k8s| Resource Docker Images
-----------------------------------------

|k8s-mdbrsc| Docker images run on Ubuntu and use Ubuntu's default
system configuration. Using the :setting:`spec.podSpec.podTemplate` 
setting, add a privileged sidecar :k8sdocs:`init container 
</concepts/workloads/pods/init-containers/>` to the |k8s-mdbrsc| 
definition to tune the underlying Ubuntu system configuration in the 
|k8s-mdbrsc| containers.

.. example:: 

   |k8s-mdbrsc| Docker images use the Ubuntu default ``keepalive`` time
   of ``7200``. MongoDB recommends a shorter ``keepalive`` time of ``120``
   for database deployments.

   You can tune the ``keepalive`` time in the |k8s-mdbrscs| Docker images
   if you experience network timeouts or socket errors in communication
   between clients and |k8s-mdbrscs|.

To tune |k8s-mdbrsc| Docker images:

1. Update the |k8s-mdbrsc| definition to append a privileged sidecar 
   container to |k8s-mdbrsc| pods the |k8s-op-short| creates. 
   
   The following sample :setting:`spec.podSpec.podTemplate` changes the 
   ``keepalive`` value to the recommended value of ``120``:

   .. code-block:: yaml

      spec:
        podSpec:
          podTemplate:
            spec:
              initContainers:
              - name: "apply-sysctl-test"
                image: "busybox:latest"
                securityContext:
                  privileged: true
                command: ["sysctl", "-w", "net.ipv4.tcp_keepalive_time=120"]

#. Apply the updated resource definition:

   .. code-block:: none

      kubectl apply -f <database-resource-conf>.yaml -n <namespace>

A privileged sidecar container is added to each |k8s-pod| the 
|k8s-op-short| created using the |k8s-mdbrsc| definition.

To verify your changes:

1. Get a shell to a running container in your database resource 
   |k8s-pod|:

   .. code-block:: none
      
      kubectl exec -n <namespace> -it <pod-name> -- /bin/bash

#. Verify your changes are present. Following the example, verify that
   the ``keepalive`` time is changed:

   .. code-block:: none

      cat /proc/sys/net/ipv4/tcp_keepalive_time

   Returns:

   .. code-block:: none
      :copyable: false

      120

.. seealso:: :manual:`Operating System Configuration </administration/production-checklist-operations/#linux>` in the MongoDB Manual.