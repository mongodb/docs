===================
Backup Preparations
===================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

Before backing up your cluster or replica set, decide how to back up
the data and what data to back up. This page describes items you must
consider before starting a backup.

.. seealso::

   To learn how Backup works, see
   :ref:`mms-backup-functional-overview`.

.. cond:: onprem

   .. _backup-configuration-options:

   Backup Configuration Options
   ----------------------------

   The backup and recovery requirements of a given system vary to meet
   the cost, performance and data protection standards the system's
   owner sets.

   |mms| Enterprise Backup and Recovery supports five backup
   architectures, each with its own strengths and trade-offs. Consider
   which architecture meets the data protection requirements for your
   deployment before configuring and deploying your backup
   architecture.

   .. example::

      Consider a system whose requirements include low operational
      costs. The system's owners may have strict limits on what they
      can spend on storage for their backup and recovery configuration.
      They may accept a longer recovery time as a result.

      Conversely, consider a system whose requirements include a low
      :term:`Recovery Time Objective`. The system's owners tolerate
      greater storage costs if it results in a backup and recovery
      configuration that fulfills the recovery requirements.

   |mms| Enterprise Backup and recovery supports the following backup
   architectures:

   - A File System on a Sophisticated |san|
   - A File System on one or more |nas| devices
   - An `AWS S3 <https://aws.amazon.com/s3>`_ :term:`Blockstore`
   - MongoDB Blockstore in a Highly Available configuration
   - MongoDB Blockstore in a Standalone configuration

   .. important::

      The backup architecture features and concerns are provided as
      guidance for developing your own data protection requirements.
      They do not cover every scenario nor are they representative of
      every deployment.

   Backup Method Features
   ~~~~~~~~~~~~~~~~~~~~~~

   .. list-table::
      :header-rows: 1
      :widths: 25 15 15 15 15 15
      :stub-columns: 1

      * - Backup System Feature
        - File System on |san|
        - File System on |nas|
        - AWS S3 Blockstore
        - MongoDB :abbr:`HA (High Availability)` Blockstore
        - MongoDB Blockstore

      * - Snapshot Types
        - Complete
        - Complete
        - Many partial
        - Many partial
        - Many partial

      * - Backup Data Deduplication
        - If SAN supports
        - No
        - Yes
        - Yes
        - Yes

      * - Backup Data Compression
        - Yes
        - Depends
        - Yes
        - Yes
        - Yes

      * - Backup Data Replication
        - If SAN supports
        - No
        - No
        - Yes
        - No

      * - Backup Storage Cost
        - Higher
        - Medium
        - Lower
        - Higher
        - Lower

      * - Staff Time to Manage Backups
        - Medium
        - Medium
        - Lower
        - Higher
        - Medium

      * - Backup |rto|
        - Lower
        - Medium
        - Lower
        - Lower
        - Medium

   .. topic:: When Do You Use a Particular Backup Method?

       - If you do not want to maintain separate backup systems nor do
         you want your staff to maintain them, consider backing up to a
         **MongoDB** or **S3 blockstore**.

       - If you need to restore data without relying on MongoDB
         database, consider backing up to a **file system on a** |san|
         **or** |nas| **device** or an **S3 blockstore**.

       - If you are backing up large amounts of data or frequently need
         to restore data, consider either a **file system on a** |san|,
         **S3 blockstore** or a **MongoDB blockstore configured as a
         replica set or sharded cluster**.

       - If you want to minimize internal storage and maintenance
         costs, consider backing up to a **MongoDB standalone
         blockstore** or an **S3 blockstore**.

       - If you have a |san| with advanced features like high
         availability, compression, :term:`deduplication`, etc.,
         consider using that |san| **for file system** backups.

.. _limits-of-backup:

Backup Sizing Recommendation
----------------------------

When sizing the backup of your data, MongoDB recommends keeping replica
set size to 2 TB or less of
:manual:`uncompressed data </reference/command/dbStats#dbStats.dataSize>`.
If your data increases beyond 2 TB, you should shard that database and
keep each shard to 2 TB or less of uncompressed data.

These size recommendations are a best practice, and are not due to a
limitation of the MongoDB database or |mms|.

Backup and restore can use great amounts of CPU, memory, storage and
network bandwidth.

.. example::

   Your stated network throughput, like 10 Gbps, 100 Gbps, is a
   *theoretical* maximum. That value doesn't account for sharing or
   throttling of network traffic.

   Consider the following scenario:

   - You want to back up a 2 TB database.
   - Your hosts support a 10 Gbps TCP connection from |mms| to its
     backup storage.
   - The network connection has very low packet loss and a low round
     trip delay time.

   A *full* backup of your data would take *more than 30 hours to
   complete*. [*]_

   This doesn't account for disk read and write speeds, which can be,
   at most, 3 Gbps reads and 1 Gbps writes for a single or mirrored
   NVMe storage device.

   The time required to complete each successive incremental backup
   depends on write load.

   If you :doc:`shard this database </tutorial/convert-replica-set-to-sharded-cluster>`
   into 4 shards, each shard runs its backup separately. This results
   in a backup that takes less than 8 hours to complete.

   .. [*] These throughput figures were calculated using the `Network Throughput Calculator <https://wintelguy.com/wanperf.pl>`__ and assume no additional network compression.

.. _snapshot-frequency-and-retention:

Snapshot Frequency and Retention Policy
---------------------------------------

.. cond:: onprem

   By default, |mms| takes a base snapshot of your data every 24 hours.

.. cond:: cloud

   By default, |mms| takes a base snapshot of your data every 6 hours.

If desired, administrators can change the frequency of base snapshots
to 6, 8, 12, or 24 hours. |mms| creates snapshots automatically on a
schedule; you cannot take snapshots on demand.

|mms| retains snapshots for the time periods listed in the following
table. If you terminate a backup, |mms| immediately deletes the
backup's snapshots.

.. cond:: onprem

   .. list-table::
      :widths: 30 30 40
      :header-rows: 1

      * - Snapshot
        - Default Retention Policy
        - Maximum Retention Policy
      * - Base snapshot
        - 2 days
        - 5 days (30 days if frequency is 24 hours)
      * - Daily snapshot
        - 0 days
        - 1 year
      * - Weekly snapshot
        - 2 weeks
        - 1 year
      * - Monthly snapshot
        - 1 month
        - 7 years

.. cond:: cloud

   .. list-table::
      :widths: 30 30 40
      :header-rows: 1

      * - Snapshot
        - Default Retention Policy
        - Maximum Retention Policy
      * - Base snapshot
        - 2 days
        - 5 days (30 days if frequency is 24 hours)
      * - Daily snapshot
        - 7 days
        - 1 year
      * - Weekly snapshot
        - 4 weeks
        - 1 year
      * - Monthly snapshot
        - 13 months
        - 7 years

.. include:: /includes/extracts/backup-preparations-pricing.rst

You can change a backed-up deployment's schedule through its
:guilabel:`Edit Snapshot Schedule` menu option, available through the
:guilabel:`Backup` page. Administrators can change snapshot frequency
and retention through the
:doc:`snapshotSchedule resource in the API </reference/api/snapshot-schedule>`.

.. _changing-reference-time:

Changing the reference time changes the time of the next scheduled
snapshot:

- If the new reference time is before the current reference time, the
  next snapshot occurs at the new reference time tomorrow. See
  the first two rows of the table below for examples.

- If the new reference time is after the current reference time, and
  you make the change before the current reference time, the next
  snapshot occurs at the new reference time today. See the third row
  of the table below for an example.

- If the new reference time is after the current reference time, but
  you make the change after the current reference time, the next
  snapshot occurs at the new reference time tomorrow. See the fourth
  row of the table below for an example.

.. list-table::
   :widths: 25 25 25 25
   :header-rows: 1

   * - Time of Change

     - Current Reference Time

     - New Reference Time

     - Time of Next Snapshot

   * - 08:00 UTC

     - 12:00 UTC

     - 10:00 UTC

     - 10:00 UTC tomorrow

   * - 13:00 UTC

     - 12:00 UTC

     - 10:00 UTC

     - 10:00 UTC tomorrow

   * - 08:00 UTC

     - 12:00 UTC

     - 14:00 UTC

     - 14:00 UTC today

   * - 13:00 UTC

     - 12:00 UTC

     - 14:00 UTC

     - 14:00 UTC tomorrow

If you change the schedule to save fewer snapshots, |mms| does **not**
delete existing snapshots to conform to the new schedule. To delete
unneeded snapshots, see :doc:`/tutorial/delete-backup-snapshots`.

Limits
~~~~~~

- |mms| does not backup deployments where the total number of
  collections on the deployment meets or exceeds ``100,000``.

- |mms| does not replicate index collection options.

Encryption
~~~~~~~~~~

.. cond:: onprem

   |onprem| can encrypt any backup job stored in a
   :term:`head database` running
   :product:`MongoDB Enterprise <mongodb-enterprise-advanced>` between
   |fcv-link| 3.4 and 4.0 with the
   :ref:`WiredTiger <encrypted-storage-engine>` storage engine.

   .. seealso::

      To learn more about configuring backup encryption, see
      :doc:`/tutorial/encrypt-snapshots`.

.. cond:: cloud

   You can't store backups using WiredTiger encryption. You can store backups using the WiredTiger storage engine if you don't enable encryption. If you restore from a backup, you restore unencrypted files.

.. _4.2-backup-considerations:

Backup Considerations
---------------------

Databases Running FCV 4.2
~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /release-notes/release-advisories/advisories-v4.2-backup.rst

Databases not Running FCV 4.2
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. important::

   .. include:: /includes/fact-backup-standalone-restriction.rst

The following considerations apply when your databases run any version
of MongoDB 4.0 or earlier or when they run MongoDB 4.2 with
``"featureCompatibilityVersion" : 4.0``

.. _namespaces-filter:

Namespaces Filter
~~~~~~~~~~~~~~~~~

The namespaces filter lets you specify which databases and collections
to back up. You create either a :guilabel:`Blacklist` of those to
exclude or a :guilabel:`Whitelist` of those to include. You make your
selections when starting a backup and can later edit them as needed. If
you change the filter in a way that adds data to your backup, a resync
is required.

Use the blacklist to prevent backup of collections that contain logging
data, caches, or other ephemeral data. Excluding these kinds of
databases and collections will allow you to reduce backup time and
costs. Using a blacklist is often preferable to using a whitelist as a
whitelist requires you to intentionally opt in to every namespace you
want backed up.

.. note::

   MongoDB deployments with ``"featureCompatibilityVersion" : 4.2``
   do not support namespaces filters.

.. _considerations-backup-storage-engine:

Storage Engine
~~~~~~~~~~~~~~

To backup MongoDB clusters, use the :ref:`WiredTiger storage engine <storage-wiredtiger>` storage engine.

If your current backing databases use MMAPv1, upgrade to WiredTiger:

* :manual:`Change Sharded Cluster to WiredTiger </tutorial/change-sharded-cluster-wiredtiger>`

* :manual:`Change Replica Set to WiredTiger </tutorial/change-replica-set-wiredtiger>`

With WiredTiger, |mms| limits backups to deployments with fewer than
100,000 files. Files includes collections and indexes.

MongoDB 4.2 removes MMAPv1 storage. To learn more about storage
engines, see :manual:`Storage </core/storage>` in the MongoDB manual.

Resyncing Production Deployments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For production deployments, it is recommended that as a best practice
you periodically (annually) :doc:`resync </tutorial/resync-backup>` all
backed-up replica sets. When you resync, data is read from a secondary
in each replica set. During resync, no new snapshots are generated.

You may also want to resync your backup after:

- A reduction in data size, such that the size on disk of |mms|'s copy
  of the data is also reduced. This scenario also includes if you:

  - Have a :manual:`TTL index </core/index-ttl>` in place, which
    periodically deletes documents.

  - :manual:`Drop a collection </reference/method/db.collection.drop>`
    (MMAPv1 only).

  - Run a :term:`sharded cluster <sharded cluster>`, and there have
    been a lot of chunks moved off a particular shard.

- A switch in storage engines, if you want |mms| to provide snapshots
  in the new storage engine format.

- A manual build of an index on a replica set in a rolling fashion
  (as per :manual:`Build Indexes on Replica Sets
  </tutorial/build-indexes-on-replica-sets>` in the MongoDB manual).

.. _checkpoint:

Checkpoints
~~~~~~~~~~~

.. include:: /includes/admonitions/important/checkpoints-fcv-4-0-only.rst

For :term:`sharded clusters <sharded cluster>`, checkpoints provide
additional restore points between snapshots. With
checkpoints enabled, |mms| creates restoration points at
configurable intervals of every 15, 30 or 60 minutes between
snapshots. To enable checkpoints, see :ref:`enable checkpoints
<enable-cluster-checkpoints>`.

To create a checkpoint, |mms| stops the :term:`balancer`
and inserts a token into the :term:`oplog` of each
:term:`shard` and :term:`config server` in the cluster. These
checkpoint tokens are lightweight and do not have a consequential
impact on performance or disk use.

Backup does not require checkpoints, and they are disabled by default.

Restoring from a checkpoint requires |mms| to apply the oplog of
each shard and config server to the last snapshot captured before the
checkpoint. Restoration from a checkpoint takes longer than
restoration from a snapshot.

.. _snapshot-while-balancer-enabled:

Snapshots when Agent Cannot Stop Balancer
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For :term:`sharded clusters <sharded cluster>`, |mms| disables the
:term:`balancer` before taking a cluster snapshot. In certain
situations, such as a long migration or no running :program:`mongos`,
|mms| tries to disable the balancer but cannot. In such cases, |mms|
will continue to take cluster snapshots but will flag the snapshots
with a warning that data may be incomplete and/or inconsistent. Cluster
snapshots taken during an active balancing operation run the risk of
data loss or orphaned data.

Snapshots when Agent Cannot Contact a ``mongod``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For :term:`sharded clusters <sharded cluster>`, if the {+bagent+}
cannot reach a :program:`mongod` process, whether a shard or config
server, then the agent cannot insert a synchronization :term:`oplog`
token. If this happens, |mms| does not create the snapshot and
displays a warning message.
