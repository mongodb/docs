.. index:: fundamentals; sharding
.. _sharding-fundamentals:

=====================
Sharding Fundamentals
=====================

.. default-domain:: mongodb

MongoDB's sharding system allows users to :term:`partition` the data of a
:term:`collection` within a database to distribute documents
across a number of :program:`mongod` instances or :term:`shards <shard>`.

Sharded clusters allow increases in write capacity, provide the ability to
support larger working sets, and raise the limits of total data size beyond
the physical resources of a single node.

This document provides an overview of the fundamental concepts and
operation of sharding with MongoDB.

.. seealso:: The ":doc:`/sharding`" index for a list of all documents
   in this manual that contain information related to the operation
   and use of sharded clusters in MongoDB. This includes:

   - :doc:`/core/sharding-internals`
   - :doc:`/administration/sharding`
   - :doc:`/administration/sharding-architectures`

   If you are not yet familiar with sharding, see the :doc:`Sharding
   FAQ </faq/sharding>` document.

Overview
--------

Features
~~~~~~~~

With sharding MongoDB automatically distributes data among a
collection of :program:`mongod` instances. Sharding, as implemented in
MongoDB has the following features:

.. glossary::

   Range-based Data Partitioning
      MongoDB distributes documents among :term:`shards <shard>` based
      on the value of the :ref:`shard key <sharding-shard-key>`. Each
      :term:`chunk` represents a block of :term:`documents <document>`
      with values that fall within a specific range. When chunks grow
      beyond the :ref:`chunk size <sharding-chunk-size>`, MongoDB
      divides the chunks into smaller chunks (i.e. :term:`splitting
      <split>`) based on the shard key.

   Automatic Data Volume Distribution
      The sharding system automatically balances data across the
      cluster without intervention from the application
      layer. Effective automatic sharding depends on a well chosen
      :ref:`shard key <sharding-shard-key>`, but requires no
      additional complexity, modifications, or intervention from
      developers.

   Transparent Query Routing
      Sharding is completely transparent to the application layer,
      because all connections to a cluster go through
      :program:`mongos`. Sharding in MongoDB requires some
      :ref:`basic initial configuration <sharding-procedure-setup>`,
      but ongoing function is entirely transparent to the application.

   Horizontal Capacity
      Sharding increases capacity in two ways:

      #. Effective partitioning of data can provide additional write
         capacity by distributing the write load over a number of
         :program:`mongod` instances.

      #. Given a shard key with sufficient :ref:`cardinality
         <sharding-shard-key-cardinality>`, partitioning data allows
         users to increase the potential amount of data to mange
         with MongoDB and expand the :term:`working set`.

A typical :term:`sharded cluster` consists of:

- 3 config servers that store metadata. The metadata maps :term:`chunks
  <chunk>` to shards.

- More than one :term:`replica sets <replica set>` that hold
  data. These are the :term:`shards <shard>`.

- A number of lightweight routing processes, called :doc:`mongos
  </reference/mongos>` instances. The :program:`mongos` process routes
  operations to the correct shard based the cluster configuration.

Indications
~~~~~~~~~~~

While sharding is a powerful and compelling feature, it comes with
significant :ref:`infrastructure requirements <sharding-requirements>`
and some limited complexity costs. As a result, use
sharding only as necessary, and when indicated by actual operational
requirements. Consider the following overview of indications it may be
time to consider sharding.

You should consider deploying a :term:`sharded cluster`, if:

- your data set approaches or exceeds the storage capacity of a single
  node in your system.

- the size of your system's active :term:`working set` *will soon*
  exceed the capacity of the *maximum* amount of RAM for your system.

- your system has a large amount of write activity, a single
  MongoDB instance cannot write data fast enough to meet demand, and
  all other approaches have not reduced contention.

If these attributes are not present in your system, sharding will only
add additional complexity to your system without providing much
benefit. When designing your data model, if you will eventually need a
sharded cluster, consider which collections you will want to shard and
the corresponding shard keys.

.. _sharding-capacity-planning:

.. warning::

   It takes time and resources to deploy sharding, and if your system
   has *already* reached or exceeded its capacity, you will have a
   difficult time deploying sharding without impacting your
   application.

   As a result, if you think you will need to partition your database
   in the future, **do not** wait until your system is overcapacity to
   enable sharding.

.. index:: sharding; requirements
.. _sharding-requirements:

Requirements
------------

.. _sharding-requirements-infrastructure:

Infrastructure
~~~~~~~~~~~~~~

A :term:`sharded cluster` has the following components:

- Three :term:`config servers <config database>`.

  These special :program:`mongod` instances store the metadata for the
  cluster. The :program:`mongos` instances cache this data and use it
  to determine which :term:`shard` is responsible for which
  :term:`chunk`.

  For development and testing purposes you may deploy a cluster with a single
  configuration server process, but always use exactly three config
  servers for redundancy and safety in production.

- Two or more shards. Each shard consists of one or more :program:`mongod`
  instances that store the data for the shard.

  These "normal" :program:`mongod` instances hold all of the
  actual data for the cluster.

  Typically each shard is a :term:`replica sets <replica set>`. Each
  replica set consists of multiple :program:`mongod` instances. The members
  of the replica set provide redundancy and high available for the data in each shard.

  .. warning::

     MongoDB enables data :term:`partitioning <partition>`, or
     sharding, on a *per collection* basis. You *must* access all data
     in a sharded cluster via the :program:`mongos` instances as below.
     If you connect directly to a :program:`mongod` in a sharded cluster
     you will see its fraction of cluster's data. The data on any
     given shard may be somewhat random: MongoDB provides no grantee
     that any two contiguous chunks will reside on a single shard.

- One or more :program:`mongos` instances.

  These instance direct queries from the application layer to the
  shards that hold the data. The :program:`mongos` instances have no
  persistent state or data files and only cache metadata in RAM from
  the config servers.

  .. note::

     In most situations :program:`mongos` instances use minimal
     resources, and you can run them on your application servers
     without impacting application performance. However, if you use
     the :term:`aggregation framework` some processing may occur on
     the :program:`mongos` instances, causing that :program:`mongos`
     to require more system resources.

Data
~~~~

Your cluster must manage a significant quantity of data for sharding
to have an effect on your collection. The default :term:`chunk` size
is 64 megabytes, [#chunk-size]_ and the :ref:`balancer
<sharding-balancing>` will not begin moving data until the imbalance
of chunks in the cluster exceeds the :ref:`migration threshold
<sharding-migration-thresholds>`.

Practically, this means that unless your cluster has many hundreds of
megabytes of data, chunks will remain on a single shard.

While there are some exceptional situations where you may need to
shard a small collection of data, most of the time the additional
complexity added by sharding the small collection is not worth the additional
complexity and overhead unless
you need additional concurrency or capacity for some reason. If you
have a small data set, usually a properly configured
single MongoDB instance or replica set will be more than sufficient
for your persistence layer needs.

.. [#chunk-size] :term:`chunk` size is :option:`user configurable
   <mongos --chunkSize>`.  However, the default value is of 64
   megabytes is ideal for most deployments. See the
   ":ref:`sharding-chunk-size`" section in the
   :doc:`sharding-internals` document for more information.

.. index:: sharding; localhost
.. _sharding-localhost:

Sharding and "localhost" Addresses
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Because all components of a :term:`sharded cluster` must communicate
with each other over the network, there are special restrictions
regarding the use of localhost addresses:

If you use either "localhost" or "``127.0.0.1``" as the host
identifier, then you must use "localhost" or "``127.0.0.1``" for *all*
host settings for any MongoDB instances in the cluster.  This applies
to both the ``host`` argument to :dbcommand:`addShard` and the value
to the :option:`mongos --configdb` run time option. If you mix
localhost addresses with remote host address, MongoDB will produce
errors.

.. index:: shard key
   single: sharding; shard key

.. _sharding-shard-key:
.. _shard-key:

Shard Keys
----------

.. todo:: link this section to <glossary:shard key>

"Shard keys" refer to the :term:`field` that exists in every
:term:`document` in a collection that MongoDB uses to distribute
documents among the :term:`shards <shard>`. Shard keys, like
:term:`indexes <index>`, can be either a single field, or may be a
compound key, consisting of multiple fields.

Remember, MongoDB's sharding is range-based: each :term:`chunk` holds
documents having specific range of values for the "shard key". Thus,
choosing the correct shard key can have a great impact on the
performance, capability, and functioning of your database and cluster.

Appropriate shard key choice depends on the schema of your data and
the way that your application queries and writes data to the database.

The ideal shard key:

- is easily divisible which makes it easy for MongoDB to distribute
  content among the shards. Shard keys that have a limited number of
  possible values are not ideal as they can result in some chunks that
  are "unsplitable." See the ":ref:`sharding-shard-key-cardinality`"
  section for more information.

- will distribute write operations among the cluster, to prevent any
  single shard from becoming a bottleneck. Shard keys that have a high
  correlation with insert time are poor choices for this reason;
  however, shard keys that have higher "randomness" satisfy this
  requirement better. See the   ":ref:`sharding-shard-key-write-scaling`"
  section for additional background.

- will make it possible for the :program:`mongos` to return most query
  operations directly from a single *specific* :program:`mongod`
  instance. Your shard key should be the primary field used by your
  queries, and fields with a high degree of "randomness" are poor
  choices for this reason. See the ":ref:`sharding-shard-key-query-isolation`"
  section for specific examples.

The challenge when selecting a shard key is that there is not always
an obvious choice. Often, an existing field in your collection may not be
the optimal key. In those situations, computing a special purpose
shard key into an additional field or using a compound shard key may
help produce one that is more ideal.

.. index:: sharding; config servers
.. index:: config servers
.. _sharding-config-server:

Config Servers
--------------

Config servers maintain the shard metadata in a config
database.  The :term:`config database <config database>` stores
the relationship between :term:`chunks <chunk>` and where they reside
within a :term:`sharded cluster`. Without a config database, the
:program:`mongos` instances would be unable to route queries or write
operations within the cluster.

Config servers *do not* run as replica sets. Instead, a :term:`cluster
<sharded cluster>` operates with a group of *three* config servers that use a
two-phase commit process that ensures immediate consistency and
reliability.

For testing purposes you may deploy a cluster with a single
config server, but this is not recommended for production.

.. warning::

   If your cluster has a single config server, this
   :program:`mongod` is a single point of failure.  If the instance is
   inaccessible the cluster is not accessible. If you cannot recover
   the data on a config server, the cluster will be inoperable.

   **Always** use three config servers for production deployments.

The actual load on configuration servers is small because each
:program:`mongos` instances maintains a cached copy of the configuration
database. MongoDB only writes data to the config server to:

- create splits in existing chunks, which happens as data in
  existing chunks exceeds the maximum chunk size.

- migrate a chunk between shards.

Additionally, all config servers must be available on initial setup
of a sharded cluster, each :program:`mongos` instance must be able
to write to the ``config.version`` collection.

If one or two configuration instances become unavailable, the
cluster's metadata becomes *read only*. It is still possible to read
and write data from the shards, but no chunk migrations or splits will
occur until all three servers are accessible. At the same time, config
server data is only read in the following situations:

- A new :program:`mongos` starts for the first time, or an existing
  :program:`mongos` restarts.

- After a chunk migration, the :program:`mongos` instances update
  themselves with the new cluster metadata.

If all three config servers are inaccessible, you can continue to use
the cluster as long as you don't restart the :program:`mongos`
instances until the after config servers are accessible again. If you
restart the :program:`mongos` instances and there are no accessible
config servers, the :program:`mongos` would be unable to direct
queries or write operations to the cluster.

Because the configuration data is small relative to the amount of data
stored in a cluster, the amount of activity is relatively low, and 100%
up time is not required for a functioning sharded cluster. As a result,
backing up the config servers is not difficult. Backups of config
servers are critical as clusters become totally inoperable when
you lose all configuration instances and data. Precautions to ensure
that the config servers remain available and intact are critical.

.. note::

   Configuration servers store metadata for a single sharded cluster.
   You must have a separate configuration server or servers for each
   cluster you administer.

.. index:: mongos
.. _sharding-mongos:

:program:`mongos` and Querying
------------------------------

.. seealso:: ":doc:`/reference/mongos`" and the :program:`mongos`\-only
   settings: ":setting:`test`" and :setting:`chunkSize`.

Operations
~~~~~~~~~~

The :program:`mongos` provides a single unified interface to a sharded
cluster for applications using MongoDB. Except for the selection of a
:term:`shard key`, application developers and administrators need not
consider any of the :doc:`internal details of sharding <sharding-internals>`.

:program:`mongos` caches data from the :ref:`config server
<sharding-config-server>`, and uses this to route operations from
applications and clients to the :program:`mongod` instances.
:program:`mongos` have no *persistent* state and consume
minimal system resources.

The most common practice is to run :program:`mongos` instances on the
same systems as your application servers, but you can maintain
:program:`mongos` instances on the shards or on other dedicated
resources.

.. note::

   .. versionchanged:: 2.1

   Some aggregation operations using the :dbcommand:`aggregate`
   command (i.e. :method:`db.collection.aggregate()`,) will cause
   :program:`mongos` instances to require more CPU resources than in
   previous versions. This modified performance profile may dictate
   alternate architecture decisions if you use the :term:`aggregation
   framework` extensively in a sharded environment.

.. _sharding-query-routing:

Routing
~~~~~~~

:program:`mongos` uses information from :ref:`config servers
<sharding-config-server>` to route operations to the cluster as
efficiently as possible. In general, operations in a sharded
environment are either:

1. Targeted at a single shard or a limited group of shards based on
   the shard key.

2. Broadcast to all shards in the cluster that hold documents in a
   collection.

When possible you should design your operations to be as targeted as
possible. Operations have the following targeting characteristics:

- Query operations broadcast to all shards [#namespace-exception]_
  **unless** the :program:`mongos` can determine which shard or shard
  stores this data.

  For queries that include the shard key, :program:`mongos` can target
  the query at a specific shard. For example:

  For queries that contain a component of the shard key, the
  :program:`mongos` may be able to target the query at a limited
  subset of shards.

- All :method:`insert() <db.collection.insert()>` operations target to
  one shard.

- All single :method:`update() <db.collection.update()>` operations
  target to one shard. This includes :term:`upsert` operations.

- The :program:`mongos` broadcasts multi-update operations to every
  shard.

- The :program:`mongos` broadcasts :method:`remove()
  <db.collection.remove()>` operations to every shard unless the
  operation specifies the shard key in full.

While some operations must broadcast to all shards, you can improve
performance by using as many targeted operations as possible by
ensuring that your operations include the shard key.

.. [#namespace-exception] If a shard does not store chunks from a
   given collection, queries for documents in that collection are not
   broadcast to that shard.

Sharded Query Response Process
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To route a query to a :term:`cluster <sharded cluster>`,
:program:`mongos` uses the following process:

#. Determine the list of :term:`shards <shard>` that must receive the query.

   In some cases, when the :term:`shard key` or a prefix of the shard
   key is a part of the query, the :program:`mongos` can route the
   query to a subset of the shards. Otherwise, the :program:`mongos`
   must direct the query to *all* shards.

   .. example::

      Given the following shard key:

      .. code-block:: javascript

         { zipcode: 1, u_id: 1, c_date: 1 }

      Depending on the distribution of chunks in the cluster, the
      :program:`mongos` may be able to target the query at a subset of
      shards, if the query contains the following fields:

      .. code-block:: javascript

         { zipcode: 1 }
         { zipcode: 1, u_id: 1 }
         { zipcode: 1, u_id: 1, c_date: 1 }

#. Establish a cursor on all targeted shards.

   When the first batch of results return from the cursors:

   a. For query with sorted results (i.e. using
      :method:`cursor.sort()`) the :program:`mongos` performs a merge
      sort of all queries.

   b. For a query with unsorted results, the :program:`mongos` returns
      a result cursor that "round robins" results from all cursors on
      the shards.

      .. versionchanged:: 2.0.5
         Before 2.0.5, the :program:`mongos` exhausted each cursor,
         one by one.

.. index:: balancing
.. _sharding-balancing:

Balancing and Distribution
--------------------------

Balancing refers to the process that MongoDB uses to redistribute data
within a :term:`sharded cluster` when some :term:`shards <shard>` have a
greater number of :term:`chunks <chunk>` than other shards. The
balancing process attempts to minimize the impact that balancing can
have on the cluster, by:

- Only moving one chunk at a time.

- Only initiating a balancing round when the difference in number of
  chunks between the shard with the greatest and the shard with the
  least number of chunks exceeds the :ref:`migration threshold
  <sharding-migration-thresholds>`.

Additionally, you may disable the balancer on a temporary basis for
maintenance and limit the window during which it runs to prevent the
balancing process from impacting production traffic.

.. seealso:: The ":ref:`"Balancing Internals
   <sharding-balancing-internals>`" and :ref:`Balancer Operations
   <sharding-balancing-operations>` for more information on balancing.

.. note::

   The balancing procedure for :term:`sharded clusters <sharded cluster>`
   is entirely transparent to the user and application layer. This
   documentation is only included for your edification and possible
   troubleshooting purposes.

.. index:: sharding; security
.. _sharding-security:

Security
--------

.. todo:: migrate this content to /administration/security.txt when that
   document exists. See DOCS-79 for tracking this document.

.. note::

   You should always run all :program:`mongod` components in trusted
   networking environments that control access to the cluster using
   network rules and restrictions to ensure that only known traffic
   reaches your :program:`mongod` and :program:`mongos` instances.

.. warning::  Limitations

   .. versionchanged:: 2.2
      Read only authentication is fully supported in shard
      clusters. Previously, in version 2.0, sharded clusters would not
      enforce read-only limitations.

   .. versionchanged:: 2.0
      Sharded clusters support authentication. Previously, in version
      1.8, sharded clusters will not support authentication and access
      control. You must run your sharded systems in trusted
      environments.

To control access to a sharded cluster, you must set the
:setting:`keyFile` option on all components of the sharded cluster. Use
the :option:`--keyFile <mongos --keyFile>` run-time option or the
:setting:`keyFile` configuration option for all :program:`mongos`,
configuration instances, and shard :program:`mongod` instances.

There are two classes of security credentials in a sharded cluster:
credentials for "admin" users (i.e. for the :term:`admin database`) and
credentials for all other databases. These credentials reside in
different locations within the cluster and have different roles:

#. Admin database credentials reside on the config servers, to receive
   admin access to the cluster you *must* authenticate a session while
   connected to a :program:`mongos` instance using the
   :term:`admin database`.

#. Other database credentials reside on the *primary* shard for the
   database.

This means that you *can* authenticate to these users and databases
while connected directly to the primary shard for a database. However,
for clarity and consistency all interactions between the client and
the database should use a :program:`mongos` instance.

.. note::

   Individual shards can store administrative credentials to their
   instance, which only permit access to a single shard. MongoDB
   stores these credentials in the shards' :term:`admin databases <admin database>` and these
   credentials are *completely* distinct from the cluster-wide
   administrative credentials.
