=====================
Aggregation Framework
=====================

.. versionadded:: 2.1

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

Overview
--------

The MongoDB aggregation framework provides a means to calculate
aggregated values without having to use :term:`map-reduce`. While
map-reduce is powerful, it is often more difficult than
necessary for many simple aggregation tasks, such as totaling or
averaging field values.

If you're familiar with :term:`SQL`, the aggregation framework
provides similar functionality to ``GROUP BY`` and related SQL
operators as well as simple forms of "self joins." Additionally, the
aggregation framework provides projection capabilities to reshape the
returned data. Using the projections in the aggregation framework, you
can add computed fields, create new virtual sub-objects, and extract
sub-fields into the top-level of results.

.. seealso:: A presentation from MongoSV 2011: `MongoDB's New
   Aggregation Framework <http://www.mongodb.com/presentations/mongosv-2011/mongodbs-new-aggregation-framework>`_.

   Additionally, consider :doc:`/tutorial/aggregation-examples` and
   :doc:`/reference/aggregation` for more documentation.

Framework Components
--------------------

This section provides an introduction to the two concepts that
underpin the aggregation framework: :term:`pipelines <pipeline>` and
:term:`expressions <expression>`.

.. _aggregation-pipelines:

Pipelines
~~~~~~~~~

Conceptually, documents from a collection pass through an
aggregation pipeline, which transforms these objects as they pass through.
For those familiar with UNIX-like shells (e.g. bash,) the concept is
analogous to the pipe (i.e. ``|``) used to string text filters together.

In a shell environment the pipe redirects a stream of characters from
the output of one process to the input of the next. The MongoDB
aggregation pipeline streams MongoDB documents from one :ref:`pipeline
operator <aggregation-pipeline-operator-reference>` to the next to process the
documents. Pipeline operators can be repeated in the pipe.

All pipeline operators process a stream of documents and the
pipeline behaves as if the operation scans a :term:`collection` and
passes all matching documents into the "top" of the pipeline.
Each operator in the pipeline transforms each document as it passes
through the pipeline.

.. note::

   Pipeline operators need not produce one output document for every
   input document: operators may also generate new documents or filter
   out documents.

.. include:: /includes/warning-aggregation-types.rst

.. seealso:: The ":doc:`/reference/aggregation`" includes
   documentation of the following pipeline operators:

   - :pipeline:`$project`
   - :pipeline:`$match`
   - :pipeline:`$limit`
   - :pipeline:`$skip`
   - :pipeline:`~stage.$unwind`
   - :pipeline:`~stage.$group`
   - :pipeline:`$sort`

.. _aggregation-expressions:

Expressions
~~~~~~~~~~~

:ref:`Expressions <aggregation-expression-operators>` produce output
documents based on calculations performed on input documents. The
aggregation framework defines expressions using a document format
using prefixes.

Expressions are stateless and are only evaluated when seen by the
aggregation process. All aggregation expressions can only operate on
the current document in the pipeline, and cannot integrate data from
other documents.

The :term:`accumulator` expressions used in the :pipeline:`~stage.$group`
operator maintain that state (e.g.  totals, maximums, minimums, and
related data) as documents progress through the :term:`pipeline`.

.. seealso:: :ref:`Aggregation expressions
   <aggregation-expression-operators>` for additional examples of the
   expressions provided by the aggregation framework.

Use
---

Invocation
~~~~~~~~~~

Invoke an :term:`aggregation` operation with the :method:`~db.collection.aggregate()`
wrapper in the :binary:`~bin.mongo` shell or the :dbcommand:`aggregate`
:term:`database command`. Always call :method:`~db.collection.aggregate()` on a
collection object that determines the input documents of the aggregation :term:`pipeline`.
The arguments to the :method:`~db.collection.aggregate()` method specify a sequence of :ref:`pipeline
operators <aggregation-pipeline-operator-reference>`, where each
operator may have a number of operands.

First, consider a :term:`collection` of documents named ``articles``
using the following format:

.. code-block:: javascript

   {
    title : "this is my title" ,
    author : "bob" ,
    posted : new Date () ,
    pageViews : 5 ,
    tags : [ "fun" , "good" , "fun" ] ,
    comments : [
                { author :"joe" , text : "this is cool" } ,
                { author :"sam" , text : "this is bad" }
    ],
    other : { foo : 5 }
   }

The following example aggregation operation pivots data to
create a set of author names grouped by tags applied to an
article. Call the aggregation framework by issuing the following
command:

.. code-block:: javascript

   db.articles.aggregate(
     { $project : {
        author : 1,
        tags : 1,
     } },
     { $unwind : "$tags" },
     { $group : {
        _id : { tags : "$tags" },
        authors : { $addToSet : "$author" }
     } }
   );

The aggregation pipeline begins with the :term:`collection`
``articles`` and selects the ``author`` and ``tags`` fields using the
:pipeline:`$project` aggregation operator. The
:expression:`~stage.$unwind` operator produces one output document per
tag. Finally, the :expression:`~stage.$group` operator pivots these fields.

Result
~~~~~~

The aggregation operation in the previous section returns a
:term:`document` with two fields:

- ``result`` which holds an array of documents returned by the :term:`pipeline`

- ``ok`` which holds the value ``1``, indicating success, or another value
  if there was an error

As a document, the result is subject to the :ref:`BSON
Document size <limit-bson-document-size>` limit, which is currently 16 megabytes.

.. _aggregation-optimize-performance:

Optimizing Performance
----------------------

Because you will always call :dbcommand:`aggregate` on a
:term:`collection` object, which logically inserts the *entire* collection into
the aggregation pipeline, you may want to optimize the operation
by avoiding scanning the entire collection whenever possible.

.. _aggregation-pipeline-operators-and-performance:

Pipeline Operators and Indexes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Depending on the order in which they appear in the pipeline,
aggregation operators can take advantage of indexes.

The following pipeline operators take advantage of an index when they
occur at the beginning of the pipeline:

- :pipeline:`$match`
- :pipeline:`$sort`
- :pipeline:`$limit`
- :pipeline:`$skip`.

The above operators can also use an index when placed **before** the
following aggregation operators:

- :pipeline:`$project`
- :pipeline:`~stage.$unwind`
- :pipeline:`$group`.

Early Filtering
~~~~~~~~~~~~~~~

If your aggregation operation requires only a subset of the data in a
collection, use the :pipeline:`$match` operator to restrict which items go
in to the top of the pipeline, as in a query. When placed early in a
pipeline, these :pipeline:`$match` operations use suitable indexes
to scan only the matching documents in a collection.

Placing a :pipeline:`$match` pipeline stage followed by a
:pipeline:`$sort` stage at the
start of the pipeline is logically equivalent to a single query
with a sort, and can use an index.

.. OMMITED: this feature is pending SERVER-4506. Other optimizations
.. are pending SERVER-4507 SERVER-4644 SERVER-4656 SERVER-4816
..
.. :term:`Aggregation` operations have an optimization phase, before
.. execution, which attempts to re-arrange the pipeline by moving
.. :pipeline:`$match` operators towards the beginning to the
.. greatest extent possible. For example, if a :term:`pipeline` begins
.. with a :pipeline:`$project` that renames fields, followed by a
.. :pipeline:`$match`, the optimizer can improve performance
.. without affecting the result by moving the :pipeline:`$match`
.. operator in front of the :pipeline:`$project`.

In future versions there may be an optimization phase in the
pipeline that reorders the operations to increase performance without
affecting the result. However, at this time place
:pipeline:`$match` operators at the beginning of the pipeline when
possible.

Memory for Cumulative Operators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Certain pipeline operators require access to the entire input set
before they can produce any output. For example, :pipeline:`$sort`
must receive all of the input from the preceding :term:`pipeline`
operator before it can produce its first output document. The current
implementation of :pipeline:`$sort` does not go to disk in these
cases: in order to sort the contents of the pipeline, the entire input
must fit in memory.

:pipeline:`$group` has similar characteristics: Before any
:pipeline:`$group` passes its output along the pipeline, it must
operator, this frequently does not require as much memory as
:pipeline:`$sort`, because it only needs to retain one record for
each unique key in the grouping specification.

The current implementation of the aggregation framework logs a warning
if a cumulative operator consumes 5% or more of the physical memory on
the host. Cumulative operators produce an error if they consume 10% or
more of the physical memory on the host.

Sharded Operation
-----------------

.. note::

   .. versionchanged:: 2.1

   Some aggregation operations using :dbcommand:`aggregate` will
   cause :binary:`~bin.mongos` instances to require more CPU resources
   than in previous versions. This modified performance profile may
   dictate alternate architectural decisions if you use the
   :term:`aggregation framework` extensively in a sharded environment.

The aggregation framework is compatible with sharded collections.

When operating on a sharded collection, the aggregation pipeline
splits into two parts. The aggregation framework pushes all of the
operators up to the first :pipeline:`$group` or :pipeline:`$sort`
operation to each shard.  [#match-sharding]_ Then, a second pipeline
on the :binary:`~bin.mongos` runs. This pipeline consists of the first
:pipeline:`$group` or :pipeline:`$sort` and any remaining pipeline
operators, and runs on the results received from the shards.

The :pipeline:`$group` operator brings in any "sub-totals" from
the shards and combines them: in some cases these may be
structures. For example, the :expression:`~grp.$avg` expression
maintains a total and count for each shard; :binary:`~bin.mongos` combines
these values and then divides.

.. [#match-sharding] If an early :pipeline:`$match` can exclude
   shards through the use of the shard key in the predicate, then
   these operators are only pushed to the relevant shards.

Limitations
-----------

Aggregation operations with the :dbcommand:`aggregate` command have
the following limitations:

- The pipeline cannot operate on values of the following types:
  ``Binary``, ``Symbol``, ``MinKey``, ``MaxKey``, ``DBRef``, ``Code``,
  ``CodeWScope``.

- Output from the :term:`pipeline` can only contain 16 megabytes. If
  your result set exceeds this limit, the :dbcommand:`aggregate`
  command produces an error.

- If any single aggregation operation consumes more than 10 percent of
  system RAM the operation will produce an error.
