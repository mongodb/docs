.. index:: administration; sharding
.. _sharding-administration:

==============================
Sharded Cluster Administration
==============================

.. default-domain:: mongodb

This document describes common administrative tasks for sharded
clusters. For a full introduction to sharding in MongoDB see
:doc:`/core/sharding`, and for a complete overview of all sharding
documentation in the MongoDB Manual, see :doc:`/sharding`. The
:doc:`/administration/sharding-architectures` document provides an
overview of deployment possibilities to help deploy a sharded
cluster. Finally, the :doc:`/core/sharding-internals` document
provides a more detailed introduction to sharding when troubleshooting
issues or understanding your cluster's behavior.

.. contents:: Sharding Procedures:
   :backlinks: none
   :local:

.. _sharding-procedure-setup:

Set up a Sharded Cluster
------------------------

Before deploying a cluster, see the requirements listed in
:ref:`Requirements for Sharded Clusters <sharding-requirements>`.

For testing purposes, you can run all the required shard :program:`mongod` processes on a
single server. For production, use the configurations described in
:doc:`/administration/replication-architectures`.

.. include:: /includes/warning-sharding-hostnames.rst

If you have an existing replica set, you can use the
:doc:`/tutorial/convert-replica-set-to-replicated-shard-cluster`
tutorial as a guide. If you're deploying a cluster from scratch, see
the :doc:`/tutorial/deploy-shard-cluster` tutorial for more detail or
use the following procedure as a quick starting point:

1. Create data directories for each of the three (3) config server instances.

#. Start the three config server instances. For example, to start a
   config server instance running on TCP port ``27018`` with the data
   stored in ``/data/configdb``, type the following:

   .. code-block:: sh

      mongod --configsvr --dbpath /data/configdb --port 27018

   For additional command options, see :doc:`/reference/mongod`
   and :doc:`/reference/configuration-options`.

   .. include:: /includes/note-config-server-startup.rst

#. Start the three :program:`mongos` instances. For example, to start a
   :program:`mongos` instance running on the following hosts:

   - ``mongos0.example.net``
   - ``mongos1.example.net``
   - ``mongos2.example.net``

   You would issue the following command:

   .. code-block:: sh

      mongos --configdb mongos0.example.net,mongos1.example.net,mongos2.example.net

#. Connect to one of the :program:`mongos` instances. For example, if
   a :program:`mongos` is accessible at ``mongos0.example.net`` on
   port ``27017``, issue the following command:

   .. code-block:: sh

      mongo mongos0.example.net

#. Add shards to the cluster. From the :program:`mongo` shell connected
   to the :program:`mongos` instance, call the :method:`sh.addShard()`
   method for each shard to add to the cluster.

   For example:

   .. code-block:: javascript

      sh.addShard( "mongodb0.example.net:27027" )

   If ``mongodb0.example.net:27027`` is a member of a replica
   set, MongoDB will discover all other members of the replica set.

   .. note:: In production deployments, all shards should be replica sets.

   Repeat this step for each new shard in your cluster.

   .. optional::

      You can specify a name for the shard and a maximum size. See
      :dbcommand:`addShard`.

   .. note::

      .. versionchanged:: 2.0.3

      Before version 2.0.3, you must specify the shard in the following
      form:

      .. code-block:: sh

         replicaSetName/<seed1>,<seed2>,<seed3>

      For example, if the name of the replica set is ``repl0``, then
      your :method:`sh.addShard()` command would be:

      .. code-block:: javascript

         sh.addShard( "repl0/mongodb0.example.net:27027,mongodb1.example.net:27017,mongodb2.example.net:27017" )

#. Enable sharding for each database you want to shard.
   While sharding operates on a per-collection basis, you must enable
   sharding for each database that holds collections you want to shard.
   This step is a meta-data change and will not redistribute your data.

   To enable sharding for a given database, use the
   :dbcommand:`enableSharding` command or the
   :method:`sh.enableSharding()` shell function, as shown below. Replace
   ``<database>`` with the name of the database on which to enable
   sharding.

   .. code-block:: javascript

      db.runCommand( { enableSharding: <database> } )

   Or:

   .. code-block:: javascript

      sh.enableSharding(<database>)

   .. note::

      MongoDB creates databases automatically upon their first use.

      Once you enable sharding for a database, MongoDB assigns a
      :term:`primary shard` for that database, where MongoDB stores all data
      before sharding begins.

.. _sharding-administration-shard-collection:

#. Enable sharding on a per-collection basis.

   Finally, you must explicitly specify collections to shard. The
   collections must belong to a database for which you have enabled
   sharding. When you shard a collection, you also choose the shard
   key. To shard a collection, run the :dbcommand:`shardCollection`
   command or the :method:`sh.shardCollection()` shell helper.

   .. code-block:: javascript

      db.runCommand( { shardCollection: "<database>.<collection>", key: "<shard-key>" } )

   Or:

   .. code-block:: javascript

      sh.shardCollection("<database>.<collection>", "key")

   For example:

   .. code-block:: javascript

      db.runCommand( { shardCollection: "myapp.users", key: {username: 1} } )

   Or:

   .. code-block:: javascript

      sh.shardCollection("myapp.users", {username: 1})

   The choice of shard key is incredibly important: it affects
   everything about the cluster from the efficiency of your queries to
   the distribution of data. Furthermore, you cannot change a
   collection's shard key after setting it.

   See the :ref:`Shard Key Overview <sharding-shard-key>` and the
   more in depth documentation of :ref:`Shard Key Qualities
   <sharding-internals-shard-keys>` to help you select better shard
   keys.

   If you do not specify a shard key, MongoDB will shard the
   collection using the ``_id`` field.

Cluster Management
------------------

Once you have a running sharded cluster, you will need to maintain it.
This section describes common maintenance procedure, including: how to
add and remove nodes, how to manually split chunks, and how to disable
the balancer for backups.

List all Shards
~~~~~~~~~~~~~~~

To list the current set of configured shards and verify that all shards
have been committed to the system, run the :dbcommand:`listShards`
command:

.. code-block:: javascript

   db.runCommand( { listshards : 1 } )

.. _sharding-procedure-add-shard:

Add a Shard to a Cluster
~~~~~~~~~~~~~~~~~~~~~~~~

To add a shard to an *existing* sharded cluster, use the following
procedure:

#. Connect to a :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. First, you need to tell the cluster where to find the individual
   shards. You can do this using the :dbcommand:`addShard` command or
   the :method:`sh.addShard()` helper:

   .. code-block:: javascript

      sh.addShard( "<hostname>:<port>" )

   Replace ``<hostname>`` and ``<port>`` with the hostname and TCP
   port number of where the shard is accessible.

   For example:

   .. code-block:: javascript

      sh.addShard( "mongodb0.example.net:27027" )

   .. note:: In production deployments, all shards should be replica sets.

   Repeat for each shard in your cluster.

   .. optional::

      You may specify a "name" as an argument to the
      :dbcommand:`addShard`, as follows:

      .. code-block:: javascript

         db.runCommand( { addShard: mongodb0.example.net, name: "mongodb0" } )

      Or:

      .. code-block:: javascript

         sh.addShard( mongodb0.example.net, name: "mongodb0" )

      If you do not specify a shard name, then MongoDB will assign a
      name upon creation.

   .. versionchanged:: 2.0.3

      Before version 2.0.3, you must specify the shard in the
      following form: the replica set name, followed by a forward
      slash, followed by a comma-separated list of seeds for the
      replica set. For example, if the name of the replica set is
      "myapp1", then your :method:`sh.addShard()` command might resemble:

      .. code-block:: javascript

         sh.addShard( "repl0/mongodb0.example.net:27027,mongodb1.example.net:27017,mongodb2.example.net:27017" )

.. note::

   It may take some time for :term:`chunks <chunk>` to migrate to the
   new shard.

   See the :ref:`Balancing and Distribution <sharding-balancing>`
   section for an overview of the balancing operation and the :ref:`Balancing Internals
   <sharding-balancing-internals>` section for additional information.

.. _sharding-procedure-remove-shard:

Remove a Shard from a Cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To remove a :term:`shard` from a :term:`sharded cluster`, you must:

- Migrate :term:`chunks <chunk>` to another shard or database.

- Ensure that this shard is not the :term:`primary shard` for any databases in
  the cluster. If it is, move the "primary" status for these databases
  to other shards.

- Finally, remove the shard from the cluster's configuration.

.. note::

   To successfully migrate data from a shard, the :term:`balancer`
   process **must** be active.

The procedure to remove a shard is as follows:

#. Connect to a :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Determine the name of the shard you will be removing.

   You must specify the name of the shard. You may have specified this
   shard name when you first ran the :dbcommand:`addShard` command. If not,
   you can find out the name of the shard by running the
   :dbcommand:`listShards` or :dbcommand:`printShardingStatus`
   commands or the :method:`sh.status()` shell helper.

   The following examples will remove a shard named ``mongodb0`` from the cluster.

#. Begin removing chunks from the shard.

   Start by running the :dbcommand:`removeShard` command. This will
   start "draining" or migrating chunks from the shard you're removing
   to another shard in the cluster.

   .. code-block:: javascript

      db.runCommand( { removeshard: "mongodb0" } )

   This operation will return the following response immediately:

   .. code-block:: javascript

      { msg : "draining started successfully" , state: "started" , shard :"mongodb0" , ok : 1 }

   Depending on your network capacity and the amount of data in the
   shard, this operation can take anywhere from a few minutes to several
   days to complete.

#. View progress of the migration.

   You can run the :dbcommand:`removeShard` again at any stage of the
   process to view the progress of the migration, as follows:

   .. code-block:: javascript

      db.runCommand( { removeShard: "mongodb0" } )

   The output should look something like this:

   .. code-block:: javascript

      { msg: "draining ongoing" , state: "ongoing" , remaining: { chunks: 42, dbs : 1 }, ok: 1 }

   In the ``remaining`` sub-document ``{ chunks: xx, dbs: y }``, a
   counter displays the remaining number of chunks that MongoDB must
   migrate to other shards and the number of MongoDB databases that have
   "primary" status on this shard.

   Continue checking the status of the :dbcommand:`removeShard` command
   until the remaining number of chunks to transfer is 0.

#. Move any databases to other shards in the cluster as needed.

   This is only necessary when removing a shard that is also the
   :term:`primary shard` for one or more databases.

   Issue the following command at the :program:`mongo` shell:

   .. code-block:: javascript

      db.runCommand( { movePrimary: "myapp", to: "mongodb1" })

   This command will migrate all remaining non-sharded data in the
   database named ``myapp`` to the shard named ``mongodb1``.

   .. warning::

      Do not run the :dbcommand:`movePrimary` until you have *finished*
      draining the shard.

   The command will not return until MongoDB completes moving all
   data. The response from this command will resemble the following:

   .. code-block:: javascript

      { "primary" : "mongodb1", "ok" : 1 }

#. Run :dbcommand:`removeShard` again to clean up all metadata
   information and finalize the shard removal, as follows:

   .. code-block:: javascript

      db.runCommand( { removeshard: "mongodb0" } )

   When successful, this command will return a document like this:

   .. code-block:: javascript

      { msg: "remove shard completed successfully" , stage: "completed", host: "mongodb0", ok : 1 }

Once the value of the ``stage`` field is "completed," you may safely
stop the processes comprising the ``mongodb0`` shard.

Chunk Management
----------------

This section describes various operations on :term:`chunks <chunk>` in
:term:`sharded clusters <sharded cluster>`. MongoDB automates these
processes; however, in some cases, particularly when you're setting up
a sharded cluster, you may need to create and manipulate chunks
directly.

.. _sharding-procedure-create-split:

Split Chunks
~~~~~~~~~~~~

Normally, MongoDB splits a :term:`chunk` following inserts when a
chunk exceeds the :ref:`chunk size <sharding-chunk-size>`. The
:term:`balancer` may migrate recently split chunks to a new shard
immediately if :program:`mongos` predicts future insertions will
benefit from the move.

The MongoDB treats all chunks the same, whether split manually or
automatically by the system.

.. warning::

   You cannot merge or combine chunks once you have split them.

You may want to split chunks manually if:

- you have a large amount of data in your cluster and very few
  :term:`chunks <chunk>`,
  as is the case after deploying a cluster using existing data.

- you expect to add a large amount of data that would
  initially reside in a single chunk or shard.

.. example::

   You plan to insert a large amount of data with :term:`shard key`
   values between ``300`` and ``400``, *but* all values of your shard
   keys are between ``250`` and ``500`` are in a single chunk.

Use :method:`sh.status()` to determine the current chunks ranges across
the cluster.

To split chunks manually, use the :dbcommand:`split` command with
operators: ``middle`` and ``find``. The equivalent shell helpers are
:method:`sh.splitAt()` or :method:`sh.splitFind()`.

.. example::

   The following command will split the chunk that contains
   the value of ``63109`` for the ``zipcode`` field in the ``people``
   collection of the ``records`` database:

   .. code-block:: javascript

      sh.splitFind( "records.people", { "zipcode": 63109 } )

:method:`sh.splitFind()` will split the chunk that contains the
*first* document returned that matches this query into two equally
sized chunks. You must specify the full namespace
(i.e. "``<database>.<collection>``") of the sharded collection to
:method:`sh.splitFind()`. The query in :method:`sh.splitFind()` need
not contain the shard key, though it almost always makes sense to
query for the shard key in this case, and including the shard key will
expedite the operation.

Use :method:`sh.splitAt()` to split a chunk in two using the queried
document as the partition point:

.. code-block:: javascript

   sh.splitAt( "records.people", { "zipcode": 63109 } )

However, the location of the document that this query finds with
respect to the other documents in the chunk does not affect how the
chunk splits.

.. _sharding-administration-pre-splitting:
.. _sharding-administration-create-chunks:

Create Chunks (Pre-Splitting)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In most situations a :term:`sharded cluster` will create and distribute
chunks automatically without user intervention. However, in a limited
number of use profiles, MongoDB cannot create enough chunks or
distribute data fast enough to support required throughput. Consider
the following scenarios:

- you must partition an existing data collection that resides on a
  single shard.

- you must ingest a large volume of data into a cluster that
  isn't balanced, or where the ingestion of data will lead to an
  imbalance of data.

  This can arise in an initial data loading, or in a case where you
  must insert a large volume of data into a single chunk, as is the
  case when you must insert at the beginning or end of the chunk
  range, as is the case for monotonically increasing or decreasing
  shard keys.

Preemptively splitting chunks increases cluster throughput for these
operations, by reducing the overhead of migrating chunks that hold
data during the write operation. MongoDB only creates splits after an
insert operation, and can only migrate a single chunk at a time. Chunk
migrations are resource intensive and further complicated by large
write volume to the migrating chunk.

To create and migrate chunks manually, use the following procedure:

#. Split empty chunks in your collection by manually performing
   :dbcommand:`split` command on chunks.

   .. example::

      To create chunks for documents in the ``myapp.users``
      collection, using the ``email`` field as the :term:`shard key`,
      use the following operation in the :program:`mongo` shell:

        .. code-block:: javascript

           for ( var x=97; x<97+26; x++ ){
             for( var y=97; y<97+26; y+=6 ) {
               var prefix = String.fromCharCode(x) + String.fromCharCode(y);
               db.runCommand( { split : "myapp.users" , middle : { email : prefix } } );
             }
           }

      This assumes a collection size of 100 million documents.

#. Migrate chunks manually using the :dbcommand:`moveChunk` command:

   .. example::

      To migrate all of the manually created user profiles evenly,
      putting each prefix chunk on the next shard from the other, run
      the following commands in the mongo shell:

        .. code-block:: javascript

           var shServer = [ "sh0.example.net", "sh1.example.net", "sh2.example.net", "sh3.example.net", "sh4.example.net" ];
           for ( var x=97; x<97+26; x++ ){
             for( var y=97; y<97+26; y+=6 ) {
               var prefix = String.fromCharCode(x) + String.fromCharCode(y);
               db.adminCommand({moveChunk : "myapp.users", find : {email : prefix}, to : shServer[(y-97)/6]})
             }
           }

   You can also let the balancer automatically distribute the new
   chunks.

.. _sharding-balancing-modify-chunk-size:

Modify Chunk Size
~~~~~~~~~~~~~~~~~

When you initialize a sharded cluster, the default chunk size is 64
megabytes. This default chunk size works well for most deployments. However, if you
notice that automatic migrations are incurring a level of I/O that
your hardware cannot handle, you may want to reduce the chunk
size. For the automatic splits and migrations, a small chunk size
leads to more rapid and frequent migrations.

To modify the chunk size, use the following procedure:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the config database:

   .. code-block:: javascript

      use config

#. Issue the following :method:`save() <db.collection.save()>`
   operation:

   .. code-block:: javascript

      db.settings.save( { _id:"chunksize", value: <size> } )

   Where the value of ``<size>`` reflects the new chunk size in
   megabytes. Here, you're essentially writing a document whose values
   store the global chunk size configuration value.

.. note::

   The :setting:`chunkSize` and :option:`--chunkSize <mongos --chunkSize>`
   options, passed at runtime to the :program:`mongos` **do not**
   affect the chunk size after you have initialized the cluster.

   To eliminate confusion you should *always* set chunk size using the
   above procedure and never use the runtime options.

Modifying the chunk size has several limitations:

- Automatic splitting only occurs when inserting :term:`documents
  <document>` or updating existing documents.

- If you lower the chunk size it may take time for all chunks to split to
  the new size.

- Splits cannot be "undone."

If you increase the chunk size, existing chunks must grow through
insertion or updates until they reach the new size.

.. _sharding-balancing-manual-migration:

Migrate Chunks
~~~~~~~~~~~~~~

In most circumstances, you should let the automatic balancer
migrate :term:`chunks <chunk>` between :term:`shards <shard>`.
However, you may want to migrate chunks manually in a few cases:

- If you create chunks by :term:`pre-splitting` the data in your
  collection, you will have to migrate chunks manually to distribute
  chunks evenly across the shards. Use pre-splitting in limited
  situations, to support bulk data ingestion.

- If the balancer in an active cluster cannot distribute chunks within
  the balancing window, then you will have to migrate chunks manually.

See the :ref:`chunk migration <sharding-chunk-migration>` section to
understand the internal process of how chunks move
between shards.

To migrate chunks, use the :dbcommand:`moveChunk` command.

.. note::

   To return a list of shards, use the :dbcommand:`listShards`
   command.

   Specify shard names using the :dbcommand:`addShard` command
   using the ``name`` argument. If you do not specify a name in the
   :dbcommand:`addShard` command, MongoDB will assign a name
   automatically.

The following example assumes that the field ``username`` is the
:term:`shard key` for a collection named ``users`` in the ``myapp``
database, and that the value ``smith`` exists within the :term:`chunk`
you want to migrate.

To move this chunk, you would issue the following command from a :program:`mongo`
shell connected to any :program:`mongos` instance.

.. code-block:: javascript

   db.adminCommand({moveChunk : "myapp.users", find : {username : "smith"}, to : "mongodb-shard3.example.net"})

This command moves the chunk that includes the shard key value "smith" to the
:term:`shard` named ``mongodb-shard3.example.net``. The command will
block until the migration is complete.

See :ref:`sharding-administration-create-chunks` for an introduction
to pre-splitting.

.. versionadded:: 2.2
   :dbcommand:`moveChunk` command has the: ``_secondaryThrottle``
   paramenter. When set to ``true``, MongoDB ensures that
   :term:`secondary` members have replicated operations before allowing
   new chunk migrations.

.. warning::

   The :dbcommand:`moveChunk` command may produce the following error
   message:

   .. code-block:: none

      The collection's metadata lock is already taken.

   These errors occur when clients have too many open :term:`cursors
   <cursor>` that access the chunk you are migrating. You can either
   wait until the cursors complete their operation or close the
   cursors manually.

   .. todo:: insert link to killing a cursor.

.. index:: balancing; operations
.. _sharding-balancing-operations:

Balancer Operations
-------------------

This section provides an overview of common administrative procedures
related to balancing and the balancing process.

.. seealso:: :ref:`sharding-balancing` and the
   :dbcommand:`moveChunk` that provides manual :term:`chunk`
   migrations.

.. _sharding-balancing-check-lock:

Check the Balancer Lock
~~~~~~~~~~~~~~~~~~~~~~~

To see if the balancer process is active in your :term:`cluster
<sharded cluster>`, do the following:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the config database:

   .. code-block:: javascript

      use config

#. Use the following query to return the balancer lock:

   .. code-block:: javascript

      db.locks.find( { _id : "balancer" } ).pretty()

   You can also use the following shell helper to return the same
   information:

   .. code-block:: javascript

      sh.getBalancerState().pretty()

When this command returns, you will see output like the following:

.. code-block:: javascript

   {   "_id" : "balancer",
   "process" : "mongos0.example.net:1292810611:1804289383",
     "state" : 2,
        "ts" : ObjectId("4d0f872630c42d1978be8a2e"),
      "when" : "Mon Dec 20 2010 11:41:10 GMT-0500 (EST)",
       "who" : "mongos0.example.net:1292810611:1804289383:Balancer:846930886",
       "why" : "doing balance round" }


Here's what this tells you:

- The balancer originates from the :program:`mongos` running on the
  system with the hostname ``mongos0.example.net``.

- The value in the ``state`` field indicates that a :program:`mongos`
  has the lock. For version 2.0 and later, the value of an active lock
  is ``2``; for earlier versions the value is ``1``.

  .. note::

     Use the :method:`sh.isBalancerRunning()` helper in the
     :program:`mongo` shell to determine if the balancer is
     running, as follows:

     .. code-block:: javascript

        sh.isBalancerRunning()

.. _sharding-schedule-balancing-window:

Schedule the Balancing Window
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some situations, particularly when your data set grows slowly and a
migration can impact performance, it's useful to be able to ensure
that the balancer is active only at certain times.  Use the following
procedure to specify a window during which the :term:`balancer` will
be able to migrate chunks:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the config database:

   .. code-block:: javascript

      use config

#. Use an operation modeled on the following example :method:`update()
   <db.collection.update()>` operation to modify the balancer's
   window:

   .. code-block:: javascript

      db.settings.update({ _id : "balancer" }, { $set : { activeWindow : { start : "<start-time>", stop : "<stop-time>" } } }, true )

   Replace ``<start-time>`` and ``<end-time>`` with time values using
   two digit hour and minute values (e.g ``HH:MM``) that describe the
   beginning and end boundaries of the balancing window.
   These times will be evaluated relative to the time zone of each individual
   :program:`mongos` instance in the sharded cluster.
   For instance, running the following
   will force the balancer to run between 11PM and 6AM local time only:

   .. code-block:: javascript

      db.settings.update({ _id : "balancer" }, { $set : { activeWindow : { start : "23:00", stop : "6:00" } } }, true )

.. note::

   The balancer window must be sufficient to *complete* the migration
   of all data inserted during the day.

   As data insert rates can change based on activity and usage
   patterns, it is important to ensure that the balancing window you
   select will be sufficient to support the needs of your deployment.

Remove a Balancing Window Schedule
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you have :ref:`set the balancing window
<sharding-schedule-balancing-window>` and wish to remove the schedule
so that the balancer is always running, issue the following sequence
of operations:

.. code-block:: javascript

   use config
   db.settings.update({ _id : "balancer" }, { $unset : { activeWindow : true })

.. _sharding-balancing-disable-temporally:

Disable the Balancer
~~~~~~~~~~~~~~~~~~~~

By default the balancer may run at any time and only moves chunks as
needed. To disable the balancer for a short period of time and prevent
all migration, use the following procedure:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to disable the balancer:

   .. code-block:: javascript

      sh.setBalancerState(false)

#. Later, issue the following command to enable the balancer:

   .. code-block:: javascript

      sh.setBalancerState(true)

.. note::

   If a balancing round is in progress, the system will complete the
   current round before the balancer is officially disabled.  After
   disabling, you can use the :method:`sh.getBalancerState()` shell
   function to determine whether the balancer is in fact disabled.

The above process and the :method:`sh.setBalancerState()` helper provide a
wrapper on the following process, which may be useful if you need to
run this operation from a driver that does not have helper functions:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the config database:

   .. code-block:: javascript

      use config

#. Issue the following update to disable the balancer:

   .. code-block:: javascript

      db.settings.update( { _id: "balancer" }, { $set : { stopped: true } } , true );

#. To enable the balancer again, alter the value of "stopped" as follows:

   .. code-block:: javascript

      db.settings.update( { _id: "balancer" }, { $set : { stopped: false } } , true );

.. index:: config servers; operations
.. _sharding-procedure-config-server:

Config Server Maintenance
-------------------------

Config servers store all cluster metadata, most importantly,
the mapping from :term:`chunks <chunk>` to :term:`shards <shard>`.
This section provides an overview of the basic
procedures to migrate, replace, and maintain these servers.

.. seealso:: :ref:`sharding-config-server`

Deploy Three Config Servers for Production Deployments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For redundancy, all production :term:`sharded clusters <sharded cluster>`
should deploy three config servers processes on three different
machines.

Do not use only a single config server for production deployments.
Only use a single config server deployments for testing.  You should
upgrade to three config servers immediately if you are shifting to
production.  The following process shows how to convert a test
deployment with only one config server to production deployment with
three config servers.

#. Shut down all existing MongoDB processes. This includes:

   - all :program:`mongod` instances or :term:`replica sets <replica set>`
     that provide your shards.

   - the :program:`mongod` instance that provides your existing config
     database.

   - all :program:`mongos` instances in your cluster.

#. Copy the entire :setting:`dbpath` file system tree from the
   existing config server to the two machines that will provide the
   additional config servers. These commands, issued on the system
   with the existing config database, ``mongo-config0.example.net`` may
   resemble the following:

   .. code-block:: sh

      rsync -az /data/configdb mongo-config1.example.net:/data/configdb
      rsync -az /data/configdb mongo-config2.example.net:/data/configdb

#. Start all three config servers, using the same invocation that you
   used for the single config server.

   .. code-block:: sh

      mongod --configsvr

#. Restart all shard :program:`mongod` and :program:`mongos` processes.

.. _sharding-process-config-server-migrate-same-hostname:

Migrate Config Servers with the Same Hostname
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use this process when you need to migrate a config server to a new
system but the new system will be accessible using the same host
name.

#. Shut down the config server that you're moving.

   This will render all config data for your cluster :ref:`read only
   <sharding-config-server>`.

#. Change the DNS entry that points to the system that provided the old
   config server, so that the *same* hostname points to the new
   system.

   How you do this depends on how you organize your DNS and
   hostname resolution services.

#. Move the entire :setting:`dbpath` file system tree from the old
   config server to the new config server. This command, issued on the
   old config server system, may resemble the following:

   .. code-block:: sh

      rsync -az /data/configdb mongo-config0.example.net:/data/configdb

#. Start the config instance on the new system. The default invocation
   is:

   .. code-block:: sh

      mongod --configsrv

When you start the third config server, your cluster will become
writable and it will be able to create new splits and migrate chunks
as needed.

.. _sharding-process-config-server-migrate-different-hostname:

Migrate Config Servers with Different Hostnames
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use this process when you need to migrate a config database to a new
server and it *will not* be accessible via the same hostname. If
possible, avoid changing the hostname so that you can use the
:ref:`previous procedure <sharding-process-config-server-migrate-same-hostname>`.

#. Shut down the :ref:`config server <sharding-config-server>` you're moving.

   This will render all config data for your cluster "read only:"

   .. code-block:: sh

      rsync -az /data/configdb mongodb.config2.example.net:/data/configdb

#. Start the config instance on the new system. The default invocation
   is:

   .. code-block:: sh

      mongod --configsrv

#. Shut down all existing MongoDB processes. This includes:

   - all :program:`mongod` instances or :term:`replica sets <replica set>`
     that provide your shards.

   - the :program:`mongod` instances that provide your existing
     config databases.

   - all :program:`mongos` instances in your cluster.

#. Restart all :program:`mongod` processes that provide the shard
   servers.

#. Update the :option:`--configdb <mongos --configdb>` parameter (or
   :setting:`configdb`) for all :program:`mongos` instances and
   restart all :program:`mongos` instances.

Replace a Config Server
~~~~~~~~~~~~~~~~~~~~~~~

Use this procedure only if you need to replace one of your config
servers after it becomes inoperable (e.g. hardware failure.) This
process assumes that the hostname of the instance will not change. If
you must change the hostname of the instance, use the process for
:ref:`migrating a config server to a different hostname
<sharding-process-config-server-migrate-different-hostname>`.

#. Provision a new system, with the same hostname as the previous
   host.

   You will have to ensure that the new system has the same IP address
   and hostname as the system it's replacing *or* you will need to
   modify the DNS records and wait for them to propagate.

#. Shut down *one* (and only one) of the existing config servers. Copy
   all this host's :setting:`dbpath` file system tree from the current system
   to the system that will provide the new config server. This
   command, issued on the system with the data files, may resemble the
   following:

   .. code-block:: sh

      rsync -az /data/configdb mongodb.config2.example.net:/data/configdb

#. Restart the config server process that you used in the previous
   step to copy the data files to the new config server instance.

#. Start the new config server instance. The default invocation is:

   .. code-block:: sh

      mongod --configsrv

Backup Cluster Metadata
~~~~~~~~~~~~~~~~~~~~~~~

The cluster will remain operational [#read-only]_ without one
of the config databases :program:`mongod` instances, creating a backup
of the cluster metadata from the config database is straight forward:

#. Shut down one of the :term:`config databases <config database>`.

#. Create a full copy of the data files (i.e. the path specified by
   the :setting:`dbpath` option for the config instance.)

#. Restart the original configuration server.

.. seealso:: :doc:`backups`.

.. [#read-only] While one of the three config servers unavailable, no
   the cluster cannot split any chunks nor can it migrate chunks
   between shards. Your application will be able to write data to the
   cluster. The :ref:`sharding-config-server` section of the
   documentation provides more information on this topic.

.. index:: troubleshooting; sharding
.. index:: sharding; troubleshooting
.. _sharding-troubleshooting:

Troubleshooting
---------------

The two most important factors in maintaining a successful sharded cluster are:

- :ref:`choosing an appropriate shard key <sharding-internals-shard-keys>` and

- :ref:`sufficient capacity to support current and future operations
  <sharding-capacity-planning>`.

You can prevent most issues encountered with sharding by ensuring that
you choose the best possible :term:`shard key` for your deployment and
ensure that you are always adding additional capacity to your cluster
well before the current resources become saturated. Continue reading
for specific issues you may encounter in a production environment.

.. _sharding-troubleshooting-not-splitting:

All Data Remains on One Shard
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Your cluster must have sufficient data for sharding to make
sense. Sharding works by migrating chunks between the shards until
each shard has roughly the same number of chunks.

The default chunk size is 64 megabytes. MongoDB will not begin
migrations until the imbalance of chunks in the cluster exceeds the
:ref:`migration threshold <sharding-migration-thresholds>`. While the
default chunk size is configurable with the :setting:`chunkSize`
setting, these behaviors help prevent unnecessary chunk migrations,
which can degrade the performance of your cluster as a whole.

If you have just deployed a sharded cluster, make sure that you have
enough data to make sharding effective. If you do not have sufficient
data to create more than eight 64 megabyte chunks, then all data will
remain on one shard. Either lower the :ref:`chunk size
<sharding-chunk-size>` setting, or add more data to the cluster.

As a related problem, the system will split chunks only on
inserts or updates, which means that if you configure sharding and do not
continue to issue insert and update operations, the database will not
create any chunks. You can either wait until your application inserts
data *or* :ref:`split chunks manually <sharding-procedure-create-split>`.

Finally, if your shard key has a low :ref:`cardinality
<sharding-shard-key-cardinality>`, MongoDB may not be able to create
sufficient splits among the data.

One Shard Receives too much Traffic
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some situations, a single shard or a subset of the cluster will
receive a disproportionate portion of the traffic and workload. In
almost all cases this is the result of a shard key that does not
effectively allow :ref:`write scaling <sharding-shard-key-write-scaling>`.

It's also possible that you have "hot chunks." In this case, you may
be able to solve the problem by splitting and then migrating parts of
these chunks.

In the worst case, you may have to consider re-sharding your data
and :ref:`choosing a different shard key <sharding-internals-choose-shard-key>`
to correct this pattern.

The Cluster does not Balance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you have just deployed your sharded cluster, you may want to
consider the :ref:`troubleshooting suggestions for a new cluster where
data remains on a single shard <sharding-troubleshooting-not-splitting>`.

If the cluster was initially balanced, but later developed an uneven
distribution of data, consider the following possible causes:

- You have deleted or removed a significant amount of data from the
  cluster. If you have added additional data, it may have a
  different distribution with regards to its shard key.

- Your :term:`shard key` has low :ref:`cardinality <sharding-shard-key-cardinality>`
  and MongoDB cannot split the chunks any further.

- Your data set is growing faster than the balancer can distribute
  data around the cluster. This is uncommon and
  typically is the result of:

  - a :ref:`balancing window <sharding-schedule-balancing-window>` that
    is too short, given the rate of data growth.

  - an uneven distribution of :ref:`write operations
    <sharding-shard-key-write-scaling>` that requires more data
    migration. You may have to choose a different shard key to resolve
    this issue.

  - poor network connectivity between shards, which may lead to chunk
    migrations that take too long to complete. Investigate your
    network configuration and interconnections between shards.

Migrations Render Cluster Unusable
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If migrations impact your cluster or application's performance,
consider the following options, depending on the nature of the impact:

#. If migrations only interrupt your clusters sporadically, you can
   limit the :ref:`balancing window
   <sharding-schedule-balancing-window>` to prevent balancing activity
   during peak hours. Ensure that there is enough time remaining to
   keep the data from becoming out of balance again.

#. If the balancer is always migrating chunks to the detriment of
   overall cluster performance:

   - You may want to attempt :ref:`decreasing the chunk size <sharding-balancing-modify-chunk-size>`
     to limit the size of the migration.

   - Your cluster may be over capacity, and you may want to attempt to
     :ref:`add one or two shards <sharding-procedure-add-shard>` to
     the cluster to distribute load.

It's also possible, that your shard key causes your
application to direct all writes to a single shard. This kind of
activity pattern can require the balancer to migrate most data soon after writing
it. Consider redeploying your cluster  with a shard key that provides
better :ref:`write scaling <sharding-shard-key-write-scaling>`.

Disable Balancing During Backups
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If MongoDB migrates a :term:`chunk` during a :doc:`backup
</administration/backups>`, you can end with an inconsistent snapshot
of your :term:`sharded cluster`. Never run a backup while the balancer is
active. To ensure that the balancer is inactive during your backup
operation:

- Set the :ref:`balancing window <sharding-schedule-balancing-window>`
  so that the balancer is inactive during the backup. Ensure that the
  backup can complete while you have the balancer disabled.

- :ref:`manually disable the balancer <sharding-balancing-disable-temporally>`
  for the duration of the backup procedure.

Confirm that the balancer is not active using the
:method:`sh.getBalancerState()` method before starting a backup
operation. When the backup procedure is complete you can reactivate
the balancer process.
