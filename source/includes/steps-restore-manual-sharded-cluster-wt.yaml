stepnum: 1
level: 4
source:
  file: steps-source-backup-tab.yaml
  ref: select-backup-tab-overview-page
---
stepnum: 2
level: 4
source:
  file: steps-source-prepare-restore-snapshot.yaml
  ref: click-deployment-select-restore
---
stepnum: 3
level: 4
title: "Select the Restore Point."
ref: select-shard-pit-manual
content: |

  a. Choose the point from which you want to restore your backup.

     .. include:: /includes/backup/select-restore-point.rst

  b. Click :guilabel:`Next`.

---
title: "Click :guilabel:`Download` to Restore the Files Manually."
stepnum: 4
level: 4
ref: select-restore-method

---
stepnum: 5
level: 4
source:
  file: steps-source-download-restore-snapshot.yaml
  ref: select-restore-destination
---
stepnum: 6
level: 4
title: "Retrieve the Snapshots."
ref: retrieve
content: |

  |mms| creates links to the snapshot. By default, these links are
  available for an hour and can be used just once.

  To download the snapshots:

  a. If you closed the restore panel, click :guilabel:`Backup`, then
     :guilabel:`Restore History`.

  b. When the restore job completes, click :guilabel:`(get link)`
     for each :term:`shard` and for one of the :term:`config servers
     <config server>` appears.

  c. Click:

     - The copy button to the right of the link to copy the link to
       use it later, or
     - :guilabel:`Download` to download the snapshot immediately.

---
stepnum: 7
level: 4
title: "Move the Snapshot Data Files to the Target Host."
ref: copy
content: |
  Before moving the snapshot's data files to the target host, check
  whether the target host contains any existing files and delete them.
  
  Extract the snapshot archive for the :term:`config server` and for
  each :term:`shard` to a temporary location.

  The following commands use </path/to/snapshot/> as a temporary
  path.

  .. code-block:: sh

     tar -xvf <backupSnapshot>.tar.gz
     mv <backupSnapshot> </path/to/snapshot>
---
stepnum: 8
level: 4
title: "Unmanage the Sharded Cluster."
ref: unmanage
content: |

  Before attempting to restore the data manually,
  :doc:`remove the sharded cluster from Automation
  </tutorial/unmanage-deployment>`.

  .. note::

     Steps 8 to 16 use the |csrs| files downloaded in Step 7.

---
stepnum: 9
level: 4
title: "Stop the Running MongoDB Processes."
ref: shutdown-mongod
content: |

  If restoring to an existing cluster, shut down the |mongod| or
  |mongos| process on the target host. Using a |mongo|
  shell, connect to a host running:

  .. list-table::
     :widths: 20 80

     * - |mongos|
       - Run :method:`db.shutdownServer() <db.shutdownServer>` from the ``admin`` database:

         .. code-block:: javascript

            db.getSiblingDB("admin").shutdownServer()

     * - |mongod|
       - Run :method:`db.isMaster() <db.isMaster>`:

         .. list-table::
            :widths: 20 20 60
            :header-rows: 1

            * - :data:`~isMaster.ismaster` returns
              - Member is
              - To Shut Down
            * - ``false``
              - :term:`secondary <Secondary>`
              - Run :method:`db.shutdownServer() <db.shutdownServer>`
                from the ``admin`` database.

                .. code-block:: javascript

                   db.getSiblingDB("admin").shutdownServer()

            * - ``true``
              - :term:`primary <Primary>`
              -

                a. Use :method:`rs.status() <rs.status>` to identify the other
                   members.
                b. Connect to each secondary and shut down their
                   |mongod| processes *first*.
                c. After the primary detects that a majority of members
                   are offline, it steps down.
                d. After the primary steps down (:method:`db.isMaster`
                   returns :data:`ismaster: false`), shut down the primary's
                   |mongod|.

---
stepnum: 10
level: 4
title: "Copy the Completed Snapshots to Restore to Other Hosts."
ref: distribute
content: |

  - For the config server, copy the restored config server database to
    the working database path of each :term:`replica set` member.

  - For each shard, copy the restored shard database to the working
    database path of each replica set member.

---
stepnum: 11
level: 4
title: "Drop the Minimum Valid Timestamp."
ref: drop-minvalid-csrs-timestamp
content: |

  Issue the following command:

  .. code-block:: javascript

     db.getSiblingDB("local").replset.minvalid.drop()

---
stepnum: 12
level: 4
title: "Verify Hardware and Software Requirements."
ref: verify-requirements
content: |

  .. list-table::
     :widths: 30 70
     :stub-columns: 1

     * - Storage Capacity
       - The target host hardware needs enough free storage space for
         the restored data. If you want to keep any existing sharded
         cluster data on this host, make sure the host has enough free
         space for both data sets.

     * - MongoDB Version
       - The target host and source host must run the same MongoDB
         Server version. To check the MongoDB version, run ``mongod
         --version`` from a terminal or shell.

  To learn more about installation, see :manual:`/installation`.



---
stepnum: 13
level: 4
title: "Prepare Data and Log Directories on the Target Host."
ref: prepare-directories-csrs
content: |

  a. Create a directory tree on the target host for the restored
     database files and logs.

     .. code-block:: sh

        mkdir -p </path/to/datafiles>/log

  b. Grant the user that runs the |mongod| read, write, and execute
     permissions for everything in that directory.

     .. code-block:: sh

        chown -R mongodb:mongodb </path/to/datafiles>
        chmod -R 770 </path/to/datafiles>

---
stepnum: 14
level: 4
title: "Create Configuration File."
ref: create-replset-config-csrs
content: |

  a. Create a mongod configuration file in your database directory
     using your preferred text editor.

     .. code-block:: sh

        vi </path/to/datafiles>/mongod.conf

     .. note::

        If you have access to the original configuration file for the
        |mongod|, you can copy it to your database directory on the
        target host instead.

  b. Grant the user that runs the |mongod| read and write permissions
     on your configuration file.

     .. code-block:: sh

        chown mongodb:mongodb </path/to/datafiles>/mongod.conf
        chmod 644 </path/to/datafiles>/mongod.conf

  c. Modify your configuration as you require for your deployment.

     .. list-table::
        :widths: 30 70
        :header-rows: 1

        * - Setting
          - Required Value

        * - :setting:`storage.dbPath`
          - Path to your data directory

        * - :setting:`systemLog.path`
          - Path to your log directory

        * - :setting:`net.bindIp`
          - IP address of the host machine

        * - :setting:`replication.replSetName`
          - Same value across each member in any given replica set

        * - :setting:`sharding.clusterRole`
          - Same value across each member in any given replica set

---
stepnum: 15
level: 4
title: "Restore the CSRS Primary |mongod| Data Files."
ref: restore-backup-files-csrs
content: |

  a. Copy the |mongod| data files from the backup data location to the
     data directory you created:

     .. code-block:: sh

        cp -a </path/to/snapshot/> </path/to/datafiles>

     The ``-a`` option recursively copies the contents of the source
     path to the destination path while preserving folder and file
     permissions.

  #. Open your replica set configuration file in your preferred text
     editor.

  #. Comment out or omit the following
     :ref:`configuration file <configuration-file>` settings:

     .. code-block:: yaml

        #replication
        #  replSetName: <myCSRSName>
        #sharding
        #  clusterRole: configsvr

  #. Start the |mongod|, specifying:

     - The ``--config`` option and the full path to the configuration
       file, and
     - The ``disableLogicalSessionCacheRefresh`` server parameter.
       Depending on your path, you may need to specify
       the path to the |mongod| binary.

       .. code-block:: sh

          mongod --config </path/to/datafiles>/mongod.conf \
                 --setParameter disableLogicalSessionCacheRefresh=true

       If you have |mongod| configured to run as a system service,
       start it using the recommended process for your platform's
       service manager.

  #. After the |mongod| starts, connect to it using the
     |mongo| shell.
---
stepnum: 16
level: 4
title: "Remove Replica Set-Related Collections from the ``local`` Database."
ref: remove-replset-config-csrs
content: |

  .. include:: /includes/fact-restore-manual-user-role.rst
  
  Run the following commands to remove the previous replica set
  configuration and other non-oplog, replication-related collections.

  .. code-block:: javascript

     db.getSiblingDB("local").replset.minvalid.drop()
     db.getSiblingDB("local").replset.oplogTruncateAfterPoint.drop()
     db.getSiblingDB("local").replset.election.drop()
     db.getSiblingDB("local").system.replset.remove( {} )

  A successful response should look like this:

  .. code-block:: javascript
     :copyable: false

     > db.getSiblingDB("local").replset.minvalid.drop()
     true
     > db.getSiblingDB("local").replset.oplogTruncateAfterPoint.drop()
     true
     > db.getSiblingDB("local").replset.election.drop()
     true
     > db.getSiblingDB("local").system.replset.remove( {} )
     WriteResult( { "nRemoved" : 1 } )
---
stepnum: 17
level: 4
title: "Add a New Replica Set Configuration."
ref: add-new-replset-config-csrs
content: |

  Insert the following document into the ``system.replset`` collection
  in the ``local`` database. Change
  ``<replaceMeWithTheCSRSName>`` to the name of your replica set
  and ``<port>`` to the port of your replica set.

  .. code-block:: javascript
     :linenos:

     db.getSiblingDB("local").system.replset.insert( {
       "_id" : "<replaceMeWithTheCSRSName>",
       "version" : NumberInt(1),
       "configsvr" : true,
       "protocolVersion" : NumberInt(1),
       "members" : [
         {
           "_id" : NumberInt(0),
           "host" : "localhost:<port>"
         }
       ],
       "settings" : {

       }
     } )

  A successful response should look like this:

  .. code-block:: javascript
     :copyable: false

     WriteResult( { "nInserted" : 1 } )


---
stepnum: 18
level: 4
title: "Insert the Minimum Valid Timestamp."
ref: set-minvalid-csrs-timestamp
content: |

  Issue the following command:

  .. code-block:: javascript

     db.getSiblingDB("local").replset.minvalid.insert( {
       "_id" : ObjectId(),
       "t" : NumberLong(-1),
       "ts" : Timestamp(0,1)
     } );



---
stepnum: 19
level: 4
title: "Set the Restore Point to the ``Restore Timestamp`` values from the ``restoreInfo`` file."
ref: set-restore-point-csrs
content: |
  Set the ``oplogTruncateAfterPoint`` document to the ``restoreTS.getTime()`` and
  ``restoreTS.getInc()`` values provided in the
  ``Restore Timestamp`` field of the :ref:`restoreInfo.txt <com-restore-info-rs>` file.

  .. code-block:: javascript

     truncateAfterPoint = Timestamp(restoreTS.getTime(), restoreTS.getInc())
     db.getSiblingDB("local").replset.oplogTruncateAfterPoint.insert( {
        "_id": "oplogTruncateAfterPoint",
        "oplogTruncateAfterPoint": truncateAfterPoint
     } )

  A successful response should look like this:

  .. code-block:: javascript
     :copyable: false

     WriteResult( { "nInserted" : 1 } )

  .. note::
     Each member has its own ``restoreInfo.txt`` file, but the
     ``Restore Timestamp`` values should be the same in each file.

---
stepnum: 20
level: 4
title: "Restart as a Single-Node Replica Set to Recover the Oplog."
ref: restart-special-db-csrs-1
content: |

  Start the |mongod|. Depending on your path, you may need to specify
  the path to the |mongod| binary. The |mongod| replays the oplog up to the
  ``Restore timestamp``.

  .. important::

     In the following command, you must use ``<ephemeralPort>``. This
     port must differ from the ``<port>`` you set in Step 14.


  .. code-block:: sh

     mongod --dbpath </path/to/datafiles> \
            --port <ephemeralPort> \
            --replSet <replaceMeWithTheCSRSName> \
            --configsvr

---
stepnum: 21
level: 4
title: "Stop the Temporary Single-Node Config Server Replica Set."
ref: stop-temp-replset-1
content: |

  .. code-block:: sh

     mongod --dbpath </path/to/datafiles> \
            --port <ephemeralPort> \
            --shutdown

---
stepnum: 22
title: "Run the MongoDB Backup Restore Utility (Point-in-Time Restore Only)."
level: 4
ref: mbru-binary-shard
content: |

  a. Download the MongoDB Backup Restore Utility to your host.

     If you closed the restore panel, click :guilabel:`Backup`, then
     :guilabel:`More` and then :guilabel:`Download MongoDB Backup
     Restore Utility`.

  #. Start a |mongod| instance without authentication
     enabled using the extracted snapshot directory as the data
     directory. Depending on your path, you may need to specify
     the path to the |mongod| binary.

     .. code-block:: sh

        mongod --port <port number> \
               --dbpath <temp-database-path> \
               --setParameter ttlMonitorEnabled=false

     .. warning::
        The MongoDB Backup Restore Utility doesn't support
        authentication, so you can't start this temporary database with
        authentication.

  #. Run the MongoDB Backup Restore Utility on your target  host.
     Run it once for the replica set.

     .. include:: /includes/fact-pre-configured-mbru-command.rst

     The ``mongodb-backup-restore-util`` command uses the following
     options:

     .. cond:: cloud

        .. include:: /includes/fact-restore-manual-replica-set-cloud.rst

     .. cond:: onprem

        .. include:: /includes/fact-restore-manual-replica-set-onprem.rst

     :icon:`check-circle` means that if you copied the
     ``mongodb-backup-restore-util`` command provided in
     |application|, this field is pre-configured.

  #. Issue the following command. Depending on your path, you may need to specify
     the path to the |mongod| binary.

     .. code-block:: sh

        mongod --dbpath </path/to/datafiles> \
               --port <ephemeralPort> \
               --shutdown

---
stepnum: 23
level: 4
title: "Restart as a Standalone to Recover the Oplog."
ref: restart-standalone-oplog-recovery
content: |

  Start the |mongod| with the following ``setParameter`` options set
  to ``true``:

  - ``recoverFromOplogAsStandalone``
  - ``takeUnstableCheckpointOnShutdown``

  Depending on your path, you may need to specify
  the path to the |mongod| binary. The |mongod| replays
  the oplog up to the ``Restore timestamp``.

  .. code-block:: sh

     mongod --dbpath </path/to/datafiles> --port <ephemeralPort> \
            --setParameter recoverFromOplogAsStandalone=true \
            --setParameter takeUnstableCheckpointOnShutdown=true

---
stepnum: 24
level: 4
title: "Stop the Temporary |csrs| Standalone Used to Recover the Oplog."
ref: stop-temp-csrs-standalone-2
content: |

  Issue the following command. Depending on your path, you may need to specify
  the path to the |mongod| binary.

  .. code-block:: sh

     mongod --dbpath </path/to/datafiles> \
            --port <ephemeralPort> \
            --shutdown

---
stepnum: 25
level: 4
title: "Restart as a Standalone to Clear Old Settings."
ref: restart-special-db-2
content: |

  Start the |mongod|. Depending on your path, you may need to specify
  the path to the |mongod| binary.

  .. code-block:: sh

     mongod --dbpath </path/to/datafiles> --port <ephemeralPort>

---
stepnum: 26
level: 4
title: "Clean Documents in Config Database that Reference the Previous Sharded Cluster Configuration."
ref: clean-config-server
content: |

  .. code-block:: javascript
     :linenos:

     db.getSiblingDB("config").mongos.remove( {} );
     db.getSiblingDB("config").lockpings.remove( {} );
     db.getSiblingDB("config").databases.updateMany( {"primary": SOURCE_SHARD_0_NAME},{"$set" : {"primary": DEST_SHARD_0_NAME}} );
     db.getSiblingDB("config").databases.updateMany( {"primary": SOURCE_SHARD_1_NAME},{"$set" : {"primary": DEST_SHARD_1_NAME}} );
     db.getSiblingDB("config").databases.updateMany( {"primary": SOURCE_SHARD_2_NAME},{"$set" : {"primary": DEST_SHARD_2_NAME}} );
     db.getSiblingDB("config").chunks.updateMany( {"shard": SOURCE_SHARD_0_NAME},{"$set" : {"shard": DEST_SHARD_0_NAME, "history": []}} );
     db.getSiblingDB("config").chunks.updateMany( {"shard": SOURCE_SHARD_1_NAME},{"$set" : {"shard": DEST_SHARD_1_NAME, "history": []}} );
     db.getSiblingDB("config").chunks.updateMany( {"shard": SOURCE_SHARD_2_NAME},{"$set" : {"shard": DEST_SHARD_2_NAME, "history": []}} );
     db.getSiblingDB("config").collections.updateMany( {"primary": SOURCE_SHARD_0_NAME},{"$set" : {"primary": DEST_SHARD_0_NAME}} );
     db.getSiblingDB("config").collections.updateMany( {"primary": SOURCE_SHARD_1_NAME},{"$set" : {"primary": DEST_SHARD_1_NAME}} );
     db.getSiblingDB("config").collections.updateMany( {"primary": SOURCE_SHARD_2_NAME},{"$set" : {"primary": DEST_SHARD_2_NAME}} );
     db.getSiblingDB("config").shards.remove( {} );
     db.getSiblingDB("config").shards.insert( {
       "_id": DEST_SHARD_0_NAME,
       "host": DEST_SHARD_0_NAME + "/" + DEST_SHARD_0_HOSTNAME,
       "State": 1,
     } );
     db.getSiblingDB("config").shards.insert( {
       "_id": DEST_SHARD_1_NAME,
       "host": DEST_SHARD_1_NAME + "/" + DEST_SHARD_1_HOSTNAME,
       "State": 1,
     } );
     db.getSiblingDB("config").shards.insert( {
       "_id": DEST_SHARD_2_NAME,
       "host": DEST_SHARD_2_NAME + "/" + DEST_SHARD_2_HOSTNAME,
       "State": 1,
     } )

  .. note::

     This example covers a three shard cluster. Replace the following
     values with those in your configuration:

     - ``SOURCE_SHARD_<X>_NAME``
     - ``<DEST_SHARD_<X>_NAME``
     - ``DEST_SHARD_<X>_HOSTNAME``
---
stepnum: 27
level: 4
title: "Restart the |mongod| as a New Single-node Replica Set."
ref: restart-as-repl-csrs
content: |

  a. Open the configuration file in your preferred text editor.

  #. Uncomment or add the following configuration file options:

     .. code-block:: yaml

        replication
          replSetName: myNewCSRSName
        sharding
          clusterRole: configsvr

  #. To change the replica set name, update the
     :setting:`replication.replSetName <replication.replSetName>` field with the new
     name before proceeding.

  #. Start the |mongod| with the updated configuration file. Depending on your
     path, you may need to specify the path to the |mongod| binary.

     .. code-block:: sh

        mongod --config </path/to/datafiles>/mongod.conf

     If you have |mongod| configured to run as a system service, start
     it using the recommended process for your platform's service
     manager.

  #. After the |mongod| starts, connect to it using the |mongo| shell.

---
stepnum: 28
level: 4
title: "Initiate the New Replica Set."
ref: initiate-csrs
content: |
  Initiate the replica set using :method:`rs.initiate() <rs.initiate>` with the
  default settings.

  .. code-block: javascript

     rs.initiate( {
       _id : <replaceMeWithTheCSRSName>,
       members: [ {
         _id : 0,
         host : <host:port>
       } ]
     } )

  Once the operation completes, use :method:`rs.status() <rs.status>` to check
  that the member has become the :term:`primary <Primary>`.
---
stepnum: 29
level: 4
title: "Add Additional Replica Set Members."
ref: add-members-csrs
content: |

  a. For each replica set member in the |csrs|, start the |mongod| on
     its host.

  b. Once you have started up all remaining members of the cluster
     successfully, connect a |mongo| shell to the primary
     replica set member.

  c. From the primary, use the :method:`rs.add() <rs.add>` method to add each
     member of the replica set. Include the replica set name as the
     prefix, followed by the hostname and port of the member's |mongod|
     process:

     .. code-block:: javascript

        rs.add("myNewCSRSName/config2.example.net:<port>")
        rs.add("myNewCSRSName/config3.example.net:<port>")

  d. If you want to add the member with specific replica
     :rsconf:`member <members[n]>` configuration settings, you can pass
     a document to :method:`rs.add() <rs.add>` that defines the member hostname
     and any :rsconf:`members[n]` settings your deployment requires.

     .. code-block:: javascript

        rs.add(
         {
           "host" : "myNewCSRSName/config2.example.net:<port>",
           priority: <int>,
           votes: <int>,
           tags: <int>
         }
        )

  e. Each new member performs an :ref:`initial sync
     <replica-set-initial-sync>` to catch up to the primary. Depending
     on data volume, network, and host performance factors, initial
     sync might take a while to complete.

  f. The replica set might elect a new primary while you add additional
     members. You can only run :method:`rs.add() <rs.add>` from the primary. To
     identify which member is the current primary, use
     :method:`rs.status() <rs.status>`.

---
stepnum: 30
level: 4
title: "Configure Any Additional Required Replication Settings."
ref: configure-replication-csrs
content: |

  The :method:`rs.reconfig() <rs.reconfig>` method updates the replica set
  configuration based on a configuration document passed in as a
  parameter.

  a. Run :method:`rs.reconfig() <rs.reconfig>` against the primary member of the
     replica set.

  b. Reference the original configuration file output of the replica
     set and apply settings as needed.

---
stepnum: 31
level: 4
title: "Remove Replica Set-Related Collections from the ``local`` Database."
ref: remove-replset-config-shard
content: |

  .. note::

     These steps appear repetitive, but cover shards instead of the
     |csrs|.

  .. include:: /includes/fact-restore-manual-user-role.rst

  Run the following commands to remove the previous replica set
  configuration and other non-oplog, replication-related collections.

  .. code-block:: javascript

     db.getSiblingDB("local").replset.minvalid.drop()
     db.getSiblingDB("local").replset.oplogTruncateAfterPoint.drop()
     db.getSiblingDB("local").replset.election.drop()
     db.getSiblingDB("local").system.replset.remove( {} )

  A successful response should look like this:

  .. code-block:: javascript
     :copyable: false

     > db.getSiblingDB("local").replset.minvalid.drop()
     true
     > db.getSiblingDB("local").replset.oplogTruncateAfterPoint.drop()
     true
     > db.getSiblingDB("local").replset.election.drop()
     true
     > db.getSiblingDB("local").system.replset.remove( {} )
     WriteResult( { "nRemoved" : 1 } )

---
stepnum: 32
level: 4
title: "Insert the Minimum Valid Timestamp."
ref: set-minvalid-shard-timestamp
content: |

  Issue the following command:

  .. code-block:: javascript

     db.getSiblingDB("local").replset.minvalid.insert( {
       "_id" : ObjectId(),
       "t" : NumberLong(-1),
       "ts" : Timestamp(0,1)
     } );
---
stepnum: 33
level: 4
title: "Add a New Replica Set Configuration."
ref: add-new-replset-config
content: |

  Insert the following document into the ``system.replset`` collection
  in the ``local`` database. Change
  ``<replaceMeWithTheShardName>`` to the name of your replica set
  and ``<port>`` to the port of your replica set.

  .. code-block:: javascript
     :linenos:

     db.getSiblingDB("local").system.replset.insert( {
       "_id" : "<replaceMeWithTheShardName>",
       "version" : NumberInt(1),
       "protocolVersion" : NumberInt(1),
       "members" : [
         {
           "_id" : NumberInt(0),
           "host" : "localhost:<port>"
         }
       ],
       "settings" : {

       }
     } )

  A successful response should look like this:

  .. code-block:: javascript
     :copyable: false

     WriteResult( { "nInserted" : 1 } )

---
stepnum: 34
level: 4
title: "Set the Restore Point to the ``Restore Timestamp`` value from the ``restoreInfo`` file."
ref: set-restore-point
content: |
  Set the ``oplogTruncateAfterPoint`` document to the values in the ``Restore Timestamp`` field
  given in the :ref:`restoreInfo.txt <com-restore-info-sc>` file.

  .. code-block:: javascript

     truncateAfterPoint = Timestamp(restoreTS.getTime(), restoreTS.getInc())
     db.getSiblingDB("local").replset.oplogTruncateAfterPoint.insert( {
        "_id": "oplogTruncateAfterPoint",
        "oplogTruncateAfterPoint": truncateAfterPoint
     } )

  A successful response should look like this:

  .. code-block:: javascript
     :copyable: false

     WriteResult( { "nInserted" : 1 } )

---
stepnum: 35
level: 4
title: "Restart as a Single-Node Replica Set to Recover the Oplog."
ref: restart-special-db-3
content: |

  Start the |mongod|. Depending on your path, you may need to specify
  the path to the |mongod| binary. The |mongod| replays the oplog up to
  the ``Restore timestamp``.

  .. important::

     This command uses ``<ephemeralPort>``. This port must differ
     from the ``<port>`` you set in **Step 14**.

  .. code-block:: sh

     mongod --dbpath </path/to/datafiles> \
            --port <ephemeralPort> \
            --replSet <replaceMeWithTheShardName>

---
stepnum: 36
level: 4
title: "Stop the Temporary Single-Node Shard Replica Set."
ref: stop-temp-replset-2
content: |

  .. code-block:: sh

     mongod --dbpath </path/to/datafiles> \
            --port <ephemeralPort> \
            --shutdown

---
stepnum: 37
title: "Run the MongoDB Backup Restore Utility (Point-in-Time Restore Only)."
level: 4
ref: mbru-binary-csrs
content: |

  a. Download the MongoDB Backup Restore Utility to your host.

     If you closed the restore panel, click :guilabel:`Backup`, then
     :guilabel:`More` and then :guilabel:`Download MongoDB Backup
     Restore Utility`.

  #. Start a |mongod| instance without authentication
     enabled using the extracted snapshot directory as the data
     directory. Depending on your path, you may need to specify
     the path to the |mongod| binary.

     .. code-block:: sh

        mongod --port <port number> \
               --dbpath <temp-database-path> \
               --setParameter ttlMonitorEnabled=false

     .. warning::
        The MongoDB Backup Restore Utility doesn't support
        authentication, so you can't start this temporary database with
        authentication.

  #. Run the MongoDB Backup Restore Utility on your target  host.
     Run it once for the replica set.

     .. include:: /includes/fact-pre-configured-mbru-command.rst

     The ``mongodb-backup-restore-util`` command uses the following
     options:

     .. cond:: cloud

        .. include:: /includes/fact-restore-manual-replica-set-cloud.rst

     .. cond:: onprem

        .. include:: /includes/fact-restore-manual-replica-set-onprem.rst

     :icon:`check-circle` means that if you copied the
     ``mongodb-backup-restore-util`` command provided in
     |application|, this field is pre-configured.

---
stepnum: 38
level: 4
title: "Stop the Temporary Shard Standalone."
ref: stop-temp-shard-standalone-1
content: |

  Issue the following command. Depending on your path, you may need to specify
  the path to the |mongod| binary.

  .. code-block:: sh

     mongod --dbpath </path/to/datafiles> \
            --port <ephemeralPort> \
            --shutdown

---
stepnum: 39
level: 4
title: "Restart as a Standalone to Clear Old Settings."
ref: restart-special-db-4
content: |

  Start the |mongod|. Depending on your path, you may need to specify
  the path to the |mongod| binary.

  .. code-block:: sh

     mongod --dbpath </path/to/datafiles> --port <ephemeralPort>

---
stepnum: 40
level: 4
title: "Clean Documents in the Admin and Config Databases."
ref: clean-admin-config-dbs
content: |

  .. code-block:: javascript
     :linenos:

     db.getSiblingDB("admin").system.version.remove( {"_id": "minOpTimeRecovery"} );

     db.getSiblingDB("admin").system.version.updateOne(
       { "_id": "shardIdentity" },
       {
         $set: {
           "shardName": <ShardName>,
           configsvrConnectionString: NEW_CONFIG_NAME + "/" + NEW_CONFIG_HOSTNAME,
           clusterId: <clusterId>
         }
       }
     );

     db.getSiblingDB("config").cache.collections.drop()
     db.getSiblingDB("config").cache.databases.drop()
     db.getSiblingDB("config").cache.chunks.config.system.sessions.drop()

  .. note::

     Replace the following values with those in your configuration:

     - ``<ShardName>``
     - ``<clusterId>``
     - ``NEW_CONFIG_NAME``
     - ``NEW_CONFIG_HOSTNAME``

---
stepnum: 41
level: 4
title: "Stop the Temporary Document Cleaning Standalone."
ref: stop-temp-shard-standalone-2
content: |

  Depending on your path, you may need to specify the path to the
  |mongod| binary.

  .. code-block:: sh

     mongod --dbpath </path/to/datafiles> \
            --port <ephemeralPort> \
            --shutdown
---
stepnum: 42
level: 4
title: Connect a ``mongo`` Shell to the ``mongod`` Instance.
ref: connect-to-instance
content: |

  From the host running this |mongod| process, start the
  |mongo| shell. Depending on your path, you may need to specify
  the path to the |mongo| shell. To connect to the |mongod| listening to
  localhost on port ``<port>``, run:

  .. code-block:: sh

     mongo --port <port>

  After the |mongod| starts accepting connections, continue.
---
stepnum: 43
level: 4
title: Initiate the New Replica Set.
ref: initiate-new-replset
content: |

  Run :method:`rs.initiate() <rs.initiate>` on the replica set:

  .. code-block:: javascript

     rs.initiate( {
       _id : <replaceMeWithTheShardName>,
       members: [ {
         _id : 0,
         host : <host:port>
       } ]
     } )

  MongoDB initiates a set that consists of the current member and that
  uses the default replica set configuration.

---
stepnum: 44
level: 4
title: "Shut Down the New Replica Set."
ref: shutdown-new-replset
content: |

  .. code-block:: javascript

     db.shutdownServer( {} );

---
stepnum: 45
level: 4
title: "Reimport the Sharded Cluster."
ref: reimport
content: |

  To manage the sharded cluster with Automation again,
  :doc:`import the sharded cluster </tutorial/add-existing-mongodb-processes>`
  back into |mms|.

---
stepnum: 46
level: 4
title: "Start the Sharded Cluster Balancer."
ref: start-balancer
content: |

  Once a restore completes, the
  :manual:`sharded cluster balancer </core/sharding-balancer-administration>`
  is turned off. To start the balancer:

  a. Click :guilabel:`Deployment`.
  #. Click :icon:`ellipsis-h` on the card for your desired sharded
     cluster.
  #. Click :guilabel:`Manager Balancer`.
  #. Toggle to :guilabel:`Yes`.
  #. Click :icon:`pencil-alt` to the right of :guilabel:`Set the Balancer State`.
  #. Toggle to :guilabel:`Yes`.
  #. Click :guilabel:`Save`.
  #. Click :guilabel:`Review & Deploy` to save the changes.

stepnum: 47
level: 4
title: "Restore the Shard Primary |mongod| Data Files."
ref: restore-backup-files
content: |

  a. Copy the |mongod| data files from the backup data location to the
     data directory you created:

     .. code-block:: sh

        cp -a </path/to/snapshot/> </path/to/datafiles>

     The ``-a`` option recursively copies the contents of the source
     path to the destination path while preserving folder and file
     permissions.

  #. Open your replica set configuration file in your preferred text
     editor.

  #. Comment out or omit the following
     :ref:`configuration file <configuration-file>` settings:

     .. code-block:: yaml

        #replication
        #  replSetName: <myShardName>
        #sharding
        #  clusterRole: shardsvr

  #. Start the |mongod|, specifying:

     - The ``--config`` option and the full
       path to the configuration file, and
     - The ``disableLogicalSessionCacheRefresh`` server parameter.

     Depending on your path, you may need to specify
     the path to the |mongod| binary.

      .. code-block:: sh

         mongod --config </path/to/datafiles>/mongod.conf \
                --setParameter disableLogicalSessionCacheRefresh=true

      If you have |mongod| configured to run as a system service, start
      it using the recommended process for your platform's service
      manager.

  #. After the |mongod| starts, connect to it using the
     |mongo| shell.
---
stepnum: 48
level: 4
title: "Create a Temporary User with the ``__system`` Role."
ref: create-system-user
content: |

  .. important::

     Skip this step if the cluster does not enforce authentication.

  Clusters that enforce :ref:`authentication <authentication>` limit
  who can change the :data:`admin.system.version` collection. Clusters
  limit permission to users with the :authrole:`__system` role.

  .. warning::

     The ``__system`` role allows a user to take any action
     against any object in the database.

     **Do not** keep this user active beyond the scope of this
     procedure. This procedure includes instructions for removing the
     user created in this step.

     Consider creating this user with the ``clientSource``
     :ref:`authentication restriction <method-createUser-authentication-restrictions>`
     configured such that only the specified hosts can
     authenticate as the privileged user.

  a. Authenticate as a user with either the :authrole:`userAdmin` role
     on the ``admin`` database or the :authrole:`userAdminAnyDatabase`
     role:

     .. code-block:: javascript
        :copyable: false

        db.getSiblingDB("admin").auth("<myUserAdmin>","<replaceMeWithAStrongPassword>")

  #. Create a user with the :authrole:`__system` role:

     .. code-block:: javascript

        db.getSiblingDB("admin").createUser(
          {
            user: "<myTempSystemUserWithTotalAccess>",
            pwd: "<replaceMeWithAStrongPassword>",
            roles: [ "__system" ]
          }
        )

     Make these passwords random, long, and complex. Keep the system
     secure and prevent or delay malicious access.

  #. Authenticate as the privileged user:

     .. code-block:: javascript

        db.getSiblingDB("admin").auth("<myTempSystemUserWithTotalAccess>","<replaceMeWithAStrongPassword>")

---
stepnum: 49
level: 4
title: "Restart the |mongod| as a New Single-node Replica Set."
ref: restart-as-repl-shard
content: |

  a. :ref:`Shut down <terminate-mongod-processes>` the |mongod|.

  b. Open the configuration file in your preferred text editor.

  c. Uncomment or add the following configuration file options:

     .. code-block:: yaml

        replication
          replSetName: myNewShardName
        sharding
          clusterRole: shardsvr

  d. To change the replica set name, update the
     :setting:`replication.replSetName <replication.replSetName>` field with
     the new name before proceeding.

  e. Start the |mongod| with the updated configuration file. Depending on your
     path, you may need to specify the path to the |mongod| binary.

     .. code-block:: sh

        mongod --config </path/to/datafiles>/mongod.conf

     If you have |mongod| configured to run as a system service, start
     it using the recommended process for your platform's service
     manager.

  f. After the |mongod| starts, connect to it using the
     |mongo| shell.

---
stepnum: 50
level: 4
title: "Initiate the New Replica Set."
ref: initiate-shard
content: |
  Initiate the replica set using :method:`rs.initiate() <rs.initiate>` with the
  default settings.

  .. code-block: javascript

     rs.initiate( {
       _id : <replaceMeWithTheShardName>,
       members: [ {
         _id : 0,
         host : <host:port>
       } ]
     } )

  Once the operation completes, use :method:`rs.status() <rs.status>` to check
  that the member has become the :term:`primary <Primary>`.
---
stepnum: 51
level: 4
title: "Add Additional Replica Set Members."
ref: add-members-shard
content: |

  a. For each replica set member in the shard replica set, start the
     |mongod| on its host.

  b. Once you have started up all remaining members of the cluster
     successfully, connect a |mongo| shell to the primary
     replica set member.

  c. From the primary, use the :method:`rs.add() <rs.add>` method to add each
     member of the replica set. Include the replica set name as the
     prefix, followed by the hostname and port of the member's |mongod|
     process:

     .. code-block:: javascript

        rs.add("myNewShardName/repl2.example.net:<port>")
        rs.add("myNewShardName/repl3.example.net:<port>")

  d. If you want to add the member with specific replica
     :rsconf:`member <members[n]>` configuration settings, you can pass
     a document to :method:`rs.add() <rs.add>` that defines the member hostname
     and any :rsconf:`members[n]` settings your deployment requires.

     .. code-block:: javascript

        rs.add(
         {
           "host" : "myNewShardName/repl2.example.net:<port>",
           priority: <int>,
           votes: <int>,
           tags: <int>
         }
        )

  e. Each new member performs an :ref:`initial sync
     <replica-set-initial-sync>` to catch up to the primary. Depending
     on data volume, network, and host performance factors, initial
     sync might take a while to complete.

  f. The replica set might elect a new primary while you add additional
     members. You can only run :method:`rs.add() <rs.add>` from the primary.
     To identify which member is the current primary, use
     :method:`rs.status() <rs.status>`.

---
stepnum: 52
level: 4
title: "Configure Any Additional Required Replication Settings."
ref: configure-replication-shard
content: |

  The :method:`rs.reconfig() <rs.reconfig>` method updates the replica set
  configuration based on a configuration document passed in as a
  parameter.

  a. Run :method:`rs.reconfig() <rs.reconfig>` against the primary member of the
     replica set.

  b. Reference the original configuration file output of the replica
     set and apply settings as needed.
---
stepnum: 53
level: 4
title: "Remove the Temporary Privileged User."
ref: remove-system-user
content: |

  For clusters enforcing authentication, remove the privileged user
  created earlier in this procedure:

  a. Authenticate as a user with the :authrole:`userAdmin` role on the
     ``admin`` database or :authrole:`userAdminAnyDatabase` role:

     .. code-block:: javascript
        :copyable: false

        db.getSiblingDB("admin").auth("<myUserAdmin>","<replaceMeWithAStrongPassword>")

  #. Delete the privileged user:

     .. code-block:: javascript

        db.getSiblingDB("admin").removeUser("<myTempSystemUserWithTotalAccess>")

---
stepnum: 54
level: 4
title: "Restart Each |mongos|."
ref: restart-mongos
content: |

  Restart each |mongos| in the cluster.

  .. code-block:: shell

     mongos --config </path/to/config/>mongos.conf

  Include all other command line options as required by your
  deployment.

  If the |csrs| replica set name or any member hostname changed, update
  the |mongos| configuration file setting :setting:`sharding.configDB`
  with updated configuration server connection string:

  .. code-block:: yaml

     sharding:
       configDB: "myNewCSRSName/config1.example.net:<port>,config2.example.net:<port>,config3.example.net:<port>"

---
stepnum: 55
level: 4
title: "Verify that You Can Access the Cluster."
ref: verify-cluster
content: |

  a. Connect a |mongo| shell to one of the |mongos|
     processes for the cluster.

  b. Use :method:`sh.status() <sh.status>` to check the overall cluster status.

     If :method:`sh.status() <sh.status>` indicates that the balancer is not
     running, use :method:`sh.startBalancer() <sh.startBalancer>` to restart
     the balancer.

  c. To confirm that you can access all shards and they are
     communicating, insert test data into a temporary sharded
     collection.

  d. Confirm that data is being split and migrated between each shard
     in your cluster.

     You can connect a |mongo| shell to each shard primary
     and use :method:`db.collection.find() <db.collection.find>` to validate that
     the data was sharded as expected.

...

