.. _recover-om-yes-k8sop-cluster:

=======================================================
Recover Ops Manager and AppDB if Operator Cluster is Up
=======================================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

If |k8s| clusters running the |application| instances or {+appdb+} nodes
fail, but the operator cluster is available, you can use the |k8s-op-short|
to reconfigure deployments of the {+appdb+}\'s replica set and the
|application| instances based on the following scenarios:

- If some or all |application| instances fail, no data is lost because the
  |application| is stateless. To increase the availability of the |application|,
  add new |application| instances to already configured and available |k8s| member
  clusters, or add new |k8s| clusters for running the |application| instances.

- If only a minority of replica set's nodes fail and the majority of nodes
  in a replica set are available, during the reconciliation process, the
  |k8s-op-short| ignores the failed |k8s| clusters and the {+appdb+}
  remains in a writable state.

  Use the :opsmgrkube:`spec.applicationDatabase.clusterSpecList` settings
  to add {+appdb+}\'s replica set nodes to already configured and
  available member |k8s| clusters, or add new |k8s| clusters on which
  you deploy {+appdb+}\'s failed replica set members.
  You can also scale down the replica set's nodes on a failed |k8s| cluster
  to reconfigure the replica set to not contain these nodes anymore.

- If a majority of replica set's nodes fail, the replica set can't form
  a voting majority to elect a primary node. To learn more, see
  :manual:`Replica Set Deployment Architectures </core/replica-set-architectures/>`.
  In this case, if at least one node in an {+appdb+}\'s replica set
  remains available, then no data is lost. Because there is no primary node
  in a replica set, you must
  :ref:`forcibly reconfigure the replica set <recover-appdb-forced-reconfig>`
  to add new replica set nodes. The nodes will form a voting majority allowing
  the replica set to elect a primary. New {+appdb+} instances will sync with
  the healthy nodes to receive the data.

- If all |k8s| member clusters hosting the {+appdb+}\'s replica set
  nodes fail, this causes an irreversible data loss (|onprem| doesn't back
  up the {+appdb+}). If possible, use an odd number of member
  |k8s| clusters and distribute your {+appdb+} nodes across
  data centers, zones, or |k8s| clusters. To learn more, see :manual:`Replica Sets Distributed Across Two or More Data Centers </core/replica-set-architecture-geographically-distributed/>`.
